{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:royalblue; font-size:3em\"> This serves as a baseline notebook to be imported by other notebooks </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning) \n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "njobs = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('../../data/train.csv',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1775\n",
      "1     104\n",
      "Name: fraud, dtype: int64\n",
      "0    0.944651\n",
      "1    0.055349\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train.fraud.value_counts())\n",
    "print(train.fraud.value_counts() / len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['scannedLineItems'] = train['scannedLineItemsPerSecond'] * train['totalScanTimeInSeconds']\n",
    "train['pricePerScannedLineItem'] = train['grandTotal'] / train['scannedLineItems']\n",
    "train['scansWithoutRegistrationPerScannedLineItem'] = train['scansWithoutRegistration'] / train['scannedLineItems']\n",
    "train['quantityModificationsPerScannedLineItem'] = train['quantityModifications'] / train['scannedLineItems']\n",
    "train['lineItemVoidsPerSecond'] = train['lineItemVoids'] / train['totalScanTimeInSeconds']\n",
    "train['scansWithoutRegistrationPerSecond'] = train['scansWithoutRegistration'] / train['totalScanTimeInSeconds']\n",
    "train['quantityModificationsPerSecond'] = train['quantityModifications'] / train['totalScanTimeInSeconds']\n",
    "train['secondsPerEuro'] = train['totalScanTimeInSeconds'] / train['grandTotal']\n",
    "train['lineItemVoidsPerEuro'] = train['lineItemVoids'] / train['grandTotal']\n",
    "train['scansWithoutRegistrationPerEuro'] = train['scansWithoutRegistration'] / train['grandTotal']\n",
    "train['quantityModificationsPerEuro'] = train['quantityModifications'] / train['grandTotal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Cross validation\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Split *train* dataset to feature and target sets \n",
    "X = train.drop('fraud',axis=1)\n",
    "Y = train['fraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    score = ((-25)*fp + (-5)*fn + 5*tp) / len(y_true)\n",
    "    return (score)\n",
    "\n",
    "my_custom_score = make_scorer(my_custom_loss_func, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run loops for training and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def run():\n",
    "    global result_table\n",
    "    # three types of data preparation: No preparation, MaxMinScaler, StandardScaler\n",
    "    for data_preparation_step in range(1,5):\n",
    "        if (data_preparation_step == 1):  \n",
    "            X_scaled = X\n",
    "            data_preparation = \"No Scaling\"\n",
    "        elif (data_preparation_step == 2):\n",
    "            feature_scaler = MinMaxScaler()  \n",
    "            X_scaled = pd.DataFrame(feature_scaler.fit_transform(X.values), columns=X.columns, index=X.index) \n",
    "            data_preparation = \"MinMaxScaler\"\n",
    "        elif (data_preparation_step == 3):\n",
    "            feature_scaler = StandardScaler()  \n",
    "            X_scaled = pd.DataFrame(feature_scaler.fit_transform(X.values), columns=X.columns, index=X.index)\n",
    "            data_preparation = \"StandardScaler\"\n",
    "        elif (data_preparation_step == 4):\n",
    "            transformer = FunctionTransformer(np.log1p, validate=True)  \n",
    "            X_scaled = pd.DataFrame(transformer.transform(X), columns=X.columns, index=X.index)\n",
    "            data_preparation = \"LogScaler\"      \n",
    "\n",
    "        for feature_count in range(1,len(list(X))+1):\n",
    "\n",
    "            for model in model_tuning_factory:   # replace with model_tuning_factory_randomized for faster results\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                best_features = SelectKBest(f_classif, k=feature_count).fit(X_scaled,Y)\n",
    "                best_feature_list = X.columns[best_features.get_support()]\n",
    "                X_selected_features = X[best_feature_list]\n",
    "\n",
    "                model.seed = 42\n",
    "                model.fit(X_selected_features,Y)\n",
    "                model_name = model.best_estimator_.__class__.__name__\n",
    "                score_mean = model.cv_results_['mean_test_score'][model.best_index_]\n",
    "                score_std = model.cv_results_['std_test_score'][model.best_index_]\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                print(\"Finished \" + model.best_estimator_.__class__.__name__ + \" with \" + data_preparation + \" and \" + str(feature_count) + \" features after \" + str(end_time - start_time) + \" seconds\")\n",
    "\n",
    "                result_table = result_table.append({\n",
    "                 \"Model\": model_name,\n",
    "                 \"Data Preparation\": data_preparation,\n",
    "                 \"Feature Count\": feature_count,\n",
    "                 \"Features\": best_feature_list.values,\n",
    "                 \"Optimal Parameters\": model.best_params_,\n",
    "                 \"Monetary Value Per Instance - Mean\":  score_mean,\n",
    "                 \"Monetary Value Per Instance - Standard Deviation\": score_std,\n",
    "                 \"Time needed\": end_time - start_time,    \n",
    "                 \"Raw Model\": model.best_estimator_\n",
    "                  }, ignore_index=True)\n",
    "\n",
    "    result_table.sort_values(by = \"Monetary Value Per Instance - Mean\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot number of features against monetary value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_number_features():\n",
    "    plt.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "    plt.scatter(result_table[\"Feature Count\"], result_table[\"Monetary Value Per Instance - Mean\"])\n",
    "    plt.xlabel('Number of features', fontsize=16)\n",
    "    plt.ylabel('Monetary Value Per Instance - Mean', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class to store the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-0b24783c38e5>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0b24783c38e5>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    self.best_model = result_table.loc[,][\"Raw Model\"]\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class BestModel:\n",
    "    best_model = None\n",
    "    best_model_features = None\n",
    "    rank = 0\n",
    "    \n",
    "    def __init__(self, rank):\n",
    "        self.rank = rank\n",
    "    \n",
    "    def set(self):\n",
    "        global result_table\n",
    "        index = result_table[\"Monetary Value Per Instance - Mean\"].astype(float).argmax() - self.rank\n",
    "        self.best_model = result_table.loc[index,][\"Raw Model\"]\n",
    "        self.best_model_features = result_table.loc[index,][\"Features\"]\n",
    "    \n",
    "    def predict(self):\n",
    "        self.set()\n",
    "        return self.best_model.predict(X[self.best_model_features])\n",
    "    \n",
    "    def print_best_model(self):\n",
    "        print(self.best_model)\n",
    "        print(self.best_model_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate performance of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monetary_value(best_model):\n",
    "        cm = confusion_matrix(Y , best_model.predict())\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        size = tn + fp + fn + tp\n",
    "        print(\"True negative: \", tn)\n",
    "        print(\"False positive: \", fp)\n",
    "        print(\"False negative: \", fn)\n",
    "        print(\"True positive: \", tp)\n",
    "        score = (-25)*fp + (-5)*fn + 5*tp\n",
    "        print(score, 'for ', size, ' instances in the test set')\n",
    "        print(score/size, ' per instance in the test set')\n",
    "        return score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
