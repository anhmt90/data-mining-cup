{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning) \n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "njobs = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv',sep='|')\n",
    "test=pd.read_csv('test.csv',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1775\n",
      "1     104\n",
      "Name: fraud, dtype: int64\n",
      "0    0.944651\n",
      "1    0.055349\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train.fraud.value_counts())\n",
    "print(train.fraud.value_counts() / len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derviable directly from given features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['scannedLineItems'] = train['scannedLineItemsPerSecond'] * train['totalScanTimeInSeconds']\n",
    "train['pricePerScannedLineItem'] = train['grandTotal'] / train['scannedLineItems']\n",
    "train['scansWithoutRegistrationPerScannedLineItem'] = train['scansWithoutRegistration'] / train['scannedLineItems']\n",
    "train['quantityModificationsPerScannedLineItem'] = train['quantityModifications'] / train['scannedLineItems']\n",
    "train['lineItemVoidsPerSecond'] = train['lineItemVoids'] / train['totalScanTimeInSeconds']\n",
    "train['scansWithoutRegistrationPerSecond'] = train['scansWithoutRegistration'] / train['totalScanTimeInSeconds']\n",
    "train['quantityModificationsPerSecond'] = train['quantityModifications'] / train['totalScanTimeInSeconds']\n",
    "train['secondsPerEuro'] = train['totalScanTimeInSeconds'] / train['grandTotal']\n",
    "train['lineItemVoidsPerEuro'] = train['lineItemVoids'] / train['grandTotal']\n",
    "train['scansWithoutRegistrationPerEuro'] = train['scansWithoutRegistration'] / train['grandTotal']\n",
    "train['quantityModificationsPerEuro'] = train['quantityModifications'] / train['grandTotal']\n",
    "\n",
    "test['scannedLineItems'] = test['scannedLineItemsPerSecond'] * test['totalScanTimeInSeconds']\n",
    "test['pricePerScannedLineItem'] = test['grandTotal'] / test['scannedLineItems']\n",
    "test['scansWithoutRegistrationPerScannedLineItem'] = test['scansWithoutRegistration'] / test['scannedLineItems']\n",
    "test['quantityModificationsPerScannedLineItem'] = test['quantityModifications'] / test['scannedLineItems']\n",
    "test['lineItemVoidsPerSecond'] = test['lineItemVoids'] / test['totalScanTimeInSeconds']\n",
    "test['scansWithoutRegistrationPerSecond'] = test['scansWithoutRegistration'] / test['totalScanTimeInSeconds']\n",
    "test['quantityModificationsPerSecond'] = test['quantityModifications'] / test['totalScanTimeInSeconds']\n",
    "test['secondsPerEuro'] = test['totalScanTimeInSeconds'] / test['grandTotal']\n",
    "test['lineItemVoidsPerEuro'] = test['lineItemVoids'] / test['grandTotal']\n",
    "test['scansWithoutRegistrationPerEuro'] = test['scansWithoutRegistration'] / test['grandTotal']\n",
    "test['quantityModificationsPerEuro'] = test['quantityModifications'] / test['grandTotal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test['grandTotal'] == 0.00, 'secondsPerEuro'] = test['totalScanTimeInSeconds'] / 0.01\n",
    "test.loc[test['grandTotal'] == 0.00, 'lineItemVoidsPerEuro'] = test['lineItemVoids'] / 0.01\n",
    "test.loc[test['grandTotal'] == 0.00, 'scansWithoutRegistrationPerEuro'] = test['scansWithoutRegistration'] / 0.01\n",
    "test.loc[test['grandTotal'] == 0.00, 'quantityModificationsPerEuro'] = test['quantityModifications'] / 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trustLevel</th>\n",
       "      <td>0.325775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scannedLineItems</th>\n",
       "      <td>0.229154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <td>0.050871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scansWithoutRegistrationPerScannedLineItem</th>\n",
       "      <td>0.044364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <td>0.032057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantityModificationsPerScannedLineItem</th>\n",
       "      <td>0.029491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <td>0.028249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineItemVoids</th>\n",
       "      <td>0.027422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <td>0.025343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scansWithoutRegistrationPerSecond</th>\n",
       "      <td>0.024737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pricePerScannedLineItem</th>\n",
       "      <td>0.024292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantityModificationsPerSecond</th>\n",
       "      <td>0.019878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantityModificationsPerEuro</th>\n",
       "      <td>0.018813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scansWithoutRegistrationPerEuro</th>\n",
       "      <td>0.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineItemVoidsPerEuro</th>\n",
       "      <td>0.018623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secondsPerEuro</th>\n",
       "      <td>0.018309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valuePerSecond</th>\n",
       "      <td>0.018001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grandTotal</th>\n",
       "      <td>0.016820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineItemVoidsPerSecond</th>\n",
       "      <td>0.015279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantityModifications</th>\n",
       "      <td>0.013722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Importance\n",
       "trustLevel                                    0.325775\n",
       "scannedLineItems                              0.229154\n",
       "totalScanTimeInSeconds                        0.050871\n",
       "scansWithoutRegistrationPerScannedLineItem    0.044364\n",
       "lineItemVoidsPerPosition                      0.032057\n",
       "quantityModificationsPerScannedLineItem       0.029491\n",
       "scansWithoutRegistration                      0.028249\n",
       "lineItemVoids                                 0.027422\n",
       "scannedLineItemsPerSecond                     0.025343\n",
       "scansWithoutRegistrationPerSecond             0.024737\n",
       "pricePerScannedLineItem                       0.024292\n",
       "quantityModificationsPerSecond                0.019878\n",
       "quantityModificationsPerEuro                  0.018813\n",
       "scansWithoutRegistrationPerEuro               0.018800\n",
       "lineItemVoidsPerEuro                          0.018623\n",
       "secondsPerEuro                                0.018309\n",
       "valuePerSecond                                0.018001\n",
       "grandTotal                                    0.016820\n",
       "lineItemVoidsPerSecond                        0.015279\n",
       "quantityModifications                         0.013722"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "y = train['fraud']\n",
    "x = train.drop('fraud',axis=1)\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier(criterion = 'entropy')\n",
    "model.fit(x, y)\n",
    "\n",
    "pd.DataFrame(model.feature_importances_, list(x), columns =['Importance']).sort_values(by='Importance', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying oversampling to dataset -> works but we will probably won't use it\n",
    "- Classical Oversampling\n",
    "- SMOTE Technique\n",
    "- ADASYN Technique\n",
    "\n",
    "Each one has a slightly different approach for generating synthetic instances\n",
    "- Simply duplicated fraud instances\n",
    "- ADASYN focuses on generating samples next to the original samples which are wrongly classified using a k-Nearest Neighbors classifier\n",
    "- SMOTE will not make any distinction between easy and hard samples to be classified using the nearest neighbors rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1775\n",
      "0    1775\n",
      "Name: fraud, dtype: int64\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# !pip install imblearn  # might be necessary for installation\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def randomOverSampling(train):\n",
    "    ros = RandomOverSampler(random_state=42, ratio = 1)\n",
    "    X_train_extended, Y_train_extended = ros.fit_resample(train.drop('fraud',axis=1), train['fraud'])\n",
    "\n",
    "    new_train = pd.DataFrame(X_train_extended, columns=train.drop('fraud',axis=1).columns) \n",
    "    new_train[\"fraud\"] = Y_train_extended\n",
    "    return new_train\n",
    "\n",
    "print(randomOverSampling(train).fraud.value_counts())\n",
    "print(randomOverSampling(train).fraud.value_counts() / len(randomOverSampling(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1775\n",
      "0    1775\n",
      "Name: fraud, dtype: int64\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#!pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def smoteOverSamling(train):\n",
    "    sm = SMOTE(random_state=42, k_neighbors = 3, ratio = 1)\n",
    "    X_train_extended, Y_train_extended = sm.fit_sample(train.drop('fraud',axis=1), train['fraud'])\n",
    "\n",
    "    new_train = pd.DataFrame(X_train_extended, columns=train.drop('fraud',axis=1).columns) \n",
    "    new_train [\"fraud\"] = Y_train_extended\n",
    "    return new_train\n",
    "    \n",
    "print(smoteOverSamling(train).fraud.value_counts())\n",
    "print(smoteOverSamling(train).fraud.value_counts() / len(smoteOverSamling(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1775\n",
      "1    1747\n",
      "Name: fraud, dtype: int64\n",
      "0    0.503975\n",
      "1    0.496025\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "def adasynOverSamling(train):\n",
    "    ada = ADASYN(random_state=42, n_neighbors = 3, ratio = 1)\n",
    "    X_train_extended, Y_train_extended = ada.fit_resample(train.drop('fraud',axis=1), train['fraud'])\n",
    "\n",
    "    new_train = pd.DataFrame(X_train_extended, columns=train.drop('fraud',axis=1).columns) \n",
    "    new_train[\"fraud\"] = Y_train_extended\n",
    "    return new_train\n",
    "    \n",
    "print(adasynOverSamling(train).fraud.value_counts())\n",
    "print(adasynOverSamling(train).fraud.value_counts() / len(adasynOverSamling(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually apply one of these techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = smoteOverSamling(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Custom Score Function based on the given cost matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    score = ((-25)*fp + (-5)*fn + 5*tp) / len(y_true)\n",
    "    return (score)\n",
    "\n",
    "my_custom_score = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "loo = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building model factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "#from sklearn.multioutput import ClassifierChain\n",
    "#from sklearn.multioutput import MultiOutputClassifier\n",
    "#from sklearn.multiclass import OutputCodeClassifier\n",
    "#from sklearn.multiclass import OneVsOneClassifier\n",
    "#from sklearn.multiclass import OneVsRestClassifier\n",
    "#from sklearn.ensemble.voting_classifier import VotingClassifier\n",
    "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "model_tuning_factory = [   \n",
    "    GridSearchCV(LogisticRegression(max_iter = 10000), \n",
    "                 dict(solver = ['newton-cg', 'lbfgs', 'liblinear'], \n",
    "                      fit_intercept = [True, False],\n",
    "                      C = np.arange(0.1, 2.0, 0.1)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(KNeighborsClassifier(), \n",
    "                 dict(\n",
    "                     n_neighbors = [1, 3, 5, 10, 15],\n",
    "                     weights = ['uniform', 'distance'],\n",
    "                     p = [1, 2, 3]\n",
    "                 ),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "  #  GridSearchCV(NearestCentroid(),    # cause some problems\n",
    "  #               dict(metric = ['euclidean', 'manhattan', 'chebyshev', 'minkowski', 'mahalanobis']),\n",
    "  #               cv = skf,\n",
    "  #               scoring = my_custom_score,\n",
    "  #               refit = True,\n",
    "  #               n_jobs = -1),\n",
    "    GridSearchCV(DecisionTreeClassifier(),\n",
    "                 dict(criterion = ['entropy', 'gini'],\n",
    "                      splitter = ['best', 'random'],\n",
    "                      max_depth = range(1,50,2),\n",
    "                      min_samples_split = range(2,10,2)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(ExtraTreeClassifier(),\n",
    "                 dict(criterion = ['entropy', 'gini'],\n",
    "                      splitter = ['best', 'random'],\n",
    "                      max_depth = range(1,50,2),\n",
    "                      min_samples_split = range(2,10,2)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(ExtraTreesClassifier(),\n",
    "                 dict(n_estimators = range(5,200,5),\n",
    "                      criterion = ['entropy', 'gini']),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),    \n",
    "    GridSearchCV(RandomForestClassifier(),\n",
    "                 dict(n_estimators = range(5,200,5),\n",
    "                      criterion = ['entropy', 'gini']),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),    \n",
    "    GridSearchCV(BernoulliNB(),\n",
    "                 dict(binarize  = [0.0, 0.2, 0.5, 1, 2],\n",
    "                      alpha = [0, 0.2, 0.5, 1, 2]),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),     \n",
    "    GridSearchCV(GaussianNB(),\n",
    "                 dict(),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),     \n",
    "    GridSearchCV(GradientBoostingClassifier(),\n",
    "                 dict(loss = ['deviance', 'exponential'],\n",
    "                      n_estimators = [20, 50,100, 150, 200]),\n",
    "                 #dict(n_estimators = range(1,150)),\n",
    "                 #     learning_rate = np.arange(0.01, 1.0, 0.01)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(AdaBoostClassifier(),\n",
    "                 dict(n_estimators = [20, 50,100, 150, 200],\n",
    "                      algorithm = ['SAMME', 'SAMME.R'],\n",
    "                      base_estimator = [DecisionTreeClassifier(max_depth=1), # default setting\n",
    "                                        LogisticRegression(),\n",
    "                                        ExtraTreesClassifier(),\n",
    "                              #          GradientBoostingClassifier(), # takes a long time (-> + 30 sec)\n",
    "                              #          RandomForestClassifier(),  # takes a long time (-> + 30 sec)\n",
    "                                        DecisionTreeClassifier(),\n",
    "                                        ExtraTreeClassifier(),\n",
    "                                        None] # default\n",
    "                      ),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1), \n",
    "    GridSearchCV(XGBClassifier(),\n",
    "                 dict(objective = ['binary:logistic'],\n",
    "                      eval_metric = ['error'],\n",
    "                      base_score = [0.3, 0.5, 0.7],\n",
    "                      learning_rate = [0.01, 0.1, 0.2, 0.3],\n",
    "                      n_estimators = [10, 20, 50, 100]),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(XGBClassifier(),\n",
    "                 dict(objective = ['binary:logistic'],\n",
    "                      eval_metric = ['error'],\n",
    "                   #   base_score = [0.3, 0.5, 0.7],\n",
    "                   #   learning_rate = [0.01, 0.1, 0.2, 0.3],\n",
    "                      n_estimators = [10, 20, 50, 100],\n",
    "                      base_estimator = [DecisionTreeClassifier(max_depth=1), # default setting for AdaBoost\n",
    "                                        LogisticRegression(),\n",
    "                                        ExtraTreesClassifier(),\n",
    "                              #          GradientBoostingClassifier(), # takes a long time (-> + 30 sec)\n",
    "                              #          RandomForestClassifier(),  # takes a long time (-> + 30 sec)\n",
    "                                        DecisionTreeClassifier(),\n",
    "                                        ExtraTreeClassifier(),\n",
    "                                        None]),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(BaggingClassifier(), \n",
    "                 dict(n_estimators = [5, 10, 20, 50, 100, 150, 200],\n",
    "                      base_estimator = [DecisionTreeClassifier(max_depth=1), # default setting for AdaBoost\n",
    "                                        LogisticRegression(),\n",
    "                                        ExtraTreesClassifier(),\n",
    "                              #          GradientBoostingClassifier(), # takes a long time (-> + 30 sec)\n",
    "                              #          RandomForestClassifier(),  # takes a long time (-> + 30 sec)\n",
    "                                        DecisionTreeClassifier(),\n",
    "                                        ExtraTreeClassifier(),\n",
    "                                        None]),   # standard -> 'Decision Tree'\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1), \n",
    "    GridSearchCV(Perceptron(),\n",
    "                 dict(penalty = ['l2', 'l1', 'elasticnet', None],\n",
    "                      alpha = np.arange(0.0005, 0.001, 0.0005),\n",
    "                      fit_intercept = [True, False],\n",
    "                      max_iter = range(5,100,5)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1), \n",
    "    GridSearchCV(MLPClassifier(),\n",
    "                 dict(solver = ['lbfgs', 'sgd', 'adam'],\n",
    "                      activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "                      learning_rate = ['constant', 'invscaling', 'adaptive']),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1), \n",
    "#    GridSearchCV(LinearDiscriminantAnalysis(),                       # tsne dimensions cause problems\n",
    "#                 dict(solver  = ['svd', 'lsqr', 'eigen'],\n",
    "#                      n_components = range(1,20)),\n",
    "#                 cv = skf,\n",
    "#                 scoring = my_custom_score,\n",
    "#                 refit = True,\n",
    "#                 n_jobs = -1),\n",
    "    GridSearchCV(QuadraticDiscriminantAnalysis(),\n",
    "                 dict(reg_param = np.arange(0.1, 1.0, 0.1)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(SVC(),                                           # gets very slow at some point\n",
    "                 dict(C = [0.01, 0.1, 0.5, 1, 2]),\n",
    "                  cv = skf,\n",
    "                  scoring = my_custom_score,\n",
    "                  refit = True,\n",
    "                  n_jobs = -1)\n",
    "]       \n",
    "\n",
    "\n",
    "model_tuning_factory_temp = [\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished LogisticRegression with No Oversampling and No Scaling and 1 features after 6.61 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 2 features after 7.04 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 3 features after 10.93 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 4 features after 11.28 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 5 features after 11.65 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 6 features after 14.5 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 7 features after 9.54 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 8 features after 10.42 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 9 features after 18.5 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 10 features after 14.44 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 11 features after 10.72 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 12 features after 14.86 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 13 features after 20.12 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 14 features after 19.5 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 15 features after 24.2 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 16 features after 36.38 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 17 features after 41.86 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 18 features after 41.8 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 19 features after 65.63 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 20 features after 86.26 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 1 features after 1.72 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 2 features after 2.22 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 3 features after 2.13 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 4 features after 2.26 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 5 features after 2.71 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 6 features after 2.76 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 7 features after 2.61 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 8 features after 2.63 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 9 features after 2.72 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 10 features after 2.68 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 11 features after 2.97 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 12 features after 3.0 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 13 features after 3.03 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 14 features after 3.66 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 15 features after 3.4 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 16 features after 4.38 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 17 features after 5.84 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 18 features after 4.32 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 19 features after 4.01 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 20 features after 5.14 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 1 features after 8.22 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 2 features after 8.34 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 3 features after 7.45 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 4 features after 8.46 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 5 features after 8.17 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 6 features after 8.53 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 7 features after 9.12 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 8 features after 8.96 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 9 features after 9.59 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 10 features after 9.07 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 11 features after 9.35 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 12 features after 10.31 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 13 features after 10.2 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 14 features after 9.99 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 15 features after 10.85 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 16 features after 11.93 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 17 features after 9.68 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 18 features after 9.36 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 19 features after 9.98 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 20 features after 12.94 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 1 features after 7.44 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 2 features after 7.77 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 3 features after 7.52 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 4 features after 8.2 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 5 features after 7.79 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 6 features after 8.39 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 7 features after 7.67 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 8 features after 8.1 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 9 features after 9.15 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 10 features after 9.27 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 11 features after 9.42 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 12 features after 9.33 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 13 features after 8.66 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 14 features after 8.62 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 15 features after 9.08 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 16 features after 14.55 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 17 features after 9.43 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 18 features after 9.66 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 19 features after 9.57 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 20 features after 10.11 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 1 features after 32.32 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 2 features after 24.08 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 3 features after 26.68 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 4 features after 27.19 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 5 features after 27.38 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 6 features after 27.31 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 7 features after 28.95 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 8 features after 29.24 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 9 features after 29.32 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 10 features after 28.81 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 11 features after 30.37 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 12 features after 31.41 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 13 features after 34.88 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 14 features after 30.74 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 15 features after 32.13 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 16 features after 33.33 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 17 features after 32.35 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 18 features after 33.45 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 19 features after 33.46 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 20 features after 34.77 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 1 features after 37.04 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 2 features after 26.92 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 3 features after 34.1 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 4 features after 35.0 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 5 features after 36.96 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 6 features after 37.24 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 7 features after 37.78 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 8 features after 38.07 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 9 features after 42.3 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 10 features after 44.42 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 11 features after 43.53 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 12 features after 46.5 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 13 features after 46.74 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 14 features after 47.69 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 15 features after 45.07 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 16 features after 41.95 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 17 features after 33.82 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 18 features after 34.23 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 19 features after 36.95 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 20 features after 36.57 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 1 features after 0.72 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 2 features after 0.77 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 3 features after 0.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 4 features after 0.66 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 5 features after 0.6 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 6 features after 0.54 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 7 features after 0.57 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 8 features after 0.55 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 9 features after 0.56 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 10 features after 0.55 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 11 features after 1.05 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 12 features after 0.73 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 13 features after 0.68 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 14 features after 0.66 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 15 features after 0.59 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 16 features after 0.95 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 17 features after 0.74 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 18 features after 0.7 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 19 features after 0.72 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 20 features after 0.67 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 1 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 2 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 3 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 4 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 5 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 6 features after 0.02 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 7 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 8 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 9 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 10 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 11 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 12 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 13 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 14 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 15 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 16 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 17 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 18 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 19 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 20 features after 0.03 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 1 features after 5.5 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 2 features after 4.95 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 3 features after 5.42 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 4 features after 6.27 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 5 features after 6.7 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 6 features after 5.71 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 7 features after 7.29 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 8 features after 7.39 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 9 features after 7.55 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 10 features after 8.29 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 11 features after 7.92 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 12 features after 8.85 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 13 features after 6.13 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 14 features after 8.97 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 15 features after 7.69 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 16 features after 8.35 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 17 features after 8.1 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 18 features after 8.03 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 19 features after 8.59 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 20 features after 7.05 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 1 features after 65.52 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 2 features after 66.99 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 3 features after 18.91 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 4 features after 23.44 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 5 features after 24.5 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 6 features after 18.0 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 7 features after 25.36 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 8 features after 25.83 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 9 features after 21.44 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 10 features after 26.41 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 11 features after 23.7 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 12 features after 23.23 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 13 features after 24.27 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 14 features after 28.81 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 15 features after 37.0 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 16 features after 29.47 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 17 features after 33.61 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 18 features after 45.11 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 19 features after 40.6 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 20 features after 34.94 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 1 features after 26.24 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 2 features after 18.89 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 3 features after 27.7 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 4 features after 34.01 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 5 features after 33.88 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 6 features after 33.96 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 7 features after 33.28 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 8 features after 30.48 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 9 features after 31.77 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 10 features after 32.24 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 11 features after 30.69 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 12 features after 31.94 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 13 features after 33.55 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 14 features after 33.39 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 15 features after 29.27 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 16 features after 29.94 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 17 features after 30.77 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 18 features after 30.83 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 19 features after 30.77 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 20 features after 33.21 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 1 features after 20.1 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 2 features after 22.53 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 3 features after 20.36 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 4 features after 19.57 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 5 features after 22.16 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 6 features after 19.25 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 7 features after 19.14 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 8 features after 20.55 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 9 features after 19.23 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 10 features after 17.98 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 11 features after 20.49 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 12 features after 18.99 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 13 features after 18.93 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 14 features after 15.03 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 15 features after 17.69 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 16 features after 17.2 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 17 features after 15.04 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 18 features after 12.56 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 19 features after 16.79 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 20 features after 17.18 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 1 features after 44.77 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 2 features after 43.35 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 3 features after 30.43 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 4 features after 53.62 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 5 features after 54.28 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 6 features after 26.76 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 7 features after 29.99 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 8 features after 34.54 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 9 features after 31.3 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 10 features after 36.3 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 11 features after 33.33 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished BaggingClassifier with No Oversampling and No Scaling and 12 features after 31.51 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 13 features after 32.44 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 14 features after 25.47 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 15 features after 40.93 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 16 features after 35.72 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 17 features after 35.39 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 18 features after 39.53 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 19 features after 42.18 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 20 features after 56.86 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 1 features after 3.82 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 2 features after 3.44 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 3 features after 3.5 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 4 features after 3.33 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 5 features after 3.48 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 6 features after 3.45 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 7 features after 3.5 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 8 features after 3.6 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 9 features after 3.68 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 10 features after 4.33 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 11 features after 8.14 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 12 features after 8.12 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 13 features after 5.17 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 14 features after 5.05 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 15 features after 4.8 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 16 features after 4.67 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 17 features after 7.08 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 18 features after 8.57 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 19 features after 9.37 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 20 features after 5.88 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 1 features after 10.65 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 2 features after 24.47 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 3 features after 20.56 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 4 features after 26.68 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 5 features after 29.07 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 6 features after 21.88 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 7 features after 21.82 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 8 features after 29.12 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 9 features after 29.76 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 10 features after 23.52 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 11 features after 24.72 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 12 features after 29.42 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 13 features after 31.22 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 14 features after 25.49 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 15 features after 24.45 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 16 features after 37.25 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 17 features after 25.08 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 18 features after 23.95 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 19 features after 27.1 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 20 features after 25.36 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 1 features after 0.22 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 2 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 3 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 4 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 5 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 6 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 7 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 8 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 9 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 10 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 11 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 12 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 13 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 14 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 15 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 16 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 17 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 18 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 19 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 20 features after 0.26 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 1 features after 0.29 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 2 features after 0.28 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 3 features after 1.71 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 4 features after 1.73 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 5 features after 1.74 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 6 features after 1.75 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 7 features after 1.77 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 8 features after 1.84 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 9 features after 1.86 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 10 features after 2.29 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 11 features after 2.16 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 12 features after 2.22 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 13 features after 2.21 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 14 features after 2.34 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 15 features after 2.17 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 16 features after 2.39 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 17 features after 2.49 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished SVC with No Oversampling and No Scaling and 18 features after 2.4 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 19 features after 2.5 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 20 features after 2.54 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 1 features after 2.31 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 2 features after 3.29 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 3 features after 6.01 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 4 features after 6.19 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 5 features after 6.76 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 6 features after 8.31 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 7 features after 7.85 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 8 features after 8.75 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 9 features after 9.75 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 10 features after 9.77 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 11 features after 9.99 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 12 features after 10.2 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 13 features after 10.99 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 14 features after 11.88 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 15 features after 14.11 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 16 features after 21.52 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 17 features after 23.25 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 18 features after 24.54 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 19 features after 39.4 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 20 features after 48.43 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 1 features after 1.48 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 2 features after 1.49 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 3 features after 1.47 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 4 features after 1.58 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 5 features after 1.9 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 6 features after 1.93 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 7 features after 2.03 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 8 features after 1.93 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 9 features after 2.16 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 10 features after 2.09 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 11 features after 2.08 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 12 features after 2.21 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 13 features after 2.44 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 14 features after 2.89 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 15 features after 2.43 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 16 features after 3.06 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 17 features after 3.0 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 18 features after 4.3 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 19 features after 4.16 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 20 features after 3.79 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 1 features after 5.94 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 2 features after 6.26 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 3 features after 6.55 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 4 features after 6.09 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 5 features after 8.41 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 6 features after 10.4 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 7 features after 10.37 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 8 features after 6.45 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 9 features after 8.84 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 10 features after 10.92 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 11 features after 10.6 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 12 features after 8.31 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 13 features after 6.17 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 14 features after 8.74 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 15 features after 11.94 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 16 features after 12.74 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 17 features after 10.9 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 18 features after 7.1 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 19 features after 13.33 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 20 features after 13.9 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 1 features after 7.49 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 2 features after 6.24 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 3 features after 9.66 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 4 features after 10.05 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 5 features after 9.3 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 6 features after 9.05 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 7 features after 10.55 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 8 features after 10.41 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 9 features after 9.23 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 10 features after 10.23 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 11 features after 11.53 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 12 features after 7.53 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 13 features after 10.41 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 14 features after 12.32 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 15 features after 10.4 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 16 features after 8.21 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 17 features after 11.15 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 18 features after 12.12 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 19 features after 7.39 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 20 features after 9.57 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 1 features after 21.15 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 2 features after 23.49 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 3 features after 27.41 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 4 features after 24.97 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 5 features after 29.44 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 6 features after 27.92 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 7 features after 28.44 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 8 features after 26.35 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 9 features after 31.84 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 10 features after 26.68 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 11 features after 24.77 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 12 features after 54.22 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 13 features after 27.26 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 14 features after 26.2 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 15 features after 34.82 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 16 features after 29.75 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 17 features after 24.88 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 18 features after 36.73 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 19 features after 29.08 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 20 features after 26.22 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 1 features after 14.96 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 2 features after 24.01 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 3 features after 23.37 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 4 features after 28.39 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 5 features after 25.36 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 6 features after 26.84 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 7 features after 29.48 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 8 features after 30.44 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 9 features after 35.61 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 10 features after 44.1 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 11 features after 38.41 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 12 features after 44.9 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 13 features after 36.01 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 14 features after 49.63 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 15 features after 54.04 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 16 features after 49.93 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 17 features after 40.42 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 18 features after 48.29 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 19 features after 50.4 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 20 features after 53.33 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 1 features after 1.29 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 2 features after 1.74 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 3 features after 1.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 4 features after 1.59 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 5 features after 1.41 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 6 features after 1.01 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 7 features after 1.56 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 8 features after 1.43 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 9 features after 1.28 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 10 features after 1.79 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 11 features after 1.89 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 12 features after 1.22 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 13 features after 1.47 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 14 features after 1.66 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 15 features after 1.54 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 16 features after 1.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 17 features after 1.71 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 18 features after 1.64 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 19 features after 2.01 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 20 features after 0.71 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 1 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 2 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 3 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 4 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 5 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 6 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 7 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 8 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 9 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 10 features after 0.07 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 11 features after 0.07 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 12 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 13 features after 0.08 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 14 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 15 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 16 features after 0.08 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 17 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 18 features after 0.08 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 19 features after 0.07 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 20 features after 0.06 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 1 features after 8.08 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 2 features after 9.01 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 3 features after 9.25 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 4 features after 8.73 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 5 features after 9.55 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 6 features after 10.75 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 7 features after 8.08 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 8 features after 8.54 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 9 features after 9.21 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 10 features after 8.41 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 11 features after 7.47 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 12 features after 8.31 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 13 features after 8.14 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 14 features after 11.36 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 15 features after 10.78 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 16 features after 11.38 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 17 features after 11.4 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 18 features after 11.52 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 19 features after 10.45 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 20 features after 10.35 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 1 features after 164.33 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 2 features after 161.62 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 3 features after 38.17 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 4 features after 40.21 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 5 features after 43.88 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 6 features after 47.02 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 7 features after 43.55 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 8 features after 55.17 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 9 features after 65.8 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 10 features after 72.19 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 11 features after 64.5 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 12 features after 71.85 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 13 features after 85.56 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 14 features after 71.4 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 15 features after 128.47 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 16 features after 136.8 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 17 features after 106.85 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 18 features after 27.36 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 19 features after 29.4 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 20 features after 29.22 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 1 features after 19.13 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 2 features after 19.2 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 3 features after 18.96 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 4 features after 19.59 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 5 features after 20.0 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 6 features after 19.19 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 7 features after 19.57 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 8 features after 20.37 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 9 features after 19.76 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 10 features after 18.74 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 11 features after 19.56 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 12 features after 19.4 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 13 features after 19.7 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 14 features after 19.17 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 15 features after 19.11 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 16 features after 18.94 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 17 features after 18.76 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 18 features after 19.08 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 19 features after 18.56 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 20 features after 18.47 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 1 features after 10.92 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 2 features after 12.17 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 3 features after 11.92 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 4 features after 12.38 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 5 features after 11.75 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 6 features after 13.34 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 7 features after 12.39 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 8 features after 13.08 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 9 features after 12.18 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 10 features after 11.01 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 11 features after 11.56 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 12 features after 12.77 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 13 features after 10.96 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 14 features after 11.49 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 15 features after 12.5 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 16 features after 10.13 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 17 features after 12.1 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 18 features after 10.77 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 19 features after 11.57 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 20 features after 10.45 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 1 features after 23.77 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 2 features after 26.58 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 3 features after 42.12 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 4 features after 33.63 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 5 features after 31.06 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 6 features after 44.72 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 7 features after 56.18 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 8 features after 47.67 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 9 features after 47.68 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 10 features after 52.95 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 11 features after 87.28 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 12 features after 61.77 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 13 features after 65.52 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 14 features after 67.58 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 15 features after 119.5 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 16 features after 45.63 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 17 features after 45.99 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 18 features after 56.81 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 19 features after 75.1 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 20 features after 71.01 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 1 features after 3.98 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 2 features after 8.15 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 3 features after 4.44 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 4 features after 3.64 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 5 features after 3.41 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 6 features after 3.36 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 7 features after 3.53 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 8 features after 3.63 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 9 features after 3.58 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 10 features after 3.62 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 11 features after 3.78 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 12 features after 3.78 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 13 features after 6.23 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 14 features after 5.86 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 15 features after 4.73 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 16 features after 4.45 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 17 features after 4.43 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 18 features after 4.69 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 19 features after 7.32 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 20 features after 5.05 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 1 features after 11.19 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 2 features after 25.0 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 3 features after 22.06 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 4 features after 26.44 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 5 features after 21.47 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 6 features after 22.42 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 7 features after 27.57 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 8 features after 25.66 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 9 features after 29.4 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 10 features after 24.11 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 11 features after 23.47 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 12 features after 23.32 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 13 features after 32.04 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 14 features after 24.0 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 15 features after 23.18 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 16 features after 32.4 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 17 features after 24.6 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 18 features after 24.01 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 19 features after 33.71 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 20 features after 27.51 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 1 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 2 features after 0.22 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 3 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 4 features after 0.29 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 5 features after 0.36 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 6 features after 0.29 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 7 features after 0.3 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 8 features after 0.32 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 9 features after 0.41 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 10 features after 0.45 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 11 features after 0.29 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 12 features after 0.35 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 13 features after 0.4 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 14 features after 0.41 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 15 features after 0.32 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 16 features after 0.35 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 17 features after 0.28 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 18 features after 0.29 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 19 features after 0.3 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 20 features after 0.32 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 1 features after 0.34 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 2 features after 0.42 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 3 features after 2.24 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 4 features after 2.07 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 5 features after 2.0 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 6 features after 2.0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished SVC with No Oversampling and MinMaxScaler and 7 features after 1.87 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 8 features after 1.84 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 9 features after 1.85 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 10 features after 1.91 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 11 features after 1.92 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 12 features after 1.95 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 13 features after 1.95 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 14 features after 1.97 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 15 features after 2.0 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 16 features after 2.04 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 17 features after 2.07 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 18 features after 2.1 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 19 features after 2.11 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 20 features after 2.09 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 1 features after 1.88 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 2 features after 2.58 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 3 features after 5.33 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 4 features after 5.47 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 5 features after 7.51 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 6 features after 9.27 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 7 features after 7.83 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 8 features after 8.02 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 9 features after 9.22 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 10 features after 9.63 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 11 features after 9.42 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 12 features after 9.5 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 13 features after 10.27 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 14 features after 11.13 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 15 features after 12.9 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 16 features after 18.72 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 17 features after 31.58 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 18 features after 23.75 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 19 features after 43.06 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 20 features after 47.83 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 1 features after 1.14 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 2 features after 1.18 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 3 features after 1.14 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 4 features after 1.24 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 5 features after 1.3 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 6 features after 1.39 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 7 features after 1.48 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 8 features after 1.45 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 9 features after 1.52 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 10 features after 1.81 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 11 features after 2.18 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 12 features after 2.55 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 13 features after 2.85 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 14 features after 2.24 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 15 features after 2.32 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 16 features after 3.08 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 17 features after 3.68 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 18 features after 3.07 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 19 features after 2.93 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 20 features after 2.99 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 1 features after 5.18 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 2 features after 5.04 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 3 features after 5.11 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 4 features after 6.46 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 5 features after 6.6 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 6 features after 7.17 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 7 features after 7.25 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 8 features after 6.09 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 9 features after 5.87 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 10 features after 5.99 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 11 features after 5.8 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 12 features after 6.04 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 13 features after 6.19 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 14 features after 6.13 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 15 features after 6.9 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 16 features after 6.8 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 17 features after 6.88 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 18 features after 7.08 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 19 features after 7.36 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 20 features after 7.44 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 1 features after 5.39 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 2 features after 5.58 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 3 features after 6.27 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 4 features after 5.83 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 5 features after 5.4 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 6 features after 5.29 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 7 features after 6.86 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 8 features after 6.71 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 9 features after 6.53 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 10 features after 5.68 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 11 features after 5.62 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 12 features after 5.58 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 13 features after 5.39 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 14 features after 5.49 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 15 features after 5.53 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 16 features after 5.54 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 17 features after 5.69 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 18 features after 5.6 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 19 features after 5.62 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 20 features after 5.79 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 1 features after 15.98 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 2 features after 16.08 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 3 features after 17.03 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 4 features after 16.02 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 5 features after 16.04 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 6 features after 17.12 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 7 features after 20.74 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 8 features after 19.16 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 9 features after 17.27 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 10 features after 18.82 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 11 features after 22.42 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 12 features after 21.09 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 13 features after 18.65 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 14 features after 18.31 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 15 features after 19.05 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 16 features after 19.45 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 17 features after 21.44 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 18 features after 23.12 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 19 features after 19.22 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 20 features after 20.09 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 1 features after 16.36 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 2 features after 16.27 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 3 features after 22.96 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 4 features after 21.12 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 5 features after 20.35 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 6 features after 20.34 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 7 features after 22.25 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 8 features after 22.31 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 9 features after 25.27 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 10 features after 24.75 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 11 features after 25.56 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 12 features after 25.55 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 13 features after 32.12 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 14 features after 25.91 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 15 features after 33.81 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 16 features after 30.36 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 17 features after 30.23 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 18 features after 30.02 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 19 features after 32.09 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 20 features after 31.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 1 features after 0.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 2 features after 0.74 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 3 features after 0.76 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 4 features after 0.69 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 5 features after 0.94 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 6 features after 0.76 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 7 features after 0.69 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 8 features after 0.79 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 9 features after 0.79 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 10 features after 0.77 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 11 features after 0.85 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 12 features after 0.75 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 13 features after 0.72 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 14 features after 0.83 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 15 features after 0.8 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 16 features after 0.74 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 17 features after 0.87 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 18 features after 0.8 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 19 features after 0.81 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 20 features after 0.82 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 1 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 2 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 3 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 4 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 5 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 6 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 7 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 8 features after 0.05 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished GaussianNB with No Oversampling and StandardScaler and 9 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 10 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 11 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 12 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 13 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 14 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 15 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 16 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 17 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 18 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 19 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 20 features after 0.06 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 1 features after 6.0 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 2 features after 6.75 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 3 features after 7.01 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 4 features after 7.14 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 5 features after 6.73 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 6 features after 7.47 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 7 features after 7.78 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 8 features after 7.98 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 9 features after 8.05 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 10 features after 7.81 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 11 features after 7.79 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 12 features after 7.37 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 13 features after 8.29 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 14 features after 7.31 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 15 features after 9.7 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 16 features after 12.63 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 17 features after 11.82 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 18 features after 12.57 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 19 features after 9.77 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 20 features after 12.03 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 1 features after 138.15 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 2 features after 119.11 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 3 features after 48.84 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 4 features after 48.11 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 5 features after 56.97 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 6 features after 83.88 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 7 features after 54.3 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 8 features after 63.51 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 9 features after 36.65 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 10 features after 45.07 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 11 features after 58.21 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 12 features after 85.5 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 13 features after 85.96 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 14 features after 155.0 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 15 features after 35.65 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 16 features after 54.24 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 17 features after 82.99 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 18 features after 101.56 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 19 features after 91.45 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 20 features after 110.06 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 1 features after 21.64 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 2 features after 20.49 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 3 features after 21.04 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 4 features after 21.32 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 5 features after 20.79 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 6 features after 21.28 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 7 features after 22.3 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 8 features after 21.53 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 9 features after 21.09 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 10 features after 21.19 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 11 features after 21.46 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 12 features after 21.79 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 13 features after 22.29 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 14 features after 23.28 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 15 features after 23.2 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 16 features after 22.37 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 17 features after 24.03 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 18 features after 24.39 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 19 features after 24.56 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 20 features after 26.21 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 1 features after 13.41 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 2 features after 13.29 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 3 features after 13.52 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 4 features after 13.5 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 5 features after 13.94 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 6 features after 13.77 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 7 features after 12.5 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 8 features after 13.86 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 9 features after 13.35 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished XGBClassifier with No Oversampling and StandardScaler and 10 features after 13.87 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 11 features after 13.95 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 12 features after 14.21 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 13 features after 13.67 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 14 features after 14.26 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 15 features after 14.09 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 16 features after 14.54 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 17 features after 15.2 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 18 features after 14.08 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 19 features after 13.84 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 20 features after 14.54 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 1 features after 91.4 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 2 features after 105.31 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 3 features after 80.13 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 4 features after 135.45 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 5 features after 32.6 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 6 features after 30.55 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 7 features after 34.94 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 8 features after 26.32 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 9 features after 38.48 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 10 features after 46.38 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 11 features after 39.57 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 12 features after 32.14 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 13 features after 47.91 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 14 features after 49.69 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 15 features after 51.73 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 16 features after 34.31 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 17 features after 70.34 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 18 features after 34.55 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 19 features after 46.78 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 20 features after 76.96 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 1 features after 2.79 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 2 features after 5.28 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 3 features after 7.36 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 4 features after 4.06 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 5 features after 3.6 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 6 features after 3.35 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 7 features after 3.41 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 8 features after 9.13 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 9 features after 5.82 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 10 features after 4.19 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 11 features after 4.77 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 12 features after 8.77 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 13 features after 5.2 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 14 features after 4.16 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 15 features after 4.19 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 16 features after 4.04 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 17 features after 4.12 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 18 features after 6.4 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 19 features after 9.29 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 20 features after 5.67 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 1 features after 10.69 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 2 features after 26.22 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 3 features after 22.39 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 4 features after 20.07 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 5 features after 21.05 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 6 features after 20.55 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 7 features after 31.1 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 8 features after 22.87 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 9 features after 40.66 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 10 features after 26.33 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 11 features after 26.13 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 12 features after 26.41 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 13 features after 35.91 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 14 features after 35.28 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 15 features after 31.15 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 16 features after 28.39 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 17 features after 28.92 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 18 features after 27.75 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 19 features after 29.2 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 20 features after 29.83 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 1 features after 0.18 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 2 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 3 features after 0.67 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 4 features after 0.76 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 5 features after 0.91 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 6 features after 1.01 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 7 features after 1.56 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 8 features after 1.47 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 9 features after 1.39 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 10 features after 0.29 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 11 features after 0.25 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 12 features after 2.12 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 13 features after 1.47 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 14 features after 1.52 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 15 features after 1.02 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 16 features after 1.51 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 17 features after 0.33 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 18 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 19 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 20 features after 0.25 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 1 features after 0.26 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 2 features after 0.31 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 3 features after 1.66 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 4 features after 2.41 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 5 features after 3.87 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 6 features after 3.6 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 7 features after 3.2 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 8 features after 3.94 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 9 features after 3.11 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 10 features after 1.84 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 11 features after 2.71 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 12 features after 4.14 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 13 features after 3.62 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 14 features after 4.22 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 15 features after 3.91 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 16 features after 2.46 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 17 features after 2.19 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 18 features after 4.33 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 19 features after 4.34 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 20 features after 3.67 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 1 features after 3.64 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 2 features after 4.62 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 3 features after 7.17 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 4 features after 10.69 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 5 features after 5.08 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 6 features after 5.03 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 7 features after 12.0 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 8 features after 11.88 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 9 features after 13.14 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 10 features after 16.42 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 11 features after 25.66 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 12 features after 29.08 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 13 features after 30.09 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 14 features after 31.99 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 15 features after 23.44 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 16 features after 27.48 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 17 features after 28.2 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 18 features after 31.47 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 19 features after 47.77 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 20 features after 64.04 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 1 features after 1.86 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 2 features after 2.04 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 3 features after 1.79 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 4 features after 1.44 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 5 features after 1.59 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 6 features after 2.67 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 7 features after 2.04 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 8 features after 3.24 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 9 features after 2.64 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 10 features after 2.06 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 11 features after 3.42 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 12 features after 2.84 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 13 features after 3.74 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 14 features after 2.77 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 15 features after 3.16 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 16 features after 3.88 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 17 features after 4.48 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 18 features after 3.27 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 19 features after 4.75 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 20 features after 4.64 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 1 features after 6.38 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 2 features after 8.96 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 3 features after 7.63 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 4 features after 8.03 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 5 features after 7.96 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 6 features after 8.1 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 7 features after 7.91 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 8 features after 8.45 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 9 features after 7.07 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 10 features after 6.88 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 11 features after 6.17 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 12 features after 6.25 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 13 features after 6.08 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 14 features after 6.77 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 15 features after 6.19 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 16 features after 7.8 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 17 features after 7.41 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 18 features after 6.85 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 19 features after 7.06 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 20 features after 7.09 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 1 features after 4.71 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 2 features after 5.11 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 3 features after 4.81 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 4 features after 4.77 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 5 features after 4.82 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 6 features after 6.87 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 7 features after 5.56 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 8 features after 5.35 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 9 features after 5.13 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 10 features after 5.31 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 11 features after 5.63 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 12 features after 5.37 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 13 features after 5.41 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 14 features after 5.2 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 15 features after 5.26 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 16 features after 5.44 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 17 features after 5.55 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 18 features after 6.04 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 19 features after 7.1 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 20 features after 6.17 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 1 features after 14.55 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 2 features after 15.04 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 3 features after 16.83 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 4 features after 17.31 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 5 features after 16.24 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 6 features after 17.32 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 7 features after 19.15 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 8 features after 18.32 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 9 features after 17.51 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 10 features after 19.2 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 11 features after 20.95 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 12 features after 18.2 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 13 features after 19.76 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 14 features after 20.08 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 15 features after 29.22 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 16 features after 20.39 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 17 features after 20.38 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 18 features after 18.43 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 19 features after 19.68 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 20 features after 19.94 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 1 features after 14.31 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 2 features after 22.57 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 3 features after 17.33 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 4 features after 19.03 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 5 features after 20.4 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 6 features after 23.48 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 7 features after 21.84 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 8 features after 22.15 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 9 features after 25.81 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 10 features after 26.32 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 11 features after 27.52 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 12 features after 25.82 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 13 features after 25.57 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 14 features after 26.45 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 15 features after 30.67 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 16 features after 31.71 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 17 features after 29.43 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 18 features after 29.74 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 19 features after 35.71 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 20 features after 30.45 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 1 features after 0.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 2 features after 0.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 3 features after 0.65 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 4 features after 0.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 5 features after 0.75 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 6 features after 0.69 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 7 features after 0.7 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 8 features after 0.71 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 9 features after 0.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 10 features after 0.71 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 11 features after 0.71 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 12 features after 0.7 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 13 features after 0.69 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 14 features after 0.68 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 15 features after 0.64 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 16 features after 0.61 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 17 features after 0.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 18 features after 0.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 19 features after 0.7 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished BernoulliNB with No Oversampling and LogScaler and 20 features after 0.67 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 1 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 2 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 3 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 4 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 5 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 6 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 7 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 8 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 9 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 10 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 11 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 12 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 13 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 14 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 15 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 16 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 17 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 18 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 19 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 20 features after 0.03 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 1 features after 5.92 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 2 features after 6.48 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 3 features after 4.73 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 4 features after 5.31 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 5 features after 5.16 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 6 features after 5.0 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 7 features after 8.51 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 8 features after 7.29 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 9 features after 6.61 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 10 features after 5.98 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 11 features after 5.82 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 12 features after 8.35 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 13 features after 8.08 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 14 features after 7.02 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 15 features after 6.54 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 16 features after 6.34 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 17 features after 7.81 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 18 features after 8.56 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 19 features after 7.25 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 20 features after 7.06 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 1 features after 59.31 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 2 features after 57.12 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 3 features after 22.52 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 4 features after 14.91 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 5 features after 25.43 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 6 features after 17.78 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 7 features after 28.32 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 8 features after 18.28 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 9 features after 32.53 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 10 features after 23.56 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 11 features after 32.55 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 12 features after 29.55 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 13 features after 34.33 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 14 features after 47.01 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 15 features after 121.12 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 16 features after 144.99 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 17 features after 135.37 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 18 features after 144.27 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 19 features after 99.56 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 20 features after 96.39 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 1 features after 24.17 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 2 features after 21.1 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 3 features after 21.78 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 4 features after 21.32 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 5 features after 21.67 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 6 features after 21.41 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 7 features after 22.73 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 8 features after 21.48 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 9 features after 21.37 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 10 features after 21.74 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 11 features after 20.87 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 12 features after 22.6 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 13 features after 22.19 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 14 features after 23.51 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 15 features after 22.13 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 16 features after 23.92 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 17 features after 23.64 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 18 features after 24.85 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 19 features after 22.61 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 20 features after 25.25 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 1 features after 14.09 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 2 features after 13.69 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 3 features after 12.84 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 4 features after 11.94 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished XGBClassifier with No Oversampling and LogScaler and 5 features after 11.72 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 6 features after 11.6 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 7 features after 11.42 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 8 features after 12.36 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 9 features after 12.4 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 10 features after 13.48 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 11 features after 13.37 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 12 features after 13.95 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 13 features after 14.7 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 14 features after 13.7 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 15 features after 14.26 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 16 features after 13.53 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 17 features after 13.57 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 18 features after 14.91 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 19 features after 14.5 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 20 features after 13.72 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 1 features after 60.54 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 2 features after 48.59 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 3 features after 51.26 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 4 features after 66.13 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 5 features after 57.74 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 6 features after 50.94 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 7 features after 80.45 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 8 features after 93.82 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 9 features after 86.91 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 10 features after 69.47 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 11 features after 108.55 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 12 features after 108.01 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 13 features after 124.4 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 14 features after 161.92 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 15 features after 122.81 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 16 features after 29.75 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 17 features after 35.52 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 18 features after 35.34 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 19 features after 45.48 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 20 features after 44.44 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 1 features after 3.49 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 2 features after 6.87 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 3 features after 5.47 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 4 features after 5.8 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 5 features after 5.08 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 6 features after 4.37 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 7 features after 4.51 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 8 features after 4.09 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 9 features after 4.12 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 10 features after 4.21 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 11 features after 3.84 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 12 features after 3.79 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 13 features after 5.93 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 14 features after 4.87 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 15 features after 6.36 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 16 features after 4.35 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 17 features after 4.11 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 18 features after 6.71 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 19 features after 5.4 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 20 features after 7.06 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 1 features after 10.27 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 2 features after 26.78 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 3 features after 29.0 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 4 features after 29.44 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 5 features after 32.88 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 6 features after 32.51 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 7 features after 21.58 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 8 features after 25.0 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 9 features after 24.55 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 10 features after 21.75 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 11 features after 22.72 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 12 features after 27.84 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 13 features after 23.32 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 14 features after 26.53 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 15 features after 27.55 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 16 features after 28.28 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 17 features after 28.97 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 18 features after 28.3 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 19 features after 29.8 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 20 features after 28.76 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 1 features after 0.22 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 2 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 3 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 4 features after 0.22 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 5 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 6 features after 0.21 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 7 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 8 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 9 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 10 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 11 features after 0.23 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 12 features after 0.89 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 13 features after 0.34 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 14 features after 0.31 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 15 features after 0.33 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 16 features after 0.37 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 17 features after 0.31 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 18 features after 0.3 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 19 features after 0.3 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 20 features after 0.32 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 1 features after 0.6 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 2 features after 0.54 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 3 features after 0.88 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 4 features after 0.68 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 5 features after 0.53 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 6 features after 0.71 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 7 features after 2.25 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 8 features after 1.84 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 9 features after 1.83 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 10 features after 1.85 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 11 features after 2.75 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 12 features after 3.82 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 13 features after 2.39 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 14 features after 2.78 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 15 features after 2.07 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 16 features after 2.07 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 17 features after 2.86 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 18 features after 3.61 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 19 features after 2.66 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 20 features after 2.4 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Oversampling Strategy</th>\n",
       "      <th>Data Preparation</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Feature Selection Technique</th>\n",
       "      <th>Features</th>\n",
       "      <th>Optimal Parameters</th>\n",
       "      <th>Monetary Value Per Instance - Mean</th>\n",
       "      <th>Monetary Value Per Instance - Standard Deviation</th>\n",
       "      <th>Raw Model</th>\n",
       "      <th>Cross Validation Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06113533973693848, 0.1522...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03849694728851318, 0.1111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.043284034729003905, 0.111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03919510841369629, 0.1388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.103015</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.043283915519714354, 0.105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.118947</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.27336826324462893, 0.1812...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.025432181358337403, 0.097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.118947</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.1654575824737549, 0.10551...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.02942171096801758, 0.0845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03171484470367432, 0.0952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.103015</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.048366665840148926, 0.119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.118947</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.16805057525634765, 0.1037...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.103015</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04318397045135498, 0.1047...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03390898704528809, 0.0981...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.102813</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.17962005138397216, 0.1369...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.059194</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06023867130279541, 0.2386...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.059194</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05874247550964355, 0.1494...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.102813</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.3041860580444336, 0.25930...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.059194</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.051860690116882324, 0.160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.102813</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.17523112297058105, 0.1351...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.102813</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.17313728332519532, 0.1894...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.059194</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07799050807952881, 0.2633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.102813</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.1412179708480835, 0.17553...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.102813</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.15877537727355956, 0.1141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.113088</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.15906999111175538, 0.1242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.101862</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0335101842880249, 0.10073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.101862</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.09344995021820068, 0.1945...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.101862</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.042286133766174315, 0.121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.113088</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.26040289402008054, 0.1775...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.113088</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.1711425542831421, 0.12476...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.101862</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03670167922973633, 0.0877...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.1521930694580078, 0.10462...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04817039966583252, 0.1117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04497933387756348, 0.1119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.047671842575073245, 0.116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.103536</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03979306221008301, 0.1205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.15687742233276367, 0.0913...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.093421</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03161232471466065, 0.0993...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.093421</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.027127003669738768, 0.100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.7000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.7000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.17346522808074952, 0.1419...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0981372594833374, 0.20116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.057944655418395996, 0.108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.17223942279815674, 0.1202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.167352557182312, 0.092950...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.7000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.7000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.1612694025039673, 0.13753...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.7000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.7000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2939136505126953, 0.24544...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.16785142421722413, 0.1121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04986319541931152, 0.1174...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.28763043880462646, 0.1764...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.103137</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05026490688323974, 0.1440...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.103137</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05544557571411133, 0.1582...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.8, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.117519</td>\n",
       "      <td>LogisticRegression(C=0.8, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.30189244747161864, 0.4564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.097959</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.021231985092163085, 0.062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.123364</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.34388012886047364, 0.7888...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.097959</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03480620384216308, 0.1215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.097959</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.029920077323913573, 0.106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05774524211883545, 0.1460...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06372911930084228, 0.2796...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07280523777008056, 0.2100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.103137</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06801788806915283, 0.1509...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.052160024642944336, 0.144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0752985954284668, 0.19527...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.13523645401000978, 0.3425...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.11947979927062988, 0.3696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07100903987884521, 0.3016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.097959</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.021940898895263673, 0.064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09015867710113526, 0.2539...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.103137</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04188234806060791, 0.1107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.094946</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06721956729888916, 0.1405...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.100877</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2289875030517578, 1.01817...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.101973</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04098949432373047, 0.1219...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.137511</td>\n",
       "      <td>LogisticRegression(C=0.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.21003875732421876, 0.5953...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.104109</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.3262270450592041, 0.57665...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.101973</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.08317737579345703, 0.1961...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.101973</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05624942779541016, 0.1344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.100877</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.19128904342651368, 0.5572...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2103</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.108848</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03530504703521729, 0.1235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.094946</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.057844924926757815, 0.134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2106</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.094946</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.050564026832580565, 0.143...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.100877</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.21312987804412842, 0.5690...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.101973</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06003890037536621, 0.1580...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.104109</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.18101446628570556, 0.2972...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.137511</td>\n",
       "      <td>LogisticRegression(C=0.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.18799772262573242, 0.3461...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.094946</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.08188107013702392, 0.2176...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.100877</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.19197847843170165, 0.5672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.137511</td>\n",
       "      <td>LogisticRegression(C=0.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.21093599796295165, 0.3604...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.104109</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.19537763595581054, 0.3093...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.8, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.8, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.18709993362426758, 0.4300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.8, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.8, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.21472642421722413, 0.5442...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.7000000000000001, 'fit_intercept': Tru...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.7000000000000001, class...</td>\n",
       "      <td>{'mean_fit_time': [0.1734311103820801, 0.37040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.8, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.8, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.2207099437713623, 0.47792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.9, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.9, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.19248578548431397, 0.3709...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.8, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.8, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.38387322425842285, 0.7942...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.9, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.121215</td>\n",
       "      <td>LogisticRegression(C=0.9, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.26718547344207766, 0.3141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107680</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.039394593238830565, 0.106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.2901237964630127, 0.61365...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107680</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03670144081115723, 0.1009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107680</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.09983272552490234, 0.2144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.20325686931610107, 1.0600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.21482634544372559, 0.6812...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.21512532234191895, 0.6758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.22569668292999268, 0.6586...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.102981</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03390836715698242, 0.1103...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.039194679260253905, 0.123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.20265860557556153, 0.6600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.25202648639678954, 0.7661...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2111</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.102908</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.046973657608032224, 0.121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.212432336807251, 0.680579...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.23636844158172607, 0.6654...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.22769105434417725, 0.7410...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2990999698638916, 0.62991...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.28184669017791747, 0.7094...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0429844856262207, 0.10831...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.3118659496307373, 0.96202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.261102294921875, 0.758272...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05575072765350342, 0.1100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.080961</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05096371173858642, 0.1580...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07011222839355469, 0.2252...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0611363410949707, 0.12536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.111028</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06043722629547119, 0.1543...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.080961</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05515222549438477, 0.1266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.111028</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.051561594009399414, 0.139...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.080961</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04497950077056885, 0.1192...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 140}</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.077010</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.008078384399414062, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.080961</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04916832447052002, 0.1450...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.111028</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06582322120666503, 0.1478...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04148845672607422, 0.1375...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.059639811515808105, 0.153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.080961</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.050066018104553224, 0.166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.111028</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0560492992401123, 0.12596...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05495035648345947, 0.1513...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05804433822631836, 0.1329...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.054054546356201175, 0.284...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09893362522125244, 0.3279...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.11100223064422607, 0.3664...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092207</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04597628116607666, 0.1395...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 150}</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.101174</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'mean_fit_time': [0.034308433532714844, 0.092...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.038193678855895995, 0.107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 150}</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.101174</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'mean_fit_time': [0.0322134256362915, 0.08786...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03889622688293457, 0.1376...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09065699577331543, 0.2680...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.101856</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0434755802154541, 0.11658...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06472690105438232, 0.1714...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.13174645900726317, 0.3373...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06213345527648926, 0.2024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.093109</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.053156685829162595, 0.173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.055550003051757814, 0.155...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04936537742614746, 0.2084...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.14670746326446532, 0.3679...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.093109</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04817130565643311, 0.1212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07479920387268066, 0.2064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2079</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10910756587982177, 0.3909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09948463439941406, 0.3176...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05624775886535645, 0.1454...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0632176399230957, 0.18749...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0556509256362915, 0.24145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.062432384490966795, 0.195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07709395885467529, 0.1967...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.13094871044158934, 0.3776...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05295827388763428, 0.1430...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.11549084186553955, 0.3311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10541601181030273, 0.3519...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10811076164245606, 0.3136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.093109</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0559502124786377, 0.28443...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.11608922481536865, 0.3448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.120078444480896, 0.370708...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.13324284553527832, 0.3540...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.13025109767913817, 0.3415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.1284557342529297, 0.35704...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08576996326446533, 0.3108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07699398994445801, 0.2177...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06502611637115478, 0.3037...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06401519775390625, 0.1706...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0630272388458252, 0.16964...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.062333083152770995, 0.207...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07789082527160644, 0.2380...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0636291742324829, 0.21951...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 150}</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.101174</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'mean_fit_time': [0.028714919090270997, 0.082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08317697048187256, 0.2898...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06313116550445556, 0.1747...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04956729412078857, 0.2117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.061734700202941896, 0.265...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06432785987854003, 0.2578...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.046769213676452634, 0.171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.093109</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05884232521057129, 0.1451...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05445435047149658, 0.2066...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 150}</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.101174</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'mean_fit_time': [0.029418802261352538, 0.091...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05703821182250977, 0.1568...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.097381</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03211398124694824, 0.0977...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.063942</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04358339309692383, 0.1932...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05026543140411377, 0.1406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.097381</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.029720139503479005, 0.086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.097381</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0981374740600586, 0.18909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.091349</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.034208273887634276, 0.125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0455775260925293, 0.11289...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04686872959136963, 0.1165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04527881145477295, 0.1108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06991252899169922, 0.1460...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.091349</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03989269733428955, 0.1277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06562378406524658, 0.1389...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.100336</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05704696178436279, 0.1268...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.117714</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.058243942260742185, 0.163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.091349</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.055152058601379395, 0.133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.048369836807250974, 0.150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 30}</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.077892</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.007579636573791504, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07470011711120605, 0.1448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.063942</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.044377803802490234, 0.121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0774925708770752, 0.23407...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.117714</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.09873580932617188, 0.2120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.100336</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.050764036178588864, 0.120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0635291337966919, 0.17652...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.091349</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.060038208961486816, 0.180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0685157060623169, 0.14640...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06063730716705322, 0.1532...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.097381</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.041388964653015135, 0.112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.100336</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06471807956695556, 0.1615...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.117714</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05345642566680908, 0.1421...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.117714</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.060737204551696775, 0.131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.100336</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05016477108001709, 0.1428...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.049267935752868655, 0.113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.08327717781066894, 0.2025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.063942</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.050265169143676756, 0.219...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05434753894805908, 0.1184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06532518863677979, 0.2764...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.104727</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06163454055786133, 0.1838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.123936</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06442737579345703, 0.1520...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.123936</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05235929489135742, 0.1379...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.044579267501831055, 0.208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.123936</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04667432308197021, 0.1226...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 200}</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.136070</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'mean_fit_time': [0.048470139503479004, 0.122...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06721937656402588, 0.2572...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 45}</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.104852</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.0044878959655761715, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03251252174377441, 0.0914...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04019174575805664, 0.1086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04368095397949219, 0.1145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 60}</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.087785</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.008377623558044434, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04725942611694336, 0.1317...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05854318141937256, 0.2138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07958698272705078, 0.2445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06382904052734376, 0.1862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.104727</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05754454135894775, 0.1469...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05844368934631348, 0.1783...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.034106206893920896, 0.087...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06233024597167969, 0.1788...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05654888153076172, 0.1912...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05535285472869873, 0.2403...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06083688735961914, 0.2303...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.104727</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05963718891143799, 0.3151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.123936</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.040691065788269046, 0.146...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.046475696563720706, 0.143...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019917726516723634, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0021944522857666017, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0016881942749023438, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.003789830207824707, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.18169987201690674, 0.2131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.004487824440002441, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.004887080192565918, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel]</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.001994824409484863, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel]</td>\n",
       "      <td>{'activation': 'identity', 'learning_rate': 'c...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>MLPClassifier(activation='identity', alpha=0.0...</td>\n",
       "      <td>{'mean_fit_time': [0.05205860137939453, 0.1279...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019945621490478514, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002792644500732422, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0037900209426879883, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.006781864166259766, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.003490734100341797, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004787039756774902, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001994943618774414, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.009474682807922363, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.004787421226501465, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020009279251098633, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[lineItemVoids, scansWithoutRegistration, quan...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.0056848287582397464, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.004089236259460449, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0021057844161987303, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[lineItemVoids, scansWithoutRegistration, quan...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.004487967491149903, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020943880081176758, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002194118499755859, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.005984020233154297, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.00329129695892334, 0.0032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.005585122108459473, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0023935794830322265], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002296781539916992, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-0.279404</td>\n",
       "      <td>0.276866</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.00907599925994873, 0.0153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.303353</td>\n",
       "      <td>0.049882</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020952701568603517, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.303353</td>\n",
       "      <td>0.049882</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002097344398498535, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.303353</td>\n",
       "      <td>0.055660</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002197599411010742, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.003789472579956055, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020944356918334963, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0031915903091430664, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.00418856143951416, 0.0057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0021973133087158205, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019999027252197267, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.003690004348754883, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002991938591003418, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0030914783477783204, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019889116287231446, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004188895225524902, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0024930953979492186, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020002126693725586, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.087123</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002792811393737793, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004786992073059082, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.005884313583374023, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0021907567977905275, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0031913995742797853, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001989579200744629, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019997596740722657, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.005086207389831543, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020969867706298827, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.005086421966552734, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.005285835266113282, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.087123</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002094459533691406, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.087123</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020998716354370117, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001992058753967285, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020879507064819336, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019947052001953124, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0022903919219970704, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002100372314453125, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.067319</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0027925252914428713, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.089731</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0022937536239624025, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0029920101165771484, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002592945098876953, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0017952203750610351, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.089731</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0022950172424316406, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0029918432235717775, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0021960258483886717, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0022879123687744142, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.089731</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020972490310668945, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.084035</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0024932146072387694, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.080324</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019973039627075194, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.116680</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.003789854049682617, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.005684852600097656, 0.007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.116680</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002197265625, 0.001991724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002299594879150391, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002001667022705078, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.116680</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.005086350440979004, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.080324</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001894998550415039, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020942926406860352, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001989150047302246, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.080324</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020943403244018553, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 20}</td>\n",
       "      <td>-0.359234</td>\n",
       "      <td>0.282543</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.004186010360717774, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.369878</td>\n",
       "      <td>0.113116</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020979881286621095, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.369878</td>\n",
       "      <td>0.113116</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020946502685546876, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.369878</td>\n",
       "      <td>0.113116</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0032900094985961912, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.369878</td>\n",
       "      <td>0.113116</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019019126892089843, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 110}</td>\n",
       "      <td>-0.372539</td>\n",
       "      <td>0.262374</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.007878804206848144, 0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0015957355499267578, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019971847534179686, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0017929315567016602, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004089069366455078, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.497605</td>\n",
       "      <td>0.379382</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0021942138671875, 0.00259...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.497605</td>\n",
       "      <td>0.379382</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0027927875518798826, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.497605</td>\n",
       "      <td>0.379382</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004088902473449707, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.497605</td>\n",
       "      <td>0.379382</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0018944263458251954, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.502927</td>\n",
       "      <td>0.321477</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.00219419002532959, 0.0025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.502927</td>\n",
       "      <td>0.321477</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020946502685546876, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.502927</td>\n",
       "      <td>0.321477</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0032912254333496093, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.502927</td>\n",
       "      <td>0.321477</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0018979310989379883, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.582757</td>\n",
       "      <td>0.441433</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.03510630130767822, 0.0263...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.582757</td>\n",
       "      <td>0.441433</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0024933576583862304, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.582757</td>\n",
       "      <td>0.441433</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0032911300659179688, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.582757</td>\n",
       "      <td>0.441433</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001898050308227539, 0.001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.651943</td>\n",
       "      <td>0.307925</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0027924060821533205], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.651943</td>\n",
       "      <td>0.307925</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091859817504883], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.651943</td>\n",
       "      <td>0.307925</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002692770957946777], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.670569</td>\n",
       "      <td>0.286324</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002892303466796875, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.670569</td>\n",
       "      <td>0.286324</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025930404663085938, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.670569</td>\n",
       "      <td>0.286324</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025929927825927733, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, scannedLineItemsPerSecond]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-0.734433</td>\n",
       "      <td>0.260090</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010172724723815918, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-0.761043</td>\n",
       "      <td>0.273077</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002892303466796875, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-0.761043</td>\n",
       "      <td>0.273077</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029915809631347657, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-0.761043</td>\n",
       "      <td>0.273077</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0026925325393676756, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, scannedLineItemsPerSecond]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.763704</td>\n",
       "      <td>0.305585</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.009076595306396484, 0.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.846195</td>\n",
       "      <td>0.341143</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0026930570602416992], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.957956</td>\n",
       "      <td>0.295386</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025930404663085938, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.2}</td>\n",
       "      <td>-0.987227</td>\n",
       "      <td>0.332005</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029918432235717775, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.2}</td>\n",
       "      <td>-0.987227</td>\n",
       "      <td>0.332005</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0026925325393676756, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.2}</td>\n",
       "      <td>-0.987227</td>\n",
       "      <td>0.332005</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002692842483520508, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-1.016498</td>\n",
       "      <td>0.302507</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029920101165771484, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-1.016498</td>\n",
       "      <td>0.302507</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0027925968170166016, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-1.016498</td>\n",
       "      <td>0.302507</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00299220085144043, 0.0028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItemsPerSecond]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-1.035125</td>\n",
       "      <td>0.329418</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.014860272407531738, 0.028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'reg_param': 0.4}</td>\n",
       "      <td>-1.048430</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0026929378509521484, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.2}</td>\n",
       "      <td>-1.098989</td>\n",
       "      <td>0.291602</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028922319412231444, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.2}</td>\n",
       "      <td>-1.098989</td>\n",
       "      <td>0.291602</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0027927398681640626, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.2}</td>\n",
       "      <td>-1.098989</td>\n",
       "      <td>0.291602</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029919624328613283, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.136243</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003191494941711426, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.136243</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0032910585403442385, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.136243</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034906148910522463, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.136243</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0032912254333496093, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.136243</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031913280487060546, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.136243</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031914710998535156, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003490734100341797, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00359039306640625, 0.0029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00249326229095459, 0.0034...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003191518783569336, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002991914749145508, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002393364906311035, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029920339584350586, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031914710998535156, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034906625747680663, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.168175</td>\n",
       "      <td>0.342141</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0035904645919799805, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.168175</td>\n",
       "      <td>0.342141</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0030917882919311523, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.168175</td>\n",
       "      <td>0.342141</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031913995742797853, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-1.232038</td>\n",
       "      <td>0.318737</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00329132080078125, 0.0029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.271953</td>\n",
       "      <td>0.393375</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025930881500244142, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.271953</td>\n",
       "      <td>0.393375</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0030918121337890625, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.271953</td>\n",
       "      <td>0.393375</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0023936986923217773, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.271953</td>\n",
       "      <td>0.393375</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028922319412231444, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-1.452900</td>\n",
       "      <td>0.432521</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002892279624938965, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.548696</td>\n",
       "      <td>0.534504</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0031911134719848633], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.628526</td>\n",
       "      <td>0.515212</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0025931358337402343], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.628526</td>\n",
       "      <td>0.515212</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0029921531677246094], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.628526</td>\n",
       "      <td>0.515212</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002892279624938965], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.998403</td>\n",
       "      <td>0.470626</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0027925968170166016], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.998403</td>\n",
       "      <td>0.470626</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016954183578491212], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.998403</td>\n",
       "      <td>0.470626</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0029920101165771484], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.027674</td>\n",
       "      <td>0.373413</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031911611557006838, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.027674</td>\n",
       "      <td>0.373413</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031915903091430664, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.027674</td>\n",
       "      <td>0.373413</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031913995742797853, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.048962</td>\n",
       "      <td>0.480463</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0014959096908569336], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.054284</td>\n",
       "      <td>0.419648</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003191566467285156, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.067589</td>\n",
       "      <td>0.410509</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003490900993347168, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.067589</td>\n",
       "      <td>0.410509</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0036903858184814454, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.067589</td>\n",
       "      <td>0.410509</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0027927875518798826, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.107504</td>\n",
       "      <td>0.433860</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025929927825927733, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.134114</td>\n",
       "      <td>0.446705</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0039894342422485355, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.134114</td>\n",
       "      <td>0.446705</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003390860557556152, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.134114</td>\n",
       "      <td>0.446705</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003490638732910156, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.134114</td>\n",
       "      <td>0.446705</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003291201591491699, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.134114</td>\n",
       "      <td>0.446705</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003191566467285156, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.200639</td>\n",
       "      <td>0.433446</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003789949417114258, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.200639</td>\n",
       "      <td>0.433446</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0036899328231811525, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.200639</td>\n",
       "      <td>0.433446</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003789663314819336, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.200639</td>\n",
       "      <td>0.433446</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003989481925964355, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.251197</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002393651008605957], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.251197</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0028923511505126952], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.251197</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002991771697998047], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.251197</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0031917333602905274], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.251197</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001896977424621582], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.251197</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0020916938781738283], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.256519</td>\n",
       "      <td>0.387237</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0027925968170166016, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.277807</td>\n",
       "      <td>0.480363</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001695537567138672], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.291112</td>\n",
       "      <td>0.481289</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0015959501266479491], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.291112</td>\n",
       "      <td>0.481289</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0030920028686523436], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.291112</td>\n",
       "      <td>0.481289</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018947362899780274], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.291112</td>\n",
       "      <td>0.481289</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002892303466796875], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.315061</td>\n",
       "      <td>0.366846</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002393770217895508, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.574415</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016014814376831056], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.574415</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0029918193817138673], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.574415</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0021942138671875], 'std_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.376264</td>\n",
       "      <td>0.370081</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003191089630126953, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003690171241760254, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.366881</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003590106964111328, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.366881</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0030860185623168947, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0035902738571166994, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0036902427673339844, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0044880867004394535, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.366881</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0035904407501220702, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.366881</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003091740608215332, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.676956</td>\n",
       "      <td>0.667603</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016979455947875976], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.676956</td>\n",
       "      <td>0.667603</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032913923263549806], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.676956</td>\n",
       "      <td>0.667603</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994466781616211], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.788717</td>\n",
       "      <td>0.432177</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0015957355499267578], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.249069</td>\n",
       "      <td>0.700153</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016956090927124023], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.249069</td>\n",
       "      <td>0.700153</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0029920339584350586], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.249069</td>\n",
       "      <td>0.700153</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001795196533203125], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.400745</td>\n",
       "      <td>0.469608</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0015952110290527344], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.400745</td>\n",
       "      <td>0.462822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016927242279052735], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.467270</td>\n",
       "      <td>0.531802</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0014960527420043944], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.706759</td>\n",
       "      <td>0.728619</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016924142837524414], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.706759</td>\n",
       "      <td>0.728619</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0035910606384277344], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.706759</td>\n",
       "      <td>0.728619</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017951250076293946], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.111229</td>\n",
       "      <td>0.765794</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0033912897109985352], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.111229</td>\n",
       "      <td>0.765794</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017014741897583008], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.111229</td>\n",
       "      <td>0.765794</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001795196533203125], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.132517</td>\n",
       "      <td>0.752621</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018015146255493165], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.132517</td>\n",
       "      <td>0.752621</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.00359039306640625], 'std_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.132517</td>\n",
       "      <td>0.752621</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001894974708557129], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.177754</td>\n",
       "      <td>0.637230</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0035907745361328123], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.177754</td>\n",
       "      <td>0.637230</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002094554901123047], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.177754</td>\n",
       "      <td>0.637230</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018978357315063477], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.204364</td>\n",
       "      <td>0.487839</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0015927791595458985], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.683342</td>\n",
       "      <td>0.517235</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0038900375366210938], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.683342</td>\n",
       "      <td>0.517235</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002692985534667969], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.683342</td>\n",
       "      <td>0.517235</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016952753067016602], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.709952</td>\n",
       "      <td>0.394694</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001792144775390625], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.749867</td>\n",
       "      <td>0.412888</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017979860305786133], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.843002</td>\n",
       "      <td>0.431181</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001791667938232422], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.962746</td>\n",
       "      <td>0.415310</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018949508666992188], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.109101</td>\n",
       "      <td>0.429031</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017982959747314454], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018980741500854493], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0015957117080688476], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018950223922729493], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091764450073242], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0035906076431274415], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0033910989761352537], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017977714538574218], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018950700759887695], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0033912181854248045], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0037900447845458985], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0031916141510009766], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001997685432434082], 'std...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2284 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Model Oversampling Strategy Data Preparation Feature Count Feature Selection Technique                                           Features                                 Optimal Parameters  Monetary Value Per Instance - Mean  Monetary Value Per Instance - Standard Deviation                                          Raw Model                           Cross Validation Results\n",
       "1482             AdaBoostClassifier       No Oversampling   StandardScaler             7                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.077832  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06113533973693848, 0.1522...\n",
       "340              AdaBoostClassifier       No Oversampling       No Scaling             7                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.077832  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03849694728851318, 0.1111...\n",
       "2053             AdaBoostClassifier       No Oversampling        LogScaler             7                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.077832  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.043284034729003905, 0.111...\n",
       "911              AdaBoostClassifier       No Oversampling     MinMaxScaler             7                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.077832  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03919510841369629, 0.1388...\n",
       "1534                  XGBClassifier       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.167642                                          0.103015  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.043283915519714354, 0.105...\n",
       "16               LogisticRegression       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.167642                                          0.118947  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.27336826324462893, 0.1812...\n",
       "909              AdaBoostClassifier       No Oversampling     MinMaxScaler             6                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.167642                                          0.066132  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.025432181358337403, 0.097...\n",
       "587              LogisticRegression       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.167642                                          0.118947  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.1654575824737549, 0.10551...\n",
       "2051             AdaBoostClassifier       No Oversampling        LogScaler             6                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.167642                                          0.066132  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.02942171096801758, 0.0845...\n",
       "1480             AdaBoostClassifier       No Oversampling   StandardScaler             6                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.167642                                          0.066132  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03171484470367432, 0.0952...\n",
       "392                   XGBClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.167642                                          0.103015  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.048366665840148926, 0.119...\n",
       "1158             LogisticRegression       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.167642                                          0.118947  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.16805057525634765, 0.1037...\n",
       "963                   XGBClassifier       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.167642                                          0.103015  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04318397045135498, 0.1047...\n",
       "338              AdaBoostClassifier       No Oversampling       No Scaling             6                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.167642                                          0.066132  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03390898704528809, 0.0981...\n",
       "20               LogisticRegression       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.164981                                          0.102813  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.17962005138397216, 0.1369...\n",
       "923              AdaBoostClassifier       No Oversampling     MinMaxScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.059194  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06023867130279541, 0.2386...\n",
       "2065             AdaBoostClassifier       No Oversampling        LogScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.059194  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05874247550964355, 0.1494...\n",
       "28               LogisticRegression       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.164981                                          0.102813  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.3041860580444336, 0.25930...\n",
       "352              AdaBoostClassifier       No Oversampling       No Scaling            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.059194  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.051860690116882324, 0.160...\n",
       "591              LogisticRegression       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.164981                                          0.102813  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.17523112297058105, 0.1351...\n",
       "599              LogisticRegression       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.164981                                          0.102813  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.17313728332519532, 0.1894...\n",
       "1494             AdaBoostClassifier       No Oversampling   StandardScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.059194  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07799050807952881, 0.2633...\n",
       "1170             LogisticRegression       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.164981                                          0.102813  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.1412179708480835, 0.17553...\n",
       "1162             LogisticRegression       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.164981                                          0.102813  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.15877537727355956, 0.1141...\n",
       "1166             LogisticRegression       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.162320                                          0.113088  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.15906999111175538, 0.1242...\n",
       "1530                  XGBClassifier       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.101862  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0335101842880249, 0.10073...\n",
       "388                   XGBClassifier       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.101862  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.09344995021820068, 0.1945...\n",
       "2107                  XGBClassifier       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.101862  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.042286133766174315, 0.121...\n",
       "24               LogisticRegression       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.162320                                          0.113088  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.26040289402008054, 0.1775...\n",
       "595              LogisticRegression       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.162320                                          0.113088  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.1711425542831421, 0.12476...\n",
       "959                   XGBClassifier       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.101862  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.03670167922973633, 0.0877...\n",
       "1160             LogisticRegression       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.1521930694580078, 0.10462...\n",
       "967                   XGBClassifier       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.102299  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04817039966583252, 0.1117...\n",
       "969                   XGBClassifier       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.102299  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04497933387756348, 0.1119...\n",
       "1540                  XGBClassifier       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.102299  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.047671842575073245, 0.116...\n",
       "2109                  XGBClassifier       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.159659                                          0.103536  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.03979306221008301, 0.1205...\n",
       "1164             LogisticRegression       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.15687742233276367, 0.0913...\n",
       "2097                  XGBClassifier       No Oversampling        LogScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.093421  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.03161232471466065, 0.0993...\n",
       "2095                  XGBClassifier       No Oversampling        LogScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.093421  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.027127003669738768, 0.100...\n",
       "597              LogisticRegression       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.7000000000000002, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.7000000000000002, class...  {'mean_fit_time': [0.17346522808074952, 0.1419...\n",
       "398                   XGBClassifier       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.102299  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0981372594833374, 0.20116...\n",
       "1538                  XGBClassifier       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.102299  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.057944655418395996, 0.108...\n",
       "22               LogisticRegression       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.17223942279815674, 0.1202...\n",
       "593              LogisticRegression       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.167352557182312, 0.092950...\n",
       "1168             LogisticRegression       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.7000000000000002, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.7000000000000002, class...  {'mean_fit_time': [0.1612694025039673, 0.13753...\n",
       "26               LogisticRegression       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.7000000000000002, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.7000000000000002, class...  {'mean_fit_time': [0.2939136505126953, 0.24544...\n",
       "589              LogisticRegression       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.16785142421722413, 0.1121...\n",
       "396                   XGBClassifier       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.102299  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04986319541931152, 0.1174...\n",
       "18               LogisticRegression       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.28763043880462646, 0.1764...\n",
       "2100                  XGBClassifier       No Oversampling        LogScaler            10                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.156998                                          0.103137  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05026490688323974, 0.1440...\n",
       "958                   XGBClassifier       No Oversampling     MinMaxScaler            10                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.156998                                          0.103137  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05544557571411133, 0.1582...\n",
       "1735             LogisticRegression       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.8, 'fit_intercept': True, 'solver': 'l...                            0.156998                                          0.117519  LogisticRegression(C=0.8, class_weight=None, d...  {'mean_fit_time': [0.30189244747161864, 0.4564...\n",
       "2049             AdaBoostClassifier       No Oversampling        LogScaler             5                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.097959  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.021231985092163085, 0.062...\n",
       "1743             LogisticRegression       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.0, 'fit_intercept': True, 'solver': 'l...                            0.156998                                          0.123364  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.34388012886047364, 0.7888...\n",
       "1478             AdaBoostClassifier       No Oversampling   StandardScaler             5                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.097959  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03480620384216308, 0.1215...\n",
       "907              AdaBoostClassifier       No Oversampling     MinMaxScaler             5                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.097959  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.029920077323913573, 0.106...\n",
       "2066             AdaBoostClassifier       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05774524211883545, 0.1460...\n",
       "1503             AdaBoostClassifier       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06372911930084228, 0.2796...\n",
       "932              AdaBoostClassifier       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07280523777008056, 0.2100...\n",
       "1529                  XGBClassifier       No Oversampling   StandardScaler            10                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.156998                                          0.103137  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06801788806915283, 0.1509...\n",
       "2064             AdaBoostClassifier       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.052160024642944336, 0.144...\n",
       "2074             AdaBoostClassifier       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0752985954284668, 0.19527...\n",
       "2072             AdaBoostClassifier       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.13523645401000978, 0.3425...\n",
       "2070             AdaBoostClassifier       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.11947979927062988, 0.3696...\n",
       "2068             AdaBoostClassifier       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07100903987884521, 0.3016...\n",
       "336              AdaBoostClassifier       No Oversampling       No Scaling             5                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.097959  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.021940898895263673, 0.064...\n",
       "361              AdaBoostClassifier       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09015867710113526, 0.2539...\n",
       "387                   XGBClassifier       No Oversampling       No Scaling            10                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.156998                                          0.103137  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04188234806060791, 0.1107...\n",
       "964                   XGBClassifier       No Oversampling     MinMaxScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.094946  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06721956729888916, 0.1405...\n",
       "36               LogisticRegression       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.100877  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.2289875030517578, 1.01817...\n",
       "2094                  XGBClassifier       No Oversampling        LogScaler             7                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.101973  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04098949432373047, 0.1219...\n",
       "32               LogisticRegression       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.6, 'fit_intercept': True, 'solver': 'l...                            0.154337                                          0.137511  LogisticRegression(C=0.6, class_weight=None, d...  {'mean_fit_time': [0.21003875732421876, 0.5953...\n",
       "30               LogisticRegression       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.104109  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.3262270450592041, 0.57665...\n",
       "381                   XGBClassifier       No Oversampling       No Scaling             7                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.101973  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.08317737579345703, 0.1961...\n",
       "1523                  XGBClassifier       No Oversampling   StandardScaler             7                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.101973  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05624942779541016, 0.1344...\n",
       "1749             LogisticRegression       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.100877  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.19128904342651368, 0.5572...\n",
       "2103                  XGBClassifier       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.154337                                          0.108848  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.03530504703521729, 0.1235...\n",
       "1535                  XGBClassifier       No Oversampling   StandardScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.094946  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.057844924926757815, 0.134...\n",
       "2106                  XGBClassifier       No Oversampling        LogScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.094946  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.050564026832580565, 0.143...\n",
       "607              LogisticRegression       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.100877  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.21312987804412842, 0.5690...\n",
       "952                   XGBClassifier       No Oversampling     MinMaxScaler             7                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.101973  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06003890037536621, 0.1580...\n",
       "1172             LogisticRegression       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.104109  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.18101446628570556, 0.2972...\n",
       "1174             LogisticRegression       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.6, 'fit_intercept': True, 'solver': 'l...                            0.154337                                          0.137511  LogisticRegression(C=0.6, class_weight=None, d...  {'mean_fit_time': [0.18799772262573242, 0.3461...\n",
       "393                   XGBClassifier       No Oversampling       No Scaling            13                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.094946  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.08188107013702392, 0.2176...\n",
       "1178             LogisticRegression       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.100877  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.19197847843170165, 0.5672...\n",
       "603              LogisticRegression       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.6, 'fit_intercept': True, 'solver': 'l...                            0.154337                                          0.137511  LogisticRegression(C=0.6, class_weight=None, d...  {'mean_fit_time': [0.21093599796295165, 0.3604...\n",
       "601              LogisticRegression       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.104109  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.19537763595581054, 0.3093...\n",
       "1747             LogisticRegression       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.8, 'fit_intercept': True, 'solver': 'n...                            0.151676                                          0.124252  LogisticRegression(C=0.8, class_weight=None, d...  {'mean_fit_time': [0.18709993362426758, 0.4300...\n",
       "605              LogisticRegression       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.8, 'fit_intercept': True, 'solver': 'n...                            0.151676                                          0.124252  LogisticRegression(C=0.8, class_weight=None, d...  {'mean_fit_time': [0.21472642421722413, 0.5442...\n",
       "1741             LogisticRegression       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.7000000000000001, 'fit_intercept': Tru...                            0.151676                                          0.124252  LogisticRegression(C=0.7000000000000001, class...  {'mean_fit_time': [0.1734311103820801, 0.37040...\n",
       "1176             LogisticRegression       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.8, 'fit_intercept': True, 'solver': 'n...                            0.151676                                          0.124252  LogisticRegression(C=0.8, class_weight=None, d...  {'mean_fit_time': [0.2207099437713623, 0.47792...\n",
       "1745             LogisticRegression       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.9, 'fit_intercept': True, 'solver': 'n...                            0.151676                                          0.124252  LogisticRegression(C=0.9, class_weight=None, d...  {'mean_fit_time': [0.19248578548431397, 0.3709...\n",
       "34               LogisticRegression       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.8, 'fit_intercept': True, 'solver': 'n...                            0.151676                                          0.124252  LogisticRegression(C=0.8, class_weight=None, d...  {'mean_fit_time': [0.38387322425842285, 0.7942...\n",
       "1733             LogisticRegression       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.9, 'fit_intercept': True, 'solver': 'n...                            0.151676                                          0.121215  LogisticRegression(C=0.9, class_weight=None, d...  {'mean_fit_time': [0.26718547344207766, 0.3141...\n",
       "1532                  XGBClassifier       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.151676                                          0.107680  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.039394593238830565, 0.106...\n",
       "1739             LogisticRegression       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.6, 'fit_intercept': True, 'solver': 'l...                            0.151676                                          0.124252  LogisticRegression(C=0.6, class_weight=None, d...  {'mean_fit_time': [0.2901237964630127, 0.61365...\n",
       "961                   XGBClassifier       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.151676                                          0.107680  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.03670144081115723, 0.1009...\n",
       "390                   XGBClassifier       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.151676                                          0.107680  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.09983272552490234, 0.2144...\n",
       "40               LogisticRegression       No Oversampling       No Scaling            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.20325686931610107, 1.0600...\n",
       "1182             LogisticRegression       No Oversampling   StandardScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.21482634544372559, 0.6812...\n",
       "1181             LogisticRegression       No Oversampling   StandardScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.21512532234191895, 0.6758...\n",
       "1180             LogisticRegression       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.22569668292999268, 0.6586...\n",
       "2105                  XGBClassifier       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.149015                                          0.102981  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.03390836715698242, 0.1103...\n",
       "965                   XGBClassifier       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.149015                                          0.098901  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.039194679260253905, 0.123...\n",
       "1751             LogisticRegression       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.20265860557556153, 0.6600...\n",
       "1752             LogisticRegression       No Oversampling        LogScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.25202648639678954, 0.7661...\n",
       "2111                  XGBClassifier       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.149015                                          0.102908  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.046973657608032224, 0.121...\n",
       "609              LogisticRegression       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.212432336807251, 0.680579...\n",
       "39               LogisticRegression       No Oversampling       No Scaling            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.23636844158172607, 0.6654...\n",
       "38               LogisticRegression       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.22769105434417725, 0.7410...\n",
       "1737             LogisticRegression       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.149015                                          0.114885  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.2990999698638916, 0.62991...\n",
       "610              LogisticRegression       No Oversampling     MinMaxScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.28184669017791747, 0.7094...\n",
       "394                   XGBClassifier       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.149015                                          0.098901  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.0429844856262207, 0.10831...\n",
       "1753             LogisticRegression       No Oversampling        LogScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.3118659496307373, 0.96202...\n",
       "611              LogisticRegression       No Oversampling     MinMaxScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.261102294921875, 0.758272...\n",
       "1536                  XGBClassifier       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.149015                                          0.098901  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05575072765350342, 0.1100...\n",
       "1487             AdaBoostClassifier       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.146354                                          0.080961  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05096371173858642, 0.1580...\n",
       "383                   XGBClassifier       No Oversampling       No Scaling             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.089183  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07011222839355469, 0.2252...\n",
       "954                   XGBClassifier       No Oversampling     MinMaxScaler             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.089183  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0611363410949707, 0.12536...\n",
       "962                   XGBClassifier       No Oversampling     MinMaxScaler            12                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.111028  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06043722629547119, 0.1543...\n",
       "2062             AdaBoostClassifier       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.146354                                          0.080961  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05515222549438477, 0.1266...\n",
       "2104                  XGBClassifier       No Oversampling        LogScaler            12                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.111028  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.051561594009399414, 0.139...\n",
       "2060             AdaBoostClassifier       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.146354                                          0.080961  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04497950077056885, 0.1192...\n",
       "158            ExtraTreesClassifier       No Oversampling       No Scaling             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...      {'criterion': 'entropy', 'n_estimators': 140}                            0.146354                                          0.077010  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.008078384399414062, 0.013...\n",
       "345              AdaBoostClassifier       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.146354                                          0.080961  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04916832447052002, 0.1450...\n",
       "391                   XGBClassifier       No Oversampling       No Scaling            12                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.111028  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06582322120666503, 0.1478...\n",
       "1525                  XGBClassifier       No Oversampling   StandardScaler             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.089183  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04148845672607422, 0.1375...\n",
       "2096                  XGBClassifier       No Oversampling        LogScaler             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.089183  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.059639811515808105, 0.153...\n",
       "916              AdaBoostClassifier       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.146354                                          0.080961  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.050066018104553224, 0.166...\n",
       "1533                  XGBClassifier       No Oversampling   StandardScaler            12                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.111028  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0560492992401123, 0.12596...\n",
       "354              AdaBoostClassifier       No Oversampling       No Scaling            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05495035648345947, 0.1513...\n",
       "2063             AdaBoostClassifier       No Oversampling        LogScaler            12                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05804433822631836, 0.1329...\n",
       "1492             AdaBoostClassifier       No Oversampling   StandardScaler            12                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.054054546356201175, 0.284...\n",
       "2069             AdaBoostClassifier       No Oversampling        LogScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09893362522125244, 0.3279...\n",
       "2071             AdaBoostClassifier       No Oversampling        LogScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.11100223064422607, 0.3664...\n",
       "2113                  XGBClassifier       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.092207  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04597628116607666, 0.1395...\n",
       "1446     GradientBoostingClassifier       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 150}                            0.143693                                          0.101174  ([DecisionTreeRegressor(criterion='friedman_ms...  {'mean_fit_time': [0.034308433532714844, 0.092...\n",
       "348              AdaBoostClassifier       No Oversampling       No Scaling            11                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.038193678855895995, 0.107...\n",
       "304      GradientBoostingClassifier       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 150}                            0.143693                                          0.101174  ([DecisionTreeRegressor(criterion='friedman_ms...  {'mean_fit_time': [0.0322134256362915, 0.08786...\n",
       "2061             AdaBoostClassifier       No Oversampling        LogScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03889622688293457, 0.1376...\n",
       "363              AdaBoostClassifier       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09065699577331543, 0.2680...\n",
       "2058             AdaBoostClassifier       No Oversampling        LogScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.101856  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0434755802154541, 0.11658...\n",
       "364              AdaBoostClassifier       No Oversampling       No Scaling            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06472690105438232, 0.1714...\n",
       "2075             AdaBoostClassifier       No Oversampling        LogScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.13174645900726317, 0.3373...\n",
       "1490             AdaBoostClassifier       No Oversampling   StandardScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06213345527648926, 0.2024...\n",
       "2112                  XGBClassifier       No Oversampling        LogScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.093109  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.053156685829162595, 0.173...\n",
       "362              AdaBoostClassifier       No Oversampling       No Scaling            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.055550003051757814, 0.155...\n",
       "2067             AdaBoostClassifier       No Oversampling        LogScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04936537742614746, 0.2084...\n",
       "2078             AdaBoostClassifier       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.14670746326446532, 0.3679...\n",
       "399                   XGBClassifier       No Oversampling       No Scaling            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.093109  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04817130565643311, 0.1212...\n",
       "365              AdaBoostClassifier       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07479920387268066, 0.2064...\n",
       "2079             AdaBoostClassifier       No Oversampling        LogScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10910756587982177, 0.3909...\n",
       "2080             AdaBoostClassifier       No Oversampling        LogScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09948463439941406, 0.3176...\n",
       "360              AdaBoostClassifier       No Oversampling       No Scaling            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05624775886535645, 0.1454...\n",
       "366              AdaBoostClassifier       No Oversampling       No Scaling            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0632176399230957, 0.18749...\n",
       "358              AdaBoostClassifier       No Oversampling       No Scaling            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0556509256362915, 0.24145...\n",
       "356              AdaBoostClassifier       No Oversampling       No Scaling            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.062432384490966795, 0.195...\n",
       "367              AdaBoostClassifier       No Oversampling       No Scaling            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07709395885467529, 0.1967...\n",
       "2076             AdaBoostClassifier       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.13094871044158934, 0.3776...\n",
       "350              AdaBoostClassifier       No Oversampling       No Scaling            12                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05295827388763428, 0.1430...\n",
       "2077             AdaBoostClassifier       No Oversampling        LogScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.11549084186553955, 0.3311...\n",
       "2073             AdaBoostClassifier       No Oversampling        LogScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10541601181030273, 0.3519...\n",
       "1504             AdaBoostClassifier       No Oversampling   StandardScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10811076164245606, 0.3136...\n",
       "1541                  XGBClassifier       No Oversampling   StandardScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.093109  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0559502124786377, 0.28443...\n",
       "1509             AdaBoostClassifier       No Oversampling   StandardScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.11608922481536865, 0.3448...\n",
       "1508             AdaBoostClassifier       No Oversampling   StandardScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.120078444480896, 0.370708...\n",
       "1507             AdaBoostClassifier       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.13324284553527832, 0.3540...\n",
       "1506             AdaBoostClassifier       No Oversampling   StandardScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.13025109767913817, 0.3415...\n",
       "1505             AdaBoostClassifier       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.1284557342529297, 0.35704...\n",
       "1502             AdaBoostClassifier       No Oversampling   StandardScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08576996326446533, 0.3108...\n",
       "935              AdaBoostClassifier       No Oversampling     MinMaxScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07699398994445801, 0.2177...\n",
       "1500             AdaBoostClassifier       No Oversampling   StandardScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06502611637115478, 0.3037...\n",
       "936              AdaBoostClassifier       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06401519775390625, 0.1706...\n",
       "937              AdaBoostClassifier       No Oversampling     MinMaxScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0630272388458252, 0.16964...\n",
       "1498             AdaBoostClassifier       No Oversampling   StandardScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.062333083152770995, 0.207...\n",
       "938              AdaBoostClassifier       No Oversampling     MinMaxScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07789082527160644, 0.2380...\n",
       "1496             AdaBoostClassifier       No Oversampling   StandardScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0636291742324829, 0.21951...\n",
       "2019     GradientBoostingClassifier       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 150}                            0.143693                                          0.101174  ([DecisionTreeRegressor(criterion='friedman_ms...  {'mean_fit_time': [0.028714919090270997, 0.082...\n",
       "934              AdaBoostClassifier       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08317697048187256, 0.2898...\n",
       "931              AdaBoostClassifier       No Oversampling     MinMaxScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06313116550445556, 0.1747...\n",
       "919              AdaBoostClassifier       No Oversampling     MinMaxScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04956729412078857, 0.2117...\n",
       "927              AdaBoostClassifier       No Oversampling     MinMaxScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.061734700202941896, 0.265...\n",
       "929              AdaBoostClassifier       No Oversampling     MinMaxScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06432785987854003, 0.2578...\n",
       "925              AdaBoostClassifier       No Oversampling     MinMaxScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.046769213676452634, 0.171...\n",
       "970                   XGBClassifier       No Oversampling     MinMaxScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.093109  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05884232521057129, 0.1451...\n",
       "921              AdaBoostClassifier       No Oversampling     MinMaxScaler            12                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05445435047149658, 0.2066...\n",
       "875      GradientBoostingClassifier       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 150}                            0.143693                                          0.101174  ([DecisionTreeRegressor(criterion='friedman_ms...  {'mean_fit_time': [0.029418802261352538, 0.091...\n",
       "933              AdaBoostClassifier       No Oversampling     MinMaxScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05703821182250977, 0.1568...\n",
       "2101                  XGBClassifier       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.097381  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.03211398124694824, 0.0977...\n",
       "918              AdaBoostClassifier       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.141032                                          0.063942  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04358339309692383, 0.1932...\n",
       "1542                  XGBClassifier       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.05026543140411377, 0.1406...\n",
       "1528                  XGBClassifier       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.097381  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.029720139503479005, 0.086...\n",
       "386                   XGBClassifier       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.097381  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0981374740600586, 0.18909...\n",
       "1521                  XGBClassifier       No Oversampling   StandardScaler             6                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.091349  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.034208273887634276, 0.125...\n",
       "971                   XGBClassifier       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.0455775260925293, 0.11289...\n",
       "973                   XGBClassifier       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.04686872959136963, 0.1165...\n",
       "972                   XGBClassifier       No Oversampling     MinMaxScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.105075  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04527881145477295, 0.1108...\n",
       "1543                  XGBClassifier       No Oversampling   StandardScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.105075  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06991252899169922, 0.1460...\n",
       "2092                  XGBClassifier       No Oversampling        LogScaler             6                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.091349  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.03989269733428955, 0.1277...\n",
       "974                   XGBClassifier       No Oversampling     MinMaxScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06562378406524658, 0.1389...\n",
       "1537                  XGBClassifier       No Oversampling   StandardScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.100336  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05704696178436279, 0.1268...\n",
       "1539                  XGBClassifier       No Oversampling   StandardScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.117714  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.058243942260742185, 0.163...\n",
       "950                   XGBClassifier       No Oversampling     MinMaxScaler             6                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.091349  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.055152058601379395, 0.133...\n",
       "2115                  XGBClassifier       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.048369836807250974, 0.150...\n",
       "727            ExtraTreesClassifier       No Oversampling     MinMaxScaler             7                         RFE  [trustLevel, totalScanTimeInSeconds, scansWith...       {'criterion': 'entropy', 'n_estimators': 30}                            0.141032                                          0.077892  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.007579636573791504, 0.013...\n",
       "400                   XGBClassifier       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.07470011711120605, 0.1448...\n",
       "347              AdaBoostClassifier       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.141032                                          0.063942  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.044377803802490234, 0.121...\n",
       "402                   XGBClassifier       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.0774925708770752, 0.23407...\n",
       "397                   XGBClassifier       No Oversampling       No Scaling            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.117714  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.09873580932617188, 0.2120...\n",
       "395                   XGBClassifier       No Oversampling       No Scaling            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.100336  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.050764036178588864, 0.120...\n",
       "2116                  XGBClassifier       No Oversampling        LogScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0635291337966919, 0.17652...\n",
       "379                   XGBClassifier       No Oversampling       No Scaling             6                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.091349  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.060038208961486816, 0.180...\n",
       "2114                  XGBClassifier       No Oversampling        LogScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.105075  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.0685157060623169, 0.14640...\n",
       "1545                  XGBClassifier       No Oversampling   StandardScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06063730716705322, 0.1532...\n",
       "957                   XGBClassifier       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.097381  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.041388964653015135, 0.112...\n",
       "966                   XGBClassifier       No Oversampling     MinMaxScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.100336  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06471807956695556, 0.1615...\n",
       "2110                  XGBClassifier       No Oversampling        LogScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.117714  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.05345642566680908, 0.1421...\n",
       "968                   XGBClassifier       No Oversampling     MinMaxScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.117714  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.060737204551696775, 0.131...\n",
       "2108                  XGBClassifier       No Oversampling        LogScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.100336  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05016477108001709, 0.1428...\n",
       "1544                  XGBClassifier       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.049267935752868655, 0.113...\n",
       "403                   XGBClassifier       No Oversampling       No Scaling            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.08327717781066894, 0.2025...\n",
       "1489             AdaBoostClassifier       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.141032                                          0.063942  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.050265169143676756, 0.219...\n",
       "401                   XGBClassifier       No Oversampling       No Scaling            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.105075  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05434753894805908, 0.1184...\n",
       "928              AdaBoostClassifier       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06532518863677979, 0.2764...\n",
       "930              AdaBoostClassifier       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.104727  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06163454055786133, 0.1838...\n",
       "960                   XGBClassifier       No Oversampling     MinMaxScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.138371                                          0.123936  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06442737579345703, 0.1520...\n",
       "2102                  XGBClassifier       No Oversampling        LogScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.138371                                          0.123936  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05235929489135742, 0.1379...\n",
       "1484             AdaBoostClassifier       No Oversampling   StandardScaler             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.044579267501831055, 0.208...\n",
       "389                   XGBClassifier       No Oversampling       No Scaling            11                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.138371                                          0.123936  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04667432308197021, 0.1226...\n",
       "1453     GradientBoostingClassifier       No Oversampling   StandardScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...          {'loss': 'deviance', 'n_estimators': 200}                            0.138371                                          0.136070  ([DecisionTreeRegressor(criterion='friedman_ms...  {'mean_fit_time': [0.048470139503479004, 0.122...\n",
       "1495             AdaBoostClassifier       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06721937656402588, 0.2572...\n",
       "1875           ExtraTreesClassifier       No Oversampling        LogScaler            10                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'criterion': 'gini', 'n_estimators': 45}                            0.138371                                          0.104852  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.0044878959655761715, 0.00...\n",
       "342              AdaBoostClassifier       No Oversampling       No Scaling             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03251252174377441, 0.0914...\n",
       "349              AdaBoostClassifier       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04019174575805664, 0.1086...\n",
       "351              AdaBoostClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04368095397949219, 0.1145...\n",
       "160            ExtraTreesClassifier       No Oversampling       No Scaling             9                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...       {'criterion': 'entropy', 'n_estimators': 60}                            0.138371                                          0.087785  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.008377623558044434, 0.013...\n",
       "353              AdaBoostClassifier       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04725942611694336, 0.1317...\n",
       "355              AdaBoostClassifier       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05854318141937256, 0.2138...\n",
       "926              AdaBoostClassifier       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07958698272705078, 0.2445...\n",
       "357              AdaBoostClassifier       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06382904052734376, 0.1862...\n",
       "359              AdaBoostClassifier       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.104727  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05754454135894775, 0.1469...\n",
       "1497             AdaBoostClassifier       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05844368934631348, 0.1783...\n",
       "2055             AdaBoostClassifier       No Oversampling        LogScaler             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.034106206893920896, 0.087...\n",
       "1499             AdaBoostClassifier       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06233024597167969, 0.1788...\n",
       "924              AdaBoostClassifier       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05654888153076172, 0.1912...\n",
       "1493             AdaBoostClassifier       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05535285472869873, 0.2403...\n",
       "1491             AdaBoostClassifier       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06083688735961914, 0.2303...\n",
       "1501             AdaBoostClassifier       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.104727  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05963718891143799, 0.3151...\n",
       "1531                  XGBClassifier       No Oversampling   StandardScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.138371                                          0.123936  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.040691065788269046, 0.146...\n",
       "913              AdaBoostClassifier       No Oversampling     MinMaxScaler             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.046475696563720706, 0.143...\n",
       "...                             ...                   ...              ...           ...                         ...                                                ...                                                ...                                 ...                                               ...                                                ...                                                ...\n",
       "1612                     Perceptron       No Oversampling   StandardScaler             1                 SelectKBest                                       [trustLevel]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019917726516723634, 0.00...\n",
       "1074                     Perceptron       No Oversampling     MinMaxScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0021944522857666017, 0.00...\n",
       "1613                     Perceptron       No Oversampling   StandardScaler             1                         RFE                                   [secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0016881942749023438, 0.00...\n",
       "1629                     Perceptron       No Oversampling   StandardScaler             9                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.003789830207824707, 0.003...\n",
       "1696                            SVC       No Oversampling   StandardScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.18169987201690674, 0.2131...\n",
       "1404                    BernoulliNB       No Oversampling   StandardScaler            19                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.004487824440002441, 0.005...\n",
       "1402                    BernoulliNB       No Oversampling   StandardScaler            18                         RFE  [grandTotal, lineItemVoids, scansWithoutRegist...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.004887080192565918, 0.006...\n",
       "1673  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             1                 SelectKBest                                       [trustLevel]                                 {'reg_param': 0.1}                           -0.276743                                          0.011709  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.001994824409484863, 0.002...\n",
       "1653                  MLPClassifier       No Oversampling   StandardScaler             1                 SelectKBest                                       [trustLevel]  {'activation': 'identity', 'learning_rate': 'c...                           -0.276743                                          0.011709  MLPClassifier(activation='identity', alpha=0.0...  {'mean_fit_time': [0.05205860137939453, 0.1279...\n",
       "1652                     Perceptron       No Oversampling   StandardScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019945621490478514, 0.00...\n",
       "1651                     Perceptron       No Oversampling   StandardScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002792644500732422, 0.003...\n",
       "1650                     Perceptron       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0037900209426879883, 0.00...\n",
       "1649                     Perceptron       No Oversampling   StandardScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.006781864166259766, 0.006...\n",
       "1403                    BernoulliNB       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.003490734100341797, 0.003...\n",
       "1648                     Perceptron       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004787039756774902, 0.006...\n",
       "1647                     Perceptron       No Oversampling   StandardScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001994943618774414, 0.002...\n",
       "1406                    BernoulliNB       No Oversampling   StandardScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.009474682807922363, 0.004...\n",
       "1401                    BernoulliNB       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.004787421226501465, 0.004...\n",
       "1645                     Perceptron       No Oversampling   StandardScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020009279251098633, 0.00...\n",
       "1400                    BernoulliNB       No Oversampling   StandardScaler            17                         RFE  [lineItemVoids, scansWithoutRegistration, quan...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.0056848287582397464, 0.00...\n",
       "1399                    BernoulliNB       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.004089236259460449, 0.005...\n",
       "1643                     Perceptron       No Oversampling   StandardScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0021057844161987303, 0.00...\n",
       "1398                    BernoulliNB       No Oversampling   StandardScaler            16                         RFE  [lineItemVoids, scansWithoutRegistration, quan...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.004487967491149903, 0.003...\n",
       "1641                     Perceptron       No Oversampling   StandardScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020943880081176758, 0.00...\n",
       "1639                     Perceptron       No Oversampling   StandardScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002194118499755859, 0.002...\n",
       "1405                    BernoulliNB       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.005984020233154297, 0.004...\n",
       "1637                     Perceptron       No Oversampling   StandardScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.00329129695892334, 0.0032...\n",
       "1407                    BernoulliNB       No Oversampling   StandardScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.005585122108459473, 0.004...\n",
       "1408                     GaussianNB       No Oversampling   StandardScaler             1                 SelectKBest                                       [trustLevel]                                                 {}                           -0.276743                                          0.011709       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0023935794830322265], 'st...\n",
       "1633                     Perceptron       No Oversampling   StandardScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002296781539916992, 0.002...\n",
       "1901         RandomForestClassifier       No Oversampling        LogScaler             3                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...          {'criterion': 'gini', 'n_estimators': 10}                           -0.279404                                          0.276866  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.00907599925994873, 0.0153...\n",
       "2205                     Perceptron       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.303353                                          0.049882  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020952701568603517, 0.00...\n",
       "2203                     Perceptron       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.303353                                          0.049882  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002097344398498535, 0.002...\n",
       "2201                     Perceptron       No Oversampling        LogScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.303353                                          0.055660  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002197599411010742, 0.002...\n",
       "1636                     Perceptron       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.003789472579956055, 0.004...\n",
       "1638                     Perceptron       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020944356918334963, 0.00...\n",
       "1069                     Perceptron       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0031915903091430664, 0.00...\n",
       "1067                     Perceptron       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.00418856143951416, 0.0057...\n",
       "1640                     Perceptron       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0021973133087158205, 0.00...\n",
       "1065                     Perceptron       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019999027252197267, 0.00...\n",
       "494                      Perceptron       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.003690004348754883, 0.003...\n",
       "496                      Perceptron       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002991938591003418, 0.003...\n",
       "2213                     Perceptron       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055970  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0030914783477783204, 0.00...\n",
       "2211                     Perceptron       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055970  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019889116287231446, 0.00...\n",
       "2209                     Perceptron       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055970  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004188895225524902, 0.005...\n",
       "498                      Perceptron       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0024930953979492186, 0.00...\n",
       "2207                     Perceptron       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055970  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020002126693725586, 0.00...\n",
       "500                      Perceptron       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.087123  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002792811393737793, 0.002...\n",
       "1628                     Perceptron       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058507  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004786992073059082, 0.004...\n",
       "492                      Perceptron       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.005884313583374023, 0.006...\n",
       "484                      Perceptron       No Oversampling       No Scaling             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058507  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0021907567977905275, 0.00...\n",
       "1630                     Perceptron       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0031913995742797853, 0.00...\n",
       "486                      Perceptron       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058507  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001989579200744629, 0.002...\n",
       "488                      Perceptron       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019997596740722657, 0.00...\n",
       "1626                     Perceptron       No Oversampling   StandardScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058507  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.005086207389831543, 0.005...\n",
       "1632                     Perceptron       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020969867706298827, 0.00...\n",
       "1634                     Perceptron       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.005086421966552734, 0.004...\n",
       "490                      Perceptron       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.005285835266113282, 0.005...\n",
       "1071                     Perceptron       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.087123  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002094459533691406, 0.002...\n",
       "1642                     Perceptron       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.087123  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020998716354370117, 0.00...\n",
       "1063                     Perceptron       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001992058753967285, 0.002...\n",
       "1057                     Perceptron       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058507  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020879507064819336, 0.00...\n",
       "1059                     Perceptron       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019947052001953124, 0.00...\n",
       "1061                     Perceptron       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0022903919219970704, 0.00...\n",
       "1055                     Perceptron       No Oversampling     MinMaxScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058507  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002100372314453125, 0.002...\n",
       "2197                     Perceptron       No Oversampling        LogScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.343268                                          0.067319  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0027925252914428713, 0.00...\n",
       "1073                     Perceptron       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.343268                                          0.089731  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0022937536239624025, 0.00...\n",
       "1618                     Perceptron       No Oversampling   StandardScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.343268                                          0.057551  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0029920101165771484, 0.00...\n",
       "1620                     Perceptron       No Oversampling   StandardScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.343268                                          0.057551  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002592945098876953, 0.002...\n",
       "478                      Perceptron       No Oversampling       No Scaling             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.343268                                          0.057551  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0017952203750610351, 0.00...\n",
       "502                      Perceptron       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.343268                                          0.089731  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0022950172424316406, 0.00...\n",
       "1047                     Perceptron       No Oversampling     MinMaxScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.343268                                          0.057551  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0029918432235717775, 0.00...\n",
       "476                      Perceptron       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.343268                                          0.057551  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0021960258483886717, 0.00...\n",
       "1049                     Perceptron       No Oversampling     MinMaxScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.343268                                          0.057551  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0022879123687744142, 0.00...\n",
       "1644                     Perceptron       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.343268                                          0.089731  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020972490310668945, 0.00...\n",
       "2199                     Perceptron       No Oversampling        LogScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.084035  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0024932146072387694, 0.00...\n",
       "480                      Perceptron       No Oversampling       No Scaling             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.080324  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019973039627075194, 0.00...\n",
       "1045                     Perceptron       No Oversampling     MinMaxScaler             3                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.116680  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.003789854049682617, 0.004...\n",
       "504                      Perceptron       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.091056  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.005684852600097656, 0.007...\n",
       "474                      Perceptron       No Oversampling       No Scaling             3                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.116680  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002197265625, 0.001991724...\n",
       "1646                     Perceptron       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.091056  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002299594879150391, 0.002...\n",
       "2217                     Perceptron       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.091056  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002001667022705078, 0.002...\n",
       "1616                     Perceptron       No Oversampling   StandardScaler             3                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.116680  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.005086350440979004, 0.004...\n",
       "1051                     Perceptron       No Oversampling     MinMaxScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.080324  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001894998550415039, 0.002...\n",
       "1075                     Perceptron       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.091056  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020942926406860352, 0.00...\n",
       "2215                     Perceptron       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.091056  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001989150047302246, 0.002...\n",
       "1622                     Perceptron       No Oversampling   StandardScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.080324  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020943403244018553, 0.00...\n",
       "1860           ExtraTreesClassifier       No Oversampling        LogScaler             3                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...       {'criterion': 'entropy', 'n_estimators': 20}                           -0.359234                                          0.282543  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.004186010360717774, 0.006...\n",
       "1624                     Perceptron       No Oversampling   StandardScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.369878                                          0.113116  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020979881286621095, 0.00...\n",
       "482                      Perceptron       No Oversampling       No Scaling             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.369878                                          0.113116  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020946502685546876, 0.00...\n",
       "2195                     Perceptron       No Oversampling        LogScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.369878                                          0.113116  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0032900094985961912, 0.00...\n",
       "1053                     Perceptron       No Oversampling     MinMaxScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.369878                                          0.113116  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019019126892089843, 0.00...\n",
       "1862           ExtraTreesClassifier       No Oversampling        LogScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...      {'criterion': 'entropy', 'n_estimators': 110}                           -0.372539                                          0.262374  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.007878804206848144, 0.012...\n",
       "1044                     Perceptron       No Oversampling     MinMaxScaler             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0015957355499267578, 0.00...\n",
       "473                      Perceptron       No Oversampling       No Scaling             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019971847534179686, 0.00...\n",
       "2186                     Perceptron       No Oversampling        LogScaler             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0017929315567016602, 0.00...\n",
       "1615                     Perceptron       No Oversampling   StandardScaler             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004089069366455078, 0.003...\n",
       "1048                     Perceptron       No Oversampling     MinMaxScaler             4                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.497605                                          0.379382  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0021942138671875, 0.00259...\n",
       "1619                     Perceptron       No Oversampling   StandardScaler             4                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.497605                                          0.379382  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0027927875518798826, 0.00...\n",
       "2190                     Perceptron       No Oversampling        LogScaler             4                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.497605                                          0.379382  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004088902473449707, 0.003...\n",
       "477                      Perceptron       No Oversampling       No Scaling             4                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.497605                                          0.379382  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0018944263458251954, 0.00...\n",
       "1050                     Perceptron       No Oversampling     MinMaxScaler             5                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.502927                                          0.321477  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.00219419002532959, 0.0025...\n",
       "1621                     Perceptron       No Oversampling   StandardScaler             5                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.502927                                          0.321477  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020946502685546876, 0.00...\n",
       "2192                     Perceptron       No Oversampling        LogScaler             5                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.502927                                          0.321477  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0032912254333496093, 0.00...\n",
       "479                      Perceptron       No Oversampling       No Scaling             5                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.502927                                          0.321477  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0018979310989379883, 0.00...\n",
       "2188                     Perceptron       No Oversampling        LogScaler             3                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.582757                                          0.441433  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.03510630130767822, 0.0263...\n",
       "1046                     Perceptron       No Oversampling     MinMaxScaler             3                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.582757                                          0.441433  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0024933576583862304, 0.00...\n",
       "1617                     Perceptron       No Oversampling   StandardScaler             3                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.582757                                          0.441433  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0032911300659179688, 0.00...\n",
       "475                      Perceptron       No Oversampling       No Scaling             3                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.582757                                          0.441433  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001898050308227539, 0.001...\n",
       "269                      GaussianNB       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -0.651943                                          0.307925       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0027924060821533205], 'st...\n",
       "840                      GaussianNB       No Oversampling     MinMaxScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -0.651943                                          0.307925       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091859817504883], 'std...\n",
       "1411                     GaussianNB       No Oversampling   StandardScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -0.651943                                          0.307925       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002692770957946777], 'std...\n",
       "534   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.1}                           -0.670569                                          0.286324  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002892303466796875, 0.002...\n",
       "1105  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.1}                           -0.670569                                          0.286324  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025930404663085938, 0.00...\n",
       "1676  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.1}                           -0.670569                                          0.286324  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025929927825927733, 0.00...\n",
       "1900         RandomForestClassifier       No Oversampling        LogScaler             2                         RFE            [trustLevel, scannedLineItemsPerSecond]          {'criterion': 'gini', 'n_estimators': 10}                           -0.734433                                          0.260090  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010172724723815918, 0.017...\n",
       "1677  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                 {'reg_param': 0.30000000000000004}                           -0.761043                                          0.273077  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002892303466796875, 0.002...\n",
       "535   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                 {'reg_param': 0.30000000000000004}                           -0.761043                                          0.273077  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029915809631347657, 0.00...\n",
       "1106  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                 {'reg_param': 0.30000000000000004}                           -0.761043                                          0.273077  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0026925325393676756, 0.00...\n",
       "758          RandomForestClassifier       No Oversampling     MinMaxScaler             2                         RFE            [trustLevel, scannedLineItemsPerSecond]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.763704                                          0.305585  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.009076595306396484, 0.016...\n",
       "1981                     GaussianNB       No Oversampling        LogScaler             3                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                                 {}                           -0.846195                                          0.341143       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0026930570602416992], 'st...\n",
       "2246  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             3                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                 {'reg_param': 0.1}                           -0.957956                                          0.295386  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025930404663085938, 0.00...\n",
       "1681  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.2}                           -0.987227                                          0.332005  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029918432235717775, 0.00...\n",
       "539   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.2}                           -0.987227                                          0.332005  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0026925325393676756, 0.00...\n",
       "1110  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.2}                           -0.987227                                          0.332005  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002692842483520508, 0.002...\n",
       "1107  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                 {'reg_param': 0.30000000000000004}                           -1.016498                                          0.302507  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029920101165771484, 0.00...\n",
       "1678  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                 {'reg_param': 0.30000000000000004}                           -1.016498                                          0.302507  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0027925968170166016, 0.00...\n",
       "536   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                 {'reg_param': 0.30000000000000004}                           -1.016498                                          0.302507  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00299220085144043, 0.0028...\n",
       "185          RandomForestClassifier       No Oversampling       No Scaling             1                         RFE                        [scannedLineItemsPerSecond]          {'criterion': 'gini', 'n_estimators': 10}                           -1.035125                                          0.329418  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.014860272407531738, 0.028...\n",
       "2247  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                 {'reg_param': 0.4}                           -1.048430                                          0.348315  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0026929378509521484, 0.00...\n",
       "1680  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                 {'reg_param': 0.2}                           -1.098989                                          0.291602  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028922319412231444, 0.00...\n",
       "1109  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                 {'reg_param': 0.2}                           -1.098989                                          0.291602  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0027927398681640626, 0.00...\n",
       "538   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                 {'reg_param': 0.2}                           -1.098989                                          0.291602  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029919624328613283, 0.00...\n",
       "1114  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.136243                                          0.316511  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003191494941711426, 0.003...\n",
       "544   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.136243                                          0.316511  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0032910585403442385, 0.00...\n",
       "1686  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.136243                                          0.316511  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034906148910522463, 0.00...\n",
       "1115  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.136243                                          0.316511  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0032912254333496093, 0.00...\n",
       "1685  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.136243                                          0.316511  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031913280487060546, 0.00...\n",
       "543   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.136243                                          0.316511  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031914710998535156, 0.00...\n",
       "1683  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003490734100341797, 0.002...\n",
       "540   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00359039306640625, 0.0029...\n",
       "1684  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00249326229095459, 0.0034...\n",
       "1113  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003191518783569336, 0.003...\n",
       "1682  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002991914749145508, 0.003...\n",
       "542   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002393364906311035, 0.002...\n",
       "1112  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029920339584350586, 0.00...\n",
       "1111  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031914710998535156, 0.00...\n",
       "541   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034906625747680663, 0.00...\n",
       "545   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.168175                                          0.342141  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0035904645919799805, 0.00...\n",
       "1687  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.168175                                          0.342141  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0030917882919311523, 0.00...\n",
       "1116  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.168175                                          0.342141  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031913995742797853, 0.00...\n",
       "2248  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             5                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                 {'reg_param': 0.30000000000000004}                           -1.232038                                          0.318737  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00329132080078125, 0.0029...\n",
       "1679  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.271953                                          0.393375  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025930881500244142, 0.00...\n",
       "2250  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.271953                                          0.393375  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0030918121337890625, 0.00...\n",
       "537   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.271953                                          0.393375  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0023936986923217773, 0.00...\n",
       "1108  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.271953                                          0.393375  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028922319412231444, 0.00...\n",
       "2249  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             6                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                 {'reg_param': 0.30000000000000004}                           -1.452900                                          0.432521  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002892279624938965, 0.002...\n",
       "1982                     GaussianNB       No Oversampling        LogScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                                 {}                           -1.548696                                          0.534504       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0031911134719848633], 'st...\n",
       "270                      GaussianNB       No Oversampling       No Scaling             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.628526                                          0.515212       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0025931358337402343], 'st...\n",
       "841                      GaussianNB       No Oversampling     MinMaxScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.628526                                          0.515212       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0029921531677246094], 'st...\n",
       "1412                     GaussianNB       No Oversampling   StandardScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.628526                                          0.515212       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002892279624938965], 'std...\n",
       "1413                     GaussianNB       No Oversampling   StandardScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.998403                                          0.470626       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0027925968170166016], 'st...\n",
       "271                      GaussianNB       No Oversampling       No Scaling             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.998403                                          0.470626       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016954183578491212], 'st...\n",
       "842                      GaussianNB       No Oversampling     MinMaxScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.998403                                          0.470626       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0029920101165771484], 'st...\n",
       "546   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.027674                                          0.373413  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031911611557006838, 0.00...\n",
       "1117  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.027674                                          0.373413  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031915903091430664, 0.00...\n",
       "1688  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.027674                                          0.373413  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031913995742797853, 0.00...\n",
       "1983                     GaussianNB       No Oversampling        LogScaler             5                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                                 {}                           -2.048962                                          0.480463       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0014959096908569336], 'st...\n",
       "2254  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.054284                                          0.419648  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003191566467285156, 0.003...\n",
       "1689  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.067589                                          0.410509  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003490900993347168, 0.003...\n",
       "1118  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.067589                                          0.410509  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0036903858184814454, 0.00...\n",
       "547   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.067589                                          0.410509  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0027927875518798826, 0.00...\n",
       "2255  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.107504                                          0.433860  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025929927825927733, 0.00...\n",
       "2260  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.134114                                          0.446705  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0039894342422485355, 0.00...\n",
       "2257  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.134114                                          0.446705  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003390860557556152, 0.003...\n",
       "2259  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.134114                                          0.446705  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003490638732910156, 0.003...\n",
       "2256  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.134114                                          0.446705  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003291201591491699, 0.002...\n",
       "2258  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.134114                                          0.446705  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003191566467285156, 0.003...\n",
       "2261  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.200639                                          0.433446  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003789949417114258, 0.003...\n",
       "548   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.200639                                          0.433446  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0036899328231811525, 0.00...\n",
       "1119  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.200639                                          0.433446  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003789663314819336, 0.003...\n",
       "1690  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.200639                                          0.433446  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003989481925964355, 0.003...\n",
       "1416                     GaussianNB       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.251197                                          0.499499       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002393651008605957], 'std...\n",
       "1415                     GaussianNB       No Oversampling   StandardScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.251197                                          0.499499       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0028923511505126952], 'st...\n",
       "845                      GaussianNB       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.251197                                          0.499499       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002991771697998047], 'std...\n",
       "844                      GaussianNB       No Oversampling     MinMaxScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.251197                                          0.499499       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0031917333602905274], 'st...\n",
       "274                      GaussianNB       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.251197                                          0.499499       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001896977424621582], 'std...\n",
       "273                      GaussianNB       No Oversampling       No Scaling             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.251197                                          0.499499       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0020916938781738283], 'st...\n",
       "2251  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.256519                                          0.387237  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0027925968170166016, 0.00...\n",
       "1984                     GaussianNB       No Oversampling        LogScaler             6                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                                 {}                           -2.277807                                          0.480363       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001695537567138672], 'std...\n",
       "1985                     GaussianNB       No Oversampling        LogScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.291112                                          0.481289       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0015959501266479491], 'st...\n",
       "843                      GaussianNB       No Oversampling     MinMaxScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.291112                                          0.481289       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0030920028686523436], 'st...\n",
       "272                      GaussianNB       No Oversampling       No Scaling             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.291112                                          0.481289       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018947362899780274], 'st...\n",
       "1414                     GaussianNB       No Oversampling   StandardScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.291112                                          0.481289       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002892303466796875], 'std...\n",
       "2253  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                  {'reg_param': 0.7000000000000001}                           -2.315061                                          0.366846  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002393770217895508, 0.002...\n",
       "275                      GaussianNB       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.574415       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016014814376831056], 'st...\n",
       "846                      GaussianNB       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.574415       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0029918193817138673], 'st...\n",
       "1417                     GaussianNB       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.574415       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0021942138671875], 'std_f...\n",
       "2252  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...                  {'reg_param': 0.7000000000000001}                           -2.376264                                          0.370081  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003191089630126953, 0.002...\n",
       "1120  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                  {'reg_param': 0.7000000000000001}                           -2.607770                                          0.558007  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003690171241760254, 0.003...\n",
       "1121  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.6}                           -2.607770                                          0.366881  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003590106964111328, 0.003...\n",
       "550   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.6}                           -2.607770                                          0.366881  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0030860185623168947, 0.00...\n",
       "549   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                  {'reg_param': 0.7000000000000001}                           -2.607770                                          0.558007  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0035902738571166994, 0.00...\n",
       "2262  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                  {'reg_param': 0.7000000000000001}                           -2.607770                                          0.558007  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0036902427673339844, 0.00...\n",
       "1691  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                  {'reg_param': 0.7000000000000001}                           -2.607770                                          0.558007  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0044880867004394535, 0.00...\n",
       "2263  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.6}                           -2.607770                                          0.366881  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0035904407501220702, 0.00...\n",
       "1692  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.6}                           -2.607770                                          0.366881  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003091740608215332, 0.003...\n",
       "276                      GaussianNB       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.676956                                          0.667603       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016979455947875976], 'st...\n",
       "847                      GaussianNB       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.676956                                          0.667603       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032913923263549806], 'st...\n",
       "1418                     GaussianNB       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.676956                                          0.667603       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994466781616211], 'std...\n",
       "1986                     GaussianNB       No Oversampling        LogScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.788717                                          0.432177       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0015957355499267578], 'st...\n",
       "1419                     GaussianNB       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.249069                                          0.700153       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016956090927124023], 'st...\n",
       "848                      GaussianNB       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.249069                                          0.700153       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0029920339584350586], 'st...\n",
       "277                      GaussianNB       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.249069                                          0.700153       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001795196533203125], 'std...\n",
       "1988                     GaussianNB       No Oversampling        LogScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -3.400745                                          0.469608       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0015952110290527344], 'st...\n",
       "1989                     GaussianNB       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.400745                                          0.462822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016927242279052735], 'st...\n",
       "1987                     GaussianNB       No Oversampling        LogScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...                                                 {}                           -3.467270                                          0.531802       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0014960527420043944], 'st...\n",
       "278                      GaussianNB       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.706759                                          0.728619       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016924142837524414], 'st...\n",
       "849                      GaussianNB       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.706759                                          0.728619       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0035910606384277344], 'st...\n",
       "1420                     GaussianNB       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.706759                                          0.728619       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017951250076293946], 'st...\n",
       "850                      GaussianNB       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.111229                                          0.765794       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0033912897109985352], 'st...\n",
       "279                      GaussianNB       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.111229                                          0.765794       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017014741897583008], 'st...\n",
       "1421                     GaussianNB       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.111229                                          0.765794       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001795196533203125], 'std...\n",
       "280                      GaussianNB       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.132517                                          0.752621       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018015146255493165], 'st...\n",
       "851                      GaussianNB       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.132517                                          0.752621       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.00359039306640625], 'std_...\n",
       "1422                     GaussianNB       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.132517                                          0.752621       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001894974708557129], 'std...\n",
       "852                      GaussianNB       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.177754                                          0.637230       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0035907745361328123], 'st...\n",
       "1423                     GaussianNB       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.177754                                          0.637230       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002094554901123047], 'std...\n",
       "281                      GaussianNB       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.177754                                          0.637230       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018978357315063477], 'st...\n",
       "1990                     GaussianNB       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.204364                                          0.487839       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0015927791595458985], 'st...\n",
       "853                      GaussianNB       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.683342                                          0.517235       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0038900375366210938], 'st...\n",
       "1424                     GaussianNB       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.683342                                          0.517235       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002692985534667969], 'std...\n",
       "282                      GaussianNB       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.683342                                          0.517235       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016952753067016602], 'st...\n",
       "1991                     GaussianNB       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.709952                                          0.394694       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001792144775390625], 'std...\n",
       "1992                     GaussianNB       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.749867                                          0.412888       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017979860305786133], 'st...\n",
       "1993                     GaussianNB       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.843002                                          0.431181       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001791667938232422], 'std...\n",
       "1994                     GaussianNB       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.962746                                          0.415310       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018949508666992188], 'st...\n",
       "1995                     GaussianNB       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -5.109101                                          0.429031       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017982959747314454], 'st...\n",
       "1996                     GaussianNB       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018980741500854493], 'st...\n",
       "1997                     GaussianNB       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0015957117080688476], 'st...\n",
       "1998                     GaussianNB       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018950223922729493], 'st...\n",
       "1425                     GaussianNB       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091764450073242], 'std...\n",
       "1426                     GaussianNB       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0035906076431274415], 'st...\n",
       "1427                     GaussianNB       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0033910989761352537], 'st...\n",
       "285                      GaussianNB       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017977714538574218], 'st...\n",
       "284                      GaussianNB       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018950700759887695], 'st...\n",
       "854                      GaussianNB       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0033912181854248045], 'st...\n",
       "855                      GaussianNB       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0037900447845458985], 'st...\n",
       "856                      GaussianNB       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0031916141510009766], 'st...\n",
       "283                      GaussianNB       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001997685432434082], 'std...\n",
       "\n",
       "[2284 rows x 11 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from math import log\n",
    "\n",
    "result_table = pd.DataFrame(columns=[\"Model\", \"Oversampling Strategy\", \"Data Preparation\", \"Feature Count\", \"Feature Selection Technique\", \"Features\", \"Optimal Parameters\", \"Monetary Value Per Instance - Mean\", \"Monetary Value Per Instance - Standard Deviation\", \"Raw Model\", 'Cross Validation Results'])\n",
    "\n",
    "for oversampling_strategy in range(1,2):  # Oversamling strategies currtently not in the loop\n",
    "    if (oversampling_strategy == 1):  \n",
    "        Y = train['fraud']\n",
    "        X = train.drop('fraud',axis=1)\n",
    "        oversampling = \"No Oversampling\"\n",
    "    elif (oversampling_strategy == 2):\n",
    "        extended_train = randomOverSampling(train)\n",
    "        Y = extended_train['fraud']\n",
    "        X = extended_train.drop('fraud',axis=1)\n",
    "        oversampling = \"Random Oversampling\"\n",
    "    elif (oversampling_strategy == 3):\n",
    "        extended_train = smoteOverSamling(train)\n",
    "        Y = extended_train['fraud']\n",
    "        X = extended_train.drop('fraud',axis=1)\n",
    "        oversampling = \"SMOTE\"\n",
    "    elif (oversampling_strategy == 3):\n",
    "        extended_train = adasynOverSamling(train)\n",
    "        Y = extended_train['fraud']\n",
    "        X = extended_train.drop('fraud',axis=1)\n",
    "        oversampling = \"Adaysin\"\n",
    "            \n",
    "    # four types of data preparation: No preparation, MaxMinScaler, StandardScaler, LogScaling\n",
    "    for data_preparation_strategy in range(1,5):\n",
    "        if (data_preparation_strategy == 1):  \n",
    "            X_scaled = X\n",
    "            data_preparation = \"No Scaling\"\n",
    "        elif (data_preparation_strategy == 2):\n",
    "            feature_scaler = MinMaxScaler()  \n",
    "            X_scaled = pd.DataFrame(feature_scaler.fit_transform(X.values), columns=X.columns, index=X.index) \n",
    "            data_preparation = \"MinMaxScaler\"\n",
    "        elif (data_preparation_strategy == 3):\n",
    "            feature_scaler = StandardScaler()  \n",
    "            X_scaled = pd.DataFrame(feature_scaler.fit_transform(X.values), columns=X.columns, index=X.index)\n",
    "            data_preparation = \"StandardScaler\"\n",
    "        elif (data_preparation_strategy == 4):\n",
    "            transformer = FunctionTransformer(np.log1p, validate=True)  \n",
    "            \n",
    "            # pca and tsne feature cause an error -> therefore no log scaling            \n",
    "            X_scaled = pd.DataFrame(transformer.transform(X.iloc[:, range(0,20)]), columns=X.iloc[:, range(0,20)].columns, index=X.iloc[:, range(0,20)].index)\n",
    "            data_preparation = \"LogScaler\"    \n",
    "\n",
    "\n",
    "\n",
    "        for model in model_tuning_factory:   # replace with model_tuning_factory_randomized for faster results\n",
    "\n",
    "                \n",
    "            for feature_count in range(1,len(list(X))+1):\n",
    "   \n",
    "                random.seed = 42\n",
    "                model.seed = 42\n",
    "                start_time = time.time()              \n",
    "                \n",
    "                \n",
    "                # Solution with SelectKBest\n",
    "                best_features = SelectKBest(f_classif, k=feature_count).fit(X_scaled,Y)\n",
    "                best_feature_list = X.columns[best_features.get_support()]\n",
    "                X_selected_features = X[best_feature_list]\n",
    "                \n",
    "                model.fit(X_selected_features,Y)  \n",
    "                model_name = model.best_estimator_.__class__.__name__\n",
    "                score_mean = model.cv_results_['mean_test_score'][model.best_index_]\n",
    "                score_std = model.cv_results_['std_test_score'][model.best_index_]\n",
    "                cross_validation_results = model.cv_results_\n",
    "                \n",
    "                result_table = result_table.append({\n",
    "                 \"Model\": model_name,\n",
    "                 \"Oversampling Strategy\": oversampling,   \n",
    "                 \"Data Preparation\": data_preparation,   \n",
    "                 \"Feature Count\": feature_count,\n",
    "                 \"Feature Selection Technique\": \"SelectKBest\",   \n",
    "                 \"Features\": best_feature_list.values, \n",
    "                 \"Optimal Parameters\": model.best_params_,\n",
    "                 \"Monetary Value Per Instance - Mean\":  score_mean,\n",
    "                 \"Monetary Value Per Instance - Standard Deviation\": score_std,   \n",
    "                 \"Raw Model\": model.best_estimator_,\n",
    "                 \"Cross Validation Results\": cross_validation_results\n",
    "                  }, ignore_index=True)\n",
    "                \n",
    "\n",
    "                # Solution with Recursive Feature Elimination -> only works for some models\n",
    "                \n",
    "                if (model.estimator.__class__.__name__ == 'LogisticRegression'\n",
    "                 or model.estimator.__class__.__name__ == 'DecisionTreeClassifier'\n",
    "                 or model.estimator.__class__.__name__ == 'ExtraTreeClassifier'\n",
    "                 or model.estimator.__class__.__name__ == 'ExtraTreesClassifier'\n",
    "                 or model.estimator.__class__.__name__ == 'RandomForestClassifier'\n",
    "                 or model.estimator.__class__.__name__ == 'BernoulliNB'\n",
    "                 or model.estimator.__class__.__name__ == 'AdaBoostClassifier'\n",
    "                 or model.estimator.__class__.__name__ == 'GradientBoostingClassifier'\n",
    "                 or model.estimator.__class__.__name__ == 'XGBClassifier'    \n",
    "                 or model.estimator.__class__.__name__ == 'Perceptron'\n",
    "                 or model.estimator.__class__.__name__ == 'LinearDiscriminantAnalysis'):\n",
    "                \n",
    "                   # Traditional RFE\n",
    "                    rfe = RFE(model.estimator, n_features_to_select = feature_count)\n",
    "                    rfe = rfe.fit(X,Y)\n",
    "                    best_feature_list = np.array(list(X))[np.array(rfe.support_)]\n",
    "                    X_selected_features = X[best_feature_list]\n",
    "\n",
    "                    model.fit(X_selected_features,Y)  \n",
    "                    model_name = model.best_estimator_.__class__.__name__\n",
    "                    score_mean = model.cv_results_['mean_test_score'][model.best_index_]\n",
    "                    score_std = model.cv_results_['std_test_score'][model.best_index_]\n",
    "                    cross_validation_results = model.cv_results_\n",
    "\n",
    "\n",
    "                    result_table = result_table.append({\n",
    "                     \"Model\": model_name,\n",
    "                     \"Oversampling Strategy\": oversampling,   \n",
    "                     \"Data Preparation\": data_preparation,\n",
    "                     \"Feature Count\": feature_count,\n",
    "                     \"Feature Selection Technique\": \"RFE\",\n",
    "                     \"Features\": best_feature_list,\n",
    "                     \"Optimal Parameters\": model.best_params_,\n",
    "                     \"Monetary Value Per Instance - Mean\":  score_mean,\n",
    "                     \"Monetary Value Per Instance - Standard Deviation\": score_std,  \n",
    "                     \"Raw Model\": model.best_estimator_,\n",
    "                     \"Cross Validation Results\": cross_validation_results\n",
    "                      }, ignore_index=True)\n",
    "                    \n",
    "                end_time = time.time()\n",
    "\n",
    "                print(\"Finished \" + model.best_estimator_.__class__.__name__ + \" with \" + oversampling + \" and \" + data_preparation + \" and \" + str(feature_count) + \" features after \" + str(round(end_time - start_time, 2)) + \" seconds\")\n",
    "\n",
    "                 \n",
    "                \n",
    "            if (model.estimator.__class__.__name__ == 'LogisticRegression'\n",
    "             or model.estimator.__class__.__name__ == 'DecisionTreeClassifier'\n",
    "             or model.estimator.__class__.__name__ == 'ExtraTreeClassifier'\n",
    "             or model.estimator.__class__.__name__ == 'ExtraTreesClassifier'\n",
    "             or model.estimator.__class__.__name__ == 'RandomForestClassifier'\n",
    "             or model.estimator.__class__.__name__ == 'BernoulliNB'\n",
    "             or model.estimator.__class__.__name__ == 'AdaBoostClassifier'\n",
    "             or model.estimator.__class__.__name__ == 'GradientBoostingClassifier'\n",
    "             or model.estimator.__class__.__name__ == 'XGBClassifier'    \n",
    "             or model.estimator.__class__.__name__ == 'Perceptron'\n",
    "             or model.estimator.__class__.__name__ == 'LinearDiscriminantAnalysis'):\n",
    "\n",
    "                # RFE with Cross Validation -> determines the optimum feature count automatically\n",
    "                rfecv = RFECV(model.estimator, cv = skf)\n",
    "                rfecv = rfe.fit(X,Y)\n",
    "                best_feature_list = np.array(list(X))[np.array(rfecv.support_)]\n",
    "                X_selected_features = X[best_feature_list]\n",
    "\n",
    "                model.fit(X_selected_features,Y)  \n",
    "                model_name = model.best_estimator_.__class__.__name__\n",
    "                score_mean = model.cv_results_['mean_test_score'][model.best_index_]\n",
    "                score_std = model.cv_results_['std_test_score'][model.best_index_]\n",
    "                cross_validation_results = model.cv_results_\n",
    "\n",
    "                result_table = result_table.append({\n",
    "                 \"Model\": model_name,\n",
    "                 \"Oversampling Strategy\": oversampling,   \n",
    "                 \"Data Preparation\": data_preparation,\n",
    "                 \"Feature Count\": len(best_feature_list),\n",
    "                 \"Feature Selection Technique\": \"RFECV\",\n",
    "                 \"Features\": best_feature_list,\n",
    "                 \"Optimal Parameters\": model.best_params_,\n",
    "                 \"Monetary Value Per Instance - Mean\":  score_mean,\n",
    "                 \"Monetary Value Per Instance - Standard Deviation\": score_std,    \n",
    "                 \"Raw Model\": model.best_estimator_,\n",
    "                 \"Cross Validation Results\": cross_validation_results\n",
    "                  }, ignore_index=True)\n",
    "                    \n",
    "result_table.sort_values(by = \"Monetary Value Per Instance - Mean\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore the saved result table to avoid recomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table.to_pickle(\"result_table_training_set_final_without_PCA_and_tsne.pkl\")\n",
    "# result_table = pd.read_pickle(\"result_table_training_set_final_without_PCA_and_tsne.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Oversampling Strategy</th>\n",
       "      <th>Data Preparation</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Feature Selection Technique</th>\n",
       "      <th>Features</th>\n",
       "      <th>Optimal Parameters</th>\n",
       "      <th>Monetary Value Per Instance - Mean</th>\n",
       "      <th>Monetary Value Per Instance - Standard Deviation</th>\n",
       "      <th>Raw Model</th>\n",
       "      <th>Cross Validation Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06113533973693848, 0.1522...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.043284034729003905, 0.111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03919510841369629, 0.1388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03849694728851318, 0.1111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.103015</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.048366665840148926, 0.119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03390898704528809, 0.0981...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.103015</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04318397045135498, 0.1047...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.118947</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.16805057525634765, 0.1037...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03171484470367432, 0.0952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.02942171096801758, 0.0845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.118947</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.1654575824737549, 0.10551...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.066132</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.025432181358337403, 0.097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.118947</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.27336826324462893, 0.1812...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.103015</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.043283915519714354, 0.105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.102813</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.17523112297058105, 0.1351...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.102813</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.15877537727355956, 0.1141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.102813</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.1412179708480835, 0.17553...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.059194</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07799050807952881, 0.2633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.102813</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.17313728332519532, 0.1894...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.059194</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.051860690116882324, 0.160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.102813</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.3041860580444336, 0.25930...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.059194</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05874247550964355, 0.1494...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.059194</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06023867130279541, 0.2386...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.102813</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.17962005138397216, 0.1369...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.101862</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.042286133766174315, 0.121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.101862</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03670167922973633, 0.0877...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.113088</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.26040289402008054, 0.1775...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.113088</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.1711425542831421, 0.12476...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.101862</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.09344995021820068, 0.1945...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.101862</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0335101842880249, 0.10073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.113088</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.15906999111175538, 0.1242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0981372594833374, 0.20116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.28763043880462646, 0.1764...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04986319541931152, 0.1174...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.16785142421722413, 0.1121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.7000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.7000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2939136505126953, 0.24544...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.7000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.7000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.1612694025039673, 0.13753...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.167352557182312, 0.092950...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.17223942279815674, 0.1202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.057944655418395996, 0.108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.093421</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.027127003669738768, 0.100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.7000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.7000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.17346522808074952, 0.1419...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.047671842575073245, 0.116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04817039966583252, 0.1117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04497933387756348, 0.1119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.1521930694580078, 0.10462...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.103536</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03979306221008301, 0.1205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.100227</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.15687742233276367, 0.0913...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.093421</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03161232471466065, 0.0993...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07280523777008056, 0.2100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.103137</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04188234806060791, 0.1107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09015867710113526, 0.2539...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.097959</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.021940898895263673, 0.064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07100903987884521, 0.3016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.11947979927062988, 0.3696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.13523645401000978, 0.3425...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0752985954284668, 0.19527...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.052160024642944336, 0.144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.103137</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06801788806915283, 0.1509...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.103137</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05026490688323974, 0.1440...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05774524211883545, 0.1460...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.097959</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.029920077323913573, 0.106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.097959</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03480620384216308, 0.1215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.123364</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.34388012886047364, 0.7888...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.097959</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.021231985092163085, 0.062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.8, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.117519</td>\n",
       "      <td>LogisticRegression(C=0.8, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.30189244747161864, 0.4564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.103137</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05544557571411133, 0.1582...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06372911930084228, 0.2796...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.094946</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.08188107013702392, 0.2176...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.101973</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06003890037536621, 0.1580...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.104109</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.18101446628570556, 0.2972...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.137511</td>\n",
       "      <td>LogisticRegression(C=0.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.18799772262573242, 0.3461...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.094946</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.057844924926757815, 0.134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.100877</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.19197847843170165, 0.5672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.137511</td>\n",
       "      <td>LogisticRegression(C=0.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.21093599796295165, 0.3604...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.104109</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.19537763595581054, 0.3093...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2106</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.094946</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.050564026832580565, 0.143...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.100877</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.21312987804412842, 0.5690...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2103</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.108848</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03530504703521729, 0.1235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.100877</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.19128904342651368, 0.5572...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.101973</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05624942779541016, 0.1344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.101973</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.08317737579345703, 0.1961...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.104109</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.3262270450592041, 0.57665...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.137511</td>\n",
       "      <td>LogisticRegression(C=0.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.21003875732421876, 0.5953...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.101973</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04098949432373047, 0.1219...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.100877</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2289875030517578, 1.01817...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.094946</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06721956729888916, 0.1405...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.9, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.121215</td>\n",
       "      <td>LogisticRegression(C=0.9, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.26718547344207766, 0.3141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107680</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.09983272552490234, 0.2144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107680</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03670144081115723, 0.1009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.2901237964630127, 0.61365...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107680</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.039394593238830565, 0.106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.8, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.8, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.38387322425842285, 0.7942...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.9, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.9, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.19248578548431397, 0.3709...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.8, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.8, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.2207099437713623, 0.47792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.7000000000000001, 'fit_intercept': Tru...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.7000000000000001, class...</td>\n",
       "      <td>{'mean_fit_time': [0.1734311103820801, 0.37040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.8, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.8, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.21472642421722413, 0.5442...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.8, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.124252</td>\n",
       "      <td>LogisticRegression(C=0.8, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.18709993362426758, 0.4300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.212432336807251, 0.680579...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05575072765350342, 0.1100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.261102294921875, 0.758272...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.3118659496307373, 0.96202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0429844856262207, 0.10831...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.28184669017791747, 0.7094...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2990999698638916, 0.62991...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.22769105434417725, 0.7410...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.23636844158172607, 0.6654...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.21512532234191895, 0.6758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2111</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.102908</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.046973657608032224, 0.121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.25202648639678954, 0.7661...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.20265860557556153, 0.6600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.039194679260253905, 0.123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.102981</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03390836715698242, 0.1103...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.22569668292999268, 0.6586...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.21482634544372559, 0.6812...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 1.0, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.20325686931610107, 1.0600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 140}</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.077010</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.008078384399414062, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.111028</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0560492992401123, 0.12596...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.080961</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.050066018104553224, 0.166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.059639811515808105, 0.153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04148845672607422, 0.1375...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.111028</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06582322120666503, 0.1478...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.080961</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04916832447052002, 0.1450...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0611363410949707, 0.12536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.080961</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04497950077056885, 0.1192...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.080961</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05515222549438477, 0.1266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.111028</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06043722629547119, 0.1543...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07011222839355469, 0.2252...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.080961</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05096371173858642, 0.1580...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.111028</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.051561594009399414, 0.139...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.13025109767913817, 0.3415...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06401519775390625, 0.1706...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06502611637115478, 0.3037...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07699398994445801, 0.2177...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08576996326446533, 0.3108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.1284557342529297, 0.35704...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.120078444480896, 0.370708...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.13324284553527832, 0.3540...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.062333083152770995, 0.207...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.11608922481536865, 0.3448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.093109</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0559502124786377, 0.28443...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10811076164245606, 0.3136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10541601181030273, 0.3519...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0630272388458252, 0.16964...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 150}</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.101174</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'mean_fit_time': [0.028714919090270997, 0.082...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07789082527160644, 0.2380...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0636291742324829, 0.21951...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08317697048187256, 0.2898...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06313116550445556, 0.1747...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04956729412078857, 0.2117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.061734700202941896, 0.265...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06432785987854003, 0.2578...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.046769213676452634, 0.171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.093109</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05884232521057129, 0.1451...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05445435047149658, 0.2066...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 150}</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.101174</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'mean_fit_time': [0.029418802261352538, 0.091...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05703821182250977, 0.1568...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05295827388763428, 0.1430...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.11549084186553955, 0.3311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06213345527648926, 0.2024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.13094871044158934, 0.3776...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.13174645900726317, 0.3373...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07709395885467529, 0.1967...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05804433822631836, 0.1329...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.054054546356201175, 0.284...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09893362522125244, 0.3279...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.11100223064422607, 0.3664...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092207</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04597628116607666, 0.1395...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.038193678855895995, 0.107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 150}</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.101174</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'mean_fit_time': [0.0322134256362915, 0.08786...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.075532</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03889622688293457, 0.1376...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09065699577331543, 0.2680...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.101856</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0434755802154541, 0.11658...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06472690105438232, 0.1714...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 150}</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.101174</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'mean_fit_time': [0.034308433532714844, 0.092...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05495035648345947, 0.1513...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2079</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10910756587982177, 0.3909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.093109</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.053156685829162595, 0.173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.062432384490966795, 0.195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0556509256362915, 0.24145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0632176399230957, 0.18749...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09948463439941406, 0.3176...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05624775886535645, 0.1454...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07479920387268066, 0.2064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.093109</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04817130565643311, 0.1212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.14670746326446532, 0.3679...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04936537742614746, 0.2084...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.092806</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.055550003051757814, 0.155...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.117714</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.09873580932617188, 0.2120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.100336</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06471807956695556, 0.1615...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.100336</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.050764036178588864, 0.120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0635291337966919, 0.17652...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.091349</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.060038208961486816, 0.180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0685157060623169, 0.14640...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06063730716705322, 0.1532...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.097381</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.041388964653015135, 0.112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.063942</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.050265169143676756, 0.219...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.117714</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05345642566680908, 0.1421...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.117714</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.060737204551696775, 0.131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.100336</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05016477108001709, 0.1428...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.049267935752868655, 0.113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.08327717781066894, 0.2025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05434753894805908, 0.1184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.063942</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.044377803802490234, 0.121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0774925708770752, 0.23407...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.091349</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03989269733428955, 0.1277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07470011711120605, 0.1448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04686872959136963, 0.1165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.097381</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.03211398124694824, 0.0977...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.063942</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04358339309692383, 0.1932...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 30}</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.077892</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.007579636573791504, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.097381</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.029720139503479005, 0.086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.097381</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0981374740600586, 0.18909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.091349</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.034208273887634276, 0.125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0455775260925293, 0.11289...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05026543140411377, 0.1406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04527881145477295, 0.1108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06562378406524658, 0.1389...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.100336</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05704696178436279, 0.1268...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.117714</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.058243942260742185, 0.163...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.091349</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.055152058601379395, 0.133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.094493</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.048369836807250974, 0.150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06991252899169922, 0.1460...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05535285472869873, 0.2403...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.104727</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05754454135894775, 0.1469...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05844368934631348, 0.1783...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.034106206893920896, 0.087...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06233024597167969, 0.1788...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05654888153076172, 0.1912...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.046475696563720706, 0.143...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06083688735961914, 0.2303...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.104727</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05963718891143799, 0.3151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.123936</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.040691065788269046, 0.146...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.054354405403137206, 0.230...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.040583348274230956, 0.109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07958698272705078, 0.2445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06382904052734376, 0.1862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.104727</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06163454055786133, 0.1838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05854318141937256, 0.2138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 200}</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.136070</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'mean_fit_time': [0.048470139503479004, 0.122...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04725942611694336, 0.1317...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06532518863677979, 0.2764...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.123936</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06442737579345703, 0.1520...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.044579267501831055, 0.208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.123936</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04667432308197021, 0.1226...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.123936</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05235929489135742, 0.1379...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06721937656402588, 0.2572...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 45}</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.104852</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.0044878959655761715, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03251252174377441, 0.0914...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.093306</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04019174575805664, 0.1086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.037200760841369626, 0.037...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[quantityModifications, scansWithoutRegistrati...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.0034906625747680663, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.0036905527114868162, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.22848806381225586, 0.2906...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel]</td>\n",
       "      <td>{'n_neighbors': 15, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.0026929616928100587, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, scannedLineItemsPerSecond, valueP...</td>\n",
       "      <td>{'C': 0.1, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.011269688606262207, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, valuePerSecond, scansWithoutRegis...</td>\n",
       "      <td>{'C': 0.1, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.01136949062347412, 0.0062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, valuePerSecond, quantityModificat...</td>\n",
       "      <td>{'C': 0.1, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.010172843933105469, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[valuePerSecond, quantityModificationsPerScann...</td>\n",
       "      <td>{'C': 0.1, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.006981444358825683, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[quantityModificationsPerScannedLineItem]</td>\n",
       "      <td>{'C': 0.1, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.0032910585403442385, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel]</td>\n",
       "      <td>{'C': 0.1, 'fit_intercept': True, 'solver': 'n...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>LogisticRegression(C=0.1, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.00747990608215332, 0.0051...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.0021942138671875, 0.00289...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.23995745182037354, 0.3406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.3127604007720947, 0.37249...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.2834411859512329, 0.27985...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.22420010566711426, 0.2973...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.24484460353851317, 0.2886...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.22629404067993164, 0.2666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.24534320831298828, 0.3101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.03400943279266357, 0.0960...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.22968454360961915, 0.2610...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.003191423416137695, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0032912731170654298, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.002393794059753418, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.0032911300659179688, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.18729548454284667, 0.2516...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.19137916564941407, 0.2468...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.20574965476989746, 0.2531...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>SVC(C=0.01, cache_size=200, class_weight=None,...</td>\n",
       "      <td>{'mean_fit_time': [0.22469837665557862, 0.2949...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[quantityModifications, quantityModificationsP...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "      <td>{'mean_fit_time': [0.03560478687286377, 0.0977...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-0.279404</td>\n",
       "      <td>0.276866</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.00907599925994873, 0.0153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.303353</td>\n",
       "      <td>0.049882</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020952701568603517, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.303353</td>\n",
       "      <td>0.049882</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002097344398498535, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.303353</td>\n",
       "      <td>0.055660</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002197599411010742, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.003690004348754883, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020002126693725586, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0024930953979492186, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004188895225524902, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019889116287231446, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0030914783477783204, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002991938591003418, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019999027252197267, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.00418856143951416, 0.0057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0031915903091430664, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020944356918334963, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.003789472579956055, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.316658</td>\n",
       "      <td>0.055546</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0021973133087158205, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.005086421966552734, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002100372314453125, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0022903919219970704, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019947052001953124, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001992058753967285, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.087123</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020998716354370117, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.087123</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002094459533691406, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.005285835266113282, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020879507064819336, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020969867706298827, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019997596740722657, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001989579200744629, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0031913995742797853, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0021907567977905275, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.005884313583374023, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004786992073059082, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.087123</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002792811393737793, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.329963</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.005086207389831543, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.089731</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0022950172424316406, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.089731</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020972490310668945, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0022879123687744142, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0021960258483886717, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0029918432235717775, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002592945098876953, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0017952203750610351, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0029920101165771484, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.089731</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0022937536239624025, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.343268</td>\n",
       "      <td>0.067319</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0027925252914428713, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.080324</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020943403244018553, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001989150047302246, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020942926406860352, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.080324</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001894998550415039, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002001667022705078, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.116680</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.005086350440979004, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.116680</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002197265625, 0.001991724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002299594879150391, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.080324</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019973039627075194, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.084035</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0024932146072387694, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.005684852600097656, 0.007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.356573</td>\n",
       "      <td>0.116680</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.003789854049682617, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 20}</td>\n",
       "      <td>-0.359234</td>\n",
       "      <td>0.282543</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.004186010360717774, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.369878</td>\n",
       "      <td>0.113116</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020979881286621095, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.369878</td>\n",
       "      <td>0.113116</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0032900094985961912, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.369878</td>\n",
       "      <td>0.113116</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019019126892089843, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.369878</td>\n",
       "      <td>0.113116</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020946502685546876, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 110}</td>\n",
       "      <td>-0.372539</td>\n",
       "      <td>0.262374</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.007878804206848144, 0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0015957355499267578, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019971847534179686, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0017929315567016602, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004089069366455078, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.497605</td>\n",
       "      <td>0.379382</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0021942138671875, 0.00259...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.497605</td>\n",
       "      <td>0.379382</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0027927875518798826, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.497605</td>\n",
       "      <td>0.379382</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004088902473449707, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.497605</td>\n",
       "      <td>0.379382</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0018944263458251954, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.502927</td>\n",
       "      <td>0.321477</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0032912254333496093, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.502927</td>\n",
       "      <td>0.321477</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0018979310989379883, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.502927</td>\n",
       "      <td>0.321477</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020946502685546876, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.502927</td>\n",
       "      <td>0.321477</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.00219419002532959, 0.0025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.582757</td>\n",
       "      <td>0.441433</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0032911300659179688, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.582757</td>\n",
       "      <td>0.441433</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001898050308227539, 0.001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.582757</td>\n",
       "      <td>0.441433</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.03510630130767822, 0.0263...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.582757</td>\n",
       "      <td>0.441433</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0024933576583862304, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.651943</td>\n",
       "      <td>0.307925</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0027924060821533205], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.651943</td>\n",
       "      <td>0.307925</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091859817504883], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.651943</td>\n",
       "      <td>0.307925</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002692770957946777], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.670569</td>\n",
       "      <td>0.286324</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002892303466796875, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.670569</td>\n",
       "      <td>0.286324</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025929927825927733, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.670569</td>\n",
       "      <td>0.286324</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025930404663085938, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, scannedLineItemsPerSecond]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-0.734433</td>\n",
       "      <td>0.260090</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010172724723815918, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-0.761043</td>\n",
       "      <td>0.273077</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002892303466796875, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-0.761043</td>\n",
       "      <td>0.273077</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029915809631347657, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-0.761043</td>\n",
       "      <td>0.273077</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0026925325393676756, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, scannedLineItemsPerSecond]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.763704</td>\n",
       "      <td>0.305585</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.009076595306396484, 0.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.846195</td>\n",
       "      <td>0.341143</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0026930570602416992], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.957956</td>\n",
       "      <td>0.295386</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025930404663085938, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.2}</td>\n",
       "      <td>-0.987227</td>\n",
       "      <td>0.332005</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002692842483520508, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.2}</td>\n",
       "      <td>-0.987227</td>\n",
       "      <td>0.332005</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0026925325393676756, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.2}</td>\n",
       "      <td>-0.987227</td>\n",
       "      <td>0.332005</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029918432235717775, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-1.016498</td>\n",
       "      <td>0.302507</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00299220085144043, 0.0028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-1.016498</td>\n",
       "      <td>0.302507</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029920101165771484, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-1.016498</td>\n",
       "      <td>0.302507</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0027925968170166016, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItemsPerSecond]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-1.035125</td>\n",
       "      <td>0.329418</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.014860272407531738, 0.028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'reg_param': 0.4}</td>\n",
       "      <td>-1.048430</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0026929378509521484, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.2}</td>\n",
       "      <td>-1.098989</td>\n",
       "      <td>0.291602</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028922319412231444, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.2}</td>\n",
       "      <td>-1.098989</td>\n",
       "      <td>0.291602</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0027927398681640626, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.2}</td>\n",
       "      <td>-1.098989</td>\n",
       "      <td>0.291602</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029919624328613283, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.136243</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0032910585403442385, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.136243</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034906148910522463, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.136243</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0032912254333496093, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.136243</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031913280487060546, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.136243</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031914710998535156, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.136243</td>\n",
       "      <td>0.316511</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003191494941711426, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002991914749145508, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034906625747680663, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031914710998535156, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029920339584350586, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002393364906311035, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003191518783569336, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00249326229095459, 0.0034...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00359039306640625, 0.0029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.162853</td>\n",
       "      <td>0.349383</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003490734100341797, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.168175</td>\n",
       "      <td>0.342141</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0035904645919799805, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.168175</td>\n",
       "      <td>0.342141</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0030917882919311523, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.168175</td>\n",
       "      <td>0.342141</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031913995742797853, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-1.232038</td>\n",
       "      <td>0.318737</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00329132080078125, 0.0029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.271953</td>\n",
       "      <td>0.393375</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028922319412231444, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.271953</td>\n",
       "      <td>0.393375</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0023936986923217773, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.271953</td>\n",
       "      <td>0.393375</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0030918121337890625, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.271953</td>\n",
       "      <td>0.393375</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025930881500244142, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-1.452900</td>\n",
       "      <td>0.432521</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002892279624938965, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.548696</td>\n",
       "      <td>0.534504</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0031911134719848633], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.628526</td>\n",
       "      <td>0.515212</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0025931358337402343], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.628526</td>\n",
       "      <td>0.515212</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0029921531677246094], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.628526</td>\n",
       "      <td>0.515212</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002892279624938965], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.998403</td>\n",
       "      <td>0.470626</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0027925968170166016], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.998403</td>\n",
       "      <td>0.470626</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016954183578491212], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.998403</td>\n",
       "      <td>0.470626</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0029920101165771484], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.027674</td>\n",
       "      <td>0.373413</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031915903091430664, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.027674</td>\n",
       "      <td>0.373413</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031913995742797853, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.027674</td>\n",
       "      <td>0.373413</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031911611557006838, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.048962</td>\n",
       "      <td>0.480463</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0014959096908569336], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.054284</td>\n",
       "      <td>0.419648</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003191566467285156, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.067589</td>\n",
       "      <td>0.410509</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003490900993347168, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.067589</td>\n",
       "      <td>0.410509</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0036903858184814454, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.067589</td>\n",
       "      <td>0.410509</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0027927875518798826, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.107504</td>\n",
       "      <td>0.433860</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025929927825927733, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.134114</td>\n",
       "      <td>0.446705</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003291201591491699, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.134114</td>\n",
       "      <td>0.446705</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003191566467285156, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.134114</td>\n",
       "      <td>0.446705</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003490638732910156, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.134114</td>\n",
       "      <td>0.446705</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003390860557556152, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.134114</td>\n",
       "      <td>0.446705</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0039894342422485355, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.200639</td>\n",
       "      <td>0.433446</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003789949417114258, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.200639</td>\n",
       "      <td>0.433446</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0036899328231811525, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.200639</td>\n",
       "      <td>0.433446</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003789663314819336, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.200639</td>\n",
       "      <td>0.433446</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003989481925964355, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.251197</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0031917333602905274], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.251197</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001896977424621582], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.251197</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0020916938781738283], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.251197</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002991771697998047], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.251197</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0028923511505126952], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.251197</td>\n",
       "      <td>0.499499</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002393651008605957], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.256519</td>\n",
       "      <td>0.387237</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0027925968170166016, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.277807</td>\n",
       "      <td>0.480363</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001695537567138672], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.291112</td>\n",
       "      <td>0.481289</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0015959501266479491], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.291112</td>\n",
       "      <td>0.481289</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0030920028686523436], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.291112</td>\n",
       "      <td>0.481289</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018947362899780274], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.291112</td>\n",
       "      <td>0.481289</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002892303466796875], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.315061</td>\n",
       "      <td>0.366846</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002393770217895508, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.574415</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0021942138671875], 'std_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.574415</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0029918193817138673], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.574415</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016014814376831056], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.376264</td>\n",
       "      <td>0.370081</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003191089630126953, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003690171241760254, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.366881</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003590106964111328, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.366881</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0030860185623168947, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0035902738571166994, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0036902427673339844, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0044880867004394535, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.366881</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0035904407501220702, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-2.607770</td>\n",
       "      <td>0.366881</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003091740608215332, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.676956</td>\n",
       "      <td>0.667603</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994466781616211], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.676956</td>\n",
       "      <td>0.667603</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032913923263549806], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.676956</td>\n",
       "      <td>0.667603</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016979455947875976], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.788717</td>\n",
       "      <td>0.432177</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0015957355499267578], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.249069</td>\n",
       "      <td>0.700153</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016956090927124023], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.249069</td>\n",
       "      <td>0.700153</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0029920339584350586], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.249069</td>\n",
       "      <td>0.700153</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001795196533203125], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.400745</td>\n",
       "      <td>0.469608</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0015952110290527344], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.400745</td>\n",
       "      <td>0.462822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016927242279052735], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.467270</td>\n",
       "      <td>0.531802</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0014960527420043944], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.706759</td>\n",
       "      <td>0.728619</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0035910606384277344], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.706759</td>\n",
       "      <td>0.728619</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017951250076293946], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.706759</td>\n",
       "      <td>0.728619</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016924142837524414], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.111229</td>\n",
       "      <td>0.765794</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0033912897109985352], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.111229</td>\n",
       "      <td>0.765794</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017014741897583008], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.111229</td>\n",
       "      <td>0.765794</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001795196533203125], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.132517</td>\n",
       "      <td>0.752621</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018015146255493165], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.132517</td>\n",
       "      <td>0.752621</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.00359039306640625], 'std_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.132517</td>\n",
       "      <td>0.752621</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001894974708557129], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.177754</td>\n",
       "      <td>0.637230</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0035907745361328123], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.177754</td>\n",
       "      <td>0.637230</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002094554901123047], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.177754</td>\n",
       "      <td>0.637230</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018978357315063477], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.204364</td>\n",
       "      <td>0.487839</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0015927791595458985], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.683342</td>\n",
       "      <td>0.517235</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0038900375366210938], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.683342</td>\n",
       "      <td>0.517235</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016952753067016602], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.683342</td>\n",
       "      <td>0.517235</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002692985534667969], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.709952</td>\n",
       "      <td>0.394694</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001792144775390625], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.749867</td>\n",
       "      <td>0.412888</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017979860305786133], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.843002</td>\n",
       "      <td>0.431181</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001791667938232422], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-4.962746</td>\n",
       "      <td>0.415310</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018949508666992188], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.109101</td>\n",
       "      <td>0.429031</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017982959747314454], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017977714538574218], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0031916141510009766], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0037900447845458985], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0033912181854248045], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018950700759887695], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018950223922729493], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0033910989761352537], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0035906076431274415], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091764450073242], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0015957117080688476], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018980741500854493], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-5.162320</td>\n",
       "      <td>0.471822</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001997685432434082], 'std...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2284 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Model Oversampling Strategy Data Preparation Feature Count Feature Selection Technique                                           Features                                 Optimal Parameters  Monetary Value Per Instance - Mean  Monetary Value Per Instance - Standard Deviation                                          Raw Model                           Cross Validation Results\n",
       "1482             AdaBoostClassifier       No Oversampling   StandardScaler             7                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.077832  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06113533973693848, 0.1522...\n",
       "2053             AdaBoostClassifier       No Oversampling        LogScaler             7                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.077832  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.043284034729003905, 0.111...\n",
       "911              AdaBoostClassifier       No Oversampling     MinMaxScaler             7                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.077832  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03919510841369629, 0.1388...\n",
       "340              AdaBoostClassifier       No Oversampling       No Scaling             7                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.077832  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03849694728851318, 0.1111...\n",
       "392                   XGBClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.167642                                          0.103015  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.048366665840148926, 0.119...\n",
       "338              AdaBoostClassifier       No Oversampling       No Scaling             6                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.167642                                          0.066132  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03390898704528809, 0.0981...\n",
       "963                   XGBClassifier       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.167642                                          0.103015  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04318397045135498, 0.1047...\n",
       "1158             LogisticRegression       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.167642                                          0.118947  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.16805057525634765, 0.1037...\n",
       "1480             AdaBoostClassifier       No Oversampling   StandardScaler             6                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.167642                                          0.066132  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03171484470367432, 0.0952...\n",
       "2051             AdaBoostClassifier       No Oversampling        LogScaler             6                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.167642                                          0.066132  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.02942171096801758, 0.0845...\n",
       "587              LogisticRegression       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.167642                                          0.118947  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.1654575824737549, 0.10551...\n",
       "909              AdaBoostClassifier       No Oversampling     MinMaxScaler             6                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.167642                                          0.066132  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.025432181358337403, 0.097...\n",
       "16               LogisticRegression       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.167642                                          0.118947  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.27336826324462893, 0.1812...\n",
       "1534                  XGBClassifier       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.167642                                          0.103015  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.043283915519714354, 0.105...\n",
       "591              LogisticRegression       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.164981                                          0.102813  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.17523112297058105, 0.1351...\n",
       "1162             LogisticRegression       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.164981                                          0.102813  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.15877537727355956, 0.1141...\n",
       "1170             LogisticRegression       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.164981                                          0.102813  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.1412179708480835, 0.17553...\n",
       "1494             AdaBoostClassifier       No Oversampling   StandardScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.059194  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07799050807952881, 0.2633...\n",
       "599              LogisticRegression       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.164981                                          0.102813  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.17313728332519532, 0.1894...\n",
       "352              AdaBoostClassifier       No Oversampling       No Scaling            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.059194  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.051860690116882324, 0.160...\n",
       "28               LogisticRegression       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.164981                                          0.102813  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.3041860580444336, 0.25930...\n",
       "2065             AdaBoostClassifier       No Oversampling        LogScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.059194  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05874247550964355, 0.1494...\n",
       "923              AdaBoostClassifier       No Oversampling     MinMaxScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.059194  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06023867130279541, 0.2386...\n",
       "20               LogisticRegression       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.164981                                          0.102813  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.17962005138397216, 0.1369...\n",
       "2107                  XGBClassifier       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.101862  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.042286133766174315, 0.121...\n",
       "959                   XGBClassifier       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.101862  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.03670167922973633, 0.0877...\n",
       "24               LogisticRegression       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.162320                                          0.113088  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.26040289402008054, 0.1775...\n",
       "595              LogisticRegression       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.162320                                          0.113088  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.1711425542831421, 0.12476...\n",
       "388                   XGBClassifier       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.101862  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.09344995021820068, 0.1945...\n",
       "1530                  XGBClassifier       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.101862  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0335101842880249, 0.10073...\n",
       "1166             LogisticRegression       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.162320                                          0.113088  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.15906999111175538, 0.1242...\n",
       "398                   XGBClassifier       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.102299  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0981372594833374, 0.20116...\n",
       "18               LogisticRegression       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.28763043880462646, 0.1764...\n",
       "396                   XGBClassifier       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.102299  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04986319541931152, 0.1174...\n",
       "589              LogisticRegression       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.16785142421722413, 0.1121...\n",
       "26               LogisticRegression       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.7000000000000002, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.7000000000000002, class...  {'mean_fit_time': [0.2939136505126953, 0.24544...\n",
       "1168             LogisticRegression       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.7000000000000002, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.7000000000000002, class...  {'mean_fit_time': [0.1612694025039673, 0.13753...\n",
       "593              LogisticRegression       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.167352557182312, 0.092950...\n",
       "22               LogisticRegression       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.17223942279815674, 0.1202...\n",
       "1538                  XGBClassifier       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.102299  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.057944655418395996, 0.108...\n",
       "2095                  XGBClassifier       No Oversampling        LogScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.093421  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.027127003669738768, 0.100...\n",
       "597              LogisticRegression       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.7000000000000002, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.7000000000000002, class...  {'mean_fit_time': [0.17346522808074952, 0.1419...\n",
       "1540                  XGBClassifier       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.102299  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.047671842575073245, 0.116...\n",
       "967                   XGBClassifier       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.102299  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04817039966583252, 0.1117...\n",
       "969                   XGBClassifier       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.102299  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04497933387756348, 0.1119...\n",
       "1160             LogisticRegression       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.1521930694580078, 0.10462...\n",
       "2109                  XGBClassifier       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.159659                                          0.103536  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.03979306221008301, 0.1205...\n",
       "1164             LogisticRegression       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.100227  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.15687742233276367, 0.0913...\n",
       "2097                  XGBClassifier       No Oversampling        LogScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.093421  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.03161232471466065, 0.0993...\n",
       "932              AdaBoostClassifier       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07280523777008056, 0.2100...\n",
       "387                   XGBClassifier       No Oversampling       No Scaling            10                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.156998                                          0.103137  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04188234806060791, 0.1107...\n",
       "361              AdaBoostClassifier       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09015867710113526, 0.2539...\n",
       "336              AdaBoostClassifier       No Oversampling       No Scaling             5                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.097959  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.021940898895263673, 0.064...\n",
       "2068             AdaBoostClassifier       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07100903987884521, 0.3016...\n",
       "2070             AdaBoostClassifier       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.11947979927062988, 0.3696...\n",
       "2072             AdaBoostClassifier       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.13523645401000978, 0.3425...\n",
       "2074             AdaBoostClassifier       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0752985954284668, 0.19527...\n",
       "2064             AdaBoostClassifier       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.052160024642944336, 0.144...\n",
       "1529                  XGBClassifier       No Oversampling   StandardScaler            10                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.156998                                          0.103137  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06801788806915283, 0.1509...\n",
       "2100                  XGBClassifier       No Oversampling        LogScaler            10                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.156998                                          0.103137  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05026490688323974, 0.1440...\n",
       "2066             AdaBoostClassifier       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05774524211883545, 0.1460...\n",
       "907              AdaBoostClassifier       No Oversampling     MinMaxScaler             5                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.097959  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.029920077323913573, 0.106...\n",
       "1478             AdaBoostClassifier       No Oversampling   StandardScaler             5                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.097959  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03480620384216308, 0.1215...\n",
       "1743             LogisticRegression       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.0, 'fit_intercept': True, 'solver': 'l...                            0.156998                                          0.123364  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.34388012886047364, 0.7888...\n",
       "2049             AdaBoostClassifier       No Oversampling        LogScaler             5                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.097959  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.021231985092163085, 0.062...\n",
       "1735             LogisticRegression       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.8, 'fit_intercept': True, 'solver': 'l...                            0.156998                                          0.117519  LogisticRegression(C=0.8, class_weight=None, d...  {'mean_fit_time': [0.30189244747161864, 0.4564...\n",
       "958                   XGBClassifier       No Oversampling     MinMaxScaler            10                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.156998                                          0.103137  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05544557571411133, 0.1582...\n",
       "1503             AdaBoostClassifier       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.070483  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06372911930084228, 0.2796...\n",
       "393                   XGBClassifier       No Oversampling       No Scaling            13                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.094946  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.08188107013702392, 0.2176...\n",
       "952                   XGBClassifier       No Oversampling     MinMaxScaler             7                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.101973  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06003890037536621, 0.1580...\n",
       "1172             LogisticRegression       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.104109  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.18101446628570556, 0.2972...\n",
       "1174             LogisticRegression       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.6, 'fit_intercept': True, 'solver': 'l...                            0.154337                                          0.137511  LogisticRegression(C=0.6, class_weight=None, d...  {'mean_fit_time': [0.18799772262573242, 0.3461...\n",
       "1535                  XGBClassifier       No Oversampling   StandardScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.094946  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.057844924926757815, 0.134...\n",
       "1178             LogisticRegression       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.100877  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.19197847843170165, 0.5672...\n",
       "603              LogisticRegression       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.6, 'fit_intercept': True, 'solver': 'l...                            0.154337                                          0.137511  LogisticRegression(C=0.6, class_weight=None, d...  {'mean_fit_time': [0.21093599796295165, 0.3604...\n",
       "601              LogisticRegression       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.104109  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.19537763595581054, 0.3093...\n",
       "2106                  XGBClassifier       No Oversampling        LogScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.094946  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.050564026832580565, 0.143...\n",
       "607              LogisticRegression       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.100877  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.21312987804412842, 0.5690...\n",
       "2103                  XGBClassifier       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.154337                                          0.108848  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.03530504703521729, 0.1235...\n",
       "1749             LogisticRegression       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.100877  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.19128904342651368, 0.5572...\n",
       "1523                  XGBClassifier       No Oversampling   StandardScaler             7                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.101973  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05624942779541016, 0.1344...\n",
       "381                   XGBClassifier       No Oversampling       No Scaling             7                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.101973  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.08317737579345703, 0.1961...\n",
       "30               LogisticRegression       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.104109  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.3262270450592041, 0.57665...\n",
       "32               LogisticRegression       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.6, 'fit_intercept': True, 'solver': 'l...                            0.154337                                          0.137511  LogisticRegression(C=0.6, class_weight=None, d...  {'mean_fit_time': [0.21003875732421876, 0.5953...\n",
       "2094                  XGBClassifier       No Oversampling        LogScaler             7                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.101973  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04098949432373047, 0.1219...\n",
       "36               LogisticRegression       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.100877  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.2289875030517578, 1.01817...\n",
       "964                   XGBClassifier       No Oversampling     MinMaxScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.154337                                          0.094946  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06721956729888916, 0.1405...\n",
       "1733             LogisticRegression       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.9, 'fit_intercept': True, 'solver': 'n...                            0.151676                                          0.121215  LogisticRegression(C=0.9, class_weight=None, d...  {'mean_fit_time': [0.26718547344207766, 0.3141...\n",
       "390                   XGBClassifier       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.151676                                          0.107680  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.09983272552490234, 0.2144...\n",
       "961                   XGBClassifier       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.151676                                          0.107680  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.03670144081115723, 0.1009...\n",
       "1739             LogisticRegression       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.6, 'fit_intercept': True, 'solver': 'l...                            0.151676                                          0.124252  LogisticRegression(C=0.6, class_weight=None, d...  {'mean_fit_time': [0.2901237964630127, 0.61365...\n",
       "1532                  XGBClassifier       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.151676                                          0.107680  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.039394593238830565, 0.106...\n",
       "34               LogisticRegression       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.8, 'fit_intercept': True, 'solver': 'n...                            0.151676                                          0.124252  LogisticRegression(C=0.8, class_weight=None, d...  {'mean_fit_time': [0.38387322425842285, 0.7942...\n",
       "1745             LogisticRegression       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.9, 'fit_intercept': True, 'solver': 'n...                            0.151676                                          0.124252  LogisticRegression(C=0.9, class_weight=None, d...  {'mean_fit_time': [0.19248578548431397, 0.3709...\n",
       "1176             LogisticRegression       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.8, 'fit_intercept': True, 'solver': 'n...                            0.151676                                          0.124252  LogisticRegression(C=0.8, class_weight=None, d...  {'mean_fit_time': [0.2207099437713623, 0.47792...\n",
       "1741             LogisticRegression       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.7000000000000001, 'fit_intercept': Tru...                            0.151676                                          0.124252  LogisticRegression(C=0.7000000000000001, class...  {'mean_fit_time': [0.1734311103820801, 0.37040...\n",
       "605              LogisticRegression       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.8, 'fit_intercept': True, 'solver': 'n...                            0.151676                                          0.124252  LogisticRegression(C=0.8, class_weight=None, d...  {'mean_fit_time': [0.21472642421722413, 0.5442...\n",
       "1747             LogisticRegression       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 0.8, 'fit_intercept': True, 'solver': 'n...                            0.151676                                          0.124252  LogisticRegression(C=0.8, class_weight=None, d...  {'mean_fit_time': [0.18709993362426758, 0.4300...\n",
       "609              LogisticRegression       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.212432336807251, 0.680579...\n",
       "1536                  XGBClassifier       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.149015                                          0.098901  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05575072765350342, 0.1100...\n",
       "611              LogisticRegression       No Oversampling     MinMaxScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.261102294921875, 0.758272...\n",
       "1753             LogisticRegression       No Oversampling        LogScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.3118659496307373, 0.96202...\n",
       "394                   XGBClassifier       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.149015                                          0.098901  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.0429844856262207, 0.10831...\n",
       "610              LogisticRegression       No Oversampling     MinMaxScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.28184669017791747, 0.7094...\n",
       "1737             LogisticRegression       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.149015                                          0.114885  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.2990999698638916, 0.62991...\n",
       "38               LogisticRegression       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.22769105434417725, 0.7410...\n",
       "39               LogisticRegression       No Oversampling       No Scaling            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.23636844158172607, 0.6654...\n",
       "1181             LogisticRegression       No Oversampling   StandardScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.21512532234191895, 0.6758...\n",
       "2111                  XGBClassifier       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.149015                                          0.102908  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.046973657608032224, 0.121...\n",
       "1752             LogisticRegression       No Oversampling        LogScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.25202648639678954, 0.7661...\n",
       "1751             LogisticRegression       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.20265860557556153, 0.6600...\n",
       "965                   XGBClassifier       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.149015                                          0.098901  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.039194679260253905, 0.123...\n",
       "2105                  XGBClassifier       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.149015                                          0.102981  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.03390836715698242, 0.1103...\n",
       "1180             LogisticRegression       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.22569668292999268, 0.6586...\n",
       "1182             LogisticRegression       No Oversampling   StandardScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.21482634544372559, 0.6812...\n",
       "40               LogisticRegression       No Oversampling       No Scaling            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'C': 1.0, 'fit_intercept': True, 'solver': 'n...                            0.149015                                          0.114885  LogisticRegression(C=1.0, class_weight=None, d...  {'mean_fit_time': [0.20325686931610107, 1.0600...\n",
       "158            ExtraTreesClassifier       No Oversampling       No Scaling             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...      {'criterion': 'entropy', 'n_estimators': 140}                            0.146354                                          0.077010  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.008078384399414062, 0.013...\n",
       "1533                  XGBClassifier       No Oversampling   StandardScaler            12                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.111028  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0560492992401123, 0.12596...\n",
       "916              AdaBoostClassifier       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.146354                                          0.080961  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.050066018104553224, 0.166...\n",
       "2096                  XGBClassifier       No Oversampling        LogScaler             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.089183  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.059639811515808105, 0.153...\n",
       "1525                  XGBClassifier       No Oversampling   StandardScaler             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.089183  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04148845672607422, 0.1375...\n",
       "391                   XGBClassifier       No Oversampling       No Scaling            12                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.111028  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06582322120666503, 0.1478...\n",
       "345              AdaBoostClassifier       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.146354                                          0.080961  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04916832447052002, 0.1450...\n",
       "954                   XGBClassifier       No Oversampling     MinMaxScaler             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.089183  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0611363410949707, 0.12536...\n",
       "2060             AdaBoostClassifier       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.146354                                          0.080961  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04497950077056885, 0.1192...\n",
       "2062             AdaBoostClassifier       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.146354                                          0.080961  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05515222549438477, 0.1266...\n",
       "962                   XGBClassifier       No Oversampling     MinMaxScaler            12                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.111028  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06043722629547119, 0.1543...\n",
       "383                   XGBClassifier       No Oversampling       No Scaling             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.089183  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07011222839355469, 0.2252...\n",
       "1487             AdaBoostClassifier       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.146354                                          0.080961  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05096371173858642, 0.1580...\n",
       "2104                  XGBClassifier       No Oversampling        LogScaler            12                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.111028  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.051561594009399414, 0.139...\n",
       "1506             AdaBoostClassifier       No Oversampling   StandardScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.13025109767913817, 0.3415...\n",
       "936              AdaBoostClassifier       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06401519775390625, 0.1706...\n",
       "1500             AdaBoostClassifier       No Oversampling   StandardScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06502611637115478, 0.3037...\n",
       "935              AdaBoostClassifier       No Oversampling     MinMaxScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07699398994445801, 0.2177...\n",
       "1502             AdaBoostClassifier       No Oversampling   StandardScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08576996326446533, 0.3108...\n",
       "1505             AdaBoostClassifier       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.1284557342529297, 0.35704...\n",
       "1508             AdaBoostClassifier       No Oversampling   StandardScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.120078444480896, 0.370708...\n",
       "1507             AdaBoostClassifier       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.13324284553527832, 0.3540...\n",
       "1498             AdaBoostClassifier       No Oversampling   StandardScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.062333083152770995, 0.207...\n",
       "1509             AdaBoostClassifier       No Oversampling   StandardScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.11608922481536865, 0.3448...\n",
       "1541                  XGBClassifier       No Oversampling   StandardScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.093109  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0559502124786377, 0.28443...\n",
       "1504             AdaBoostClassifier       No Oversampling   StandardScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10811076164245606, 0.3136...\n",
       "2073             AdaBoostClassifier       No Oversampling        LogScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10541601181030273, 0.3519...\n",
       "937              AdaBoostClassifier       No Oversampling     MinMaxScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0630272388458252, 0.16964...\n",
       "2019     GradientBoostingClassifier       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 150}                            0.143693                                          0.101174  ([DecisionTreeRegressor(criterion='friedman_ms...  {'mean_fit_time': [0.028714919090270997, 0.082...\n",
       "938              AdaBoostClassifier       No Oversampling     MinMaxScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07789082527160644, 0.2380...\n",
       "1496             AdaBoostClassifier       No Oversampling   StandardScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0636291742324829, 0.21951...\n",
       "934              AdaBoostClassifier       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08317697048187256, 0.2898...\n",
       "931              AdaBoostClassifier       No Oversampling     MinMaxScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06313116550445556, 0.1747...\n",
       "919              AdaBoostClassifier       No Oversampling     MinMaxScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04956729412078857, 0.2117...\n",
       "927              AdaBoostClassifier       No Oversampling     MinMaxScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.061734700202941896, 0.265...\n",
       "929              AdaBoostClassifier       No Oversampling     MinMaxScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06432785987854003, 0.2578...\n",
       "925              AdaBoostClassifier       No Oversampling     MinMaxScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.046769213676452634, 0.171...\n",
       "970                   XGBClassifier       No Oversampling     MinMaxScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.093109  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05884232521057129, 0.1451...\n",
       "921              AdaBoostClassifier       No Oversampling     MinMaxScaler            12                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05445435047149658, 0.2066...\n",
       "875      GradientBoostingClassifier       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 150}                            0.143693                                          0.101174  ([DecisionTreeRegressor(criterion='friedman_ms...  {'mean_fit_time': [0.029418802261352538, 0.091...\n",
       "933              AdaBoostClassifier       No Oversampling     MinMaxScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05703821182250977, 0.1568...\n",
       "350              AdaBoostClassifier       No Oversampling       No Scaling            12                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05295827388763428, 0.1430...\n",
       "2077             AdaBoostClassifier       No Oversampling        LogScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.11549084186553955, 0.3311...\n",
       "1490             AdaBoostClassifier       No Oversampling   StandardScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06213345527648926, 0.2024...\n",
       "2076             AdaBoostClassifier       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.13094871044158934, 0.3776...\n",
       "2075             AdaBoostClassifier       No Oversampling        LogScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.13174645900726317, 0.3373...\n",
       "367              AdaBoostClassifier       No Oversampling       No Scaling            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07709395885467529, 0.1967...\n",
       "2063             AdaBoostClassifier       No Oversampling        LogScaler            12                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05804433822631836, 0.1329...\n",
       "1492             AdaBoostClassifier       No Oversampling   StandardScaler            12                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.054054546356201175, 0.284...\n",
       "2069             AdaBoostClassifier       No Oversampling        LogScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09893362522125244, 0.3279...\n",
       "2071             AdaBoostClassifier       No Oversampling        LogScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.11100223064422607, 0.3664...\n",
       "2113                  XGBClassifier       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.092207  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04597628116607666, 0.1395...\n",
       "348              AdaBoostClassifier       No Oversampling       No Scaling            11                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.038193678855895995, 0.107...\n",
       "304      GradientBoostingClassifier       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 150}                            0.143693                                          0.101174  ([DecisionTreeRegressor(criterion='friedman_ms...  {'mean_fit_time': [0.0322134256362915, 0.08786...\n",
       "2061             AdaBoostClassifier       No Oversampling        LogScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.075532  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03889622688293457, 0.1376...\n",
       "363              AdaBoostClassifier       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09065699577331543, 0.2680...\n",
       "2058             AdaBoostClassifier       No Oversampling        LogScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.143693                                          0.101856  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0434755802154541, 0.11658...\n",
       "364              AdaBoostClassifier       No Oversampling       No Scaling            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06472690105438232, 0.1714...\n",
       "1446     GradientBoostingClassifier       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 150}                            0.143693                                          0.101174  ([DecisionTreeRegressor(criterion='friedman_ms...  {'mean_fit_time': [0.034308433532714844, 0.092...\n",
       "354              AdaBoostClassifier       No Oversampling       No Scaling            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05495035648345947, 0.1513...\n",
       "2079             AdaBoostClassifier       No Oversampling        LogScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10910756587982177, 0.3909...\n",
       "2112                  XGBClassifier       No Oversampling        LogScaler            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.093109  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.053156685829162595, 0.173...\n",
       "356              AdaBoostClassifier       No Oversampling       No Scaling            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.062432384490966795, 0.195...\n",
       "358              AdaBoostClassifier       No Oversampling       No Scaling            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0556509256362915, 0.24145...\n",
       "366              AdaBoostClassifier       No Oversampling       No Scaling            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0632176399230957, 0.18749...\n",
       "2080             AdaBoostClassifier       No Oversampling        LogScaler            20                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09948463439941406, 0.3176...\n",
       "360              AdaBoostClassifier       No Oversampling       No Scaling            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05624775886535645, 0.1454...\n",
       "365              AdaBoostClassifier       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07479920387268066, 0.2064...\n",
       "399                   XGBClassifier       No Oversampling       No Scaling            16                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.093109  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04817130565643311, 0.1212...\n",
       "2078             AdaBoostClassifier       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.14670746326446532, 0.3679...\n",
       "2067             AdaBoostClassifier       No Oversampling        LogScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04936537742614746, 0.2084...\n",
       "362              AdaBoostClassifier       No Oversampling       No Scaling            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.143693                                          0.092806  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.055550003051757814, 0.155...\n",
       "397                   XGBClassifier       No Oversampling       No Scaling            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.117714  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.09873580932617188, 0.2120...\n",
       "966                   XGBClassifier       No Oversampling     MinMaxScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.100336  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06471807956695556, 0.1615...\n",
       "395                   XGBClassifier       No Oversampling       No Scaling            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.100336  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.050764036178588864, 0.120...\n",
       "2116                  XGBClassifier       No Oversampling        LogScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0635291337966919, 0.17652...\n",
       "379                   XGBClassifier       No Oversampling       No Scaling             6                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.091349  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.060038208961486816, 0.180...\n",
       "2114                  XGBClassifier       No Oversampling        LogScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.105075  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.0685157060623169, 0.14640...\n",
       "1545                  XGBClassifier       No Oversampling   StandardScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06063730716705322, 0.1532...\n",
       "957                   XGBClassifier       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.097381  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.041388964653015135, 0.112...\n",
       "1489             AdaBoostClassifier       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.141032                                          0.063942  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.050265169143676756, 0.219...\n",
       "2110                  XGBClassifier       No Oversampling        LogScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.117714  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.05345642566680908, 0.1421...\n",
       "968                   XGBClassifier       No Oversampling     MinMaxScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.117714  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.060737204551696775, 0.131...\n",
       "2108                  XGBClassifier       No Oversampling        LogScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.100336  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05016477108001709, 0.1428...\n",
       "1544                  XGBClassifier       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.049267935752868655, 0.113...\n",
       "403                   XGBClassifier       No Oversampling       No Scaling            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.08327717781066894, 0.2025...\n",
       "401                   XGBClassifier       No Oversampling       No Scaling            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.105075  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05434753894805908, 0.1184...\n",
       "347              AdaBoostClassifier       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.141032                                          0.063942  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.044377803802490234, 0.121...\n",
       "402                   XGBClassifier       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.0774925708770752, 0.23407...\n",
       "2092                  XGBClassifier       No Oversampling        LogScaler             6                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.091349  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.03989269733428955, 0.1277...\n",
       "400                   XGBClassifier       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.07470011711120605, 0.1448...\n",
       "973                   XGBClassifier       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.04686872959136963, 0.1165...\n",
       "2101                  XGBClassifier       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.097381  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.03211398124694824, 0.0977...\n",
       "918              AdaBoostClassifier       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.141032                                          0.063942  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04358339309692383, 0.1932...\n",
       "727            ExtraTreesClassifier       No Oversampling     MinMaxScaler             7                         RFE  [trustLevel, totalScanTimeInSeconds, scansWith...       {'criterion': 'entropy', 'n_estimators': 30}                            0.141032                                          0.077892  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.007579636573791504, 0.013...\n",
       "1528                  XGBClassifier       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.097381  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.029720139503479005, 0.086...\n",
       "386                   XGBClassifier       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.097381  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0981374740600586, 0.18909...\n",
       "1521                  XGBClassifier       No Oversampling   StandardScaler             6                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.091349  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.034208273887634276, 0.125...\n",
       "971                   XGBClassifier       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.0455775260925293, 0.11289...\n",
       "1542                  XGBClassifier       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.05026543140411377, 0.1406...\n",
       "972                   XGBClassifier       No Oversampling     MinMaxScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.105075  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04527881145477295, 0.1108...\n",
       "974                   XGBClassifier       No Oversampling     MinMaxScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06562378406524658, 0.1389...\n",
       "1537                  XGBClassifier       No Oversampling   StandardScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.100336  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05704696178436279, 0.1268...\n",
       "1539                  XGBClassifier       No Oversampling   StandardScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.117714  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.058243942260742185, 0.163...\n",
       "950                   XGBClassifier       No Oversampling     MinMaxScaler             6                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.091349  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.055152058601379395, 0.133...\n",
       "2115                  XGBClassifier       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.141032                                          0.094493  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.048369836807250974, 0.150...\n",
       "1543                  XGBClassifier       No Oversampling   StandardScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.141032                                          0.105075  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06991252899169922, 0.1460...\n",
       "1493             AdaBoostClassifier       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05535285472869873, 0.2403...\n",
       "359              AdaBoostClassifier       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.104727  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05754454135894775, 0.1469...\n",
       "1497             AdaBoostClassifier       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05844368934631348, 0.1783...\n",
       "2055             AdaBoostClassifier       No Oversampling        LogScaler             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.034106206893920896, 0.087...\n",
       "1499             AdaBoostClassifier       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06233024597167969, 0.1788...\n",
       "924              AdaBoostClassifier       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05654888153076172, 0.1912...\n",
       "913              AdaBoostClassifier       No Oversampling     MinMaxScaler             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.046475696563720706, 0.143...\n",
       "1491             AdaBoostClassifier       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06083688735961914, 0.2303...\n",
       "1501             AdaBoostClassifier       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.104727  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05963718891143799, 0.3151...\n",
       "1531                  XGBClassifier       No Oversampling   StandardScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.138371                                          0.123936  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.040691065788269046, 0.146...\n",
       "922              AdaBoostClassifier       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.054354405403137206, 0.230...\n",
       "920              AdaBoostClassifier       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.040583348274230956, 0.109...\n",
       "926              AdaBoostClassifier       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07958698272705078, 0.2445...\n",
       "357              AdaBoostClassifier       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06382904052734376, 0.1862...\n",
       "930              AdaBoostClassifier       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.104727  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06163454055786133, 0.1838...\n",
       "355              AdaBoostClassifier       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05854318141937256, 0.2138...\n",
       "1453     GradientBoostingClassifier       No Oversampling   StandardScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...          {'loss': 'deviance', 'n_estimators': 200}                            0.138371                                          0.136070  ([DecisionTreeRegressor(criterion='friedman_ms...  {'mean_fit_time': [0.048470139503479004, 0.122...\n",
       "353              AdaBoostClassifier       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04725942611694336, 0.1317...\n",
       "928              AdaBoostClassifier       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06532518863677979, 0.2764...\n",
       "960                   XGBClassifier       No Oversampling     MinMaxScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.138371                                          0.123936  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06442737579345703, 0.1520...\n",
       "1484             AdaBoostClassifier       No Oversampling   StandardScaler             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.044579267501831055, 0.208...\n",
       "389                   XGBClassifier       No Oversampling       No Scaling            11                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.138371                                          0.123936  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04667432308197021, 0.1226...\n",
       "2102                  XGBClassifier       No Oversampling        LogScaler            11                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.138371                                          0.123936  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05235929489135742, 0.1379...\n",
       "1495             AdaBoostClassifier       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06721937656402588, 0.2572...\n",
       "1875           ExtraTreesClassifier       No Oversampling        LogScaler            10                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'criterion': 'gini', 'n_estimators': 45}                            0.138371                                          0.104852  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.0044878959655761715, 0.00...\n",
       "342              AdaBoostClassifier       No Oversampling       No Scaling             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03251252174377441, 0.0914...\n",
       "349              AdaBoostClassifier       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.138371                                          0.093306  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04019174575805664, 0.1086...\n",
       "...                             ...                   ...              ...           ...                         ...                                                ...                                                ...                                 ...                                               ...                                                ...                                                ...\n",
       "800                     BernoulliNB       No Oversampling     MinMaxScaler             3                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.037200760841369626, 0.037...\n",
       "1951                    BernoulliNB       No Oversampling        LogScaler             7                         RFE  [quantityModifications, scansWithoutRegistrati...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.0034906625747680663, 0.00...\n",
       "802                     BernoulliNB       No Oversampling     MinMaxScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.0036905527114868162, 0.00...\n",
       "564                             SVC       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.22848806381225586, 0.2906...\n",
       "612            KNeighborsClassifier       No Oversampling     MinMaxScaler             1                 SelectKBest                                       [trustLevel]  {'n_neighbors': 15, 'p': 1, 'weights': 'uniform'}                           -0.276743                                          0.011709  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.0026929616928100587, 0.00...\n",
       "580              LogisticRegression       No Oversampling     MinMaxScaler             5                         RFE  [trustLevel, scannedLineItemsPerSecond, valueP...  {'C': 0.1, 'fit_intercept': True, 'solver': 'n...                           -0.276743                                          0.011709  LogisticRegression(C=0.1, class_weight=None, d...  {'mean_fit_time': [0.011269688606262207, 0.006...\n",
       "578              LogisticRegression       No Oversampling     MinMaxScaler             4                         RFE  [trustLevel, valuePerSecond, scansWithoutRegis...  {'C': 0.1, 'fit_intercept': True, 'solver': 'n...                           -0.276743                                          0.011709  LogisticRegression(C=0.1, class_weight=None, d...  {'mean_fit_time': [0.01136949062347412, 0.0062...\n",
       "576              LogisticRegression       No Oversampling     MinMaxScaler             3                         RFE  [trustLevel, valuePerSecond, quantityModificat...  {'C': 0.1, 'fit_intercept': True, 'solver': 'n...                           -0.276743                                          0.011709  LogisticRegression(C=0.1, class_weight=None, d...  {'mean_fit_time': [0.010172843933105469, 0.006...\n",
       "574              LogisticRegression       No Oversampling     MinMaxScaler             2                         RFE  [valuePerSecond, quantityModificationsPerScann...  {'C': 0.1, 'fit_intercept': True, 'solver': 'n...                           -0.276743                                          0.011709  LogisticRegression(C=0.1, class_weight=None, d...  {'mean_fit_time': [0.006981444358825683, 0.005...\n",
       "572              LogisticRegression       No Oversampling     MinMaxScaler             1                         RFE          [quantityModificationsPerScannedLineItem]  {'C': 0.1, 'fit_intercept': True, 'solver': 'n...                           -0.276743                                          0.011709  LogisticRegression(C=0.1, class_weight=None, d...  {'mean_fit_time': [0.0032910585403442385, 0.00...\n",
       "571              LogisticRegression       No Oversampling     MinMaxScaler             1                 SelectKBest                                       [trustLevel]  {'C': 0.1, 'fit_intercept': True, 'solver': 'n...                           -0.276743                                          0.011709  LogisticRegression(C=0.1, class_weight=None, d...  {'mean_fit_time': [0.00747990608215332, 0.0051...\n",
       "619            KNeighborsClassifier       No Oversampling     MinMaxScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}                           -0.276743                                          0.011709  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.0021942138671875, 0.00289...\n",
       "568                             SVC       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.23995745182037354, 0.3406...\n",
       "567                             SVC       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.3127604007720947, 0.37249...\n",
       "566                             SVC       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.2834411859512329, 0.27985...\n",
       "565                             SVC       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.22420010566711426, 0.2973...\n",
       "570                             SVC       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.24484460353851317, 0.2886...\n",
       "563                             SVC       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.22629404067993164, 0.2666...\n",
       "569                             SVC       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.24534320831298828, 0.3101...\n",
       "804                     BernoulliNB       No Oversampling     MinMaxScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.03400943279266357, 0.0960...\n",
       "562                             SVC       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.22968454360961915, 0.2610...\n",
       "620            KNeighborsClassifier       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}                           -0.276743                                          0.011709  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.003191423416137695, 0.003...\n",
       "1070                     Perceptron       No Oversampling     MinMaxScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.276743                                          0.011709  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0032912731170654298, 0.00...\n",
       "615            KNeighborsClassifier       No Oversampling     MinMaxScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}                           -0.276743                                          0.011709  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.002393794059753418, 0.002...\n",
       "622            KNeighborsClassifier       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}                           -0.276743                                          0.011709  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.0032911300659179688, 0.00...\n",
       "558                             SVC       No Oversampling       No Scaling             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.18729548454284667, 0.2516...\n",
       "559                             SVC       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.19137916564941407, 0.2468...\n",
       "560                             SVC       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.20574965476989746, 0.2531...\n",
       "561                             SVC       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                        {'C': 0.01}                           -0.276743                                          0.011709  SVC(C=0.01, cache_size=200, class_weight=None,...  {'mean_fit_time': [0.22469837665557862, 0.2949...\n",
       "803                     BernoulliNB       No Oversampling     MinMaxScaler             4                         RFE  [quantityModifications, quantityModificationsP...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...  {'mean_fit_time': [0.03560478687286377, 0.0977...\n",
       "1901         RandomForestClassifier       No Oversampling        LogScaler             3                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...          {'criterion': 'gini', 'n_estimators': 10}                           -0.279404                                          0.276866  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.00907599925994873, 0.0153...\n",
       "2205                     Perceptron       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.303353                                          0.049882  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020952701568603517, 0.00...\n",
       "2203                     Perceptron       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.303353                                          0.049882  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002097344398498535, 0.002...\n",
       "2201                     Perceptron       No Oversampling        LogScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.303353                                          0.055660  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002197599411010742, 0.002...\n",
       "494                      Perceptron       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.003690004348754883, 0.003...\n",
       "2207                     Perceptron       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055970  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020002126693725586, 0.00...\n",
       "498                      Perceptron       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0024930953979492186, 0.00...\n",
       "2209                     Perceptron       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055970  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004188895225524902, 0.005...\n",
       "2211                     Perceptron       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055970  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019889116287231446, 0.00...\n",
       "2213                     Perceptron       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055970  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0030914783477783204, 0.00...\n",
       "496                      Perceptron       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002991938591003418, 0.003...\n",
       "1065                     Perceptron       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019999027252197267, 0.00...\n",
       "1067                     Perceptron       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.00418856143951416, 0.0057...\n",
       "1069                     Perceptron       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0031915903091430664, 0.00...\n",
       "1638                     Perceptron       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020944356918334963, 0.00...\n",
       "1636                     Perceptron       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.003789472579956055, 0.004...\n",
       "1640                     Perceptron       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.316658                                          0.055546  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0021973133087158205, 0.00...\n",
       "1634                     Perceptron       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.005086421966552734, 0.004...\n",
       "1055                     Perceptron       No Oversampling     MinMaxScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058507  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002100372314453125, 0.002...\n",
       "1061                     Perceptron       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0022903919219970704, 0.00...\n",
       "1059                     Perceptron       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019947052001953124, 0.00...\n",
       "1063                     Perceptron       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001992058753967285, 0.002...\n",
       "1642                     Perceptron       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.087123  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020998716354370117, 0.00...\n",
       "1071                     Perceptron       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.087123  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002094459533691406, 0.002...\n",
       "490                      Perceptron       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.005285835266113282, 0.005...\n",
       "1057                     Perceptron       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058507  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020879507064819336, 0.00...\n",
       "1632                     Perceptron       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020969867706298827, 0.00...\n",
       "488                      Perceptron       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019997596740722657, 0.00...\n",
       "486                      Perceptron       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058507  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001989579200744629, 0.002...\n",
       "1630                     Perceptron       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0031913995742797853, 0.00...\n",
       "484                      Perceptron       No Oversampling       No Scaling             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058507  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0021907567977905275, 0.00...\n",
       "492                      Perceptron       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058101  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.005884313583374023, 0.006...\n",
       "1628                     Perceptron       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058507  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004786992073059082, 0.004...\n",
       "500                      Perceptron       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.087123  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002792811393737793, 0.002...\n",
       "1626                     Perceptron       No Oversampling   StandardScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.329963                                          0.058507  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.005086207389831543, 0.005...\n",
       "502                      Perceptron       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.343268                                          0.089731  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0022950172424316406, 0.00...\n",
       "1644                     Perceptron       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.343268                                          0.089731  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020972490310668945, 0.00...\n",
       "1049                     Perceptron       No Oversampling     MinMaxScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.343268                                          0.057551  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0022879123687744142, 0.00...\n",
       "476                      Perceptron       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.343268                                          0.057551  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0021960258483886717, 0.00...\n",
       "1047                     Perceptron       No Oversampling     MinMaxScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.343268                                          0.057551  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0029918432235717775, 0.00...\n",
       "1620                     Perceptron       No Oversampling   StandardScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.343268                                          0.057551  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002592945098876953, 0.002...\n",
       "478                      Perceptron       No Oversampling       No Scaling             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.343268                                          0.057551  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0017952203750610351, 0.00...\n",
       "1618                     Perceptron       No Oversampling   StandardScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.343268                                          0.057551  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0029920101165771484, 0.00...\n",
       "1073                     Perceptron       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.343268                                          0.089731  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0022937536239624025, 0.00...\n",
       "2197                     Perceptron       No Oversampling        LogScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.343268                                          0.067319  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0027925252914428713, 0.00...\n",
       "1622                     Perceptron       No Oversampling   StandardScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.080324  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020943403244018553, 0.00...\n",
       "2215                     Perceptron       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.091056  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001989150047302246, 0.002...\n",
       "1075                     Perceptron       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.091056  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020942926406860352, 0.00...\n",
       "1051                     Perceptron       No Oversampling     MinMaxScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.080324  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001894998550415039, 0.002...\n",
       "2217                     Perceptron       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.091056  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002001667022705078, 0.002...\n",
       "1616                     Perceptron       No Oversampling   StandardScaler             3                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.116680  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.005086350440979004, 0.004...\n",
       "474                      Perceptron       No Oversampling       No Scaling             3                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.116680  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002197265625, 0.001991724...\n",
       "1646                     Perceptron       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.091056  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002299594879150391, 0.002...\n",
       "480                      Perceptron       No Oversampling       No Scaling             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.080324  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019973039627075194, 0.00...\n",
       "2199                     Perceptron       No Oversampling        LogScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.084035  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0024932146072387694, 0.00...\n",
       "504                      Perceptron       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.091056  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.005684852600097656, 0.007...\n",
       "1045                     Perceptron       No Oversampling     MinMaxScaler             3                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.356573                                          0.116680  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.003789854049682617, 0.004...\n",
       "1860           ExtraTreesClassifier       No Oversampling        LogScaler             3                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...       {'criterion': 'entropy', 'n_estimators': 20}                           -0.359234                                          0.282543  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.004186010360717774, 0.006...\n",
       "1624                     Perceptron       No Oversampling   StandardScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.369878                                          0.113116  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020979881286621095, 0.00...\n",
       "2195                     Perceptron       No Oversampling        LogScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.369878                                          0.113116  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0032900094985961912, 0.00...\n",
       "1053                     Perceptron       No Oversampling     MinMaxScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.369878                                          0.113116  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019019126892089843, 0.00...\n",
       "482                      Perceptron       No Oversampling       No Scaling             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.369878                                          0.113116  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020946502685546876, 0.00...\n",
       "1862           ExtraTreesClassifier       No Oversampling        LogScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...      {'criterion': 'entropy', 'n_estimators': 110}                           -0.372539                                          0.262374  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.007878804206848144, 0.012...\n",
       "1044                     Perceptron       No Oversampling     MinMaxScaler             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0015957355499267578, 0.00...\n",
       "473                      Perceptron       No Oversampling       No Scaling             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019971847534179686, 0.00...\n",
       "2186                     Perceptron       No Oversampling        LogScaler             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0017929315567016602, 0.00...\n",
       "1615                     Perceptron       No Oversampling   StandardScaler             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004089069366455078, 0.003...\n",
       "1048                     Perceptron       No Oversampling     MinMaxScaler             4                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.497605                                          0.379382  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0021942138671875, 0.00259...\n",
       "1619                     Perceptron       No Oversampling   StandardScaler             4                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.497605                                          0.379382  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0027927875518798826, 0.00...\n",
       "2190                     Perceptron       No Oversampling        LogScaler             4                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.497605                                          0.379382  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004088902473449707, 0.003...\n",
       "477                      Perceptron       No Oversampling       No Scaling             4                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.497605                                          0.379382  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0018944263458251954, 0.00...\n",
       "2192                     Perceptron       No Oversampling        LogScaler             5                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.502927                                          0.321477  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0032912254333496093, 0.00...\n",
       "479                      Perceptron       No Oversampling       No Scaling             5                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.502927                                          0.321477  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0018979310989379883, 0.00...\n",
       "1621                     Perceptron       No Oversampling   StandardScaler             5                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.502927                                          0.321477  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020946502685546876, 0.00...\n",
       "1050                     Perceptron       No Oversampling     MinMaxScaler             5                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.502927                                          0.321477  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.00219419002532959, 0.0025...\n",
       "1617                     Perceptron       No Oversampling   StandardScaler             3                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.582757                                          0.441433  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0032911300659179688, 0.00...\n",
       "475                      Perceptron       No Oversampling       No Scaling             3                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.582757                                          0.441433  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001898050308227539, 0.001...\n",
       "2188                     Perceptron       No Oversampling        LogScaler             3                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.582757                                          0.441433  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.03510630130767822, 0.0263...\n",
       "1046                     Perceptron       No Oversampling     MinMaxScaler             3                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.582757                                          0.441433  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0024933576583862304, 0.00...\n",
       "269                      GaussianNB       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -0.651943                                          0.307925       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0027924060821533205], 'st...\n",
       "840                      GaussianNB       No Oversampling     MinMaxScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -0.651943                                          0.307925       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091859817504883], 'std...\n",
       "1411                     GaussianNB       No Oversampling   StandardScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -0.651943                                          0.307925       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002692770957946777], 'std...\n",
       "534   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.1}                           -0.670569                                          0.286324  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002892303466796875, 0.002...\n",
       "1676  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.1}                           -0.670569                                          0.286324  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025929927825927733, 0.00...\n",
       "1105  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             4                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.1}                           -0.670569                                          0.286324  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025930404663085938, 0.00...\n",
       "1900         RandomForestClassifier       No Oversampling        LogScaler             2                         RFE            [trustLevel, scannedLineItemsPerSecond]          {'criterion': 'gini', 'n_estimators': 10}                           -0.734433                                          0.260090  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010172724723815918, 0.017...\n",
       "1677  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                 {'reg_param': 0.30000000000000004}                           -0.761043                                          0.273077  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002892303466796875, 0.002...\n",
       "535   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                 {'reg_param': 0.30000000000000004}                           -0.761043                                          0.273077  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029915809631347657, 0.00...\n",
       "1106  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                 {'reg_param': 0.30000000000000004}                           -0.761043                                          0.273077  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0026925325393676756, 0.00...\n",
       "758          RandomForestClassifier       No Oversampling     MinMaxScaler             2                         RFE            [trustLevel, scannedLineItemsPerSecond]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.763704                                          0.305585  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.009076595306396484, 0.016...\n",
       "1981                     GaussianNB       No Oversampling        LogScaler             3                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                                 {}                           -0.846195                                          0.341143       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0026930570602416992], 'st...\n",
       "2246  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             3                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                 {'reg_param': 0.1}                           -0.957956                                          0.295386  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025930404663085938, 0.00...\n",
       "1110  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.2}                           -0.987227                                          0.332005  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002692842483520508, 0.002...\n",
       "539   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.2}                           -0.987227                                          0.332005  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0026925325393676756, 0.00...\n",
       "1681  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.2}                           -0.987227                                          0.332005  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029918432235717775, 0.00...\n",
       "536   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                 {'reg_param': 0.30000000000000004}                           -1.016498                                          0.302507  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00299220085144043, 0.0028...\n",
       "1107  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                 {'reg_param': 0.30000000000000004}                           -1.016498                                          0.302507  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029920101165771484, 0.00...\n",
       "1678  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                 {'reg_param': 0.30000000000000004}                           -1.016498                                          0.302507  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0027925968170166016, 0.00...\n",
       "185          RandomForestClassifier       No Oversampling       No Scaling             1                         RFE                        [scannedLineItemsPerSecond]          {'criterion': 'gini', 'n_estimators': 10}                           -1.035125                                          0.329418  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.014860272407531738, 0.028...\n",
       "2247  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                 {'reg_param': 0.4}                           -1.048430                                          0.348315  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0026929378509521484, 0.00...\n",
       "1680  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                 {'reg_param': 0.2}                           -1.098989                                          0.291602  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028922319412231444, 0.00...\n",
       "1109  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                 {'reg_param': 0.2}                           -1.098989                                          0.291602  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0027927398681640626, 0.00...\n",
       "538   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                 {'reg_param': 0.2}                           -1.098989                                          0.291602  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029919624328613283, 0.00...\n",
       "544   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.136243                                          0.316511  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0032910585403442385, 0.00...\n",
       "1686  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.136243                                          0.316511  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034906148910522463, 0.00...\n",
       "1115  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.136243                                          0.316511  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0032912254333496093, 0.00...\n",
       "1685  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.136243                                          0.316511  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031913280487060546, 0.00...\n",
       "543   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.136243                                          0.316511  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031914710998535156, 0.00...\n",
       "1114  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.136243                                          0.316511  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003191494941711426, 0.003...\n",
       "1682  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002991914749145508, 0.003...\n",
       "541   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034906625747680663, 0.00...\n",
       "1111  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031914710998535156, 0.00...\n",
       "1112  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029920339584350586, 0.00...\n",
       "542   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002393364906311035, 0.002...\n",
       "1113  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003191518783569336, 0.003...\n",
       "1684  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00249326229095459, 0.0034...\n",
       "540   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00359039306640625, 0.0029...\n",
       "1683  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.162853                                          0.349383  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003490734100341797, 0.002...\n",
       "545   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.168175                                          0.342141  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0035904645919799805, 0.00...\n",
       "1687  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.168175                                          0.342141  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0030917882919311523, 0.00...\n",
       "1116  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.168175                                          0.342141  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031913995742797853, 0.00...\n",
       "2248  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             5                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                 {'reg_param': 0.30000000000000004}                           -1.232038                                          0.318737  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00329132080078125, 0.0029...\n",
       "1108  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.271953                                          0.393375  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028922319412231444, 0.00...\n",
       "537   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.271953                                          0.393375  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0023936986923217773, 0.00...\n",
       "2250  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.271953                                          0.393375  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0030918121337890625, 0.00...\n",
       "1679  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.6}                           -1.271953                                          0.393375  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025930881500244142, 0.00...\n",
       "2249  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             6                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                 {'reg_param': 0.30000000000000004}                           -1.452900                                          0.432521  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002892279624938965, 0.002...\n",
       "1982                     GaussianNB       No Oversampling        LogScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                                 {}                           -1.548696                                          0.534504       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0031911134719848633], 'st...\n",
       "270                      GaussianNB       No Oversampling       No Scaling             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.628526                                          0.515212       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0025931358337402343], 'st...\n",
       "841                      GaussianNB       No Oversampling     MinMaxScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.628526                                          0.515212       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0029921531677246094], 'st...\n",
       "1412                     GaussianNB       No Oversampling   StandardScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.628526                                          0.515212       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002892279624938965], 'std...\n",
       "1413                     GaussianNB       No Oversampling   StandardScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.998403                                          0.470626       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0027925968170166016], 'st...\n",
       "271                      GaussianNB       No Oversampling       No Scaling             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.998403                                          0.470626       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016954183578491212], 'st...\n",
       "842                      GaussianNB       No Oversampling     MinMaxScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.998403                                          0.470626       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0029920101165771484], 'st...\n",
       "1117  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.027674                                          0.373413  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031915903091430664, 0.00...\n",
       "1688  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.027674                                          0.373413  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031913995742797853, 0.00...\n",
       "546   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.027674                                          0.373413  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031911611557006838, 0.00...\n",
       "1983                     GaussianNB       No Oversampling        LogScaler             5                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                                 {}                           -2.048962                                          0.480463       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0014959096908569336], 'st...\n",
       "2254  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.054284                                          0.419648  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003191566467285156, 0.003...\n",
       "1689  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.067589                                          0.410509  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003490900993347168, 0.003...\n",
       "1118  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.067589                                          0.410509  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0036903858184814454, 0.00...\n",
       "547   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.067589                                          0.410509  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0027927875518798826, 0.00...\n",
       "2255  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.107504                                          0.433860  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025929927825927733, 0.00...\n",
       "2256  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.134114                                          0.446705  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003291201591491699, 0.002...\n",
       "2258  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.134114                                          0.446705  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003191566467285156, 0.003...\n",
       "2259  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.134114                                          0.446705  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003490638732910156, 0.003...\n",
       "2257  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.134114                                          0.446705  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003390860557556152, 0.003...\n",
       "2260  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.134114                                          0.446705  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0039894342422485355, 0.00...\n",
       "2261  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.200639                                          0.433446  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003789949417114258, 0.003...\n",
       "548   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.200639                                          0.433446  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0036899328231811525, 0.00...\n",
       "1119  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.200639                                          0.433446  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003789663314819336, 0.003...\n",
       "1690  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.200639                                          0.433446  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003989481925964355, 0.003...\n",
       "844                      GaussianNB       No Oversampling     MinMaxScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.251197                                          0.499499       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0031917333602905274], 'st...\n",
       "274                      GaussianNB       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.251197                                          0.499499       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001896977424621582], 'std...\n",
       "273                      GaussianNB       No Oversampling       No Scaling             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.251197                                          0.499499       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0020916938781738283], 'st...\n",
       "845                      GaussianNB       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.251197                                          0.499499       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002991771697998047], 'std...\n",
       "1415                     GaussianNB       No Oversampling   StandardScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.251197                                          0.499499       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0028923511505126952], 'st...\n",
       "1416                     GaussianNB       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.251197                                          0.499499       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002393651008605957], 'std...\n",
       "2251  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -2.256519                                          0.387237  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0027925968170166016, 0.00...\n",
       "1984                     GaussianNB       No Oversampling        LogScaler             6                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                                 {}                           -2.277807                                          0.480363       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001695537567138672], 'std...\n",
       "1985                     GaussianNB       No Oversampling        LogScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.291112                                          0.481289       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0015959501266479491], 'st...\n",
       "843                      GaussianNB       No Oversampling     MinMaxScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.291112                                          0.481289       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0030920028686523436], 'st...\n",
       "272                      GaussianNB       No Oversampling       No Scaling             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.291112                                          0.481289       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018947362899780274], 'st...\n",
       "1414                     GaussianNB       No Oversampling   StandardScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.291112                                          0.481289       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002892303466796875], 'std...\n",
       "2253  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                  {'reg_param': 0.7000000000000001}                           -2.315061                                          0.366846  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002393770217895508, 0.002...\n",
       "1417                     GaussianNB       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.574415       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0021942138671875], 'std_f...\n",
       "846                      GaussianNB       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.574415       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0029918193817138673], 'st...\n",
       "275                      GaussianNB       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.574415       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016014814376831056], 'st...\n",
       "2252  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...                  {'reg_param': 0.7000000000000001}                           -2.376264                                          0.370081  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003191089630126953, 0.002...\n",
       "1120  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                  {'reg_param': 0.7000000000000001}                           -2.607770                                          0.558007  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003690171241760254, 0.003...\n",
       "1121  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.6}                           -2.607770                                          0.366881  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003590106964111328, 0.003...\n",
       "550   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.6}                           -2.607770                                          0.366881  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0030860185623168947, 0.00...\n",
       "549   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                  {'reg_param': 0.7000000000000001}                           -2.607770                                          0.558007  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0035902738571166994, 0.00...\n",
       "2262  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                  {'reg_param': 0.7000000000000001}                           -2.607770                                          0.558007  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0036902427673339844, 0.00...\n",
       "1691  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                  {'reg_param': 0.7000000000000001}                           -2.607770                                          0.558007  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0044880867004394535, 0.00...\n",
       "2263  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.6}                           -2.607770                                          0.366881  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0035904407501220702, 0.00...\n",
       "1692  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.6}                           -2.607770                                          0.366881  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003091740608215332, 0.003...\n",
       "1418                     GaussianNB       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.676956                                          0.667603       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994466781616211], 'std...\n",
       "847                      GaussianNB       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.676956                                          0.667603       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032913923263549806], 'st...\n",
       "276                      GaussianNB       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.676956                                          0.667603       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016979455947875976], 'st...\n",
       "1986                     GaussianNB       No Oversampling        LogScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.788717                                          0.432177       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0015957355499267578], 'st...\n",
       "1419                     GaussianNB       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.249069                                          0.700153       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016956090927124023], 'st...\n",
       "848                      GaussianNB       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.249069                                          0.700153       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0029920339584350586], 'st...\n",
       "277                      GaussianNB       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.249069                                          0.700153       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001795196533203125], 'std...\n",
       "1988                     GaussianNB       No Oversampling        LogScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -3.400745                                          0.469608       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0015952110290527344], 'st...\n",
       "1989                     GaussianNB       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.400745                                          0.462822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016927242279052735], 'st...\n",
       "1987                     GaussianNB       No Oversampling        LogScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...                                                 {}                           -3.467270                                          0.531802       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0014960527420043944], 'st...\n",
       "849                      GaussianNB       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.706759                                          0.728619       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0035910606384277344], 'st...\n",
       "1420                     GaussianNB       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.706759                                          0.728619       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017951250076293946], 'st...\n",
       "278                      GaussianNB       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.706759                                          0.728619       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016924142837524414], 'st...\n",
       "850                      GaussianNB       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.111229                                          0.765794       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0033912897109985352], 'st...\n",
       "279                      GaussianNB       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.111229                                          0.765794       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017014741897583008], 'st...\n",
       "1421                     GaussianNB       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.111229                                          0.765794       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001795196533203125], 'std...\n",
       "280                      GaussianNB       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.132517                                          0.752621       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018015146255493165], 'st...\n",
       "851                      GaussianNB       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.132517                                          0.752621       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.00359039306640625], 'std_...\n",
       "1422                     GaussianNB       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.132517                                          0.752621       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001894974708557129], 'std...\n",
       "852                      GaussianNB       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.177754                                          0.637230       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0035907745361328123], 'st...\n",
       "1423                     GaussianNB       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.177754                                          0.637230       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002094554901123047], 'std...\n",
       "281                      GaussianNB       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.177754                                          0.637230       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018978357315063477], 'st...\n",
       "1990                     GaussianNB       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.204364                                          0.487839       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0015927791595458985], 'st...\n",
       "853                      GaussianNB       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.683342                                          0.517235       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0038900375366210938], 'st...\n",
       "282                      GaussianNB       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.683342                                          0.517235       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016952753067016602], 'st...\n",
       "1424                     GaussianNB       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.683342                                          0.517235       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002692985534667969], 'std...\n",
       "1991                     GaussianNB       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.709952                                          0.394694       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001792144775390625], 'std...\n",
       "1992                     GaussianNB       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.749867                                          0.412888       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017979860305786133], 'st...\n",
       "1993                     GaussianNB       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.843002                                          0.431181       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001791667938232422], 'std...\n",
       "1994                     GaussianNB       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -4.962746                                          0.415310       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018949508666992188], 'st...\n",
       "1995                     GaussianNB       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -5.109101                                          0.429031       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017982959747314454], 'st...\n",
       "285                      GaussianNB       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017977714538574218], 'st...\n",
       "856                      GaussianNB       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0031916141510009766], 'st...\n",
       "855                      GaussianNB       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0037900447845458985], 'st...\n",
       "854                      GaussianNB       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0033912181854248045], 'st...\n",
       "284                      GaussianNB       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018950700759887695], 'st...\n",
       "1998                     GaussianNB       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018950223922729493], 'st...\n",
       "1427                     GaussianNB       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0033910989761352537], 'st...\n",
       "1426                     GaussianNB       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0035906076431274415], 'st...\n",
       "1425                     GaussianNB       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091764450073242], 'std...\n",
       "1997                     GaussianNB       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0015957117080688476], 'st...\n",
       "1996                     GaussianNB       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018980741500854493], 'st...\n",
       "283                      GaussianNB       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -5.162320                                          0.471822       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001997685432434082], 'std...\n",
       "\n",
       "[2284 rows x 11 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table = result_table.sort_values(by = \"Monetary Value Per Instance - Mean\", ascending = False)\n",
    "result_table.to_excel(\"Result-Train Set-Final-without PCA and tsne.xlsx\")\n",
    "result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result tables for all Scaling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Oversampling Strategy</th>\n",
       "      <th>Data Preparation</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Feature Selection Technique</th>\n",
       "      <th>Features</th>\n",
       "      <th>Optimal Parameters</th>\n",
       "      <th>Monetary Value Per Instance - Mean</th>\n",
       "      <th>Monetary Value Per Instance - Standard Deviation</th>\n",
       "      <th>Raw Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.118947</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.103015</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 140}</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.077010</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 150}</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.101174</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'base_estimator': ExtraTreesClassifier(bootst...</td>\n",
       "      <td>0.109101</td>\n",
       "      <td>0.078367</td>\n",
       "      <td>((ExtraTreeClassifier(class_weight=None, crite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 105}</td>\n",
       "      <td>0.101118</td>\n",
       "      <td>0.089369</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 35, 'min_sa...</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.094774</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'activation': 'logistic', 'learning_rate': 'c...</td>\n",
       "      <td>0.077169</td>\n",
       "      <td>0.071634</td>\n",
       "      <td>MLPClassifier(activation='logistic', alpha=0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 35, 'min_sa...</td>\n",
       "      <td>0.026610</td>\n",
       "      <td>0.108179</td>\n",
       "      <td>ExtraTreeClassifier(class_weight=None, criteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.093135</td>\n",
       "      <td>0.184733</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.106440</td>\n",
       "      <td>0.179312</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{'C': 0.5}</td>\n",
       "      <td>-0.146354</td>\n",
       "      <td>0.200883</td>\n",
       "      <td>SVC(C=0.5, cache_size=200, class_weight=None, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{'n_neighbors': 10, 'p': 2, 'weights': 'distan...</td>\n",
       "      <td>-0.156998</td>\n",
       "      <td>0.258287</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.167642</td>\n",
       "      <td>0.189601</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model Oversampling Strategy Data Preparation Feature Count Feature Selection Technique                                           Features                                 Optimal Parameters  Monetary Value Per Instance - Mean  Monetary Value Per Instance - Standard Deviation                                          Raw Model\n",
       "4              AdaBoostClassifier       No Oversampling   StandardScaler             7                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.077832  (DecisionTreeClassifier(class_weight=None, cri...\n",
       "1              LogisticRegression       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.167642                                          0.118947  LogisticRegression(C=1.2000000000000002, class...\n",
       "15                  XGBClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.167642                                          0.103015  XGBClassifier(base_score=0.7, booster='gbtree'...\n",
       "14           ExtraTreesClassifier       No Oversampling       No Scaling             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...      {'criterion': 'entropy', 'n_estimators': 140}                            0.146354                                          0.077010  (ExtraTreeClassifier(class_weight=None, criter...\n",
       "2      GradientBoostingClassifier       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 150}                            0.143693                                          0.101174  ([DecisionTreeRegressor(criterion='friedman_ms...\n",
       "0               BaggingClassifier       No Oversampling   StandardScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'base_estimator': ExtraTreesClassifier(bootst...                            0.109101                                          0.078367  ((ExtraTreeClassifier(class_weight=None, crite...\n",
       "6          RandomForestClassifier       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...      {'criterion': 'entropy', 'n_estimators': 105}                            0.101118                                          0.089369  (DecisionTreeClassifier(class_weight=None, cri...\n",
       "10         DecisionTreeClassifier       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'criterion': 'gini', 'max_depth': 35, 'min_sa...                            0.085152                                          0.094774  DecisionTreeClassifier(class_weight=None, crit...\n",
       "7                   MLPClassifier       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'activation': 'logistic', 'learning_rate': 'c...                            0.077169                                          0.071634  MLPClassifier(activation='logistic', alpha=0.0...\n",
       "3             ExtraTreeClassifier       No Oversampling        LogScaler             7                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'criterion': 'gini', 'max_depth': 35, 'min_sa...                            0.026610                                          0.108179  ExtraTreeClassifier(class_weight=None, criteri...\n",
       "5                      GaussianNB       No Oversampling       No Scaling             3                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -0.093135                                          0.184733       GaussianNB(priors=None, var_smoothing=1e-09)\n",
       "11  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             3                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.1}                           -0.106440                                          0.179312  QuadraticDiscriminantAnalysis(priors=None, reg...\n",
       "12                            SVC       No Oversampling        LogScaler             6                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                         {'C': 0.5}                           -0.146354                                          0.200883  SVC(C=0.5, cache_size=200, class_weight=None, ...\n",
       "8            KNeighborsClassifier       No Oversampling        LogScaler             6                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...  {'n_neighbors': 10, 'p': 2, 'weights': 'distan...                           -0.156998                                          0.258287  KNeighborsClassifier(algorithm='auto', leaf_si...\n",
       "9                      Perceptron       No Oversampling        LogScaler             5                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.167642                                          0.189601  Perceptron(alpha=0.0005, class_weight=None, ea...\n",
       "13                    BernoulliNB       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table_aggregated = pd.DataFrame(columns=[\"Model\", \"Oversampling Strategy\", \"Data Preparation\", \"Feature Count\", \"Feature Selection Technique\", \"Features\", \"Optimal Parameters\", \"Monetary Value Per Instance - Mean\", \"Monetary Value Per Instance - Standard Deviation\", \"Raw Model\"])\n",
    "\n",
    "\n",
    "for model in list(set(result_table[\"Model\"].values)):\n",
    "    sub_table = result_table[result_table[\"Model\"] == model]\n",
    "    result_table_aggregated = result_table_aggregated.append({\n",
    "        \"Model\": model,\n",
    "        \"Oversampling Strategy\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Oversampling Strategy\"],\n",
    "        \"Data Preparation\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Data Preparation\"],\n",
    "        \"Feature Count\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Feature Count\"],\n",
    "        \"Features\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Features\"],\n",
    "        \"Feature Selection Technique\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Feature Selection Technique\"],\n",
    "        \"Optimal Parameters\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Optimal Parameters\"],\n",
    "        \"Monetary Value Per Instance - Mean\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Monetary Value Per Instance - Mean\"],\n",
    "        \"Monetary Value Per Instance - Standard Deviation\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Monetary Value Per Instance - Standard Deviation\"],\n",
    "        \"Raw Model\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Raw Model\"]\n",
    "        }, ignore_index=True)\n",
    "\n",
    "result_table_aggregated.to_excel(\"Result-Train Set-Aggregated-Final-without PCA and tsne.xlsx\")\n",
    "\n",
    "result_table_aggregated.sort_values(by = \"Monetary Value Per Instance - Mean\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New aggregated result table with 'No scaling' only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Oversampling Strategy</th>\n",
       "      <th>Data Preparation</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Feature Selection Technique</th>\n",
       "      <th>Features</th>\n",
       "      <th>Optimal Parameters</th>\n",
       "      <th>Monetary Value Per Instance - Mean</th>\n",
       "      <th>Monetary Value Per Instance - Standard Deviation</th>\n",
       "      <th>Raw Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, lineItemVoids, scansWithoutRegist...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.118947</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.167642</td>\n",
       "      <td>0.103015</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 140}</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.077010</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 150}</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.101174</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 60}</td>\n",
       "      <td>0.098457</td>\n",
       "      <td>0.099356</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_estimator': ExtraTreeClassifier(class_w...</td>\n",
       "      <td>0.095796</td>\n",
       "      <td>0.052033</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 27, 'min...</td>\n",
       "      <td>0.074508</td>\n",
       "      <td>0.142174</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'activation': 'logistic', 'learning_rate': 'i...</td>\n",
       "      <td>0.069186</td>\n",
       "      <td>0.058413</td>\n",
       "      <td>MLPClassifier(activation='logistic', alpha=0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 15, 'min_sa...</td>\n",
       "      <td>-0.002661</td>\n",
       "      <td>0.153540</td>\n",
       "      <td>ExtraTreeClassifier(class_weight=None, criteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.093135</td>\n",
       "      <td>0.184733</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.106440</td>\n",
       "      <td>0.179312</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems]</td>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>-0.159659</td>\n",
       "      <td>0.208667</td>\n",
       "      <td>SVC(C=1, cache_size=200, class_weight=None, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems]</td>\n",
       "      <td>{'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.164981</td>\n",
       "      <td>0.203795</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.236828</td>\n",
       "      <td>0.106678</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[quantityModifications, quantityModificationsP...</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model Oversampling Strategy Data Preparation Feature Count Feature Selection Technique                                           Features                                 Optimal Parameters  Monetary Value Per Instance - Mean  Monetary Value Per Instance - Standard Deviation                                          Raw Model\n",
       "4              AdaBoostClassifier       No Oversampling       No Scaling             7                         RFE  [trustLevel, lineItemVoids, scansWithoutRegist...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.077832  (DecisionTreeClassifier(class_weight=None, cri...\n",
       "1              LogisticRegression       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.167642                                          0.118947  LogisticRegression(C=1.2000000000000002, class...\n",
       "15                  XGBClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.167642                                          0.103015  XGBClassifier(base_score=0.7, booster='gbtree'...\n",
       "14           ExtraTreesClassifier       No Oversampling       No Scaling             8                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...      {'criterion': 'entropy', 'n_estimators': 140}                            0.146354                                          0.077010  (ExtraTreeClassifier(class_weight=None, criter...\n",
       "2      GradientBoostingClassifier       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 150}                            0.143693                                          0.101174  ([DecisionTreeRegressor(criterion='friedman_ms...\n",
       "6          RandomForestClassifier       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...       {'criterion': 'entropy', 'n_estimators': 60}                            0.098457                                          0.099356  (DecisionTreeClassifier(class_weight=None, cri...\n",
       "0               BaggingClassifier       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_estimator': ExtraTreeClassifier(class_w...                            0.095796                                          0.052033  (ExtraTreeClassifier(class_weight=None, criter...\n",
       "10         DecisionTreeClassifier       No Oversampling       No Scaling            12                         RFE  [trustLevel, totalScanTimeInSeconds, scansWith...  {'criterion': 'entropy', 'max_depth': 27, 'min...                            0.074508                                          0.142174  DecisionTreeClassifier(class_weight=None, crit...\n",
       "7                   MLPClassifier       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'activation': 'logistic', 'learning_rate': 'i...                            0.069186                                          0.058413  MLPClassifier(activation='logistic', alpha=0.0...\n",
       "3             ExtraTreeClassifier       No Oversampling       No Scaling             6                         RFE  [trustLevel, totalScanTimeInSeconds, scansWith...  {'criterion': 'gini', 'max_depth': 15, 'min_sa...                           -0.002661                                          0.153540  ExtraTreeClassifier(class_weight=None, criteri...\n",
       "5                      GaussianNB       No Oversampling       No Scaling             3                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -0.093135                                          0.184733       GaussianNB(priors=None, var_smoothing=1e-09)\n",
       "11  QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             3                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.1}                           -0.106440                                          0.179312  QuadraticDiscriminantAnalysis(priors=None, reg...\n",
       "12                            SVC       No Oversampling       No Scaling             2                 SelectKBest                     [trustLevel, scannedLineItems]                                           {'C': 1}                           -0.159659                                          0.208667  SVC(C=1, cache_size=200, class_weight=None, co...\n",
       "8            KNeighborsClassifier       No Oversampling       No Scaling             2                 SelectKBest                     [trustLevel, scannedLineItems]  {'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}                           -0.164981                                          0.203795  KNeighborsClassifier(algorithm='auto', leaf_si...\n",
       "9                      Perceptron       No Oversampling       No Scaling             2                 SelectKBest                     [trustLevel, scannedLineItems]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.236828                                          0.106678  Perceptron(alpha=0.0005, class_weight=None, ea...\n",
       "13                    BernoulliNB       No Oversampling       No Scaling             5                         RFE  [quantityModifications, quantityModificationsP...                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table_aggregated = pd.DataFrame(columns=[\"Model\", \"Oversampling Strategy\", \"Data Preparation\", \"Feature Count\", \"Feature Selection Technique\", \"Features\", \"Optimal Parameters\", \"Monetary Value Per Instance - Mean\", \"Monetary Value Per Instance - Standard Deviation\", \"Raw Model\"])\n",
    "\n",
    "\n",
    "for model in list(set(result_table[\"Model\"].values)):\n",
    "    sub_table = result_table[(result_table[\"Model\"] == model) & (result_table[\"Data Preparation\"] == 'No Scaling')]\n",
    "    result_table_aggregated = result_table_aggregated.append({\n",
    "        \"Model\": model,\n",
    "        \"Oversampling Strategy\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Oversampling Strategy\"],\n",
    "        \"Data Preparation\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Data Preparation\"],\n",
    "        \"Feature Count\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Feature Count\"],\n",
    "        \"Features\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Features\"],\n",
    "        \"Feature Selection Technique\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Feature Selection Technique\"],\n",
    "        \"Optimal Parameters\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Optimal Parameters\"],\n",
    "        \"Monetary Value Per Instance - Mean\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Monetary Value Per Instance - Mean\"],\n",
    "        \"Monetary Value Per Instance - Standard Deviation\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Monetary Value Per Instance - Standard Deviation\"],\n",
    "        \"Raw Model\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Raw Model\"]\n",
    "        }, ignore_index=True)\n",
    "\n",
    "result_table_aggregated.sort_values(by = \"Monetary Value Per Instance - Mean\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Monetary Value Per Instance - Mean')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "\n",
    "sub_table = result_table[result_table[\"Model\"] == \"AdaBoostClassifier\"]\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(sub_table[\"Feature Count\"],\n",
    "            sub_table[\"Monetary Value Per Instance - Mean\"],\n",
    "             c = 'black'),\n",
    "\n",
    "\n",
    "plt.xlabel('Number of features', fontsize=16)\n",
    "plt.ylabel('Monetary Value Per Instance - Mean', fontsize=16)\n",
    "\n",
    "#ggplot(aes(x='Feature Count', y='Monetary Value Per Instance - Mean', color='Data Preparation'), data=sub_table)  + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Monetary Value Per Instance - Mean')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(sub_table[\"Feature Count\"][sub_table['Data Preparation'] == 'No Scaling'],\n",
    "            sub_table[\"Monetary Value Per Instance - Mean\"][sub_table['Data Preparation'] == 'No Scaling'],\n",
    "             c = 'red'),\n",
    "\n",
    "\n",
    "plt.xlabel('Number of features', fontsize=16)\n",
    "plt.ylabel('Monetary Value Per Instance - Mean', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "['totalScanTimeInSeconds' 'lineItemVoids' 'scansWithoutRegistration'\n",
      " 'scannedLineItems' 'scansWithoutRegistrationPerScannedLineItem'\n",
      " 'quantityModificationsPerSecond' 'secondsPerEuro'\n",
      " 'quantityModificationsPerEuro' 'pca_axis_2' 'tsne_axis_1' 'tsne_axis_2']\n",
      "{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "best_model = result_table_aggregated.loc[result_table_aggregated[\"Monetary Value Per Instance - Mean\"].argmax()][\"Raw Model\"]\n",
    "best_model_features = result_table_aggregated.loc[result_table_aggregated[\"Monetary Value Per Instance - Mean\"].argmax()][\"Features\"]\n",
    "best_parameters = result_table.loc[result_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Optimal Parameters\"]\n",
    "\n",
    "print(best_model)\n",
    "print(best_model_features)\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the result of all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.07719312, 0.24654045, 0.42067425, 0.56000149, 0.6918488 ,\n",
       "        0.0813822 , 0.21253138, 0.39444449, 0.54145153, 0.61146438]),\n",
       " 'std_fit_time': array([0.02842738, 0.02546927, 0.04019336, 0.10787258, 0.13792846,\n",
       "        0.01621819, 0.0218753 , 0.09674357, 0.11914641, 0.10653174]),\n",
       " 'mean_score_time': array([0.00299208, 0.00498657, 0.0159575 , 0.01466086, 0.01456122,\n",
       "        0.00388956, 0.00777924, 0.01585753, 0.02074437, 0.02493329]),\n",
       " 'std_score_time': array([3.02519263e-07, 6.30713329e-04, 1.21983838e-02, 6.43329631e-03,\n",
       "        1.68059759e-03, 5.36989939e-04, 2.74211089e-03, 1.00572440e-02,\n",
       "        6.17713624e-03, 1.10248599e-02]),\n",
       " 'param_algorithm': masked_array(data=['SAMME', 'SAMME', 'SAMME', 'SAMME', 'SAMME', 'SAMME.R',\n",
       "                    'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[20, 50, 100, 150, 200, 20, 50, 100, 150, 200],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'algorithm': 'SAMME', 'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME', 'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME', 'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME', 'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME', 'n_estimators': 200},\n",
       "  {'algorithm': 'SAMME.R', 'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME.R', 'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME.R', 'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME.R', 'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME.R', 'n_estimators': 200}],\n",
       " 'split0_test_score': array([0.18518519, 0.02645503, 0.15873016, 0.15873016, 0.15873016,\n",
       "        0.23809524, 0.15873016, 0.15873016, 0.10582011, 0.15873016]),\n",
       " 'split1_test_score': array([-0.02645503,  0.10582011,  0.10582011,  0.23809524,  0.23809524,\n",
       "         0.10582011,  0.23809524,  0.05291005,  0.05291005,  0.05291005]),\n",
       " 'split2_test_score': array([-0.21164021, -0.02645503,  0.15873016,  0.15873016,  0.15873016,\n",
       "         0.10582011,  0.15873016,  0.02645503,  0.02645503,  0.02645503]),\n",
       " 'split3_test_score': array([0.18518519, 0.18518519, 0.18518519, 0.18518519, 0.18518519,\n",
       "        0.18518519, 0.23809524, 0.29100529, 0.23809524, 0.23809524]),\n",
       " 'split4_test_score': array([0.02659574, 0.07978723, 0.07978723, 0.21276596, 0.21276596,\n",
       "        0.21276596, 0.21276596, 0.21276596, 0.21276596, 0.21276596]),\n",
       " 'split5_test_score': array([-0.05347594,  0.26737968,  0.13368984,  0.13368984,  0.13368984,\n",
       "         0.        ,  0.26737968,  0.13368984,  0.13368984,  0.26737968]),\n",
       " 'split6_test_score': array([-0.0802139 ,  0.0802139 , -0.05347594,  0.0802139 ,  0.21390374,\n",
       "        -0.10695187,  0.16042781,  0.02673797,  0.02673797,  0.0802139 ]),\n",
       " 'split7_test_score': array([0.21390374, 0.21390374, 0.26737968, 0.26737968, 0.26737968,\n",
       "        0.21390374, 0.21390374, 0.16042781, 0.21390374, 0.26737968]),\n",
       " 'split8_test_score': array([-0.29411765, -0.02673797, -0.0802139 , -0.02673797, -0.0802139 ,\n",
       "         0.02673797,  0.16042781,  0.16042781,  0.10695187,  0.16042781]),\n",
       " 'split9_test_score': array([0.05347594, 0.05347594, 0.10695187, 0.16042781, 0.21390374,\n",
       "        0.21390374, 0.21390374, 0.21390374, 0.16042781, 0.16042781]),\n",
       " 'mean_test_score': array([0.        , 0.09579564, 0.1064396 , 0.1569984 , 0.17030335,\n",
       "        0.11974454, 0.20223523, 0.14369345, 0.12772751, 0.16232038]),\n",
       " 'std_test_score': array([0.16098944, 0.09402823, 0.09954344, 0.0791676 , 0.09158262,\n",
       "        0.10913983, 0.03805113, 0.08253479, 0.07415321, 0.08223635]),\n",
       " 'rank_test_score': array([10,  9,  8,  4,  2,  7,  1,  5,  6,  3]),\n",
       " 'split0_train_score': array([0.07692308, 0.23668639, 0.27514793, 0.27514793, 0.27514793,\n",
       "        0.22781065, 0.27514793, 0.27514793, 0.27514793, 0.27514793]),\n",
       " 'split1_train_score': array([0.0887574 , 0.26331361, 0.27514793, 0.27514793, 0.27514793,\n",
       "        0.23372781, 0.27514793, 0.27514793, 0.27514793, 0.27514793]),\n",
       " 'split2_train_score': array([0.10946746, 0.23964497, 0.27514793, 0.27514793, 0.27514793,\n",
       "        0.24556213, 0.27514793, 0.27514793, 0.27514793, 0.27514793]),\n",
       " 'split3_train_score': array([0.11242604, 0.23372781, 0.27514793, 0.27514793, 0.27514793,\n",
       "        0.23372781, 0.27514793, 0.27514793, 0.27514793, 0.27514793]),\n",
       " 'split4_train_score': array([0.03843879, 0.20697812, 0.27794205, 0.27794205, 0.27794205,\n",
       "        0.25724423, 0.27794205, 0.27794205, 0.27794205, 0.27794205]),\n",
       " 'split5_train_score': array([0.10047281, 0.26300236, 0.27777778, 0.27777778, 0.27777778,\n",
       "        0.24231678, 0.27777778, 0.27777778, 0.27777778, 0.27777778]),\n",
       " 'split6_train_score': array([0.07387707, 0.26300236, 0.27777778, 0.27777778, 0.27777778,\n",
       "        0.25413712, 0.27777778, 0.27777778, 0.27777778, 0.27777778]),\n",
       " 'split7_train_score': array([0.03841608, 0.21572104, 0.27777778, 0.27777778, 0.27777778,\n",
       "        0.2570922 , 0.27777778, 0.27777778, 0.27777778, 0.27777778]),\n",
       " 'split8_train_score': array([0.12115839, 0.27186761, 0.27777778, 0.27777778, 0.27777778,\n",
       "        0.24231678, 0.27777778, 0.27777778, 0.27777778, 0.27777778]),\n",
       " 'split9_train_score': array([0.13297872, 0.26004728, 0.27777778, 0.27777778, 0.27777778,\n",
       "        0.25118203, 0.27777778, 0.27777778, 0.27777778, 0.27777778]),\n",
       " 'mean_train_score': array([0.08929158, 0.24539916, 0.27674227, 0.27674227, 0.27674227,\n",
       "        0.24451176, 0.27674227, 0.27674227, 0.27674227, 0.27674227]),\n",
       " 'std_train_score': array([0.03091366, 0.02111405, 0.00130263, 0.00130263, 0.00130263,\n",
       "        0.00989837, 0.00130263, 0.00130263, 0.00130263, 0.00130263])}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = result_table.loc[result_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Cross Validation Results\"]\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of the model on the entire train data set (when trained on the entire train data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negative:  1775\n",
      "False positive:  0\n",
      "False negative:  0\n",
      "True positive:  104\n",
      "520 for 1879 instances in the test set\n",
      "0.2767429483767962 per instance in the test set\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_monetary_value(cm):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        print(\"True negative: \", tn)\n",
    "        print(\"False positive: \", fp)\n",
    "        print(\"False negative: \", fn)\n",
    "        print(\"True positive: \", tp)\n",
    "        score = (-25)*fp + (-5)*fn + 5*tp\n",
    "        print(str(score) + \" for \" + str(sum(sum(cm))) + \" instances in the test set\")\n",
    "        print(str(score/sum(sum(cm))) + \" per instance in the test set\")\n",
    "        return score\n",
    "    \n",
    "cm = confusion_matrix(Y , best_model.predict(X[best_model_features]))\n",
    "\n",
    "monetary_value = get_monetary_value(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on the entire test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>scannedLineItems</th>\n",
       "      <th>pricePerScannedLineItem</th>\n",
       "      <th>scansWithoutRegistrationPerScannedLineItem</th>\n",
       "      <th>quantityModificationsPerScannedLineItem</th>\n",
       "      <th>lineItemVoidsPerSecond</th>\n",
       "      <th>scansWithoutRegistrationPerSecond</th>\n",
       "      <th>quantityModificationsPerSecond</th>\n",
       "      <th>secondsPerEuro</th>\n",
       "      <th>lineItemVoidsPerEuro</th>\n",
       "      <th>scansWithoutRegistrationPerEuro</th>\n",
       "      <th>quantityModificationsPerEuro</th>\n",
       "      <th>pca_axis_1</th>\n",
       "      <th>pca_axis_2</th>\n",
       "      <th>tsne_axis_1</th>\n",
       "      <th>tsne_axis_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>467</td>\n",
       "      <td>88.48</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>0.189465</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.640000</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.017131</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>5.278029</td>\n",
       "      <td>0.045208</td>\n",
       "      <td>0.090416</td>\n",
       "      <td>0.045208</td>\n",
       "      <td>0.407059</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1.288056</td>\n",
       "      <td>0.439125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1004</td>\n",
       "      <td>58.99</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026892</td>\n",
       "      <td>0.058755</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.184815</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.006972</td>\n",
       "      <td>0.005976</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>17.019834</td>\n",
       "      <td>0.118664</td>\n",
       "      <td>0.101712</td>\n",
       "      <td>0.016952</td>\n",
       "      <td>-0.404056</td>\n",
       "      <td>-0.107318</td>\n",
       "      <td>-2.048066</td>\n",
       "      <td>-0.496164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>14.00</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>11.571429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.648069</td>\n",
       "      <td>-0.481068</td>\n",
       "      <td>3.447044</td>\n",
       "      <td>0.053117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>532</td>\n",
       "      <td>84.79</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.159380</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.056429</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>6.274325</td>\n",
       "      <td>0.106145</td>\n",
       "      <td>0.035382</td>\n",
       "      <td>0.047175</td>\n",
       "      <td>0.292492</td>\n",
       "      <td>0.294786</td>\n",
       "      <td>1.344228</td>\n",
       "      <td>1.554838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>890</td>\n",
       "      <td>42.16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021348</td>\n",
       "      <td>0.047371</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.218947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.110057</td>\n",
       "      <td>0.094877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.564485</td>\n",
       "      <td>0.295128</td>\n",
       "      <td>-2.414502</td>\n",
       "      <td>1.547169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1072</td>\n",
       "      <td>12.67</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.603333</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>84.609313</td>\n",
       "      <td>0.236780</td>\n",
       "      <td>0.315706</td>\n",
       "      <td>0.078927</td>\n",
       "      <td>-0.388592</td>\n",
       "      <td>0.294494</td>\n",
       "      <td>-1.828019</td>\n",
       "      <td>1.732591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>259</td>\n",
       "      <td>93.75</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100386</td>\n",
       "      <td>0.361969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.605769</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.762667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.591763</td>\n",
       "      <td>-0.104089</td>\n",
       "      <td>-2.445146</td>\n",
       "      <td>-0.517219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1528</td>\n",
       "      <td>47.35</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009817</td>\n",
       "      <td>0.030988</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.156667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>32.270327</td>\n",
       "      <td>0.042239</td>\n",
       "      <td>0.190074</td>\n",
       "      <td>0.105597</td>\n",
       "      <td>0.458967</td>\n",
       "      <td>-0.303730</td>\n",
       "      <td>2.015723</td>\n",
       "      <td>-1.598574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>816</td>\n",
       "      <td>80.89</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017157</td>\n",
       "      <td>0.099130</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.777857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011029</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.087774</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.049450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.438512</td>\n",
       "      <td>0.506551</td>\n",
       "      <td>-2.074741</td>\n",
       "      <td>1.978520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>31.91</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1.994375</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.519524</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.501410</td>\n",
       "      <td>0.219367</td>\n",
       "      <td>0.219367</td>\n",
       "      <td>0.125353</td>\n",
       "      <td>0.211314</td>\n",
       "      <td>0.090980</td>\n",
       "      <td>1.474045</td>\n",
       "      <td>0.426568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>714</td>\n",
       "      <td>94.29</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.132059</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.857500</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.572383</td>\n",
       "      <td>0.084845</td>\n",
       "      <td>0.074239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.391550</td>\n",
       "      <td>-0.087511</td>\n",
       "      <td>-2.165339</td>\n",
       "      <td>-0.435252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>1077</td>\n",
       "      <td>66.16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015785</td>\n",
       "      <td>0.061430</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.891765</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.004643</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>16.278718</td>\n",
       "      <td>0.075574</td>\n",
       "      <td>0.120919</td>\n",
       "      <td>0.045345</td>\n",
       "      <td>0.081986</td>\n",
       "      <td>0.298952</td>\n",
       "      <td>0.382123</td>\n",
       "      <td>1.827146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>1301</td>\n",
       "      <td>84.35</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021522</td>\n",
       "      <td>0.064835</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.012500</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.007686</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>15.423829</td>\n",
       "      <td>0.035566</td>\n",
       "      <td>0.118554</td>\n",
       "      <td>0.059277</td>\n",
       "      <td>0.332642</td>\n",
       "      <td>0.084617</td>\n",
       "      <td>2.101652</td>\n",
       "      <td>0.551876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>1429</td>\n",
       "      <td>47.95</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003499</td>\n",
       "      <td>0.033555</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.590000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>29.801877</td>\n",
       "      <td>0.166840</td>\n",
       "      <td>0.020855</td>\n",
       "      <td>0.062565</td>\n",
       "      <td>0.203086</td>\n",
       "      <td>-0.094804</td>\n",
       "      <td>0.419216</td>\n",
       "      <td>-0.478411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>1196</td>\n",
       "      <td>83.77</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.070042</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.754000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009197</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.277188</td>\n",
       "      <td>0.131312</td>\n",
       "      <td>0.119374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.249300</td>\n",
       "      <td>-0.073777</td>\n",
       "      <td>-1.691172</td>\n",
       "      <td>-0.394049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>1567</td>\n",
       "      <td>75.53</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008296</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.810000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.006382</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>20.746723</td>\n",
       "      <td>0.092678</td>\n",
       "      <td>0.132398</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>-0.209044</td>\n",
       "      <td>-0.088058</td>\n",
       "      <td>-1.627112</td>\n",
       "      <td>-0.455931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>289</td>\n",
       "      <td>18.66</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086505</td>\n",
       "      <td>0.064567</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.746400</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027682</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.487674</td>\n",
       "      <td>0.428725</td>\n",
       "      <td>0.214362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.587221</td>\n",
       "      <td>0.094554</td>\n",
       "      <td>-2.528869</td>\n",
       "      <td>0.501003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>335</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032836</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.108182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>281.512605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>-0.327371</td>\n",
       "      <td>-0.500976</td>\n",
       "      <td>-1.404831</td>\n",
       "      <td>-1.984003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>1304</td>\n",
       "      <td>30.51</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.023397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.906875</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>42.740085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229433</td>\n",
       "      <td>0.098328</td>\n",
       "      <td>0.047629</td>\n",
       "      <td>-0.102692</td>\n",
       "      <td>0.390086</td>\n",
       "      <td>-0.537004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>1353</td>\n",
       "      <td>82.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>0.060621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.717143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>16.495977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024384</td>\n",
       "      <td>-0.061909</td>\n",
       "      <td>0.301631</td>\n",
       "      <td>-0.469931</td>\n",
       "      <td>1.453829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>1749</td>\n",
       "      <td>46.56</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012007</td>\n",
       "      <td>0.026621</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.217143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>37.564433</td>\n",
       "      <td>0.107388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107388</td>\n",
       "      <td>0.335438</td>\n",
       "      <td>0.480083</td>\n",
       "      <td>1.940913</td>\n",
       "      <td>2.267286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>757</td>\n",
       "      <td>91.02</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019815</td>\n",
       "      <td>0.120238</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.068000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.003963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.316853</td>\n",
       "      <td>0.098879</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.454010</td>\n",
       "      <td>0.304593</td>\n",
       "      <td>-2.222024</td>\n",
       "      <td>1.415683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>1440</td>\n",
       "      <td>15.15</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.010521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.631250</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>95.049505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396040</td>\n",
       "      <td>0.330033</td>\n",
       "      <td>0.310644</td>\n",
       "      <td>-0.317246</td>\n",
       "      <td>2.062055</td>\n",
       "      <td>-1.834159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>1101</td>\n",
       "      <td>22.55</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019982</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.025000</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>0.008174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.824834</td>\n",
       "      <td>0.354767</td>\n",
       "      <td>0.399113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.516836</td>\n",
       "      <td>0.103528</td>\n",
       "      <td>-2.477338</td>\n",
       "      <td>0.480600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6</td>\n",
       "      <td>1404</td>\n",
       "      <td>59.43</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015670</td>\n",
       "      <td>0.042329</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.701364</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>0.005698</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>23.624432</td>\n",
       "      <td>0.084133</td>\n",
       "      <td>0.134612</td>\n",
       "      <td>0.084133</td>\n",
       "      <td>0.387792</td>\n",
       "      <td>0.488340</td>\n",
       "      <td>1.978658</td>\n",
       "      <td>2.308822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>1396</td>\n",
       "      <td>13.16</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010745</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.877333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007880</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106.079027</td>\n",
       "      <td>0.835866</td>\n",
       "      <td>0.151976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.479203</td>\n",
       "      <td>0.502977</td>\n",
       "      <td>-2.042129</td>\n",
       "      <td>1.932095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>248</td>\n",
       "      <td>14.00</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100806</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.036290</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>17.714286</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.217940</td>\n",
       "      <td>0.089334</td>\n",
       "      <td>-0.608626</td>\n",
       "      <td>0.511476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>359</td>\n",
       "      <td>6.40</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>0.017827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>56.093750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.136216</td>\n",
       "      <td>-0.096246</td>\n",
       "      <td>0.416303</td>\n",
       "      <td>-0.478445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6</td>\n",
       "      <td>915</td>\n",
       "      <td>36.79</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.040208</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.255714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.870889</td>\n",
       "      <td>0.244632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.401927</td>\n",
       "      <td>0.508693</td>\n",
       "      <td>-1.884966</td>\n",
       "      <td>1.719997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>617</td>\n",
       "      <td>49.57</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035656</td>\n",
       "      <td>0.080340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.253182</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>12.447045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100867</td>\n",
       "      <td>0.020173</td>\n",
       "      <td>-0.394770</td>\n",
       "      <td>0.294185</td>\n",
       "      <td>-1.889421</td>\n",
       "      <td>1.752990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>785</td>\n",
       "      <td>69.87</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.033121</td>\n",
       "      <td>0.089006</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.687308</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>11.235151</td>\n",
       "      <td>0.028625</td>\n",
       "      <td>0.014312</td>\n",
       "      <td>0.042937</td>\n",
       "      <td>-0.085711</td>\n",
       "      <td>0.281051</td>\n",
       "      <td>0.363176</td>\n",
       "      <td>2.021664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6</td>\n",
       "      <td>1576</td>\n",
       "      <td>94.01</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>0.059651</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.875625</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.003173</td>\n",
       "      <td>0.003173</td>\n",
       "      <td>16.764174</td>\n",
       "      <td>0.063823</td>\n",
       "      <td>0.053186</td>\n",
       "      <td>0.053186</td>\n",
       "      <td>0.453287</td>\n",
       "      <td>0.491772</td>\n",
       "      <td>1.920002</td>\n",
       "      <td>2.143842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>44.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.344531</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>2.902494</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.022676</td>\n",
       "      <td>-0.305911</td>\n",
       "      <td>-0.499015</td>\n",
       "      <td>-1.549833</td>\n",
       "      <td>-2.117539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>1260</td>\n",
       "      <td>42.28</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.033556</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.456000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>29.801325</td>\n",
       "      <td>0.094607</td>\n",
       "      <td>0.165563</td>\n",
       "      <td>0.047304</td>\n",
       "      <td>0.038380</td>\n",
       "      <td>0.113650</td>\n",
       "      <td>-0.500719</td>\n",
       "      <td>0.467817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6</td>\n",
       "      <td>1206</td>\n",
       "      <td>39.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.032595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.655000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>30.679216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050878</td>\n",
       "      <td>0.017638</td>\n",
       "      <td>0.507226</td>\n",
       "      <td>-0.113821</td>\n",
       "      <td>1.689994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>883</td>\n",
       "      <td>22.42</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007928</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.202857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007928</td>\n",
       "      <td>0.005663</td>\n",
       "      <td>39.384478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312221</td>\n",
       "      <td>0.223015</td>\n",
       "      <td>0.533834</td>\n",
       "      <td>0.101324</td>\n",
       "      <td>1.874397</td>\n",
       "      <td>0.447375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1554</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010296</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.257500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>377.184466</td>\n",
       "      <td>0.970874</td>\n",
       "      <td>2.427184</td>\n",
       "      <td>0.242718</td>\n",
       "      <td>-0.285590</td>\n",
       "      <td>-0.492750</td>\n",
       "      <td>-1.539333</td>\n",
       "      <td>-2.141390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>1734</td>\n",
       "      <td>9.38</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014994</td>\n",
       "      <td>0.005409</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.360769</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>184.861407</td>\n",
       "      <td>1.066098</td>\n",
       "      <td>0.639659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.577454</td>\n",
       "      <td>-0.303080</td>\n",
       "      <td>-2.396955</td>\n",
       "      <td>-1.499305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>1182</td>\n",
       "      <td>94.67</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.080093</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.982632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>12.485476</td>\n",
       "      <td>0.042252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042252</td>\n",
       "      <td>0.186985</td>\n",
       "      <td>-0.114655</td>\n",
       "      <td>1.540088</td>\n",
       "      <td>-0.537563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>1194</td>\n",
       "      <td>75.02</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010050</td>\n",
       "      <td>0.062831</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.251667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.004188</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>15.915756</td>\n",
       "      <td>0.066649</td>\n",
       "      <td>0.106638</td>\n",
       "      <td>0.039989</td>\n",
       "      <td>0.146536</td>\n",
       "      <td>0.104531</td>\n",
       "      <td>0.382479</td>\n",
       "      <td>0.465688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5</td>\n",
       "      <td>526</td>\n",
       "      <td>92.49</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053232</td>\n",
       "      <td>0.175837</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.303214</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.020913</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.009506</td>\n",
       "      <td>5.687101</td>\n",
       "      <td>0.118932</td>\n",
       "      <td>0.010812</td>\n",
       "      <td>0.054060</td>\n",
       "      <td>0.310278</td>\n",
       "      <td>0.276153</td>\n",
       "      <td>1.974766</td>\n",
       "      <td>1.768757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>1662</td>\n",
       "      <td>70.34</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009627</td>\n",
       "      <td>0.042323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.396250</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.628092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.467701</td>\n",
       "      <td>0.109340</td>\n",
       "      <td>-2.210949</td>\n",
       "      <td>0.471721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>1060</td>\n",
       "      <td>67.97</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.064123</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.265667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>0.005660</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>15.595115</td>\n",
       "      <td>0.102987</td>\n",
       "      <td>0.088274</td>\n",
       "      <td>0.058849</td>\n",
       "      <td>0.113458</td>\n",
       "      <td>0.081560</td>\n",
       "      <td>1.632863</td>\n",
       "      <td>0.532465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5</td>\n",
       "      <td>1276</td>\n",
       "      <td>59.30</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.046473</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.766667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.517707</td>\n",
       "      <td>0.016863</td>\n",
       "      <td>0.050590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.354376</td>\n",
       "      <td>0.315715</td>\n",
       "      <td>-1.826596</td>\n",
       "      <td>1.206667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>1075</td>\n",
       "      <td>80.75</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.013953</td>\n",
       "      <td>0.075116</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.383333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>13.312693</td>\n",
       "      <td>0.111455</td>\n",
       "      <td>0.086687</td>\n",
       "      <td>0.037152</td>\n",
       "      <td>0.124672</td>\n",
       "      <td>0.101301</td>\n",
       "      <td>0.358393</td>\n",
       "      <td>0.504236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>67.99</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.102241</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.997500</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>9.780850</td>\n",
       "      <td>0.029416</td>\n",
       "      <td>0.102956</td>\n",
       "      <td>0.044124</td>\n",
       "      <td>0.253154</td>\n",
       "      <td>-0.487076</td>\n",
       "      <td>0.424097</td>\n",
       "      <td>-1.681565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5</td>\n",
       "      <td>1768</td>\n",
       "      <td>81.84</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>0.046290</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.897143</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>21.603128</td>\n",
       "      <td>0.109971</td>\n",
       "      <td>0.012219</td>\n",
       "      <td>0.012219</td>\n",
       "      <td>-0.357523</td>\n",
       "      <td>0.293651</td>\n",
       "      <td>-1.751355</td>\n",
       "      <td>1.645385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>1772</td>\n",
       "      <td>33.45</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.018877</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.787500</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>52.974589</td>\n",
       "      <td>0.269058</td>\n",
       "      <td>0.149477</td>\n",
       "      <td>0.149477</td>\n",
       "      <td>0.498718</td>\n",
       "      <td>-0.103505</td>\n",
       "      <td>1.981186</td>\n",
       "      <td>-0.538616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2</td>\n",
       "      <td>870</td>\n",
       "      <td>90.86</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.031034</td>\n",
       "      <td>0.104437</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.365185</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>9.575171</td>\n",
       "      <td>0.088048</td>\n",
       "      <td>0.088048</td>\n",
       "      <td>0.055030</td>\n",
       "      <td>0.357115</td>\n",
       "      <td>-0.315162</td>\n",
       "      <td>2.108857</td>\n",
       "      <td>-1.824888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4</td>\n",
       "      <td>1323</td>\n",
       "      <td>40.34</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>0.030491</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.068000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>32.796232</td>\n",
       "      <td>0.173525</td>\n",
       "      <td>0.148736</td>\n",
       "      <td>0.099157</td>\n",
       "      <td>0.427446</td>\n",
       "      <td>0.108353</td>\n",
       "      <td>1.351503</td>\n",
       "      <td>0.428718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>1087</td>\n",
       "      <td>40.46</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017479</td>\n",
       "      <td>0.037222</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.129474</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.866041</td>\n",
       "      <td>0.024716</td>\n",
       "      <td>0.247158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.505859</td>\n",
       "      <td>-0.093829</td>\n",
       "      <td>-2.444576</td>\n",
       "      <td>-0.487174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3</td>\n",
       "      <td>631</td>\n",
       "      <td>34.78</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.045959</td>\n",
       "      <td>0.055119</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.199310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.012678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007924</td>\n",
       "      <td>18.142611</td>\n",
       "      <td>0.230017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143761</td>\n",
       "      <td>0.258254</td>\n",
       "      <td>-0.127098</td>\n",
       "      <td>2.137155</td>\n",
       "      <td>-0.610189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>766</td>\n",
       "      <td>23.84</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026110</td>\n",
       "      <td>0.031123</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.192000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>0.007833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.130872</td>\n",
       "      <td>0.377517</td>\n",
       "      <td>0.251678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.510724</td>\n",
       "      <td>-0.497340</td>\n",
       "      <td>-2.242349</td>\n",
       "      <td>-2.108567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4</td>\n",
       "      <td>1243</td>\n",
       "      <td>89.45</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.071963</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.065909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.004827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.896031</td>\n",
       "      <td>0.044718</td>\n",
       "      <td>0.067077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.537213</td>\n",
       "      <td>0.099864</td>\n",
       "      <td>-2.521923</td>\n",
       "      <td>0.494900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4</td>\n",
       "      <td>1328</td>\n",
       "      <td>69.66</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.052455</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.951429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.006777</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>19.064025</td>\n",
       "      <td>0.129199</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.057422</td>\n",
       "      <td>0.369560</td>\n",
       "      <td>0.100507</td>\n",
       "      <td>1.330060</td>\n",
       "      <td>0.432338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5</td>\n",
       "      <td>220</td>\n",
       "      <td>33.23</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.077273</td>\n",
       "      <td>0.151045</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.954706</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>6.620524</td>\n",
       "      <td>0.180560</td>\n",
       "      <td>0.150466</td>\n",
       "      <td>0.090280</td>\n",
       "      <td>0.053972</td>\n",
       "      <td>0.295087</td>\n",
       "      <td>0.376573</td>\n",
       "      <td>1.801181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5</td>\n",
       "      <td>416</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.060096</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.016827</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>1733.333333</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>29.166667</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>-0.423952</td>\n",
       "      <td>0.293156</td>\n",
       "      <td>-1.823436</td>\n",
       "      <td>1.677843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>330</td>\n",
       "      <td>16.71</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>0.050636</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.856667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>19.748654</td>\n",
       "      <td>0.359066</td>\n",
       "      <td>0.299222</td>\n",
       "      <td>0.179533</td>\n",
       "      <td>0.147244</td>\n",
       "      <td>-0.496160</td>\n",
       "      <td>0.358784</td>\n",
       "      <td>-2.020848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5</td>\n",
       "      <td>266</td>\n",
       "      <td>58.63</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097744</td>\n",
       "      <td>0.220414</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.255000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.536926</td>\n",
       "      <td>0.136449</td>\n",
       "      <td>0.068224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.585148</td>\n",
       "      <td>0.293887</td>\n",
       "      <td>-2.444040</td>\n",
       "      <td>1.618795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4</td>\n",
       "      <td>916</td>\n",
       "      <td>18.89</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017467</td>\n",
       "      <td>0.020622</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.180625</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>0.006550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.491265</td>\n",
       "      <td>0.211752</td>\n",
       "      <td>0.317628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.494299</td>\n",
       "      <td>0.104907</td>\n",
       "      <td>-2.445191</td>\n",
       "      <td>0.481664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>6</td>\n",
       "      <td>492</td>\n",
       "      <td>29.45</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.059858</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.227083</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>16.706282</td>\n",
       "      <td>0.033956</td>\n",
       "      <td>0.271647</td>\n",
       "      <td>0.101868</td>\n",
       "      <td>-0.030359</td>\n",
       "      <td>0.490101</td>\n",
       "      <td>0.386263</td>\n",
       "      <td>2.530044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>6</td>\n",
       "      <td>604</td>\n",
       "      <td>12.81</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.036424</td>\n",
       "      <td>0.021209</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.582273</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>47.150664</td>\n",
       "      <td>0.390320</td>\n",
       "      <td>0.234192</td>\n",
       "      <td>0.390320</td>\n",
       "      <td>0.335972</td>\n",
       "      <td>0.481823</td>\n",
       "      <td>1.968992</td>\n",
       "      <td>2.293505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4</td>\n",
       "      <td>712</td>\n",
       "      <td>83.07</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.116671</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.153500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>8.571085</td>\n",
       "      <td>0.120380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048152</td>\n",
       "      <td>0.202694</td>\n",
       "      <td>0.085536</td>\n",
       "      <td>1.476217</td>\n",
       "      <td>0.521695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4</td>\n",
       "      <td>520</td>\n",
       "      <td>39.85</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017308</td>\n",
       "      <td>0.076635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.427778</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>13.048934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100376</td>\n",
       "      <td>0.050188</td>\n",
       "      <td>-0.072150</td>\n",
       "      <td>0.103467</td>\n",
       "      <td>-0.522323</td>\n",
       "      <td>0.490866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>6</td>\n",
       "      <td>244</td>\n",
       "      <td>51.97</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.069672</td>\n",
       "      <td>0.212992</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.057059</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>4.695016</td>\n",
       "      <td>0.076967</td>\n",
       "      <td>0.115451</td>\n",
       "      <td>0.019242</td>\n",
       "      <td>-0.309943</td>\n",
       "      <td>0.501196</td>\n",
       "      <td>-1.605085</td>\n",
       "      <td>2.290144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>813</td>\n",
       "      <td>11.94</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027060</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.542727</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.090452</td>\n",
       "      <td>0.083752</td>\n",
       "      <td>0.418760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.585050</td>\n",
       "      <td>-0.502719</td>\n",
       "      <td>-2.282900</td>\n",
       "      <td>-2.140231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5</td>\n",
       "      <td>877</td>\n",
       "      <td>75.98</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023945</td>\n",
       "      <td>0.086636</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.618095</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.009122</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>11.542511</td>\n",
       "      <td>0.105291</td>\n",
       "      <td>0.039484</td>\n",
       "      <td>0.013161</td>\n",
       "      <td>-0.349019</td>\n",
       "      <td>0.295377</td>\n",
       "      <td>-1.810546</td>\n",
       "      <td>1.734635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5</td>\n",
       "      <td>280</td>\n",
       "      <td>59.81</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.213607</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.848095</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>4.681491</td>\n",
       "      <td>0.150477</td>\n",
       "      <td>0.083598</td>\n",
       "      <td>0.050159</td>\n",
       "      <td>0.032398</td>\n",
       "      <td>0.292187</td>\n",
       "      <td>0.362073</td>\n",
       "      <td>1.920195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>4</td>\n",
       "      <td>1354</td>\n",
       "      <td>21.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.015510</td>\n",
       "      <td>0.016041</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.034286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>62.338858</td>\n",
       "      <td>0.092081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092081</td>\n",
       "      <td>-0.237265</td>\n",
       "      <td>0.087244</td>\n",
       "      <td>-0.585706</td>\n",
       "      <td>0.531649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4</td>\n",
       "      <td>1564</td>\n",
       "      <td>48.20</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.030818</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.448133</td>\n",
       "      <td>0.062241</td>\n",
       "      <td>0.041494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.330304</td>\n",
       "      <td>0.117252</td>\n",
       "      <td>-1.736771</td>\n",
       "      <td>0.443097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>66.48</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022901</td>\n",
       "      <td>0.507481</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.160000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.083969</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>1.970517</td>\n",
       "      <td>0.165463</td>\n",
       "      <td>0.150421</td>\n",
       "      <td>0.015042</td>\n",
       "      <td>-0.005106</td>\n",
       "      <td>-0.272038</td>\n",
       "      <td>-0.993315</td>\n",
       "      <td>-0.934356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2</td>\n",
       "      <td>1184</td>\n",
       "      <td>35.34</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016892</td>\n",
       "      <td>0.029848</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.767000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.004223</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>33.503113</td>\n",
       "      <td>0.141483</td>\n",
       "      <td>0.226372</td>\n",
       "      <td>0.056593</td>\n",
       "      <td>-0.146389</td>\n",
       "      <td>-0.301370</td>\n",
       "      <td>-0.593195</td>\n",
       "      <td>-1.917251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5</td>\n",
       "      <td>1694</td>\n",
       "      <td>52.61</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.031057</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.305000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>32.199202</td>\n",
       "      <td>0.019008</td>\n",
       "      <td>0.190078</td>\n",
       "      <td>0.095039</td>\n",
       "      <td>0.764194</td>\n",
       "      <td>0.317292</td>\n",
       "      <td>1.221473</td>\n",
       "      <td>0.752527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1</td>\n",
       "      <td>1021</td>\n",
       "      <td>15.30</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.018609</td>\n",
       "      <td>0.014985</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.805263</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>66.732026</td>\n",
       "      <td>0.065359</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.261438</td>\n",
       "      <td>0.167253</td>\n",
       "      <td>-0.512760</td>\n",
       "      <td>1.357841</td>\n",
       "      <td>-2.423085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>1309</td>\n",
       "      <td>40.16</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.030680</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.362353</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.003820</td>\n",
       "      <td>32.594622</td>\n",
       "      <td>0.199203</td>\n",
       "      <td>0.074701</td>\n",
       "      <td>0.124502</td>\n",
       "      <td>0.418606</td>\n",
       "      <td>-0.511156</td>\n",
       "      <td>1.960953</td>\n",
       "      <td>-2.223192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>6</td>\n",
       "      <td>1409</td>\n",
       "      <td>30.54</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>0.021675</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.349231</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.136215</td>\n",
       "      <td>0.163720</td>\n",
       "      <td>0.261952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.436180</td>\n",
       "      <td>0.510618</td>\n",
       "      <td>-2.063818</td>\n",
       "      <td>1.988861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>4</td>\n",
       "      <td>952</td>\n",
       "      <td>61.40</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.064496</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.011555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>15.504886</td>\n",
       "      <td>0.179153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032573</td>\n",
       "      <td>0.265730</td>\n",
       "      <td>0.122574</td>\n",
       "      <td>3.404396</td>\n",
       "      <td>0.108386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2</td>\n",
       "      <td>1484</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.005640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.298929</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>177.299881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.836320</td>\n",
       "      <td>0.358423</td>\n",
       "      <td>-0.094212</td>\n",
       "      <td>-0.314690</td>\n",
       "      <td>0.361347</td>\n",
       "      <td>-2.027979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>81.30</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>2.463636</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.710000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.405904</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.656401</td>\n",
       "      <td>0.488213</td>\n",
       "      <td>-2.255680</td>\n",
       "      <td>2.111732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>4</td>\n",
       "      <td>555</td>\n",
       "      <td>19.91</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>0.035874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.977500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>27.875439</td>\n",
       "      <td>0.200904</td>\n",
       "      <td>0.050226</td>\n",
       "      <td>0.150678</td>\n",
       "      <td>0.184926</td>\n",
       "      <td>0.104084</td>\n",
       "      <td>0.473830</td>\n",
       "      <td>0.476910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>953</td>\n",
       "      <td>86.81</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025184</td>\n",
       "      <td>0.091091</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.617083</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.010493</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>10.977998</td>\n",
       "      <td>0.115194</td>\n",
       "      <td>0.069116</td>\n",
       "      <td>0.011519</td>\n",
       "      <td>-0.346283</td>\n",
       "      <td>-0.502986</td>\n",
       "      <td>-1.690420</td>\n",
       "      <td>-2.316562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2</td>\n",
       "      <td>437</td>\n",
       "      <td>56.71</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022883</td>\n",
       "      <td>0.129771</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.671000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>7.705872</td>\n",
       "      <td>0.035267</td>\n",
       "      <td>0.123435</td>\n",
       "      <td>0.017634</td>\n",
       "      <td>-0.228421</td>\n",
       "      <td>-0.290254</td>\n",
       "      <td>-1.455960</td>\n",
       "      <td>-1.345126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>6</td>\n",
       "      <td>675</td>\n",
       "      <td>14.47</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.021437</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.803889</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>46.648238</td>\n",
       "      <td>0.138217</td>\n",
       "      <td>0.069109</td>\n",
       "      <td>0.138217</td>\n",
       "      <td>-0.198784</td>\n",
       "      <td>0.490810</td>\n",
       "      <td>-0.567895</td>\n",
       "      <td>2.334896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>581</td>\n",
       "      <td>32.72</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.034423</td>\n",
       "      <td>0.056317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.636000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>17.756724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213936</td>\n",
       "      <td>0.152812</td>\n",
       "      <td>0.370116</td>\n",
       "      <td>-0.512009</td>\n",
       "      <td>1.933192</td>\n",
       "      <td>-2.282469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>3</td>\n",
       "      <td>1277</td>\n",
       "      <td>26.15</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.377273</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.833652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.441328</td>\n",
       "      <td>-0.088447</td>\n",
       "      <td>-2.182540</td>\n",
       "      <td>-0.449952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>4</td>\n",
       "      <td>706</td>\n",
       "      <td>16.38</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.023201</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.460000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>43.101343</td>\n",
       "      <td>0.305250</td>\n",
       "      <td>0.305250</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>0.453798</td>\n",
       "      <td>0.109272</td>\n",
       "      <td>1.300240</td>\n",
       "      <td>0.426456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>47.41</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>1.156341</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.160667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.864796</td>\n",
       "      <td>0.147648</td>\n",
       "      <td>0.042185</td>\n",
       "      <td>0.084370</td>\n",
       "      <td>0.249387</td>\n",
       "      <td>-0.108368</td>\n",
       "      <td>1.411381</td>\n",
       "      <td>-0.540051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>4</td>\n",
       "      <td>594</td>\n",
       "      <td>63.48</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.043771</td>\n",
       "      <td>0.106869</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.441538</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>9.357278</td>\n",
       "      <td>0.094518</td>\n",
       "      <td>0.094518</td>\n",
       "      <td>0.031506</td>\n",
       "      <td>-0.213568</td>\n",
       "      <td>0.090515</td>\n",
       "      <td>-0.614394</td>\n",
       "      <td>0.516220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>1044</td>\n",
       "      <td>51.44</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.049272</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.857778</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>20.295490</td>\n",
       "      <td>0.174961</td>\n",
       "      <td>0.077760</td>\n",
       "      <td>0.077760</td>\n",
       "      <td>0.239975</td>\n",
       "      <td>-0.507987</td>\n",
       "      <td>1.360582</td>\n",
       "      <td>-2.396240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5</td>\n",
       "      <td>650</td>\n",
       "      <td>59.73</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.091892</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.991000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>10.882304</td>\n",
       "      <td>0.184162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016742</td>\n",
       "      <td>-0.460335</td>\n",
       "      <td>0.283556</td>\n",
       "      <td>-1.834735</td>\n",
       "      <td>1.790446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>5</td>\n",
       "      <td>338</td>\n",
       "      <td>43.91</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.129911</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.910000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.017751</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>7.697563</td>\n",
       "      <td>0.136643</td>\n",
       "      <td>0.045548</td>\n",
       "      <td>0.091095</td>\n",
       "      <td>0.660309</td>\n",
       "      <td>0.316781</td>\n",
       "      <td>3.475505</td>\n",
       "      <td>0.123218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>3</td>\n",
       "      <td>580</td>\n",
       "      <td>88.44</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.152483</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.440000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.010345</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>6.558118</td>\n",
       "      <td>0.113071</td>\n",
       "      <td>0.067843</td>\n",
       "      <td>0.056536</td>\n",
       "      <td>1.060829</td>\n",
       "      <td>-0.069564</td>\n",
       "      <td>3.626117</td>\n",
       "      <td>0.108674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>6</td>\n",
       "      <td>314</td>\n",
       "      <td>77.71</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.070064</td>\n",
       "      <td>0.247484</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.532273</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>0.031847</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>4.040664</td>\n",
       "      <td>0.064342</td>\n",
       "      <td>0.128684</td>\n",
       "      <td>0.064342</td>\n",
       "      <td>0.409609</td>\n",
       "      <td>0.490560</td>\n",
       "      <td>1.897657</td>\n",
       "      <td>2.218347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2</td>\n",
       "      <td>1625</td>\n",
       "      <td>72.28</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>0.044480</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.035000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.005538</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>22.482014</td>\n",
       "      <td>0.124516</td>\n",
       "      <td>0.027670</td>\n",
       "      <td>0.027670</td>\n",
       "      <td>-0.010886</td>\n",
       "      <td>-0.293898</td>\n",
       "      <td>-0.526151</td>\n",
       "      <td>-1.437430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>4</td>\n",
       "      <td>1225</td>\n",
       "      <td>27.98</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013061</td>\n",
       "      <td>0.022841</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.748750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>43.781272</td>\n",
       "      <td>0.285919</td>\n",
       "      <td>0.357398</td>\n",
       "      <td>0.035740</td>\n",
       "      <td>-0.256314</td>\n",
       "      <td>0.108177</td>\n",
       "      <td>-1.739169</td>\n",
       "      <td>0.482357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>666</td>\n",
       "      <td>97.66</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034535</td>\n",
       "      <td>0.146637</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.246087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.006006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006006</td>\n",
       "      <td>6.819578</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.143085</td>\n",
       "      <td>-0.518497</td>\n",
       "      <td>1.288936</td>\n",
       "      <td>-2.446570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>405</td>\n",
       "      <td>62.14</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.153432</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.142667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.517541</td>\n",
       "      <td>0.032185</td>\n",
       "      <td>0.128742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.462443</td>\n",
       "      <td>-0.491470</td>\n",
       "      <td>-2.103142</td>\n",
       "      <td>-1.925902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>403</td>\n",
       "      <td>38.54</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027295</td>\n",
       "      <td>0.095633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.503636</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019851</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>10.456668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207577</td>\n",
       "      <td>0.025947</td>\n",
       "      <td>-0.250854</td>\n",
       "      <td>-0.091152</td>\n",
       "      <td>-1.569144</td>\n",
       "      <td>-0.455662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>527</td>\n",
       "      <td>79.43</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.150721</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39.715000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018975</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>6.634773</td>\n",
       "      <td>0.125897</td>\n",
       "      <td>0.075538</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>0.211066</td>\n",
       "      <td>0.323883</td>\n",
       "      <td>-0.321653</td>\n",
       "      <td>0.723815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>652</td>\n",
       "      <td>64.01</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.802000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>10.185908</td>\n",
       "      <td>0.093735</td>\n",
       "      <td>0.031245</td>\n",
       "      <td>0.062490</td>\n",
       "      <td>0.397474</td>\n",
       "      <td>-0.496382</td>\n",
       "      <td>1.219413</td>\n",
       "      <td>-1.757077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>4</td>\n",
       "      <td>1740</td>\n",
       "      <td>83.60</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004023</td>\n",
       "      <td>0.048046</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.942857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.004023</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>20.813397</td>\n",
       "      <td>0.011962</td>\n",
       "      <td>0.083732</td>\n",
       "      <td>0.035885</td>\n",
       "      <td>0.190354</td>\n",
       "      <td>0.108247</td>\n",
       "      <td>0.396245</td>\n",
       "      <td>0.433069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2</td>\n",
       "      <td>1794</td>\n",
       "      <td>29.34</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.011148</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.467000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>61.145194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306748</td>\n",
       "      <td>0.136333</td>\n",
       "      <td>0.199407</td>\n",
       "      <td>-0.306850</td>\n",
       "      <td>1.385703</td>\n",
       "      <td>-1.713234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>3</td>\n",
       "      <td>1077</td>\n",
       "      <td>86.33</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.080158</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.776667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005571</td>\n",
       "      <td>0.009285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.475385</td>\n",
       "      <td>0.069501</td>\n",
       "      <td>0.115835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.227794</td>\n",
       "      <td>-0.071145</td>\n",
       "      <td>-1.494627</td>\n",
       "      <td>-0.318647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2</td>\n",
       "      <td>1258</td>\n",
       "      <td>36.61</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.029102</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.152500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.006359</td>\n",
       "      <td>0.003975</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>34.362196</td>\n",
       "      <td>0.218520</td>\n",
       "      <td>0.136575</td>\n",
       "      <td>0.081945</td>\n",
       "      <td>0.258578</td>\n",
       "      <td>-0.288120</td>\n",
       "      <td>0.387946</td>\n",
       "      <td>-1.289043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>6</td>\n",
       "      <td>1377</td>\n",
       "      <td>47.45</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.034459</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.450000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>29.020021</td>\n",
       "      <td>0.063224</td>\n",
       "      <td>0.126449</td>\n",
       "      <td>0.105374</td>\n",
       "      <td>0.915379</td>\n",
       "      <td>0.520606</td>\n",
       "      <td>3.539910</td>\n",
       "      <td>0.124749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>3</td>\n",
       "      <td>1788</td>\n",
       "      <td>24.98</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.012304</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.135455</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>71.577262</td>\n",
       "      <td>0.360288</td>\n",
       "      <td>0.080064</td>\n",
       "      <td>0.120096</td>\n",
       "      <td>-0.013854</td>\n",
       "      <td>-0.112010</td>\n",
       "      <td>0.345096</td>\n",
       "      <td>-0.550858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2</td>\n",
       "      <td>1162</td>\n",
       "      <td>37.26</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.032065</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.452000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>31.186259</td>\n",
       "      <td>0.268384</td>\n",
       "      <td>0.161031</td>\n",
       "      <td>0.053677</td>\n",
       "      <td>0.066070</td>\n",
       "      <td>-0.285305</td>\n",
       "      <td>-0.495923</td>\n",
       "      <td>-1.313870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>4</td>\n",
       "      <td>979</td>\n",
       "      <td>8.86</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027579</td>\n",
       "      <td>0.009050</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.328148</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>110.496614</td>\n",
       "      <td>0.902935</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.225734</td>\n",
       "      <td>-0.254634</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>-0.588204</td>\n",
       "      <td>0.536142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022147</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.294615</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>153.263708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.349869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.463786</td>\n",
       "      <td>-0.489571</td>\n",
       "      <td>-2.020708</td>\n",
       "      <td>-1.864661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>5</td>\n",
       "      <td>1777</td>\n",
       "      <td>67.23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.037833</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.410000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>26.431653</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>-0.174948</td>\n",
       "      <td>0.310909</td>\n",
       "      <td>-1.237590</td>\n",
       "      <td>1.276763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>1685</td>\n",
       "      <td>62.39</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017804</td>\n",
       "      <td>0.037027</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.079667</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>27.007533</td>\n",
       "      <td>0.048085</td>\n",
       "      <td>0.112197</td>\n",
       "      <td>0.032056</td>\n",
       "      <td>-0.266381</td>\n",
       "      <td>-0.512390</td>\n",
       "      <td>-0.639070</td>\n",
       "      <td>-2.522591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2</td>\n",
       "      <td>1246</td>\n",
       "      <td>2.33</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.110952</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>534.763948</td>\n",
       "      <td>3.862661</td>\n",
       "      <td>0.858369</td>\n",
       "      <td>0.429185</td>\n",
       "      <td>-0.376011</td>\n",
       "      <td>-0.305903</td>\n",
       "      <td>-1.770405</td>\n",
       "      <td>-1.724881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>6</td>\n",
       "      <td>303</td>\n",
       "      <td>54.13</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.178647</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.255417</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.026403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.597635</td>\n",
       "      <td>0.036948</td>\n",
       "      <td>0.147792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.565796</td>\n",
       "      <td>0.498713</td>\n",
       "      <td>-2.246209</td>\n",
       "      <td>2.142398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2</td>\n",
       "      <td>1095</td>\n",
       "      <td>9.44</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015525</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.555294</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.010046</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>115.995763</td>\n",
       "      <td>1.165254</td>\n",
       "      <td>0.635593</td>\n",
       "      <td>0.105932</td>\n",
       "      <td>-0.288443</td>\n",
       "      <td>-0.296690</td>\n",
       "      <td>-1.678647</td>\n",
       "      <td>-1.623752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>51.57</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>6.446250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.785000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.317583</td>\n",
       "      <td>-0.479721</td>\n",
       "      <td>-1.649683</td>\n",
       "      <td>-1.441282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>3</td>\n",
       "      <td>1125</td>\n",
       "      <td>64.29</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.057147</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.430000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007111</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.498833</td>\n",
       "      <td>0.124436</td>\n",
       "      <td>0.139991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.236115</td>\n",
       "      <td>-0.072329</td>\n",
       "      <td>-1.509923</td>\n",
       "      <td>-0.325212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2</td>\n",
       "      <td>1394</td>\n",
       "      <td>86.22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.061851</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.071765</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>16.167942</td>\n",
       "      <td>0.011598</td>\n",
       "      <td>0.023196</td>\n",
       "      <td>0.011598</td>\n",
       "      <td>-0.344384</td>\n",
       "      <td>-0.302984</td>\n",
       "      <td>-1.705218</td>\n",
       "      <td>-1.615325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>3</td>\n",
       "      <td>478</td>\n",
       "      <td>4.48</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.039749</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.235789</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.023013</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>106.696429</td>\n",
       "      <td>2.455357</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>-0.114081</td>\n",
       "      <td>-0.099461</td>\n",
       "      <td>-0.564797</td>\n",
       "      <td>-0.530682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>6</td>\n",
       "      <td>1746</td>\n",
       "      <td>68.99</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.039513</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.832778</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>25.308016</td>\n",
       "      <td>0.028990</td>\n",
       "      <td>0.101464</td>\n",
       "      <td>0.028990</td>\n",
       "      <td>-0.136049</td>\n",
       "      <td>0.498734</td>\n",
       "      <td>-0.591832</td>\n",
       "      <td>2.384673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>3</td>\n",
       "      <td>1700</td>\n",
       "      <td>21.46</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.012624</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.682500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>79.217148</td>\n",
       "      <td>0.139795</td>\n",
       "      <td>0.465983</td>\n",
       "      <td>0.232992</td>\n",
       "      <td>0.558587</td>\n",
       "      <td>-0.094934</td>\n",
       "      <td>1.831263</td>\n",
       "      <td>-0.519757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6</td>\n",
       "      <td>961</td>\n",
       "      <td>58.21</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019771</td>\n",
       "      <td>0.060572</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.063684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.509191</td>\n",
       "      <td>0.103075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.548874</td>\n",
       "      <td>0.495751</td>\n",
       "      <td>-2.238045</td>\n",
       "      <td>2.114063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>3</td>\n",
       "      <td>352</td>\n",
       "      <td>57.85</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008523</td>\n",
       "      <td>0.164347</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.283333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.084702</td>\n",
       "      <td>0.034572</td>\n",
       "      <td>0.069144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.336361</td>\n",
       "      <td>-0.082467</td>\n",
       "      <td>-1.916044</td>\n",
       "      <td>-0.345205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2</td>\n",
       "      <td>1154</td>\n",
       "      <td>26.67</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017331</td>\n",
       "      <td>0.023111</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.333500</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>0.003466</td>\n",
       "      <td>43.269591</td>\n",
       "      <td>0.262467</td>\n",
       "      <td>0.337458</td>\n",
       "      <td>0.149981</td>\n",
       "      <td>0.235012</td>\n",
       "      <td>-0.305194</td>\n",
       "      <td>1.454368</td>\n",
       "      <td>-1.824861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>3</td>\n",
       "      <td>470</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.017553</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.014894</td>\n",
       "      <td>0.014894</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>56.969697</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.042272</td>\n",
       "      <td>-0.086112</td>\n",
       "      <td>-0.470571</td>\n",
       "      <td>-0.417043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2</td>\n",
       "      <td>618</td>\n",
       "      <td>90.66</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021036</td>\n",
       "      <td>0.146699</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.973846</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.016181</td>\n",
       "      <td>0.008091</td>\n",
       "      <td>6.816678</td>\n",
       "      <td>0.022060</td>\n",
       "      <td>0.110302</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>0.508359</td>\n",
       "      <td>-0.300070</td>\n",
       "      <td>1.886692</td>\n",
       "      <td>-1.461049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>4</td>\n",
       "      <td>301</td>\n",
       "      <td>43.08</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036545</td>\n",
       "      <td>0.143123</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.916364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.013289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>6.987001</td>\n",
       "      <td>0.092851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>-0.289943</td>\n",
       "      <td>0.100431</td>\n",
       "      <td>-1.631992</td>\n",
       "      <td>0.475209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>5</td>\n",
       "      <td>1607</td>\n",
       "      <td>11.70</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.007281</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.925000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>137.350427</td>\n",
       "      <td>0.598291</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.328511</td>\n",
       "      <td>0.319326</td>\n",
       "      <td>-1.799030</td>\n",
       "      <td>1.136609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1</td>\n",
       "      <td>488</td>\n",
       "      <td>35.97</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014344</td>\n",
       "      <td>0.073709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.138571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.566861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.429844</td>\n",
       "      <td>-0.490044</td>\n",
       "      <td>-1.923349</td>\n",
       "      <td>-1.716013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>3</td>\n",
       "      <td>1466</td>\n",
       "      <td>18.47</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017735</td>\n",
       "      <td>0.012599</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.710385</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>79.371955</td>\n",
       "      <td>0.054142</td>\n",
       "      <td>0.378993</td>\n",
       "      <td>0.108284</td>\n",
       "      <td>-0.246775</td>\n",
       "      <td>-0.109779</td>\n",
       "      <td>-0.642605</td>\n",
       "      <td>-0.511483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4</td>\n",
       "      <td>1040</td>\n",
       "      <td>65.36</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020192</td>\n",
       "      <td>0.062846</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.112381</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.911873</td>\n",
       "      <td>0.152999</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.525571</td>\n",
       "      <td>0.098653</td>\n",
       "      <td>-2.509734</td>\n",
       "      <td>0.494890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>6</td>\n",
       "      <td>455</td>\n",
       "      <td>42.89</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.094264</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.299231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.017582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>10.608533</td>\n",
       "      <td>0.186524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069946</td>\n",
       "      <td>0.076987</td>\n",
       "      <td>0.493971</td>\n",
       "      <td>0.377488</td>\n",
       "      <td>2.139227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>4</td>\n",
       "      <td>1695</td>\n",
       "      <td>8.71</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.005139</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.580667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>194.603904</td>\n",
       "      <td>1.033295</td>\n",
       "      <td>0.229621</td>\n",
       "      <td>0.229621</td>\n",
       "      <td>-0.123339</td>\n",
       "      <td>0.097346</td>\n",
       "      <td>-0.585983</td>\n",
       "      <td>0.502683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>4</td>\n",
       "      <td>518</td>\n",
       "      <td>35.77</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.042471</td>\n",
       "      <td>0.069054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.625909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>14.481409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055913</td>\n",
       "      <td>0.055913</td>\n",
       "      <td>-0.238255</td>\n",
       "      <td>0.087977</td>\n",
       "      <td>-0.624167</td>\n",
       "      <td>0.530130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6</td>\n",
       "      <td>1183</td>\n",
       "      <td>55.17</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015216</td>\n",
       "      <td>0.046636</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.065000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>21.442813</td>\n",
       "      <td>0.163132</td>\n",
       "      <td>0.163132</td>\n",
       "      <td>0.054377</td>\n",
       "      <td>0.094516</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.372490</td>\n",
       "      <td>2.355158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>3</td>\n",
       "      <td>1230</td>\n",
       "      <td>80.65</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.065569</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.688333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>15.251085</td>\n",
       "      <td>0.123993</td>\n",
       "      <td>0.012399</td>\n",
       "      <td>0.037198</td>\n",
       "      <td>-0.086448</td>\n",
       "      <td>-0.120337</td>\n",
       "      <td>0.342721</td>\n",
       "      <td>-0.582871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>6</td>\n",
       "      <td>571</td>\n",
       "      <td>14.13</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.026270</td>\n",
       "      <td>0.024746</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>0.014011</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>40.410474</td>\n",
       "      <td>0.424628</td>\n",
       "      <td>0.566171</td>\n",
       "      <td>0.353857</td>\n",
       "      <td>0.461146</td>\n",
       "      <td>0.494981</td>\n",
       "      <td>1.952633</td>\n",
       "      <td>2.082048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2</td>\n",
       "      <td>252</td>\n",
       "      <td>26.07</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.103452</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.738000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>9.666283</td>\n",
       "      <td>0.191791</td>\n",
       "      <td>0.345224</td>\n",
       "      <td>0.076717</td>\n",
       "      <td>-0.083666</td>\n",
       "      <td>-0.295489</td>\n",
       "      <td>-0.549774</td>\n",
       "      <td>-1.630075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1</td>\n",
       "      <td>806</td>\n",
       "      <td>37.38</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>0.046377</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.340000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>21.562333</td>\n",
       "      <td>0.133761</td>\n",
       "      <td>0.133761</td>\n",
       "      <td>0.080257</td>\n",
       "      <td>0.178951</td>\n",
       "      <td>-0.493633</td>\n",
       "      <td>0.390586</td>\n",
       "      <td>-1.946326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1</td>\n",
       "      <td>1798</td>\n",
       "      <td>4.14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>434.299517</td>\n",
       "      <td>0.966184</td>\n",
       "      <td>1.932367</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.114780</td>\n",
       "      <td>-0.496235</td>\n",
       "      <td>0.379510</td>\n",
       "      <td>-2.147009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1</td>\n",
       "      <td>583</td>\n",
       "      <td>86.07</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.029160</td>\n",
       "      <td>0.147633</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.062941</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.015437</td>\n",
       "      <td>0.015437</td>\n",
       "      <td>0.008576</td>\n",
       "      <td>6.773556</td>\n",
       "      <td>0.104566</td>\n",
       "      <td>0.104566</td>\n",
       "      <td>0.058092</td>\n",
       "      <td>0.485475</td>\n",
       "      <td>-0.503528</td>\n",
       "      <td>1.881517</td>\n",
       "      <td>-2.084089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>5</td>\n",
       "      <td>735</td>\n",
       "      <td>48.40</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010884</td>\n",
       "      <td>0.065850</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.050000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.005442</td>\n",
       "      <td>15.185950</td>\n",
       "      <td>0.041322</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.353275</td>\n",
       "      <td>0.303670</td>\n",
       "      <td>1.336312</td>\n",
       "      <td>1.440252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6</td>\n",
       "      <td>926</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.026998</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>228.641975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.481481</td>\n",
       "      <td>1.234568</td>\n",
       "      <td>0.295811</td>\n",
       "      <td>0.480973</td>\n",
       "      <td>1.946658</td>\n",
       "      <td>2.329079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>3</td>\n",
       "      <td>1789</td>\n",
       "      <td>46.50</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.025992</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>38.473118</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.289340</td>\n",
       "      <td>-0.090152</td>\n",
       "      <td>0.546109</td>\n",
       "      <td>-0.407515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>5</td>\n",
       "      <td>1387</td>\n",
       "      <td>58.57</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.042228</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.570000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>23.681065</td>\n",
       "      <td>0.051221</td>\n",
       "      <td>0.119515</td>\n",
       "      <td>0.017074</td>\n",
       "      <td>0.047770</td>\n",
       "      <td>0.330549</td>\n",
       "      <td>3.250104</td>\n",
       "      <td>0.146613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3</td>\n",
       "      <td>252</td>\n",
       "      <td>72.77</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.128333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.043651</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>3.462966</td>\n",
       "      <td>0.151161</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>0.068710</td>\n",
       "      <td>0.591178</td>\n",
       "      <td>-0.100129</td>\n",
       "      <td>1.660981</td>\n",
       "      <td>-0.425891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6</td>\n",
       "      <td>133</td>\n",
       "      <td>13.79</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.103684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.644670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.600899</td>\n",
       "      <td>0.493726</td>\n",
       "      <td>-2.185368</td>\n",
       "      <td>2.021419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>5</td>\n",
       "      <td>1784</td>\n",
       "      <td>11.42</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.012892</td>\n",
       "      <td>0.006401</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.496522</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>156.217163</td>\n",
       "      <td>0.350263</td>\n",
       "      <td>0.437828</td>\n",
       "      <td>0.350263</td>\n",
       "      <td>0.149957</td>\n",
       "      <td>0.286002</td>\n",
       "      <td>1.517198</td>\n",
       "      <td>1.945273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2</td>\n",
       "      <td>392</td>\n",
       "      <td>20.86</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.053214</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.738333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.022959</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>18.791946</td>\n",
       "      <td>0.431448</td>\n",
       "      <td>0.239693</td>\n",
       "      <td>0.095877</td>\n",
       "      <td>-0.058865</td>\n",
       "      <td>-0.295941</td>\n",
       "      <td>-0.519896</td>\n",
       "      <td>-1.583911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6</td>\n",
       "      <td>1400</td>\n",
       "      <td>61.12</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.043657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.560000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.905759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.276335</td>\n",
       "      <td>0.525029</td>\n",
       "      <td>-1.310742</td>\n",
       "      <td>1.230293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>4</td>\n",
       "      <td>486</td>\n",
       "      <td>94.09</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.026749</td>\n",
       "      <td>0.193601</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.237692</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.022634</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>5.165267</td>\n",
       "      <td>0.116909</td>\n",
       "      <td>0.095653</td>\n",
       "      <td>0.053141</td>\n",
       "      <td>0.550154</td>\n",
       "      <td>0.101063</td>\n",
       "      <td>1.911370</td>\n",
       "      <td>0.481390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>82.36</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>0.232655</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.118000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>4.298203</td>\n",
       "      <td>0.048567</td>\n",
       "      <td>0.036425</td>\n",
       "      <td>0.048567</td>\n",
       "      <td>0.194579</td>\n",
       "      <td>-0.512313</td>\n",
       "      <td>1.333194</td>\n",
       "      <td>-2.401191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>2</td>\n",
       "      <td>1077</td>\n",
       "      <td>85.23</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022284</td>\n",
       "      <td>0.079136</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.551250</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.636396</td>\n",
       "      <td>0.011733</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.560821</td>\n",
       "      <td>-0.300450</td>\n",
       "      <td>-2.395571</td>\n",
       "      <td>-1.588696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>6</td>\n",
       "      <td>986</td>\n",
       "      <td>38.30</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011156</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.481818</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>0.004057</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>25.744125</td>\n",
       "      <td>0.052219</td>\n",
       "      <td>0.104439</td>\n",
       "      <td>0.052219</td>\n",
       "      <td>-0.087083</td>\n",
       "      <td>0.501789</td>\n",
       "      <td>-0.515210</td>\n",
       "      <td>2.188878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>4</td>\n",
       "      <td>883</td>\n",
       "      <td>71.85</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>0.081370</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.012458</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>12.289492</td>\n",
       "      <td>0.153097</td>\n",
       "      <td>0.125261</td>\n",
       "      <td>0.027836</td>\n",
       "      <td>0.042803</td>\n",
       "      <td>0.113504</td>\n",
       "      <td>-0.527827</td>\n",
       "      <td>0.491065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>3</td>\n",
       "      <td>1064</td>\n",
       "      <td>13.03</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022556</td>\n",
       "      <td>0.012246</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.542917</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.657713</td>\n",
       "      <td>0.844206</td>\n",
       "      <td>0.460476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.548807</td>\n",
       "      <td>-0.101111</td>\n",
       "      <td>-2.508782</td>\n",
       "      <td>-0.502444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>4</td>\n",
       "      <td>1387</td>\n",
       "      <td>45.70</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.019466</td>\n",
       "      <td>0.032949</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.692593</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>30.350109</td>\n",
       "      <td>0.087527</td>\n",
       "      <td>0.153173</td>\n",
       "      <td>0.087527</td>\n",
       "      <td>0.131617</td>\n",
       "      <td>0.084744</td>\n",
       "      <td>1.619102</td>\n",
       "      <td>0.531306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>6</td>\n",
       "      <td>701</td>\n",
       "      <td>97.76</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.037090</td>\n",
       "      <td>0.139458</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.760000</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.012839</td>\n",
       "      <td>0.004280</td>\n",
       "      <td>7.170622</td>\n",
       "      <td>0.040917</td>\n",
       "      <td>0.092062</td>\n",
       "      <td>0.030687</td>\n",
       "      <td>-0.007980</td>\n",
       "      <td>0.490875</td>\n",
       "      <td>0.332471</td>\n",
       "      <td>2.474133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>4</td>\n",
       "      <td>1649</td>\n",
       "      <td>88.15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.011522</td>\n",
       "      <td>0.053457</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.639474</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.004245</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>18.706750</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>0.079410</td>\n",
       "      <td>0.034033</td>\n",
       "      <td>0.037613</td>\n",
       "      <td>0.095235</td>\n",
       "      <td>0.363114</td>\n",
       "      <td>0.500594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>6</td>\n",
       "      <td>1074</td>\n",
       "      <td>24.42</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023277</td>\n",
       "      <td>0.022737</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.976800</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.980344</td>\n",
       "      <td>0.368550</td>\n",
       "      <td>0.245700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.566490</td>\n",
       "      <td>0.497118</td>\n",
       "      <td>-2.260485</td>\n",
       "      <td>2.198636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>68.76</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.892987</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>1.119837</td>\n",
       "      <td>0.159977</td>\n",
       "      <td>0.145433</td>\n",
       "      <td>0.058173</td>\n",
       "      <td>0.302100</td>\n",
       "      <td>-0.500545</td>\n",
       "      <td>1.288634</td>\n",
       "      <td>-2.168150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1</td>\n",
       "      <td>1598</td>\n",
       "      <td>16.47</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018773</td>\n",
       "      <td>0.010307</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.024894</td>\n",
       "      <td>0.364299</td>\n",
       "      <td>0.060716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.674898</td>\n",
       "      <td>-0.513560</td>\n",
       "      <td>-2.257268</td>\n",
       "      <td>-2.174318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>29.44</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.840000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.944000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.305707</td>\n",
       "      <td>0.033967</td>\n",
       "      <td>0.169837</td>\n",
       "      <td>0.495087</td>\n",
       "      <td>-0.506332</td>\n",
       "      <td>1.784030</td>\n",
       "      <td>-1.787131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1</td>\n",
       "      <td>1307</td>\n",
       "      <td>36.94</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005356</td>\n",
       "      <td>0.028263</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.277143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>35.381700</td>\n",
       "      <td>0.108284</td>\n",
       "      <td>0.027071</td>\n",
       "      <td>0.108284</td>\n",
       "      <td>0.326265</td>\n",
       "      <td>-0.501366</td>\n",
       "      <td>1.263510</td>\n",
       "      <td>-1.938844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2</td>\n",
       "      <td>1437</td>\n",
       "      <td>49.61</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>0.034523</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.611053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>28.965934</td>\n",
       "      <td>0.040314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080629</td>\n",
       "      <td>0.161229</td>\n",
       "      <td>-0.315550</td>\n",
       "      <td>1.420020</td>\n",
       "      <td>-1.908916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>5</td>\n",
       "      <td>872</td>\n",
       "      <td>18.26</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010321</td>\n",
       "      <td>0.020940</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.028889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.008028</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>47.754655</td>\n",
       "      <td>0.164294</td>\n",
       "      <td>0.383352</td>\n",
       "      <td>0.109529</td>\n",
       "      <td>-0.039924</td>\n",
       "      <td>0.307610</td>\n",
       "      <td>-0.502431</td>\n",
       "      <td>1.561197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>6</td>\n",
       "      <td>498</td>\n",
       "      <td>56.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014056</td>\n",
       "      <td>0.114116</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.118571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.762977</td>\n",
       "      <td>0.017596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.440682</td>\n",
       "      <td>0.506328</td>\n",
       "      <td>-1.885966</td>\n",
       "      <td>1.746130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>5</td>\n",
       "      <td>174</td>\n",
       "      <td>21.04</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.120920</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.337778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.028736</td>\n",
       "      <td>8.269962</td>\n",
       "      <td>0.285171</td>\n",
       "      <td>0.285171</td>\n",
       "      <td>0.237643</td>\n",
       "      <td>0.528609</td>\n",
       "      <td>0.299278</td>\n",
       "      <td>1.921933</td>\n",
       "      <td>1.361451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>3</td>\n",
       "      <td>1273</td>\n",
       "      <td>4.72</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013354</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.277647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>269.703390</td>\n",
       "      <td>1.483051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.059322</td>\n",
       "      <td>0.378371</td>\n",
       "      <td>-0.115637</td>\n",
       "      <td>2.115145</td>\n",
       "      <td>-0.579228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>6</td>\n",
       "      <td>1019</td>\n",
       "      <td>63.42</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012758</td>\n",
       "      <td>0.062237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.878462</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007851</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>16.067487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126143</td>\n",
       "      <td>0.015768</td>\n",
       "      <td>-0.266321</td>\n",
       "      <td>0.506886</td>\n",
       "      <td>-1.476022</td>\n",
       "      <td>2.055631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>5</td>\n",
       "      <td>1706</td>\n",
       "      <td>49.16</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016999</td>\n",
       "      <td>0.028816</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.695172</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>34.703011</td>\n",
       "      <td>0.223759</td>\n",
       "      <td>0.101709</td>\n",
       "      <td>0.020342</td>\n",
       "      <td>-0.417305</td>\n",
       "      <td>0.290406</td>\n",
       "      <td>-1.838035</td>\n",
       "      <td>1.807709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2</td>\n",
       "      <td>475</td>\n",
       "      <td>34.30</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056842</td>\n",
       "      <td>0.072211</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.270370</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016842</td>\n",
       "      <td>0.012632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.848397</td>\n",
       "      <td>0.233236</td>\n",
       "      <td>0.174927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.589245</td>\n",
       "      <td>-0.304641</td>\n",
       "      <td>-2.434409</td>\n",
       "      <td>-1.585167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2</td>\n",
       "      <td>934</td>\n",
       "      <td>56.45</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016060</td>\n",
       "      <td>0.060439</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.763333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>16.545616</td>\n",
       "      <td>0.159433</td>\n",
       "      <td>0.177148</td>\n",
       "      <td>0.035430</td>\n",
       "      <td>-0.044783</td>\n",
       "      <td>-0.292619</td>\n",
       "      <td>-0.550653</td>\n",
       "      <td>-1.667654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>6</td>\n",
       "      <td>605</td>\n",
       "      <td>91.49</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039669</td>\n",
       "      <td>0.151223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.812083</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.612745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.593271</td>\n",
       "      <td>0.494126</td>\n",
       "      <td>-2.242729</td>\n",
       "      <td>2.116446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>5</td>\n",
       "      <td>1001</td>\n",
       "      <td>17.92</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018981</td>\n",
       "      <td>0.017902</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.943158</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>55.859375</td>\n",
       "      <td>0.279018</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.167411</td>\n",
       "      <td>0.012420</td>\n",
       "      <td>0.291662</td>\n",
       "      <td>0.363158</td>\n",
       "      <td>1.954131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2</td>\n",
       "      <td>1621</td>\n",
       "      <td>33.91</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017273</td>\n",
       "      <td>0.020919</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.211071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.803008</td>\n",
       "      <td>0.235919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.645039</td>\n",
       "      <td>-0.312165</td>\n",
       "      <td>-2.441574</td>\n",
       "      <td>-1.590580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1</td>\n",
       "      <td>1197</td>\n",
       "      <td>11.15</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010860</td>\n",
       "      <td>0.009315</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.857692</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>107.354260</td>\n",
       "      <td>0.089686</td>\n",
       "      <td>0.807175</td>\n",
       "      <td>0.269058</td>\n",
       "      <td>0.097118</td>\n",
       "      <td>-0.497028</td>\n",
       "      <td>0.380270</td>\n",
       "      <td>-2.195638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>33.51</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.193333</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.155517</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>4.476276</td>\n",
       "      <td>0.238735</td>\n",
       "      <td>0.089526</td>\n",
       "      <td>0.029842</td>\n",
       "      <td>-0.450280</td>\n",
       "      <td>-0.312841</td>\n",
       "      <td>-1.811161</td>\n",
       "      <td>-1.732991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>36.88</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.024587</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.048889</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>40.672451</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.162690</td>\n",
       "      <td>0.108460</td>\n",
       "      <td>0.207546</td>\n",
       "      <td>0.491452</td>\n",
       "      <td>1.395871</td>\n",
       "      <td>2.374583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2</td>\n",
       "      <td>817</td>\n",
       "      <td>24.61</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012240</td>\n",
       "      <td>0.030122</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.461000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.013464</td>\n",
       "      <td>0.012240</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>33.197887</td>\n",
       "      <td>0.446973</td>\n",
       "      <td>0.406339</td>\n",
       "      <td>0.081268</td>\n",
       "      <td>0.018372</td>\n",
       "      <td>-0.287020</td>\n",
       "      <td>-0.559148</td>\n",
       "      <td>-1.401861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2</td>\n",
       "      <td>1525</td>\n",
       "      <td>1.98</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004590</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>770.202020</td>\n",
       "      <td>3.535354</td>\n",
       "      <td>1.515152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.316800</td>\n",
       "      <td>-0.280733</td>\n",
       "      <td>-1.658054</td>\n",
       "      <td>-1.095683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>5</td>\n",
       "      <td>478</td>\n",
       "      <td>84.37</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.039749</td>\n",
       "      <td>0.176506</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.440526</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.010460</td>\n",
       "      <td>0.010460</td>\n",
       "      <td>5.665521</td>\n",
       "      <td>0.071115</td>\n",
       "      <td>0.059263</td>\n",
       "      <td>0.059263</td>\n",
       "      <td>0.414915</td>\n",
       "      <td>0.288380</td>\n",
       "      <td>2.070851</td>\n",
       "      <td>1.650042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>5</td>\n",
       "      <td>972</td>\n",
       "      <td>71.76</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.073827</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.352000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.008230</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>13.545151</td>\n",
       "      <td>0.083612</td>\n",
       "      <td>0.111483</td>\n",
       "      <td>0.041806</td>\n",
       "      <td>0.264242</td>\n",
       "      <td>0.313588</td>\n",
       "      <td>0.379195</td>\n",
       "      <td>1.312437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>3</td>\n",
       "      <td>509</td>\n",
       "      <td>21.03</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035363</td>\n",
       "      <td>0.041316</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.168333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.203519</td>\n",
       "      <td>0.047551</td>\n",
       "      <td>0.142653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.553342</td>\n",
       "      <td>-0.101330</td>\n",
       "      <td>-2.473010</td>\n",
       "      <td>-0.486504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>5</td>\n",
       "      <td>84</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.060833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.425833</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>16.438356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782779</td>\n",
       "      <td>0.195695</td>\n",
       "      <td>-0.306115</td>\n",
       "      <td>0.302235</td>\n",
       "      <td>-1.567058</td>\n",
       "      <td>1.470711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>4</td>\n",
       "      <td>373</td>\n",
       "      <td>36.81</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072386</td>\n",
       "      <td>0.098686</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.363333</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010724</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.133116</td>\n",
       "      <td>0.108666</td>\n",
       "      <td>0.135833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.615962</td>\n",
       "      <td>0.092926</td>\n",
       "      <td>-2.577707</td>\n",
       "      <td>0.498381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>3</td>\n",
       "      <td>1315</td>\n",
       "      <td>64.80</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.049278</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.005323</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>20.293210</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.108025</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.437612</td>\n",
       "      <td>-0.090601</td>\n",
       "      <td>1.287018</td>\n",
       "      <td>-0.483106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>3</td>\n",
       "      <td>760</td>\n",
       "      <td>32.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.011842</td>\n",
       "      <td>0.043289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.655556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>23.100304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121581</td>\n",
       "      <td>0.264727</td>\n",
       "      <td>-0.106542</td>\n",
       "      <td>1.449901</td>\n",
       "      <td>-0.521575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>5</td>\n",
       "      <td>1808</td>\n",
       "      <td>44.36</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.013827</td>\n",
       "      <td>0.024535</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.774400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>40.757439</td>\n",
       "      <td>0.112714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067628</td>\n",
       "      <td>-0.076099</td>\n",
       "      <td>0.281653</td>\n",
       "      <td>0.394807</td>\n",
       "      <td>1.997327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2</td>\n",
       "      <td>1689</td>\n",
       "      <td>23.54</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.362857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>71.750212</td>\n",
       "      <td>0.424809</td>\n",
       "      <td>0.297366</td>\n",
       "      <td>0.212404</td>\n",
       "      <td>0.593354</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>1.767289</td>\n",
       "      <td>-1.242308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>73.73</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>1.152031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.266429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.868032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054252</td>\n",
       "      <td>0.040689</td>\n",
       "      <td>0.064350</td>\n",
       "      <td>0.495696</td>\n",
       "      <td>0.348556</td>\n",
       "      <td>2.204473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>5</td>\n",
       "      <td>1005</td>\n",
       "      <td>45.66</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.026866</td>\n",
       "      <td>0.045433</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.691111</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>22.010512</td>\n",
       "      <td>0.087604</td>\n",
       "      <td>0.043802</td>\n",
       "      <td>0.087604</td>\n",
       "      <td>0.095280</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>1.511743</td>\n",
       "      <td>1.986378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>3</td>\n",
       "      <td>993</td>\n",
       "      <td>57.11</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020141</td>\n",
       "      <td>0.057513</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.855500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010070</td>\n",
       "      <td>0.010070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.387498</td>\n",
       "      <td>0.175101</td>\n",
       "      <td>0.175101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.465014</td>\n",
       "      <td>-0.092257</td>\n",
       "      <td>-2.405918</td>\n",
       "      <td>-0.470611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>35.59</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.593167</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.368846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.685867</td>\n",
       "      <td>0.280978</td>\n",
       "      <td>0.056196</td>\n",
       "      <td>0.112391</td>\n",
       "      <td>0.134467</td>\n",
       "      <td>-0.318643</td>\n",
       "      <td>1.377448</td>\n",
       "      <td>-1.868006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>6</td>\n",
       "      <td>679</td>\n",
       "      <td>86.06</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.019146</td>\n",
       "      <td>0.126745</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.620000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.011782</td>\n",
       "      <td>0.011782</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>7.889844</td>\n",
       "      <td>0.092958</td>\n",
       "      <td>0.092958</td>\n",
       "      <td>0.034859</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.504049</td>\n",
       "      <td>0.374560</td>\n",
       "      <td>2.175920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2</td>\n",
       "      <td>1717</td>\n",
       "      <td>45.44</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.026465</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.672941</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>37.786092</td>\n",
       "      <td>0.088028</td>\n",
       "      <td>0.198063</td>\n",
       "      <td>0.044014</td>\n",
       "      <td>-0.106768</td>\n",
       "      <td>-0.297170</td>\n",
       "      <td>-0.578729</td>\n",
       "      <td>-1.813681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>97.26</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.097143</td>\n",
       "      <td>0.555771</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.721176</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>1.799301</td>\n",
       "      <td>0.102817</td>\n",
       "      <td>0.071972</td>\n",
       "      <td>0.010282</td>\n",
       "      <td>-0.254858</td>\n",
       "      <td>-0.494790</td>\n",
       "      <td>-1.508955</td>\n",
       "      <td>-2.094675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2</td>\n",
       "      <td>888</td>\n",
       "      <td>96.19</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>0.108322</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.687778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.231729</td>\n",
       "      <td>0.062377</td>\n",
       "      <td>0.051980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.379825</td>\n",
       "      <td>-0.286914</td>\n",
       "      <td>-2.101203</td>\n",
       "      <td>-1.326087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>4</td>\n",
       "      <td>167</td>\n",
       "      <td>30.81</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113772</td>\n",
       "      <td>0.184491</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.621579</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.059880</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>5.420318</td>\n",
       "      <td>0.324570</td>\n",
       "      <td>0.064914</td>\n",
       "      <td>0.097371</td>\n",
       "      <td>0.028507</td>\n",
       "      <td>0.090757</td>\n",
       "      <td>0.384752</td>\n",
       "      <td>0.510863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2</td>\n",
       "      <td>1314</td>\n",
       "      <td>17.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.780000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.903262</td>\n",
       "      <td>0.112486</td>\n",
       "      <td>0.056243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.341165</td>\n",
       "      <td>-0.283304</td>\n",
       "      <td>-1.684899</td>\n",
       "      <td>-1.202937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>4</td>\n",
       "      <td>849</td>\n",
       "      <td>92.81</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.109317</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.405000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011779</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>9.147721</td>\n",
       "      <td>0.107747</td>\n",
       "      <td>0.032324</td>\n",
       "      <td>0.021549</td>\n",
       "      <td>0.185274</td>\n",
       "      <td>0.120086</td>\n",
       "      <td>-0.295869</td>\n",
       "      <td>0.278374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>5</td>\n",
       "      <td>303</td>\n",
       "      <td>2.72</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.226667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.033003</td>\n",
       "      <td>0.026403</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>111.397059</td>\n",
       "      <td>3.676471</td>\n",
       "      <td>2.941176</td>\n",
       "      <td>0.367647</td>\n",
       "      <td>-0.221813</td>\n",
       "      <td>0.309872</td>\n",
       "      <td>-1.446125</td>\n",
       "      <td>1.335933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>6</td>\n",
       "      <td>186</td>\n",
       "      <td>42.15</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.226613</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.683333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.037634</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>4.412811</td>\n",
       "      <td>0.166074</td>\n",
       "      <td>0.071174</td>\n",
       "      <td>0.094899</td>\n",
       "      <td>0.332402</td>\n",
       "      <td>0.498828</td>\n",
       "      <td>1.264839</td>\n",
       "      <td>1.934676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2</td>\n",
       "      <td>1816</td>\n",
       "      <td>72.32</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014317</td>\n",
       "      <td>0.039824</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.781538</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>25.110619</td>\n",
       "      <td>0.027655</td>\n",
       "      <td>0.082965</td>\n",
       "      <td>0.069137</td>\n",
       "      <td>0.316737</td>\n",
       "      <td>-0.317950</td>\n",
       "      <td>2.086976</td>\n",
       "      <td>-1.886836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>5</td>\n",
       "      <td>471</td>\n",
       "      <td>60.59</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.048832</td>\n",
       "      <td>0.128641</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.634348</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.004246</td>\n",
       "      <td>0.008493</td>\n",
       "      <td>7.773560</td>\n",
       "      <td>0.132035</td>\n",
       "      <td>0.033009</td>\n",
       "      <td>0.066017</td>\n",
       "      <td>0.165588</td>\n",
       "      <td>0.283829</td>\n",
       "      <td>1.430841</td>\n",
       "      <td>1.856731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>5</td>\n",
       "      <td>591</td>\n",
       "      <td>11.09</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.018765</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.090000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.011844</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>53.291253</td>\n",
       "      <td>0.631199</td>\n",
       "      <td>0.090171</td>\n",
       "      <td>0.360685</td>\n",
       "      <td>0.616504</td>\n",
       "      <td>0.313159</td>\n",
       "      <td>3.442124</td>\n",
       "      <td>0.126928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>4</td>\n",
       "      <td>401</td>\n",
       "      <td>84.09</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.049875</td>\n",
       "      <td>0.209701</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.204500</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>4.768700</td>\n",
       "      <td>0.035676</td>\n",
       "      <td>0.023784</td>\n",
       "      <td>0.059460</td>\n",
       "      <td>0.365517</td>\n",
       "      <td>0.083210</td>\n",
       "      <td>2.087890</td>\n",
       "      <td>0.531684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>2</td>\n",
       "      <td>574</td>\n",
       "      <td>0.07</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015679</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8200.000000</td>\n",
       "      <td>142.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.435042</td>\n",
       "      <td>-0.293384</td>\n",
       "      <td>-1.985836</td>\n",
       "      <td>-1.258672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>6</td>\n",
       "      <td>1227</td>\n",
       "      <td>7.76</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.517333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006520</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>158.118557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.030928</td>\n",
       "      <td>0.386598</td>\n",
       "      <td>0.058595</td>\n",
       "      <td>0.498770</td>\n",
       "      <td>0.351063</td>\n",
       "      <td>2.302593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>4</td>\n",
       "      <td>605</td>\n",
       "      <td>73.28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.121124</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.328000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>8.256004</td>\n",
       "      <td>0.027293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>-0.277961</td>\n",
       "      <td>0.101422</td>\n",
       "      <td>-1.562473</td>\n",
       "      <td>0.449867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>6</td>\n",
       "      <td>877</td>\n",
       "      <td>14.63</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.030787</td>\n",
       "      <td>0.016682</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.541852</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.006842</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>59.945318</td>\n",
       "      <td>0.136705</td>\n",
       "      <td>0.410116</td>\n",
       "      <td>0.205058</td>\n",
       "      <td>-0.078370</td>\n",
       "      <td>0.485045</td>\n",
       "      <td>0.389452</td>\n",
       "      <td>2.560760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>5</td>\n",
       "      <td>1149</td>\n",
       "      <td>19.94</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013925</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.246250</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.006092</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>57.622869</td>\n",
       "      <td>0.351053</td>\n",
       "      <td>0.050150</td>\n",
       "      <td>0.200602</td>\n",
       "      <td>0.219046</td>\n",
       "      <td>0.289120</td>\n",
       "      <td>1.397904</td>\n",
       "      <td>1.686173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>76.96</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>1.509020</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.748571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>0.142931</td>\n",
       "      <td>0.103950</td>\n",
       "      <td>0.038981</td>\n",
       "      <td>-0.008287</td>\n",
       "      <td>-0.310349</td>\n",
       "      <td>0.346922</td>\n",
       "      <td>-1.870742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>6</td>\n",
       "      <td>1570</td>\n",
       "      <td>81.60</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.264000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>19.240196</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.036765</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>0.156245</td>\n",
       "      <td>0.483390</td>\n",
       "      <td>1.401493</td>\n",
       "      <td>2.479143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "      <td>55.66</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.376081</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.319167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>2.659001</td>\n",
       "      <td>0.035932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071865</td>\n",
       "      <td>0.108193</td>\n",
       "      <td>-0.320773</td>\n",
       "      <td>1.394666</td>\n",
       "      <td>-1.952027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1</td>\n",
       "      <td>1037</td>\n",
       "      <td>33.26</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005786</td>\n",
       "      <td>0.032073</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.543333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>0.005786</td>\n",
       "      <td>0.003857</td>\n",
       "      <td>31.178593</td>\n",
       "      <td>0.300661</td>\n",
       "      <td>0.180397</td>\n",
       "      <td>0.120265</td>\n",
       "      <td>0.421547</td>\n",
       "      <td>-0.491970</td>\n",
       "      <td>1.209323</td>\n",
       "      <td>-1.778674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>6</td>\n",
       "      <td>698</td>\n",
       "      <td>15.93</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.034384</td>\n",
       "      <td>0.022822</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.663750</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.005731</td>\n",
       "      <td>0.014327</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>43.816698</td>\n",
       "      <td>0.251099</td>\n",
       "      <td>0.627746</td>\n",
       "      <td>0.313873</td>\n",
       "      <td>0.360773</td>\n",
       "      <td>0.487661</td>\n",
       "      <td>1.959383</td>\n",
       "      <td>2.308124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1</td>\n",
       "      <td>1332</td>\n",
       "      <td>9.19</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010511</td>\n",
       "      <td>0.006899</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.656429</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>144.940152</td>\n",
       "      <td>0.544070</td>\n",
       "      <td>1.088139</td>\n",
       "      <td>0.435256</td>\n",
       "      <td>0.297457</td>\n",
       "      <td>-0.498436</td>\n",
       "      <td>1.259410</td>\n",
       "      <td>-2.106750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2</td>\n",
       "      <td>658</td>\n",
       "      <td>58.00</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.022796</td>\n",
       "      <td>0.088146</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.866667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.012158</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>11.344828</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>-0.096190</td>\n",
       "      <td>-0.296949</td>\n",
       "      <td>-0.555048</td>\n",
       "      <td>-1.738005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>3</td>\n",
       "      <td>723</td>\n",
       "      <td>31.00</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>0.042877</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.583333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.015214</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>23.322581</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>-0.013583</td>\n",
       "      <td>-0.090532</td>\n",
       "      <td>-0.528755</td>\n",
       "      <td>-0.525878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>6</td>\n",
       "      <td>1391</td>\n",
       "      <td>69.14</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.049705</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.828000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.007908</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>20.118600</td>\n",
       "      <td>0.159097</td>\n",
       "      <td>0.028927</td>\n",
       "      <td>0.028927</td>\n",
       "      <td>0.051391</td>\n",
       "      <td>0.510208</td>\n",
       "      <td>-0.505678</td>\n",
       "      <td>1.634006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>4</td>\n",
       "      <td>1643</td>\n",
       "      <td>73.39</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017651</td>\n",
       "      <td>0.044668</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.530690</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>22.387246</td>\n",
       "      <td>0.081755</td>\n",
       "      <td>0.013626</td>\n",
       "      <td>0.013626</td>\n",
       "      <td>-0.463446</td>\n",
       "      <td>0.084954</td>\n",
       "      <td>-2.024617</td>\n",
       "      <td>0.533434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>6</td>\n",
       "      <td>852</td>\n",
       "      <td>42.93</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034038</td>\n",
       "      <td>0.050387</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.480345</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.010563</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>19.846261</td>\n",
       "      <td>0.069881</td>\n",
       "      <td>0.209644</td>\n",
       "      <td>0.046587</td>\n",
       "      <td>-0.246956</td>\n",
       "      <td>0.489775</td>\n",
       "      <td>-0.644835</td>\n",
       "      <td>2.516904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>1</td>\n",
       "      <td>868</td>\n",
       "      <td>62.30</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010369</td>\n",
       "      <td>0.071774</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.922222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005760</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.932584</td>\n",
       "      <td>0.080257</td>\n",
       "      <td>0.096308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.390702</td>\n",
       "      <td>-0.486453</td>\n",
       "      <td>-2.012085</td>\n",
       "      <td>-1.829113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>1</td>\n",
       "      <td>1186</td>\n",
       "      <td>91.64</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007589</td>\n",
       "      <td>0.077268</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.182222</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.941947</td>\n",
       "      <td>0.043649</td>\n",
       "      <td>0.109123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.352883</td>\n",
       "      <td>-0.481367</td>\n",
       "      <td>-1.802917</td>\n",
       "      <td>-1.667604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>3</td>\n",
       "      <td>867</td>\n",
       "      <td>61.82</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.012687</td>\n",
       "      <td>0.071303</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.620000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.011534</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>14.024588</td>\n",
       "      <td>0.080880</td>\n",
       "      <td>0.161760</td>\n",
       "      <td>0.048528</td>\n",
       "      <td>0.170813</td>\n",
       "      <td>-0.092140</td>\n",
       "      <td>0.380211</td>\n",
       "      <td>-0.502559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>5</td>\n",
       "      <td>1489</td>\n",
       "      <td>93.17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003358</td>\n",
       "      <td>0.062572</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.634000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>15.981539</td>\n",
       "      <td>0.032199</td>\n",
       "      <td>0.075131</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>-0.133884</td>\n",
       "      <td>0.316928</td>\n",
       "      <td>-1.280134</td>\n",
       "      <td>1.250465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>1</td>\n",
       "      <td>309</td>\n",
       "      <td>54.72</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067961</td>\n",
       "      <td>0.177087</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.605714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>5.646930</td>\n",
       "      <td>0.018275</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.036550</td>\n",
       "      <td>-0.163369</td>\n",
       "      <td>-0.502154</td>\n",
       "      <td>-0.640747</td>\n",
       "      <td>-2.371208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>3</td>\n",
       "      <td>1709</td>\n",
       "      <td>12.78</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006437</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.161818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>133.724570</td>\n",
       "      <td>0.312989</td>\n",
       "      <td>0.078247</td>\n",
       "      <td>0.234742</td>\n",
       "      <td>0.075566</td>\n",
       "      <td>-0.103639</td>\n",
       "      <td>0.428342</td>\n",
       "      <td>-0.515022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>3</td>\n",
       "      <td>161</td>\n",
       "      <td>39.01</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.093168</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.600667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049689</td>\n",
       "      <td>0.012422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.127147</td>\n",
       "      <td>0.205076</td>\n",
       "      <td>0.051269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.484544</td>\n",
       "      <td>-0.097448</td>\n",
       "      <td>-2.346547</td>\n",
       "      <td>-0.439370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>2</td>\n",
       "      <td>208</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.091346</td>\n",
       "      <td>0.010721</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.117368</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>93.273543</td>\n",
       "      <td>0.448430</td>\n",
       "      <td>4.484305</td>\n",
       "      <td>0.896861</td>\n",
       "      <td>-0.151180</td>\n",
       "      <td>-0.299799</td>\n",
       "      <td>-0.623690</td>\n",
       "      <td>-1.723073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2</td>\n",
       "      <td>1572</td>\n",
       "      <td>83.24</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013995</td>\n",
       "      <td>0.052952</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.783636</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>18.885151</td>\n",
       "      <td>0.072081</td>\n",
       "      <td>0.036040</td>\n",
       "      <td>0.060067</td>\n",
       "      <td>0.364162</td>\n",
       "      <td>-0.316287</td>\n",
       "      <td>2.119008</td>\n",
       "      <td>-1.840066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>6</td>\n",
       "      <td>1299</td>\n",
       "      <td>54.84</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.042217</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.280000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>23.687090</td>\n",
       "      <td>0.091174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018235</td>\n",
       "      <td>-0.161877</td>\n",
       "      <td>0.510681</td>\n",
       "      <td>-1.261282</td>\n",
       "      <td>1.653437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1</td>\n",
       "      <td>1701</td>\n",
       "      <td>94.49</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008818</td>\n",
       "      <td>0.055550</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.299333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>18.001905</td>\n",
       "      <td>0.031749</td>\n",
       "      <td>0.074082</td>\n",
       "      <td>0.010583</td>\n",
       "      <td>-0.270420</td>\n",
       "      <td>-0.494255</td>\n",
       "      <td>-1.523670</td>\n",
       "      <td>-2.134762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>4</td>\n",
       "      <td>410</td>\n",
       "      <td>11.13</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.027146</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.017073</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>36.837376</td>\n",
       "      <td>0.628931</td>\n",
       "      <td>0.179695</td>\n",
       "      <td>0.179695</td>\n",
       "      <td>-0.298526</td>\n",
       "      <td>0.081630</td>\n",
       "      <td>-0.602282</td>\n",
       "      <td>0.557966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2</td>\n",
       "      <td>859</td>\n",
       "      <td>55.80</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.019790</td>\n",
       "      <td>0.064959</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.282353</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.012806</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>15.394265</td>\n",
       "      <td>0.197133</td>\n",
       "      <td>0.179211</td>\n",
       "      <td>0.071685</td>\n",
       "      <td>0.308933</td>\n",
       "      <td>-0.299581</td>\n",
       "      <td>1.388300</td>\n",
       "      <td>-1.639994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>4</td>\n",
       "      <td>1106</td>\n",
       "      <td>35.40</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011754</td>\n",
       "      <td>0.032007</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.723077</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008137</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.242938</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.450975</td>\n",
       "      <td>0.106126</td>\n",
       "      <td>-2.329400</td>\n",
       "      <td>0.458421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>6</td>\n",
       "      <td>1073</td>\n",
       "      <td>64.26</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022367</td>\n",
       "      <td>0.059888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.677500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>16.697790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046685</td>\n",
       "      <td>0.015562</td>\n",
       "      <td>-0.427074</td>\n",
       "      <td>0.490085</td>\n",
       "      <td>-1.721572</td>\n",
       "      <td>2.358052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1</td>\n",
       "      <td>489</td>\n",
       "      <td>67.27</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014315</td>\n",
       "      <td>0.137566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.610000</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.269214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.366393</td>\n",
       "      <td>-0.482060</td>\n",
       "      <td>-1.730990</td>\n",
       "      <td>-1.520159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>60.15</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1.136364</td>\n",
       "      <td>2.734091</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.406000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.365752</td>\n",
       "      <td>0.133001</td>\n",
       "      <td>0.149626</td>\n",
       "      <td>0.049875</td>\n",
       "      <td>0.011980</td>\n",
       "      <td>0.092557</td>\n",
       "      <td>0.359428</td>\n",
       "      <td>0.520287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>58.11</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.527500</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.905500</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068835</td>\n",
       "      <td>0.120461</td>\n",
       "      <td>0.120461</td>\n",
       "      <td>0.068835</td>\n",
       "      <td>0.232398</td>\n",
       "      <td>-0.307440</td>\n",
       "      <td>1.266045</td>\n",
       "      <td>-1.291368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>28.58</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>4.082857</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.198462</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244927</td>\n",
       "      <td>0.104969</td>\n",
       "      <td>0.034990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500124</td>\n",
       "      <td>-0.098199</td>\n",
       "      <td>-2.258512</td>\n",
       "      <td>-0.437479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>4</td>\n",
       "      <td>208</td>\n",
       "      <td>77.53</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.372740</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.753000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>2.682832</td>\n",
       "      <td>0.025796</td>\n",
       "      <td>0.064491</td>\n",
       "      <td>0.025796</td>\n",
       "      <td>-0.050841</td>\n",
       "      <td>0.104713</td>\n",
       "      <td>-0.561977</td>\n",
       "      <td>0.473652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>4</td>\n",
       "      <td>1331</td>\n",
       "      <td>62.88</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.047243</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.576000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>0.006762</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>21.167303</td>\n",
       "      <td>0.111323</td>\n",
       "      <td>0.143130</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>-0.104432</td>\n",
       "      <td>0.120352</td>\n",
       "      <td>-1.396567</td>\n",
       "      <td>0.415872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>5</td>\n",
       "      <td>619</td>\n",
       "      <td>32.10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033926</td>\n",
       "      <td>0.051858</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.528571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.004847</td>\n",
       "      <td>0.009693</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>19.283489</td>\n",
       "      <td>0.093458</td>\n",
       "      <td>0.186916</td>\n",
       "      <td>0.062305</td>\n",
       "      <td>-0.183667</td>\n",
       "      <td>0.294208</td>\n",
       "      <td>-0.576585</td>\n",
       "      <td>1.917542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "      <td>51.88</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>1.152889</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.995385</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.867386</td>\n",
       "      <td>0.154202</td>\n",
       "      <td>0.134927</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>-0.199700</td>\n",
       "      <td>0.491629</td>\n",
       "      <td>-0.620596</td>\n",
       "      <td>2.438824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>6</td>\n",
       "      <td>291</td>\n",
       "      <td>87.69</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.079038</td>\n",
       "      <td>0.301340</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.812609</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.024055</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.017182</td>\n",
       "      <td>3.318508</td>\n",
       "      <td>0.079827</td>\n",
       "      <td>0.011404</td>\n",
       "      <td>0.057019</td>\n",
       "      <td>0.345184</td>\n",
       "      <td>0.479855</td>\n",
       "      <td>1.864071</td>\n",
       "      <td>2.233412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>5</td>\n",
       "      <td>1077</td>\n",
       "      <td>98.03</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>0.091021</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.535333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>10.986433</td>\n",
       "      <td>0.030603</td>\n",
       "      <td>0.020402</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>-0.307090</td>\n",
       "      <td>0.299263</td>\n",
       "      <td>-1.685744</td>\n",
       "      <td>1.584835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>6</td>\n",
       "      <td>1216</td>\n",
       "      <td>1.76</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.022204</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.065185</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.004934</td>\n",
       "      <td>0.005757</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>690.909091</td>\n",
       "      <td>3.409091</td>\n",
       "      <td>3.977273</td>\n",
       "      <td>1.136364</td>\n",
       "      <td>-0.238180</td>\n",
       "      <td>0.489873</td>\n",
       "      <td>-0.638376</td>\n",
       "      <td>2.504639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>5</td>\n",
       "      <td>877</td>\n",
       "      <td>12.59</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.023945</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.599524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>69.658459</td>\n",
       "      <td>0.397141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397141</td>\n",
       "      <td>0.324896</td>\n",
       "      <td>0.279568</td>\n",
       "      <td>2.101542</td>\n",
       "      <td>1.746174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>6</td>\n",
       "      <td>1698</td>\n",
       "      <td>86.47</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>0.050925</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.860909</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>19.636868</td>\n",
       "      <td>0.069388</td>\n",
       "      <td>0.057824</td>\n",
       "      <td>0.046259</td>\n",
       "      <td>0.330386</td>\n",
       "      <td>0.499763</td>\n",
       "      <td>1.305311</td>\n",
       "      <td>2.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497871</th>\n",
       "      <td>3</td>\n",
       "      <td>214</td>\n",
       "      <td>42.32</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.102804</td>\n",
       "      <td>0.197757</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.923636</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.037383</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>5.056711</td>\n",
       "      <td>0.189036</td>\n",
       "      <td>0.236295</td>\n",
       "      <td>0.023629</td>\n",
       "      <td>-0.318887</td>\n",
       "      <td>-0.097898</td>\n",
       "      <td>-1.847975</td>\n",
       "      <td>-0.463108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497872</th>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>63.03</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.575750</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.634618</td>\n",
       "      <td>0.063462</td>\n",
       "      <td>0.031731</td>\n",
       "      <td>0.031731</td>\n",
       "      <td>-0.093333</td>\n",
       "      <td>-0.500276</td>\n",
       "      <td>-0.563991</td>\n",
       "      <td>-2.141233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497873</th>\n",
       "      <td>4</td>\n",
       "      <td>822</td>\n",
       "      <td>8.40</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.035280</td>\n",
       "      <td>0.010219</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.289655</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>97.857143</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>-0.050245</td>\n",
       "      <td>0.088211</td>\n",
       "      <td>0.382395</td>\n",
       "      <td>0.540968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497874</th>\n",
       "      <td>3</td>\n",
       "      <td>277</td>\n",
       "      <td>80.70</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.064982</td>\n",
       "      <td>0.291336</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.483333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.036101</td>\n",
       "      <td>0.014440</td>\n",
       "      <td>0.018051</td>\n",
       "      <td>3.432466</td>\n",
       "      <td>0.123916</td>\n",
       "      <td>0.049566</td>\n",
       "      <td>0.061958</td>\n",
       "      <td>0.439175</td>\n",
       "      <td>-0.110510</td>\n",
       "      <td>2.066726</td>\n",
       "      <td>-0.575874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497875</th>\n",
       "      <td>3</td>\n",
       "      <td>1291</td>\n",
       "      <td>10.97</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019365</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.002324</td>\n",
       "      <td>0.007746</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>117.684594</td>\n",
       "      <td>0.273473</td>\n",
       "      <td>0.911577</td>\n",
       "      <td>0.091158</td>\n",
       "      <td>-0.388849</td>\n",
       "      <td>-0.102270</td>\n",
       "      <td>-1.958143</td>\n",
       "      <td>-0.480060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497876</th>\n",
       "      <td>6</td>\n",
       "      <td>1723</td>\n",
       "      <td>99.46</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012768</td>\n",
       "      <td>0.057725</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.520909</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.004643</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>17.323547</td>\n",
       "      <td>0.080434</td>\n",
       "      <td>0.050271</td>\n",
       "      <td>0.050271</td>\n",
       "      <td>0.393984</td>\n",
       "      <td>0.486245</td>\n",
       "      <td>1.960557</td>\n",
       "      <td>2.257914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497877</th>\n",
       "      <td>4</td>\n",
       "      <td>889</td>\n",
       "      <td>43.05</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033746</td>\n",
       "      <td>0.048425</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.435000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.011249</td>\n",
       "      <td>0.006749</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>20.650407</td>\n",
       "      <td>0.232288</td>\n",
       "      <td>0.139373</td>\n",
       "      <td>0.092915</td>\n",
       "      <td>0.120549</td>\n",
       "      <td>0.082003</td>\n",
       "      <td>1.558144</td>\n",
       "      <td>0.521085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497878</th>\n",
       "      <td>3</td>\n",
       "      <td>1657</td>\n",
       "      <td>26.99</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015691</td>\n",
       "      <td>0.016288</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.038077</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>61.393109</td>\n",
       "      <td>0.370508</td>\n",
       "      <td>0.370508</td>\n",
       "      <td>0.185254</td>\n",
       "      <td>0.371521</td>\n",
       "      <td>-0.112003</td>\n",
       "      <td>2.102036</td>\n",
       "      <td>-0.596868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497879</th>\n",
       "      <td>6</td>\n",
       "      <td>239</td>\n",
       "      <td>39.45</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.071130</td>\n",
       "      <td>0.165063</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.320588</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>6.058302</td>\n",
       "      <td>0.101394</td>\n",
       "      <td>0.050697</td>\n",
       "      <td>0.076046</td>\n",
       "      <td>0.022993</td>\n",
       "      <td>0.491016</td>\n",
       "      <td>0.382550</td>\n",
       "      <td>2.412438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497880</th>\n",
       "      <td>5</td>\n",
       "      <td>988</td>\n",
       "      <td>22.33</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>0.022601</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.582500</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>44.245410</td>\n",
       "      <td>0.089566</td>\n",
       "      <td>0.358262</td>\n",
       "      <td>0.179131</td>\n",
       "      <td>0.429917</td>\n",
       "      <td>0.310101</td>\n",
       "      <td>1.256478</td>\n",
       "      <td>1.248799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497881</th>\n",
       "      <td>5</td>\n",
       "      <td>683</td>\n",
       "      <td>56.40</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.038067</td>\n",
       "      <td>0.082577</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.169231</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>12.109929</td>\n",
       "      <td>0.195035</td>\n",
       "      <td>0.053191</td>\n",
       "      <td>0.035461</td>\n",
       "      <td>-0.212855</td>\n",
       "      <td>0.288189</td>\n",
       "      <td>-0.590204</td>\n",
       "      <td>1.968852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497882</th>\n",
       "      <td>5</td>\n",
       "      <td>1736</td>\n",
       "      <td>23.54</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.013560</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.846667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003456</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.746814</td>\n",
       "      <td>0.254885</td>\n",
       "      <td>0.127443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.338180</td>\n",
       "      <td>0.316965</td>\n",
       "      <td>-1.869084</td>\n",
       "      <td>1.200675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497883</th>\n",
       "      <td>1</td>\n",
       "      <td>1637</td>\n",
       "      <td>55.99</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>0.034203</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.153462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>29.237364</td>\n",
       "      <td>0.017860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053581</td>\n",
       "      <td>-0.102898</td>\n",
       "      <td>-0.519752</td>\n",
       "      <td>0.340993</td>\n",
       "      <td>-2.515509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497884</th>\n",
       "      <td>6</td>\n",
       "      <td>1456</td>\n",
       "      <td>4.47</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>325.727069</td>\n",
       "      <td>1.565996</td>\n",
       "      <td>1.118568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.446601</td>\n",
       "      <td>0.508332</td>\n",
       "      <td>-2.032781</td>\n",
       "      <td>1.927589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497885</th>\n",
       "      <td>1</td>\n",
       "      <td>711</td>\n",
       "      <td>60.04</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.084444</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.003333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>11.842105</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>0.033311</td>\n",
       "      <td>0.083278</td>\n",
       "      <td>0.442906</td>\n",
       "      <td>-0.508993</td>\n",
       "      <td>1.862295</td>\n",
       "      <td>-1.996423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497886</th>\n",
       "      <td>6</td>\n",
       "      <td>65</td>\n",
       "      <td>89.17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>1.371846</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.585000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.728945</td>\n",
       "      <td>0.044858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.312971</td>\n",
       "      <td>0.515902</td>\n",
       "      <td>-1.350984</td>\n",
       "      <td>1.242417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497887</th>\n",
       "      <td>5</td>\n",
       "      <td>1277</td>\n",
       "      <td>3.81</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010180</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.293077</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.007048</td>\n",
       "      <td>0.004699</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>335.170604</td>\n",
       "      <td>2.362205</td>\n",
       "      <td>1.574803</td>\n",
       "      <td>1.049869</td>\n",
       "      <td>0.298286</td>\n",
       "      <td>0.298441</td>\n",
       "      <td>1.390683</td>\n",
       "      <td>1.576313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497888</th>\n",
       "      <td>5</td>\n",
       "      <td>690</td>\n",
       "      <td>26.73</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010145</td>\n",
       "      <td>0.038739</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.818571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.005797</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>25.813692</td>\n",
       "      <td>0.224467</td>\n",
       "      <td>0.149645</td>\n",
       "      <td>0.037411</td>\n",
       "      <td>-0.202973</td>\n",
       "      <td>0.309912</td>\n",
       "      <td>-1.421479</td>\n",
       "      <td>1.358110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497889</th>\n",
       "      <td>3</td>\n",
       "      <td>1221</td>\n",
       "      <td>71.01</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019656</td>\n",
       "      <td>0.058157</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.958750</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.194761</td>\n",
       "      <td>0.042248</td>\n",
       "      <td>0.070413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.577679</td>\n",
       "      <td>-0.103624</td>\n",
       "      <td>-2.590283</td>\n",
       "      <td>-0.505800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497890</th>\n",
       "      <td>1</td>\n",
       "      <td>1810</td>\n",
       "      <td>53.38</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009392</td>\n",
       "      <td>0.029492</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>33.907831</td>\n",
       "      <td>0.093668</td>\n",
       "      <td>0.018734</td>\n",
       "      <td>0.037467</td>\n",
       "      <td>-0.158978</td>\n",
       "      <td>-0.505922</td>\n",
       "      <td>-0.612788</td>\n",
       "      <td>-2.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497891</th>\n",
       "      <td>3</td>\n",
       "      <td>1710</td>\n",
       "      <td>39.55</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.023129</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.955000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.236410</td>\n",
       "      <td>0.126422</td>\n",
       "      <td>0.075853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.436691</td>\n",
       "      <td>-0.091548</td>\n",
       "      <td>-2.252739</td>\n",
       "      <td>-0.443313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497892</th>\n",
       "      <td>3</td>\n",
       "      <td>1181</td>\n",
       "      <td>50.24</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.025402</td>\n",
       "      <td>0.042540</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.674667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>23.507166</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.159236</td>\n",
       "      <td>0.059713</td>\n",
       "      <td>-0.075250</td>\n",
       "      <td>-0.114245</td>\n",
       "      <td>0.359917</td>\n",
       "      <td>-0.577615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497893</th>\n",
       "      <td>6</td>\n",
       "      <td>768</td>\n",
       "      <td>51.22</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029948</td>\n",
       "      <td>0.066693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.226957</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>14.994143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156189</td>\n",
       "      <td>0.019524</td>\n",
       "      <td>-0.383327</td>\n",
       "      <td>0.496569</td>\n",
       "      <td>-1.712215</td>\n",
       "      <td>2.338566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497894</th>\n",
       "      <td>4</td>\n",
       "      <td>1695</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>543.269231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.320513</td>\n",
       "      <td>-0.301135</td>\n",
       "      <td>0.102399</td>\n",
       "      <td>-1.564669</td>\n",
       "      <td>0.485726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497895</th>\n",
       "      <td>6</td>\n",
       "      <td>902</td>\n",
       "      <td>96.04</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011086</td>\n",
       "      <td>0.106475</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.604000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.009978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.391920</td>\n",
       "      <td>0.010412</td>\n",
       "      <td>0.093711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.389311</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>-1.863031</td>\n",
       "      <td>1.772185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497896</th>\n",
       "      <td>1</td>\n",
       "      <td>902</td>\n",
       "      <td>16.64</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>0.018448</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.080000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.005543</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>54.206731</td>\n",
       "      <td>0.180288</td>\n",
       "      <td>0.300481</td>\n",
       "      <td>0.120192</td>\n",
       "      <td>-0.043057</td>\n",
       "      <td>-0.493192</td>\n",
       "      <td>-0.497411</td>\n",
       "      <td>-2.050189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497897</th>\n",
       "      <td>3</td>\n",
       "      <td>630</td>\n",
       "      <td>7.77</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.295000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>81.081081</td>\n",
       "      <td>1.287001</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>-0.189188</td>\n",
       "      <td>-0.090140</td>\n",
       "      <td>-1.430743</td>\n",
       "      <td>-0.415446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497898</th>\n",
       "      <td>5</td>\n",
       "      <td>1140</td>\n",
       "      <td>38.15</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>0.033465</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.238889</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>29.882045</td>\n",
       "      <td>0.209699</td>\n",
       "      <td>0.262123</td>\n",
       "      <td>0.026212</td>\n",
       "      <td>-0.165749</td>\n",
       "      <td>0.315800</td>\n",
       "      <td>-1.402647</td>\n",
       "      <td>1.371706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497899</th>\n",
       "      <td>3</td>\n",
       "      <td>492</td>\n",
       "      <td>29.87</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030488</td>\n",
       "      <td>0.060711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.991333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>16.471376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200870</td>\n",
       "      <td>0.033478</td>\n",
       "      <td>-0.315887</td>\n",
       "      <td>-0.097712</td>\n",
       "      <td>-1.750163</td>\n",
       "      <td>-0.497995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497900</th>\n",
       "      <td>1</td>\n",
       "      <td>1078</td>\n",
       "      <td>15.34</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.014230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.670000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>70.273794</td>\n",
       "      <td>0.130378</td>\n",
       "      <td>0.130378</td>\n",
       "      <td>0.260756</td>\n",
       "      <td>0.447825</td>\n",
       "      <td>-0.493488</td>\n",
       "      <td>1.373389</td>\n",
       "      <td>-1.405849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497901</th>\n",
       "      <td>6</td>\n",
       "      <td>965</td>\n",
       "      <td>28.15</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.029171</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.815000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.008290</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>34.280639</td>\n",
       "      <td>0.284192</td>\n",
       "      <td>0.213144</td>\n",
       "      <td>0.106572</td>\n",
       "      <td>0.156169</td>\n",
       "      <td>0.504222</td>\n",
       "      <td>0.378608</td>\n",
       "      <td>2.115143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497902</th>\n",
       "      <td>1</td>\n",
       "      <td>1151</td>\n",
       "      <td>49.09</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025195</td>\n",
       "      <td>0.042650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.692759</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006950</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>23.446730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162966</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>-0.449582</td>\n",
       "      <td>-0.508612</td>\n",
       "      <td>-1.650975</td>\n",
       "      <td>-2.349825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497903</th>\n",
       "      <td>3</td>\n",
       "      <td>542</td>\n",
       "      <td>69.85</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.128875</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.492500</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>7.759485</td>\n",
       "      <td>0.057266</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>0.057266</td>\n",
       "      <td>0.182536</td>\n",
       "      <td>-0.113831</td>\n",
       "      <td>1.563627</td>\n",
       "      <td>-0.576309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497904</th>\n",
       "      <td>2</td>\n",
       "      <td>314</td>\n",
       "      <td>31.63</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.022293</td>\n",
       "      <td>0.100732</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.518571</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.028662</td>\n",
       "      <td>0.028662</td>\n",
       "      <td>0.009554</td>\n",
       "      <td>9.927284</td>\n",
       "      <td>0.284540</td>\n",
       "      <td>0.284540</td>\n",
       "      <td>0.094847</td>\n",
       "      <td>0.234081</td>\n",
       "      <td>-0.287762</td>\n",
       "      <td>0.338303</td>\n",
       "      <td>-1.328888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497905</th>\n",
       "      <td>5</td>\n",
       "      <td>149</td>\n",
       "      <td>26.28</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.093960</td>\n",
       "      <td>0.176376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.877143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>5.669711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152207</td>\n",
       "      <td>0.038052</td>\n",
       "      <td>-0.321065</td>\n",
       "      <td>0.300549</td>\n",
       "      <td>-1.616944</td>\n",
       "      <td>1.539526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497906</th>\n",
       "      <td>5</td>\n",
       "      <td>374</td>\n",
       "      <td>40.41</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.069519</td>\n",
       "      <td>0.108048</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.554231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.021390</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>9.255135</td>\n",
       "      <td>0.197971</td>\n",
       "      <td>0.049493</td>\n",
       "      <td>0.049493</td>\n",
       "      <td>-0.240248</td>\n",
       "      <td>0.286005</td>\n",
       "      <td>-0.575605</td>\n",
       "      <td>1.961350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497907</th>\n",
       "      <td>3</td>\n",
       "      <td>765</td>\n",
       "      <td>53.54</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.069987</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.149412</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>14.288383</td>\n",
       "      <td>0.205454</td>\n",
       "      <td>0.074710</td>\n",
       "      <td>0.093388</td>\n",
       "      <td>0.446651</td>\n",
       "      <td>-0.109417</td>\n",
       "      <td>2.143117</td>\n",
       "      <td>-0.575129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497908</th>\n",
       "      <td>2</td>\n",
       "      <td>1338</td>\n",
       "      <td>38.12</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011211</td>\n",
       "      <td>0.028490</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.541333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>35.099685</td>\n",
       "      <td>0.262329</td>\n",
       "      <td>0.183631</td>\n",
       "      <td>0.026233</td>\n",
       "      <td>-0.253148</td>\n",
       "      <td>-0.293300</td>\n",
       "      <td>-1.606304</td>\n",
       "      <td>-1.544726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497909</th>\n",
       "      <td>6</td>\n",
       "      <td>1050</td>\n",
       "      <td>76.05</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017143</td>\n",
       "      <td>0.072429</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.225000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>13.806706</td>\n",
       "      <td>0.131492</td>\n",
       "      <td>0.013149</td>\n",
       "      <td>0.052597</td>\n",
       "      <td>0.230571</td>\n",
       "      <td>0.488465</td>\n",
       "      <td>1.348774</td>\n",
       "      <td>2.320577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497910</th>\n",
       "      <td>1</td>\n",
       "      <td>656</td>\n",
       "      <td>8.07</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033537</td>\n",
       "      <td>0.012302</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.366818</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>81.288724</td>\n",
       "      <td>0.247831</td>\n",
       "      <td>0.867410</td>\n",
       "      <td>0.495663</td>\n",
       "      <td>0.166070</td>\n",
       "      <td>-0.511053</td>\n",
       "      <td>1.370506</td>\n",
       "      <td>-2.410851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497911</th>\n",
       "      <td>4</td>\n",
       "      <td>693</td>\n",
       "      <td>45.34</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>0.065426</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.113333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.002886</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>15.284517</td>\n",
       "      <td>0.198500</td>\n",
       "      <td>0.044111</td>\n",
       "      <td>0.110278</td>\n",
       "      <td>0.666552</td>\n",
       "      <td>0.105446</td>\n",
       "      <td>1.575251</td>\n",
       "      <td>0.342943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497912</th>\n",
       "      <td>2</td>\n",
       "      <td>1225</td>\n",
       "      <td>29.08</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017143</td>\n",
       "      <td>0.023739</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.384762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.007347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>42.125172</td>\n",
       "      <td>0.309491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137552</td>\n",
       "      <td>0.167730</td>\n",
       "      <td>-0.315987</td>\n",
       "      <td>1.417449</td>\n",
       "      <td>-1.883173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497913</th>\n",
       "      <td>6</td>\n",
       "      <td>976</td>\n",
       "      <td>89.50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.091701</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.187500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>10.905028</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.055866</td>\n",
       "      <td>0.517734</td>\n",
       "      <td>0.496522</td>\n",
       "      <td>1.771479</td>\n",
       "      <td>1.781639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497914</th>\n",
       "      <td>5</td>\n",
       "      <td>451</td>\n",
       "      <td>36.47</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.050998</td>\n",
       "      <td>0.080865</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.585652</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.013304</td>\n",
       "      <td>0.019956</td>\n",
       "      <td>0.011086</td>\n",
       "      <td>12.366328</td>\n",
       "      <td>0.164519</td>\n",
       "      <td>0.246778</td>\n",
       "      <td>0.137099</td>\n",
       "      <td>0.382066</td>\n",
       "      <td>0.288344</td>\n",
       "      <td>2.117811</td>\n",
       "      <td>1.728118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497915</th>\n",
       "      <td>1</td>\n",
       "      <td>925</td>\n",
       "      <td>97.29</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>0.105178</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.120526</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.008649</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>0.003243</td>\n",
       "      <td>9.507658</td>\n",
       "      <td>0.082228</td>\n",
       "      <td>0.092507</td>\n",
       "      <td>0.030836</td>\n",
       "      <td>0.092716</td>\n",
       "      <td>-0.500268</td>\n",
       "      <td>0.336133</td>\n",
       "      <td>-2.353075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497916</th>\n",
       "      <td>1</td>\n",
       "      <td>1646</td>\n",
       "      <td>57.60</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015188</td>\n",
       "      <td>0.034994</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.304000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.576389</td>\n",
       "      <td>0.121528</td>\n",
       "      <td>0.017361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.602324</td>\n",
       "      <td>-0.507928</td>\n",
       "      <td>-2.269138</td>\n",
       "      <td>-2.158515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497917</th>\n",
       "      <td>3</td>\n",
       "      <td>566</td>\n",
       "      <td>46.97</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017668</td>\n",
       "      <td>0.082986</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.697000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.050245</td>\n",
       "      <td>0.042580</td>\n",
       "      <td>0.127741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.425786</td>\n",
       "      <td>-0.089053</td>\n",
       "      <td>-2.216361</td>\n",
       "      <td>-0.442334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497918</th>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>11.63</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.011607</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.969167</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>86.156492</td>\n",
       "      <td>0.515907</td>\n",
       "      <td>0.773861</td>\n",
       "      <td>0.171969</td>\n",
       "      <td>-0.047894</td>\n",
       "      <td>-0.491920</td>\n",
       "      <td>-0.520650</td>\n",
       "      <td>-2.125407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497919</th>\n",
       "      <td>2</td>\n",
       "      <td>629</td>\n",
       "      <td>53.94</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>0.085755</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.568571</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.006359</td>\n",
       "      <td>11.661105</td>\n",
       "      <td>0.018539</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>0.074156</td>\n",
       "      <td>0.150477</td>\n",
       "      <td>-0.315663</td>\n",
       "      <td>1.427899</td>\n",
       "      <td>-1.941239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497920</th>\n",
       "      <td>2</td>\n",
       "      <td>611</td>\n",
       "      <td>44.62</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.073028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.487333</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011457</td>\n",
       "      <td>0.008183</td>\n",
       "      <td>13.693411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156880</td>\n",
       "      <td>0.112057</td>\n",
       "      <td>0.261445</td>\n",
       "      <td>-0.321915</td>\n",
       "      <td>2.089285</td>\n",
       "      <td>-1.877363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497921</th>\n",
       "      <td>4</td>\n",
       "      <td>950</td>\n",
       "      <td>17.65</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.018579</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.825000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>53.824363</td>\n",
       "      <td>0.226629</td>\n",
       "      <td>0.113314</td>\n",
       "      <td>0.283286</td>\n",
       "      <td>0.670363</td>\n",
       "      <td>0.105083</td>\n",
       "      <td>1.627837</td>\n",
       "      <td>0.256227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497922</th>\n",
       "      <td>5</td>\n",
       "      <td>101</td>\n",
       "      <td>57.19</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.566238</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.382917</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>1.766043</td>\n",
       "      <td>0.069942</td>\n",
       "      <td>0.122399</td>\n",
       "      <td>0.017486</td>\n",
       "      <td>-0.378796</td>\n",
       "      <td>0.295495</td>\n",
       "      <td>-1.843209</td>\n",
       "      <td>1.743744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497923</th>\n",
       "      <td>2</td>\n",
       "      <td>1699</td>\n",
       "      <td>21.95</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013537</td>\n",
       "      <td>0.012919</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.954348</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.006474</td>\n",
       "      <td>0.005886</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>77.403189</td>\n",
       "      <td>0.501139</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>0.182232</td>\n",
       "      <td>0.226269</td>\n",
       "      <td>-0.305935</td>\n",
       "      <td>1.415527</td>\n",
       "      <td>-1.808570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497924</th>\n",
       "      <td>1</td>\n",
       "      <td>1076</td>\n",
       "      <td>55.47</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.051552</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.470000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.004647</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>0.004647</td>\n",
       "      <td>19.397873</td>\n",
       "      <td>0.090139</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.090139</td>\n",
       "      <td>0.919136</td>\n",
       "      <td>-0.480284</td>\n",
       "      <td>3.563014</td>\n",
       "      <td>0.086820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497925</th>\n",
       "      <td>6</td>\n",
       "      <td>232</td>\n",
       "      <td>7.20</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012931</td>\n",
       "      <td>0.031034</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.021552</td>\n",
       "      <td>32.222222</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.644360</td>\n",
       "      <td>0.508524</td>\n",
       "      <td>1.554195</td>\n",
       "      <td>1.392249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497926</th>\n",
       "      <td>5</td>\n",
       "      <td>453</td>\n",
       "      <td>96.32</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017660</td>\n",
       "      <td>0.212627</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.040000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.703073</td>\n",
       "      <td>0.093439</td>\n",
       "      <td>0.020764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.373520</td>\n",
       "      <td>0.311056</td>\n",
       "      <td>-1.889049</td>\n",
       "      <td>1.237284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497927</th>\n",
       "      <td>5</td>\n",
       "      <td>972</td>\n",
       "      <td>45.90</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.047222</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.639286</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>21.176471</td>\n",
       "      <td>0.087146</td>\n",
       "      <td>0.217865</td>\n",
       "      <td>0.087146</td>\n",
       "      <td>0.142679</td>\n",
       "      <td>0.286934</td>\n",
       "      <td>1.460048</td>\n",
       "      <td>2.007905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497928</th>\n",
       "      <td>4</td>\n",
       "      <td>1750</td>\n",
       "      <td>76.78</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.043874</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.780000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>22.792394</td>\n",
       "      <td>0.078145</td>\n",
       "      <td>0.013024</td>\n",
       "      <td>0.013024</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>0.123184</td>\n",
       "      <td>3.183962</td>\n",
       "      <td>0.101032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497929</th>\n",
       "      <td>1</td>\n",
       "      <td>1111</td>\n",
       "      <td>5.52</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027003</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>201.268116</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>1.449275</td>\n",
       "      <td>0.181159</td>\n",
       "      <td>-0.464504</td>\n",
       "      <td>-0.509576</td>\n",
       "      <td>-1.759948</td>\n",
       "      <td>-2.285219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497930</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>41.17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>3.166923</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.940714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315764</td>\n",
       "      <td>0.097158</td>\n",
       "      <td>0.024290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.501541</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>-2.266945</td>\n",
       "      <td>1.432820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497931</th>\n",
       "      <td>4</td>\n",
       "      <td>955</td>\n",
       "      <td>94.26</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.098702</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.473333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011518</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.131551</td>\n",
       "      <td>0.116698</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.376634</td>\n",
       "      <td>0.110731</td>\n",
       "      <td>-2.025526</td>\n",
       "      <td>0.445616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497932</th>\n",
       "      <td>1</td>\n",
       "      <td>990</td>\n",
       "      <td>66.95</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.021212</td>\n",
       "      <td>0.067626</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.188095</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>14.787155</td>\n",
       "      <td>0.104556</td>\n",
       "      <td>0.149365</td>\n",
       "      <td>0.059746</td>\n",
       "      <td>0.244647</td>\n",
       "      <td>-0.504483</td>\n",
       "      <td>1.356587</td>\n",
       "      <td>-2.414823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497933</th>\n",
       "      <td>1</td>\n",
       "      <td>1549</td>\n",
       "      <td>24.13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.015578</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.043333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>64.193949</td>\n",
       "      <td>0.041442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124327</td>\n",
       "      <td>0.180683</td>\n",
       "      <td>-0.496246</td>\n",
       "      <td>0.476644</td>\n",
       "      <td>-1.811791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497934</th>\n",
       "      <td>2</td>\n",
       "      <td>1660</td>\n",
       "      <td>67.63</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.661429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.006024</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>24.545320</td>\n",
       "      <td>0.147863</td>\n",
       "      <td>0.014786</td>\n",
       "      <td>0.073932</td>\n",
       "      <td>0.561913</td>\n",
       "      <td>-0.301452</td>\n",
       "      <td>1.735964</td>\n",
       "      <td>-1.260534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497935</th>\n",
       "      <td>1</td>\n",
       "      <td>1337</td>\n",
       "      <td>87.25</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>0.065258</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.232143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>15.323782</td>\n",
       "      <td>0.114613</td>\n",
       "      <td>0.114613</td>\n",
       "      <td>0.034384</td>\n",
       "      <td>0.167772</td>\n",
       "      <td>-0.493282</td>\n",
       "      <td>0.349151</td>\n",
       "      <td>-2.056843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497936</th>\n",
       "      <td>2</td>\n",
       "      <td>1310</td>\n",
       "      <td>46.16</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.035237</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.540000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.005344</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>28.379549</td>\n",
       "      <td>0.151646</td>\n",
       "      <td>0.064991</td>\n",
       "      <td>0.021664</td>\n",
       "      <td>-0.146173</td>\n",
       "      <td>-0.285910</td>\n",
       "      <td>-1.329197</td>\n",
       "      <td>-1.262713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497937</th>\n",
       "      <td>6</td>\n",
       "      <td>473</td>\n",
       "      <td>42.82</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.019027</td>\n",
       "      <td>0.090529</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.757778</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.012685</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.008457</td>\n",
       "      <td>11.046240</td>\n",
       "      <td>0.140121</td>\n",
       "      <td>0.023354</td>\n",
       "      <td>0.093414</td>\n",
       "      <td>0.310647</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>1.274024</td>\n",
       "      <td>2.005695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497938</th>\n",
       "      <td>5</td>\n",
       "      <td>944</td>\n",
       "      <td>21.03</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.022278</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.751071</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>44.888255</td>\n",
       "      <td>0.190204</td>\n",
       "      <td>0.285307</td>\n",
       "      <td>0.095102</td>\n",
       "      <td>-0.259895</td>\n",
       "      <td>0.287508</td>\n",
       "      <td>-0.609184</td>\n",
       "      <td>2.023527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497939</th>\n",
       "      <td>6</td>\n",
       "      <td>595</td>\n",
       "      <td>83.51</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.140353</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.918333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>7.124895</td>\n",
       "      <td>0.023949</td>\n",
       "      <td>0.035924</td>\n",
       "      <td>0.023949</td>\n",
       "      <td>-0.007990</td>\n",
       "      <td>0.506929</td>\n",
       "      <td>-0.482164</td>\n",
       "      <td>1.881562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497940</th>\n",
       "      <td>2</td>\n",
       "      <td>1620</td>\n",
       "      <td>98.43</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.060759</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.921500</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>16.458397</td>\n",
       "      <td>0.020319</td>\n",
       "      <td>0.030479</td>\n",
       "      <td>0.050798</td>\n",
       "      <td>0.371833</td>\n",
       "      <td>-0.315134</td>\n",
       "      <td>2.034848</td>\n",
       "      <td>-1.807346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497941</th>\n",
       "      <td>3</td>\n",
       "      <td>310</td>\n",
       "      <td>49.27</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.158935</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.211667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>6.291861</td>\n",
       "      <td>0.081185</td>\n",
       "      <td>0.202963</td>\n",
       "      <td>0.040593</td>\n",
       "      <td>0.049810</td>\n",
       "      <td>-0.084002</td>\n",
       "      <td>-0.522279</td>\n",
       "      <td>-0.458336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497942</th>\n",
       "      <td>1</td>\n",
       "      <td>1390</td>\n",
       "      <td>60.46</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>0.043496</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.030667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.004317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.990407</td>\n",
       "      <td>0.066159</td>\n",
       "      <td>0.099239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.468515</td>\n",
       "      <td>-0.493005</td>\n",
       "      <td>-2.171754</td>\n",
       "      <td>-2.018435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497943</th>\n",
       "      <td>3</td>\n",
       "      <td>109</td>\n",
       "      <td>37.82</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.346972</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.454615</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.100917</td>\n",
       "      <td>0.082569</td>\n",
       "      <td>0.036697</td>\n",
       "      <td>2.882073</td>\n",
       "      <td>0.290851</td>\n",
       "      <td>0.237969</td>\n",
       "      <td>0.105764</td>\n",
       "      <td>0.191321</td>\n",
       "      <td>-0.110475</td>\n",
       "      <td>1.533154</td>\n",
       "      <td>-0.592224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497944</th>\n",
       "      <td>3</td>\n",
       "      <td>1726</td>\n",
       "      <td>93.51</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.054177</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.585000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.457919</td>\n",
       "      <td>0.042776</td>\n",
       "      <td>0.106940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.308373</td>\n",
       "      <td>-0.077549</td>\n",
       "      <td>-1.825610</td>\n",
       "      <td>-0.478387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497945</th>\n",
       "      <td>4</td>\n",
       "      <td>717</td>\n",
       "      <td>51.97</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009763</td>\n",
       "      <td>0.072483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.424286</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011158</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>13.796421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153935</td>\n",
       "      <td>0.076967</td>\n",
       "      <td>0.367547</td>\n",
       "      <td>0.105673</td>\n",
       "      <td>1.383200</td>\n",
       "      <td>0.455537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497946</th>\n",
       "      <td>1</td>\n",
       "      <td>1385</td>\n",
       "      <td>90.09</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009386</td>\n",
       "      <td>0.065047</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.930000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.373515</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.451828</td>\n",
       "      <td>-0.495266</td>\n",
       "      <td>-2.047025</td>\n",
       "      <td>-1.886567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497947</th>\n",
       "      <td>2</td>\n",
       "      <td>614</td>\n",
       "      <td>37.52</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.048860</td>\n",
       "      <td>0.061107</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.250667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>16.364606</td>\n",
       "      <td>0.239872</td>\n",
       "      <td>0.239872</td>\n",
       "      <td>0.053305</td>\n",
       "      <td>-0.229100</td>\n",
       "      <td>-0.309281</td>\n",
       "      <td>-0.634213</td>\n",
       "      <td>-1.971938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497948</th>\n",
       "      <td>1</td>\n",
       "      <td>379</td>\n",
       "      <td>6.76</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034301</td>\n",
       "      <td>0.017836</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.021108</td>\n",
       "      <td>0.013193</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>56.065089</td>\n",
       "      <td>1.183432</td>\n",
       "      <td>0.739645</td>\n",
       "      <td>0.295858</td>\n",
       "      <td>-0.081448</td>\n",
       "      <td>-0.497355</td>\n",
       "      <td>-0.558030</td>\n",
       "      <td>-2.194152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497949</th>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>72.47</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>2.196061</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.494000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.455361</td>\n",
       "      <td>0.110391</td>\n",
       "      <td>0.082793</td>\n",
       "      <td>0.041396</td>\n",
       "      <td>0.260391</td>\n",
       "      <td>0.511392</td>\n",
       "      <td>0.384186</td>\n",
       "      <td>1.738853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497950</th>\n",
       "      <td>3</td>\n",
       "      <td>1709</td>\n",
       "      <td>64.66</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009362</td>\n",
       "      <td>0.037835</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.041250</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.004681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.430560</td>\n",
       "      <td>0.108259</td>\n",
       "      <td>0.123724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.448012</td>\n",
       "      <td>-0.090996</td>\n",
       "      <td>-2.335179</td>\n",
       "      <td>-0.467485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497951</th>\n",
       "      <td>1</td>\n",
       "      <td>1411</td>\n",
       "      <td>36.17</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010631</td>\n",
       "      <td>0.025634</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.411333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.007087</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>39.010229</td>\n",
       "      <td>0.221178</td>\n",
       "      <td>0.276472</td>\n",
       "      <td>0.110589</td>\n",
       "      <td>0.310510</td>\n",
       "      <td>-0.498269</td>\n",
       "      <td>1.306725</td>\n",
       "      <td>-2.195733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497952</th>\n",
       "      <td>1</td>\n",
       "      <td>1323</td>\n",
       "      <td>38.93</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.009826</td>\n",
       "      <td>0.029426</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.994615</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.007559</td>\n",
       "      <td>0.006047</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>33.984074</td>\n",
       "      <td>0.256871</td>\n",
       "      <td>0.205497</td>\n",
       "      <td>0.077061</td>\n",
       "      <td>0.147743</td>\n",
       "      <td>-0.495217</td>\n",
       "      <td>0.356603</td>\n",
       "      <td>-2.157650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497953</th>\n",
       "      <td>4</td>\n",
       "      <td>505</td>\n",
       "      <td>61.61</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.043564</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.800455</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.015842</td>\n",
       "      <td>0.007921</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>8.196721</td>\n",
       "      <td>0.129849</td>\n",
       "      <td>0.064925</td>\n",
       "      <td>0.081156</td>\n",
       "      <td>0.375232</td>\n",
       "      <td>0.084578</td>\n",
       "      <td>2.143034</td>\n",
       "      <td>0.525180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497954</th>\n",
       "      <td>6</td>\n",
       "      <td>1792</td>\n",
       "      <td>98.21</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.054805</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.777059</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>18.246614</td>\n",
       "      <td>0.081458</td>\n",
       "      <td>0.020365</td>\n",
       "      <td>0.030547</td>\n",
       "      <td>0.062847</td>\n",
       "      <td>0.493280</td>\n",
       "      <td>0.401105</td>\n",
       "      <td>2.330642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497955</th>\n",
       "      <td>5</td>\n",
       "      <td>1603</td>\n",
       "      <td>33.78</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.021073</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.260000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.006238</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>47.454115</td>\n",
       "      <td>0.059207</td>\n",
       "      <td>0.296033</td>\n",
       "      <td>0.088810</td>\n",
       "      <td>0.296223</td>\n",
       "      <td>0.317961</td>\n",
       "      <td>0.387246</td>\n",
       "      <td>1.120883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497956</th>\n",
       "      <td>2</td>\n",
       "      <td>927</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.861667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179.303675</td>\n",
       "      <td>0.386847</td>\n",
       "      <td>0.580271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.419540</td>\n",
       "      <td>-0.288909</td>\n",
       "      <td>-2.062783</td>\n",
       "      <td>-1.302834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497957</th>\n",
       "      <td>5</td>\n",
       "      <td>1009</td>\n",
       "      <td>63.24</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010902</td>\n",
       "      <td>0.062676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.749091</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.010902</td>\n",
       "      <td>0.006938</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>15.955092</td>\n",
       "      <td>0.173941</td>\n",
       "      <td>0.110689</td>\n",
       "      <td>0.063251</td>\n",
       "      <td>0.365522</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>1.334533</td>\n",
       "      <td>1.481709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497958</th>\n",
       "      <td>2</td>\n",
       "      <td>257</td>\n",
       "      <td>90.49</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058366</td>\n",
       "      <td>0.352101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.032667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.840093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.462979</td>\n",
       "      <td>-0.291816</td>\n",
       "      <td>-2.142370</td>\n",
       "      <td>-1.380672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497959</th>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>53.68</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.511238</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.917143</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>1.956036</td>\n",
       "      <td>0.093145</td>\n",
       "      <td>0.018629</td>\n",
       "      <td>0.074516</td>\n",
       "      <td>0.085535</td>\n",
       "      <td>-0.122962</td>\n",
       "      <td>1.508674</td>\n",
       "      <td>-0.579206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497960</th>\n",
       "      <td>2</td>\n",
       "      <td>1553</td>\n",
       "      <td>6.14</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017386</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.227407</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>252.931596</td>\n",
       "      <td>1.628664</td>\n",
       "      <td>0.325733</td>\n",
       "      <td>0.651466</td>\n",
       "      <td>0.112413</td>\n",
       "      <td>-0.319560</td>\n",
       "      <td>1.427250</td>\n",
       "      <td>-1.957337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497961</th>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>71.89</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.133758</td>\n",
       "      <td>0.457898</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.423333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031847</td>\n",
       "      <td>0.019108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.183892</td>\n",
       "      <td>0.069551</td>\n",
       "      <td>0.041730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.547886</td>\n",
       "      <td>-0.502436</td>\n",
       "      <td>-2.263123</td>\n",
       "      <td>-2.100250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497962</th>\n",
       "      <td>4</td>\n",
       "      <td>380</td>\n",
       "      <td>17.38</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.044737</td>\n",
       "      <td>0.045737</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.022353</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>21.864212</td>\n",
       "      <td>0.345224</td>\n",
       "      <td>0.345224</td>\n",
       "      <td>0.115075</td>\n",
       "      <td>-0.127697</td>\n",
       "      <td>0.098907</td>\n",
       "      <td>-0.579296</td>\n",
       "      <td>0.504211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497963</th>\n",
       "      <td>2</td>\n",
       "      <td>1203</td>\n",
       "      <td>82.83</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.068853</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.805000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.004156</td>\n",
       "      <td>14.523723</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>0.060365</td>\n",
       "      <td>0.533351</td>\n",
       "      <td>-0.302982</td>\n",
       "      <td>1.737878</td>\n",
       "      <td>-1.260504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497964</th>\n",
       "      <td>3</td>\n",
       "      <td>1598</td>\n",
       "      <td>31.89</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017522</td>\n",
       "      <td>0.019956</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.138929</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.004380</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>50.109752</td>\n",
       "      <td>0.156789</td>\n",
       "      <td>0.219505</td>\n",
       "      <td>0.031358</td>\n",
       "      <td>-0.427080</td>\n",
       "      <td>-0.107822</td>\n",
       "      <td>-2.040401</td>\n",
       "      <td>-0.491006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497965</th>\n",
       "      <td>3</td>\n",
       "      <td>858</td>\n",
       "      <td>26.06</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>0.030373</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.212000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>32.924021</td>\n",
       "      <td>0.191865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191865</td>\n",
       "      <td>0.544451</td>\n",
       "      <td>-0.102936</td>\n",
       "      <td>1.894366</td>\n",
       "      <td>-0.498546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497966</th>\n",
       "      <td>6</td>\n",
       "      <td>449</td>\n",
       "      <td>47.71</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004454</td>\n",
       "      <td>0.106258</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.855000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.017817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.411025</td>\n",
       "      <td>0.083840</td>\n",
       "      <td>0.167680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.249344</td>\n",
       "      <td>0.526457</td>\n",
       "      <td>-1.385854</td>\n",
       "      <td>1.224655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497967</th>\n",
       "      <td>4</td>\n",
       "      <td>848</td>\n",
       "      <td>61.01</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.071946</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.336667</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>0.008255</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>13.899361</td>\n",
       "      <td>0.098345</td>\n",
       "      <td>0.114735</td>\n",
       "      <td>0.065563</td>\n",
       "      <td>0.508678</td>\n",
       "      <td>0.114009</td>\n",
       "      <td>1.209460</td>\n",
       "      <td>0.461805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497968</th>\n",
       "      <td>3</td>\n",
       "      <td>1191</td>\n",
       "      <td>69.48</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.058338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.740000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.141623</td>\n",
       "      <td>0.057571</td>\n",
       "      <td>0.086356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.257001</td>\n",
       "      <td>-0.075054</td>\n",
       "      <td>-1.558675</td>\n",
       "      <td>-0.304257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497969</th>\n",
       "      <td>6</td>\n",
       "      <td>583</td>\n",
       "      <td>70.73</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>0.121321</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.841250</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.242613</td>\n",
       "      <td>0.113106</td>\n",
       "      <td>0.014138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.398151</td>\n",
       "      <td>0.508976</td>\n",
       "      <td>-1.887037</td>\n",
       "      <td>1.748286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497970</th>\n",
       "      <td>6</td>\n",
       "      <td>229</td>\n",
       "      <td>64.02</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.074236</td>\n",
       "      <td>0.279563</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.765882</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030568</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.577007</td>\n",
       "      <td>0.109341</td>\n",
       "      <td>0.046860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.496375</td>\n",
       "      <td>0.501370</td>\n",
       "      <td>-2.147514</td>\n",
       "      <td>2.030763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497971</th>\n",
       "      <td>6</td>\n",
       "      <td>411</td>\n",
       "      <td>62.33</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>0.151655</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.308519</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.026764</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>6.593936</td>\n",
       "      <td>0.176480</td>\n",
       "      <td>0.032087</td>\n",
       "      <td>0.016044</td>\n",
       "      <td>-0.411929</td>\n",
       "      <td>0.488627</td>\n",
       "      <td>-1.692066</td>\n",
       "      <td>2.316603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497972</th>\n",
       "      <td>1</td>\n",
       "      <td>685</td>\n",
       "      <td>29.13</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.030657</td>\n",
       "      <td>0.042526</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.387143</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.013139</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>23.515276</td>\n",
       "      <td>0.034329</td>\n",
       "      <td>0.308960</td>\n",
       "      <td>0.171644</td>\n",
       "      <td>0.377302</td>\n",
       "      <td>-0.510459</td>\n",
       "      <td>1.913296</td>\n",
       "      <td>-2.275492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497973</th>\n",
       "      <td>1</td>\n",
       "      <td>1646</td>\n",
       "      <td>13.33</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004860</td>\n",
       "      <td>0.008098</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.666250</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.006683</td>\n",
       "      <td>0.006075</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>123.480870</td>\n",
       "      <td>0.825206</td>\n",
       "      <td>0.750188</td>\n",
       "      <td>0.075019</td>\n",
       "      <td>-0.144314</td>\n",
       "      <td>-0.481809</td>\n",
       "      <td>-1.318567</td>\n",
       "      <td>-1.700728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497974</th>\n",
       "      <td>3</td>\n",
       "      <td>1549</td>\n",
       "      <td>23.01</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012266</td>\n",
       "      <td>0.014855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.211053</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>67.318557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217297</td>\n",
       "      <td>0.043459</td>\n",
       "      <td>-0.371256</td>\n",
       "      <td>-0.102658</td>\n",
       "      <td>-1.896519</td>\n",
       "      <td>-0.482931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497975</th>\n",
       "      <td>6</td>\n",
       "      <td>1331</td>\n",
       "      <td>35.55</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.015778</td>\n",
       "      <td>0.026709</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.692857</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.006011</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>37.440225</td>\n",
       "      <td>0.225035</td>\n",
       "      <td>0.112518</td>\n",
       "      <td>0.056259</td>\n",
       "      <td>-0.172472</td>\n",
       "      <td>0.493332</td>\n",
       "      <td>-0.591919</td>\n",
       "      <td>2.473187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497976</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>60.86</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>3.043000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.532727</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.328623</td>\n",
       "      <td>0.049293</td>\n",
       "      <td>0.147880</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>-0.217885</td>\n",
       "      <td>0.111022</td>\n",
       "      <td>-1.527506</td>\n",
       "      <td>0.472024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497977</th>\n",
       "      <td>3</td>\n",
       "      <td>1584</td>\n",
       "      <td>76.14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>0.048068</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.007368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>20.803783</td>\n",
       "      <td>0.078802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013134</td>\n",
       "      <td>-0.359569</td>\n",
       "      <td>-0.106157</td>\n",
       "      <td>-1.883287</td>\n",
       "      <td>-0.455039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497978</th>\n",
       "      <td>6</td>\n",
       "      <td>1508</td>\n",
       "      <td>46.81</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.031041</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.405000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.215339</td>\n",
       "      <td>0.064089</td>\n",
       "      <td>0.149541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.271931</td>\n",
       "      <td>0.524596</td>\n",
       "      <td>-1.409193</td>\n",
       "      <td>1.295561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497979</th>\n",
       "      <td>6</td>\n",
       "      <td>906</td>\n",
       "      <td>55.48</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.025386</td>\n",
       "      <td>0.061236</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.412174</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>16.330209</td>\n",
       "      <td>0.162221</td>\n",
       "      <td>0.036049</td>\n",
       "      <td>0.090123</td>\n",
       "      <td>0.351504</td>\n",
       "      <td>0.481314</td>\n",
       "      <td>1.957192</td>\n",
       "      <td>2.290627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497980</th>\n",
       "      <td>6</td>\n",
       "      <td>1125</td>\n",
       "      <td>66.48</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.059093</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.006222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>16.922383</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060168</td>\n",
       "      <td>0.507549</td>\n",
       "      <td>0.507919</td>\n",
       "      <td>1.079857</td>\n",
       "      <td>1.089849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497981</th>\n",
       "      <td>2</td>\n",
       "      <td>1561</td>\n",
       "      <td>86.14</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017937</td>\n",
       "      <td>0.055183</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.076429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.006406</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>18.121662</td>\n",
       "      <td>0.116090</td>\n",
       "      <td>0.046436</td>\n",
       "      <td>0.023218</td>\n",
       "      <td>-0.223493</td>\n",
       "      <td>-0.311944</td>\n",
       "      <td>-0.612456</td>\n",
       "      <td>-1.974560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497982</th>\n",
       "      <td>6</td>\n",
       "      <td>1562</td>\n",
       "      <td>48.35</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011524</td>\n",
       "      <td>0.030954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.686111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.306101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.497936</td>\n",
       "      <td>0.506739</td>\n",
       "      <td>-2.083967</td>\n",
       "      <td>2.013481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497983</th>\n",
       "      <td>5</td>\n",
       "      <td>1070</td>\n",
       "      <td>92.80</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.021495</td>\n",
       "      <td>0.086729</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.034783</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>11.530172</td>\n",
       "      <td>0.118534</td>\n",
       "      <td>0.010776</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.183623</td>\n",
       "      <td>0.284008</td>\n",
       "      <td>1.379803</td>\n",
       "      <td>1.823357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497984</th>\n",
       "      <td>4</td>\n",
       "      <td>1128</td>\n",
       "      <td>39.32</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.034858</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.638333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>28.687691</td>\n",
       "      <td>0.101729</td>\n",
       "      <td>0.025432</td>\n",
       "      <td>0.076297</td>\n",
       "      <td>-0.063716</td>\n",
       "      <td>0.083412</td>\n",
       "      <td>0.352569</td>\n",
       "      <td>0.543517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497985</th>\n",
       "      <td>6</td>\n",
       "      <td>172</td>\n",
       "      <td>61.32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098837</td>\n",
       "      <td>0.356512</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.607059</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.029070</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>2.804958</td>\n",
       "      <td>0.081539</td>\n",
       "      <td>0.032616</td>\n",
       "      <td>0.016308</td>\n",
       "      <td>-0.331533</td>\n",
       "      <td>0.496927</td>\n",
       "      <td>-1.555691</td>\n",
       "      <td>2.213828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497986</th>\n",
       "      <td>6</td>\n",
       "      <td>369</td>\n",
       "      <td>82.69</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.224092</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.890833</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.018970</td>\n",
       "      <td>0.005420</td>\n",
       "      <td>0.005420</td>\n",
       "      <td>4.462450</td>\n",
       "      <td>0.084654</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>-0.070648</td>\n",
       "      <td>0.500310</td>\n",
       "      <td>-0.527367</td>\n",
       "      <td>2.119238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497987</th>\n",
       "      <td>5</td>\n",
       "      <td>161</td>\n",
       "      <td>88.57</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111801</td>\n",
       "      <td>0.550124</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.920556</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.031056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.817771</td>\n",
       "      <td>0.079034</td>\n",
       "      <td>0.056453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.484007</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>-2.262990</td>\n",
       "      <td>1.453666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497988</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>50.17</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1.416667</td>\n",
       "      <td>4.180833</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.951176</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.239187</td>\n",
       "      <td>0.039864</td>\n",
       "      <td>0.199322</td>\n",
       "      <td>0.099661</td>\n",
       "      <td>0.444640</td>\n",
       "      <td>-0.105156</td>\n",
       "      <td>1.934557</td>\n",
       "      <td>-0.590354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497989</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>87.02</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.230769</td>\n",
       "      <td>6.693846</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.438750</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.149391</td>\n",
       "      <td>0.068950</td>\n",
       "      <td>0.091933</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>-0.259910</td>\n",
       "      <td>-0.493856</td>\n",
       "      <td>-1.526429</td>\n",
       "      <td>-2.059335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497990</th>\n",
       "      <td>5</td>\n",
       "      <td>1312</td>\n",
       "      <td>29.28</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013720</td>\n",
       "      <td>0.022317</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.626667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>44.808743</td>\n",
       "      <td>0.204918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170765</td>\n",
       "      <td>0.369721</td>\n",
       "      <td>0.283148</td>\n",
       "      <td>2.120246</td>\n",
       "      <td>1.692070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497991</th>\n",
       "      <td>6</td>\n",
       "      <td>1768</td>\n",
       "      <td>45.98</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>0.026007</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.196000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>38.451501</td>\n",
       "      <td>0.130492</td>\n",
       "      <td>0.086994</td>\n",
       "      <td>0.065246</td>\n",
       "      <td>0.215417</td>\n",
       "      <td>0.507956</td>\n",
       "      <td>0.365629</td>\n",
       "      <td>1.862777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497992</th>\n",
       "      <td>6</td>\n",
       "      <td>1331</td>\n",
       "      <td>86.17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015026</td>\n",
       "      <td>0.064741</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.308500</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.446211</td>\n",
       "      <td>0.034815</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.558711</td>\n",
       "      <td>0.495557</td>\n",
       "      <td>-2.205069</td>\n",
       "      <td>2.095035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497993</th>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>75.16</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.174375</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.684286</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.851517</td>\n",
       "      <td>0.039915</td>\n",
       "      <td>0.079830</td>\n",
       "      <td>0.039915</td>\n",
       "      <td>-0.064012</td>\n",
       "      <td>0.484775</td>\n",
       "      <td>0.356794</td>\n",
       "      <td>2.512816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497994</th>\n",
       "      <td>2</td>\n",
       "      <td>217</td>\n",
       "      <td>30.89</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027650</td>\n",
       "      <td>0.142350</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.148333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.004608</td>\n",
       "      <td>7.024927</td>\n",
       "      <td>0.097119</td>\n",
       "      <td>0.097119</td>\n",
       "      <td>0.032373</td>\n",
       "      <td>-0.212926</td>\n",
       "      <td>-0.290807</td>\n",
       "      <td>-1.399179</td>\n",
       "      <td>-1.320059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497995</th>\n",
       "      <td>3</td>\n",
       "      <td>1123</td>\n",
       "      <td>98.64</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013357</td>\n",
       "      <td>0.087836</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.576000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.008014</td>\n",
       "      <td>0.008014</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>11.384834</td>\n",
       "      <td>0.091241</td>\n",
       "      <td>0.091241</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>-0.221773</td>\n",
       "      <td>-0.090620</td>\n",
       "      <td>-1.666022</td>\n",
       "      <td>-0.446184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497996</th>\n",
       "      <td>2</td>\n",
       "      <td>404</td>\n",
       "      <td>39.61</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.074257</td>\n",
       "      <td>0.098045</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.320333</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.017327</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>10.199445</td>\n",
       "      <td>0.252461</td>\n",
       "      <td>0.176723</td>\n",
       "      <td>0.100985</td>\n",
       "      <td>0.127347</td>\n",
       "      <td>-0.316765</td>\n",
       "      <td>1.455288</td>\n",
       "      <td>-1.925460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497997</th>\n",
       "      <td>6</td>\n",
       "      <td>1724</td>\n",
       "      <td>61.73</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.035806</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.115333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>27.928074</td>\n",
       "      <td>0.097197</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.048599</td>\n",
       "      <td>0.055712</td>\n",
       "      <td>0.493124</td>\n",
       "      <td>0.396777</td>\n",
       "      <td>2.330573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497998</th>\n",
       "      <td>4</td>\n",
       "      <td>1072</td>\n",
       "      <td>48.65</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.018657</td>\n",
       "      <td>0.045382</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.432500</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.004664</td>\n",
       "      <td>22.034943</td>\n",
       "      <td>0.041110</td>\n",
       "      <td>0.164440</td>\n",
       "      <td>0.102775</td>\n",
       "      <td>0.392459</td>\n",
       "      <td>0.089610</td>\n",
       "      <td>2.167140</td>\n",
       "      <td>0.520323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497999</th>\n",
       "      <td>4</td>\n",
       "      <td>1146</td>\n",
       "      <td>83.19</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020942</td>\n",
       "      <td>0.072592</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.466250</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>13.775694</td>\n",
       "      <td>0.120207</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.036062</td>\n",
       "      <td>-0.018873</td>\n",
       "      <td>0.085490</td>\n",
       "      <td>0.346169</td>\n",
       "      <td>0.530277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498000</th>\n",
       "      <td>4</td>\n",
       "      <td>618</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012945</td>\n",
       "      <td>0.006440</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.008091</td>\n",
       "      <td>155.276382</td>\n",
       "      <td>0.502513</td>\n",
       "      <td>0.251256</td>\n",
       "      <td>1.256281</td>\n",
       "      <td>0.472828</td>\n",
       "      <td>0.093066</td>\n",
       "      <td>1.900436</td>\n",
       "      <td>0.471335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498001</th>\n",
       "      <td>4</td>\n",
       "      <td>1120</td>\n",
       "      <td>58.73</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.018750</td>\n",
       "      <td>0.052437</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.796667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>19.070322</td>\n",
       "      <td>0.170271</td>\n",
       "      <td>0.119190</td>\n",
       "      <td>0.085135</td>\n",
       "      <td>0.417751</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>2.159581</td>\n",
       "      <td>0.548878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498002</th>\n",
       "      <td>2</td>\n",
       "      <td>878</td>\n",
       "      <td>82.84</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012528</td>\n",
       "      <td>0.094351</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.530909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>10.598745</td>\n",
       "      <td>0.048286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>-0.275274</td>\n",
       "      <td>-0.298547</td>\n",
       "      <td>-1.514400</td>\n",
       "      <td>-1.433863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498003</th>\n",
       "      <td>2</td>\n",
       "      <td>881</td>\n",
       "      <td>35.51</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030647</td>\n",
       "      <td>0.040306</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.315185</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.005675</td>\n",
       "      <td>0.005675</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>24.809913</td>\n",
       "      <td>0.140805</td>\n",
       "      <td>0.140805</td>\n",
       "      <td>0.028161</td>\n",
       "      <td>-0.428842</td>\n",
       "      <td>-0.309141</td>\n",
       "      <td>-1.906032</td>\n",
       "      <td>-1.846047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498004</th>\n",
       "      <td>1</td>\n",
       "      <td>892</td>\n",
       "      <td>95.82</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.023543</td>\n",
       "      <td>0.107422</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.562857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.011211</td>\n",
       "      <td>0.006726</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>9.309121</td>\n",
       "      <td>0.104362</td>\n",
       "      <td>0.062617</td>\n",
       "      <td>0.031309</td>\n",
       "      <td>0.056918</td>\n",
       "      <td>-0.505272</td>\n",
       "      <td>0.323255</td>\n",
       "      <td>-2.369564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498005</th>\n",
       "      <td>4</td>\n",
       "      <td>1154</td>\n",
       "      <td>9.15</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018198</td>\n",
       "      <td>0.007929</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.435714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>126.120219</td>\n",
       "      <td>1.202186</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.109290</td>\n",
       "      <td>-0.311736</td>\n",
       "      <td>0.102476</td>\n",
       "      <td>-1.858715</td>\n",
       "      <td>0.462043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498006</th>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>50.93</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.101860</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.183125</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>9.817396</td>\n",
       "      <td>0.137444</td>\n",
       "      <td>0.098174</td>\n",
       "      <td>0.039270</td>\n",
       "      <td>-0.106676</td>\n",
       "      <td>-0.300231</td>\n",
       "      <td>-0.552954</td>\n",
       "      <td>-1.736231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498007</th>\n",
       "      <td>6</td>\n",
       "      <td>438</td>\n",
       "      <td>23.23</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.025114</td>\n",
       "      <td>0.053037</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.111818</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.004566</td>\n",
       "      <td>18.854929</td>\n",
       "      <td>0.172191</td>\n",
       "      <td>0.387430</td>\n",
       "      <td>0.086096</td>\n",
       "      <td>-0.042063</td>\n",
       "      <td>0.507938</td>\n",
       "      <td>-0.523065</td>\n",
       "      <td>2.124035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498008</th>\n",
       "      <td>5</td>\n",
       "      <td>580</td>\n",
       "      <td>10.63</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.018328</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.379643</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>54.562559</td>\n",
       "      <td>0.752587</td>\n",
       "      <td>0.282220</td>\n",
       "      <td>0.282220</td>\n",
       "      <td>-0.082119</td>\n",
       "      <td>0.282185</td>\n",
       "      <td>0.356861</td>\n",
       "      <td>1.975282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498009</th>\n",
       "      <td>5</td>\n",
       "      <td>1607</td>\n",
       "      <td>71.42</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.044443</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.806667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>22.500700</td>\n",
       "      <td>0.112013</td>\n",
       "      <td>0.084010</td>\n",
       "      <td>0.042005</td>\n",
       "      <td>0.323571</td>\n",
       "      <td>0.316354</td>\n",
       "      <td>0.399155</td>\n",
       "      <td>1.137945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498010</th>\n",
       "      <td>5</td>\n",
       "      <td>182</td>\n",
       "      <td>56.29</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.309286</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.036250</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>3.233256</td>\n",
       "      <td>0.124356</td>\n",
       "      <td>0.177651</td>\n",
       "      <td>0.071061</td>\n",
       "      <td>0.410503</td>\n",
       "      <td>0.308894</td>\n",
       "      <td>1.298575</td>\n",
       "      <td>1.347872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498011</th>\n",
       "      <td>2</td>\n",
       "      <td>873</td>\n",
       "      <td>56.69</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028637</td>\n",
       "      <td>0.064937</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.267600</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.399541</td>\n",
       "      <td>0.088199</td>\n",
       "      <td>0.088199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.582822</td>\n",
       "      <td>-0.304221</td>\n",
       "      <td>-2.523409</td>\n",
       "      <td>-1.617010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498012</th>\n",
       "      <td>5</td>\n",
       "      <td>1398</td>\n",
       "      <td>27.71</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.019821</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.065769</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.007153</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>50.451101</td>\n",
       "      <td>0.360881</td>\n",
       "      <td>0.180440</td>\n",
       "      <td>0.072176</td>\n",
       "      <td>-0.213236</td>\n",
       "      <td>0.290083</td>\n",
       "      <td>-0.611810</td>\n",
       "      <td>1.999489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498013</th>\n",
       "      <td>5</td>\n",
       "      <td>1014</td>\n",
       "      <td>63.96</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.063077</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.995000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>15.853659</td>\n",
       "      <td>0.031270</td>\n",
       "      <td>0.093809</td>\n",
       "      <td>0.031270</td>\n",
       "      <td>-0.021911</td>\n",
       "      <td>0.308046</td>\n",
       "      <td>-0.489122</td>\n",
       "      <td>1.506339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498014</th>\n",
       "      <td>5</td>\n",
       "      <td>775</td>\n",
       "      <td>40.67</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016774</td>\n",
       "      <td>0.052477</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.128462</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>19.055815</td>\n",
       "      <td>0.024588</td>\n",
       "      <td>0.245881</td>\n",
       "      <td>0.098353</td>\n",
       "      <td>0.299922</td>\n",
       "      <td>0.301256</td>\n",
       "      <td>1.405477</td>\n",
       "      <td>1.577039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498015</th>\n",
       "      <td>4</td>\n",
       "      <td>1286</td>\n",
       "      <td>58.18</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006998</td>\n",
       "      <td>0.045241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.464444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>22.103816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051564</td>\n",
       "      <td>0.017188</td>\n",
       "      <td>-0.259565</td>\n",
       "      <td>0.105410</td>\n",
       "      <td>-1.556376</td>\n",
       "      <td>0.435602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498016</th>\n",
       "      <td>1</td>\n",
       "      <td>603</td>\n",
       "      <td>71.47</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.118524</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.735000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.018242</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>8.437106</td>\n",
       "      <td>0.153911</td>\n",
       "      <td>0.027984</td>\n",
       "      <td>0.041976</td>\n",
       "      <td>0.369656</td>\n",
       "      <td>-0.483745</td>\n",
       "      <td>0.332684</td>\n",
       "      <td>-0.963786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498017</th>\n",
       "      <td>2</td>\n",
       "      <td>209</td>\n",
       "      <td>91.97</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086124</td>\n",
       "      <td>0.440048</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.109444</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043062</td>\n",
       "      <td>0.014354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.272480</td>\n",
       "      <td>0.097858</td>\n",
       "      <td>0.032619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.487168</td>\n",
       "      <td>-0.298225</td>\n",
       "      <td>-2.261619</td>\n",
       "      <td>-1.476540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498018</th>\n",
       "      <td>5</td>\n",
       "      <td>1001</td>\n",
       "      <td>74.24</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008991</td>\n",
       "      <td>0.074166</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.248889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.005994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.483297</td>\n",
       "      <td>0.094289</td>\n",
       "      <td>0.080819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375193</td>\n",
       "      <td>0.313870</td>\n",
       "      <td>-2.112742</td>\n",
       "      <td>1.316592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498019</th>\n",
       "      <td>5</td>\n",
       "      <td>1324</td>\n",
       "      <td>28.98</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.021888</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.980000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>45.686680</td>\n",
       "      <td>0.069013</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.138026</td>\n",
       "      <td>0.604215</td>\n",
       "      <td>0.314179</td>\n",
       "      <td>3.388991</td>\n",
       "      <td>0.134462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498020</th>\n",
       "      <td>5</td>\n",
       "      <td>951</td>\n",
       "      <td>10.74</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005258</td>\n",
       "      <td>0.011293</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.148000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009464</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.005258</td>\n",
       "      <td>88.547486</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.186220</td>\n",
       "      <td>0.465549</td>\n",
       "      <td>0.580511</td>\n",
       "      <td>0.300506</td>\n",
       "      <td>1.736223</td>\n",
       "      <td>1.153623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498021</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>41.16</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>10.290000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.646400</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.097182</td>\n",
       "      <td>0.145773</td>\n",
       "      <td>0.170068</td>\n",
       "      <td>0.097182</td>\n",
       "      <td>0.164367</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>1.208001</td>\n",
       "      <td>0.006072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498022</th>\n",
       "      <td>6</td>\n",
       "      <td>1341</td>\n",
       "      <td>86.28</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.064340</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.392500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.002983</td>\n",
       "      <td>15.542420</td>\n",
       "      <td>0.069541</td>\n",
       "      <td>0.115902</td>\n",
       "      <td>0.046361</td>\n",
       "      <td>0.304757</td>\n",
       "      <td>0.500109</td>\n",
       "      <td>1.370524</td>\n",
       "      <td>2.253173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498023</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>56.76</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>4.054286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.352000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246653</td>\n",
       "      <td>0.088090</td>\n",
       "      <td>0.017618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.380183</td>\n",
       "      <td>-0.288317</td>\n",
       "      <td>-1.854480</td>\n",
       "      <td>-1.200103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498024</th>\n",
       "      <td>4</td>\n",
       "      <td>588</td>\n",
       "      <td>40.94</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.069626</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.274444</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>14.362482</td>\n",
       "      <td>0.024426</td>\n",
       "      <td>0.097704</td>\n",
       "      <td>0.122130</td>\n",
       "      <td>0.378489</td>\n",
       "      <td>0.086512</td>\n",
       "      <td>2.127613</td>\n",
       "      <td>0.537959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498025</th>\n",
       "      <td>3</td>\n",
       "      <td>1580</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010759</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.350588</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.003165</td>\n",
       "      <td>265.100671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.671141</td>\n",
       "      <td>0.838926</td>\n",
       "      <td>0.372136</td>\n",
       "      <td>-0.112801</td>\n",
       "      <td>2.034317</td>\n",
       "      <td>-0.552250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498026</th>\n",
       "      <td>1</td>\n",
       "      <td>1814</td>\n",
       "      <td>12.55</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>0.006918</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.140909</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.004961</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>144.541833</td>\n",
       "      <td>0.318725</td>\n",
       "      <td>0.717131</td>\n",
       "      <td>0.318725</td>\n",
       "      <td>0.323437</td>\n",
       "      <td>-0.496505</td>\n",
       "      <td>1.259555</td>\n",
       "      <td>-2.058402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498027</th>\n",
       "      <td>1</td>\n",
       "      <td>847</td>\n",
       "      <td>54.17</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028335</td>\n",
       "      <td>0.063955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.257083</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011806</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>15.635961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184604</td>\n",
       "      <td>0.036921</td>\n",
       "      <td>-0.195251</td>\n",
       "      <td>-0.504122</td>\n",
       "      <td>-0.658638</td>\n",
       "      <td>-2.439608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498028</th>\n",
       "      <td>2</td>\n",
       "      <td>856</td>\n",
       "      <td>42.39</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>0.049521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.027857</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>20.193442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117952</td>\n",
       "      <td>0.117952</td>\n",
       "      <td>0.428977</td>\n",
       "      <td>-0.308174</td>\n",
       "      <td>2.082868</td>\n",
       "      <td>-1.612717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498029</th>\n",
       "      <td>3</td>\n",
       "      <td>1380</td>\n",
       "      <td>92.66</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013768</td>\n",
       "      <td>0.067145</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.876842</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>14.893158</td>\n",
       "      <td>0.043169</td>\n",
       "      <td>0.032376</td>\n",
       "      <td>0.053961</td>\n",
       "      <td>0.391932</td>\n",
       "      <td>-0.113856</td>\n",
       "      <td>2.113660</td>\n",
       "      <td>-0.578362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498030</th>\n",
       "      <td>2</td>\n",
       "      <td>560</td>\n",
       "      <td>51.69</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030357</td>\n",
       "      <td>0.092304</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.040588</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>10.833817</td>\n",
       "      <td>0.193461</td>\n",
       "      <td>0.096731</td>\n",
       "      <td>0.019346</td>\n",
       "      <td>-0.286111</td>\n",
       "      <td>-0.297724</td>\n",
       "      <td>-1.692565</td>\n",
       "      <td>-1.584580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498031</th>\n",
       "      <td>3</td>\n",
       "      <td>865</td>\n",
       "      <td>1.37</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017341</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.091333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.012717</td>\n",
       "      <td>0.008092</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>631.386861</td>\n",
       "      <td>8.029197</td>\n",
       "      <td>5.109489</td>\n",
       "      <td>1.459854</td>\n",
       "      <td>-0.076531</td>\n",
       "      <td>-0.096455</td>\n",
       "      <td>-0.581668</td>\n",
       "      <td>-0.510481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498032</th>\n",
       "      <td>1</td>\n",
       "      <td>1377</td>\n",
       "      <td>44.34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.021060</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.528966</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>31.055480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045106</td>\n",
       "      <td>0.045106</td>\n",
       "      <td>-0.312384</td>\n",
       "      <td>-0.518153</td>\n",
       "      <td>-0.633360</td>\n",
       "      <td>-2.528799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498033</th>\n",
       "      <td>4</td>\n",
       "      <td>1162</td>\n",
       "      <td>50.89</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010327</td>\n",
       "      <td>0.043795</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.240833</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.006885</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>22.833563</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>0.058951</td>\n",
       "      <td>0.107338</td>\n",
       "      <td>0.097948</td>\n",
       "      <td>0.396946</td>\n",
       "      <td>0.495607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498034</th>\n",
       "      <td>4</td>\n",
       "      <td>442</td>\n",
       "      <td>93.62</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042986</td>\n",
       "      <td>0.211810</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.927368</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.721213</td>\n",
       "      <td>0.106815</td>\n",
       "      <td>0.106815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.440895</td>\n",
       "      <td>0.108936</td>\n",
       "      <td>-2.268001</td>\n",
       "      <td>0.498674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498035</th>\n",
       "      <td>6</td>\n",
       "      <td>426</td>\n",
       "      <td>86.54</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.203146</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.817500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>4.922579</td>\n",
       "      <td>0.069332</td>\n",
       "      <td>0.034666</td>\n",
       "      <td>0.023111</td>\n",
       "      <td>-0.013686</td>\n",
       "      <td>0.505769</td>\n",
       "      <td>-0.504177</td>\n",
       "      <td>1.956118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498036</th>\n",
       "      <td>3</td>\n",
       "      <td>1115</td>\n",
       "      <td>12.44</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008072</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.382222</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.006278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.630225</td>\n",
       "      <td>0.160772</td>\n",
       "      <td>0.562701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.419783</td>\n",
       "      <td>-0.087195</td>\n",
       "      <td>-2.167825</td>\n",
       "      <td>-0.411364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498037</th>\n",
       "      <td>2</td>\n",
       "      <td>994</td>\n",
       "      <td>51.18</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>0.051489</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.795000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>19.421649</td>\n",
       "      <td>0.175850</td>\n",
       "      <td>0.195389</td>\n",
       "      <td>0.078156</td>\n",
       "      <td>0.512460</td>\n",
       "      <td>-0.283281</td>\n",
       "      <td>1.147365</td>\n",
       "      <td>-1.182094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498038</th>\n",
       "      <td>6</td>\n",
       "      <td>1006</td>\n",
       "      <td>76.66</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.020875</td>\n",
       "      <td>0.076203</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.650476</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>13.122880</td>\n",
       "      <td>0.026089</td>\n",
       "      <td>0.130446</td>\n",
       "      <td>0.052178</td>\n",
       "      <td>0.221764</td>\n",
       "      <td>0.493659</td>\n",
       "      <td>1.374972</td>\n",
       "      <td>2.395316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498039</th>\n",
       "      <td>3</td>\n",
       "      <td>1377</td>\n",
       "      <td>87.61</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>0.063624</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.964545</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.717384</td>\n",
       "      <td>0.068485</td>\n",
       "      <td>0.022828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.432407</td>\n",
       "      <td>-0.092852</td>\n",
       "      <td>-2.211316</td>\n",
       "      <td>-0.455448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498040</th>\n",
       "      <td>5</td>\n",
       "      <td>695</td>\n",
       "      <td>3.77</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011511</td>\n",
       "      <td>0.005424</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.471250</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>184.350133</td>\n",
       "      <td>0.530504</td>\n",
       "      <td>0.530504</td>\n",
       "      <td>1.326260</td>\n",
       "      <td>0.480751</td>\n",
       "      <td>0.294168</td>\n",
       "      <td>1.935878</td>\n",
       "      <td>1.365138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498041</th>\n",
       "      <td>4</td>\n",
       "      <td>235</td>\n",
       "      <td>58.68</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.046809</td>\n",
       "      <td>0.249702</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.334545</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.038298</td>\n",
       "      <td>0.012766</td>\n",
       "      <td>4.004772</td>\n",
       "      <td>0.170416</td>\n",
       "      <td>0.153374</td>\n",
       "      <td>0.051125</td>\n",
       "      <td>0.189705</td>\n",
       "      <td>0.107790</td>\n",
       "      <td>0.357380</td>\n",
       "      <td>0.473245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498042</th>\n",
       "      <td>5</td>\n",
       "      <td>1580</td>\n",
       "      <td>59.92</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>0.037924</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.960000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.003165</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>26.368491</td>\n",
       "      <td>0.083445</td>\n",
       "      <td>0.150200</td>\n",
       "      <td>0.066756</td>\n",
       "      <td>0.591405</td>\n",
       "      <td>0.320574</td>\n",
       "      <td>0.931662</td>\n",
       "      <td>0.800498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498043</th>\n",
       "      <td>3</td>\n",
       "      <td>1467</td>\n",
       "      <td>86.97</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.059284</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.663333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>16.867885</td>\n",
       "      <td>0.080488</td>\n",
       "      <td>0.068989</td>\n",
       "      <td>0.011498</td>\n",
       "      <td>-0.184398</td>\n",
       "      <td>-0.088165</td>\n",
       "      <td>-1.532502</td>\n",
       "      <td>-0.446634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498044</th>\n",
       "      <td>2</td>\n",
       "      <td>436</td>\n",
       "      <td>56.54</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.043578</td>\n",
       "      <td>0.129679</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.975789</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>7.711355</td>\n",
       "      <td>0.159179</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>0.088433</td>\n",
       "      <td>0.398903</td>\n",
       "      <td>-0.314244</td>\n",
       "      <td>2.098449</td>\n",
       "      <td>-1.709724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498045</th>\n",
       "      <td>1</td>\n",
       "      <td>276</td>\n",
       "      <td>55.27</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.200254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.047037</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.018116</td>\n",
       "      <td>4.993667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162837</td>\n",
       "      <td>0.090465</td>\n",
       "      <td>0.313270</td>\n",
       "      <td>-0.516511</td>\n",
       "      <td>1.872921</td>\n",
       "      <td>-2.332967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498046</th>\n",
       "      <td>3</td>\n",
       "      <td>1150</td>\n",
       "      <td>31.81</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>0.027661</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.446923</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>36.152153</td>\n",
       "      <td>0.062873</td>\n",
       "      <td>0.251493</td>\n",
       "      <td>0.062873</td>\n",
       "      <td>-0.082362</td>\n",
       "      <td>-0.095233</td>\n",
       "      <td>-0.550952</td>\n",
       "      <td>-0.499150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498047</th>\n",
       "      <td>3</td>\n",
       "      <td>1257</td>\n",
       "      <td>68.08</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012729</td>\n",
       "      <td>0.054161</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.255000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>18.463572</td>\n",
       "      <td>0.029377</td>\n",
       "      <td>0.058754</td>\n",
       "      <td>0.014689</td>\n",
       "      <td>-0.319074</td>\n",
       "      <td>-0.099784</td>\n",
       "      <td>-1.836191</td>\n",
       "      <td>-0.450929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498048</th>\n",
       "      <td>6</td>\n",
       "      <td>1329</td>\n",
       "      <td>90.91</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020316</td>\n",
       "      <td>0.068405</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.367037</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.618854</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.109999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.577908</td>\n",
       "      <td>0.498470</td>\n",
       "      <td>-2.215459</td>\n",
       "      <td>2.116667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498049</th>\n",
       "      <td>6</td>\n",
       "      <td>74</td>\n",
       "      <td>39.14</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>0.528919</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.348889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>1.890649</td>\n",
       "      <td>0.229944</td>\n",
       "      <td>0.025549</td>\n",
       "      <td>0.076648</td>\n",
       "      <td>0.140629</td>\n",
       "      <td>0.499588</td>\n",
       "      <td>0.382790</td>\n",
       "      <td>1.905114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498050</th>\n",
       "      <td>5</td>\n",
       "      <td>296</td>\n",
       "      <td>90.15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101351</td>\n",
       "      <td>0.304561</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.005000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.283417</td>\n",
       "      <td>0.033278</td>\n",
       "      <td>0.044370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.643651</td>\n",
       "      <td>0.289025</td>\n",
       "      <td>-2.446935</td>\n",
       "      <td>1.607977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498051</th>\n",
       "      <td>5</td>\n",
       "      <td>550</td>\n",
       "      <td>39.44</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.071709</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.440000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>13.945233</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.050710</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>-0.086355</td>\n",
       "      <td>0.317151</td>\n",
       "      <td>-0.224669</td>\n",
       "      <td>0.960902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498052</th>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>97.15</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.630844</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48.575000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>1.585178</td>\n",
       "      <td>0.072054</td>\n",
       "      <td>0.020587</td>\n",
       "      <td>0.051467</td>\n",
       "      <td>0.758849</td>\n",
       "      <td>-0.489491</td>\n",
       "      <td>1.198422</td>\n",
       "      <td>-0.940836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498053</th>\n",
       "      <td>4</td>\n",
       "      <td>1674</td>\n",
       "      <td>22.70</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.013560</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.675000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>73.744493</td>\n",
       "      <td>0.308370</td>\n",
       "      <td>0.176211</td>\n",
       "      <td>0.220264</td>\n",
       "      <td>0.618356</td>\n",
       "      <td>0.104645</td>\n",
       "      <td>1.677158</td>\n",
       "      <td>0.414093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498054</th>\n",
       "      <td>5</td>\n",
       "      <td>1542</td>\n",
       "      <td>79.01</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.019455</td>\n",
       "      <td>0.051239</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.633667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.003243</td>\n",
       "      <td>19.516517</td>\n",
       "      <td>0.075940</td>\n",
       "      <td>0.050627</td>\n",
       "      <td>0.063283</td>\n",
       "      <td>0.279533</td>\n",
       "      <td>0.276435</td>\n",
       "      <td>2.125745</td>\n",
       "      <td>1.806038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498055</th>\n",
       "      <td>2</td>\n",
       "      <td>1210</td>\n",
       "      <td>83.79</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021488</td>\n",
       "      <td>0.069248</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.222692</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>0.004959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.440864</td>\n",
       "      <td>0.107411</td>\n",
       "      <td>0.071608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.557716</td>\n",
       "      <td>-0.302632</td>\n",
       "      <td>-2.440468</td>\n",
       "      <td>-1.617686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498056</th>\n",
       "      <td>5</td>\n",
       "      <td>289</td>\n",
       "      <td>80.54</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103806</td>\n",
       "      <td>0.278685</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.684667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.588279</td>\n",
       "      <td>0.037249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.675826</td>\n",
       "      <td>0.284363</td>\n",
       "      <td>-2.426243</td>\n",
       "      <td>1.566113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498057</th>\n",
       "      <td>6</td>\n",
       "      <td>1208</td>\n",
       "      <td>44.88</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.020695</td>\n",
       "      <td>0.037152</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.795200</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>26.916221</td>\n",
       "      <td>0.222816</td>\n",
       "      <td>0.200535</td>\n",
       "      <td>0.044563</td>\n",
       "      <td>-0.167101</td>\n",
       "      <td>0.495667</td>\n",
       "      <td>-0.624764</td>\n",
       "      <td>2.493826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498058</th>\n",
       "      <td>2</td>\n",
       "      <td>1401</td>\n",
       "      <td>17.78</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.012691</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.780000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>78.796400</td>\n",
       "      <td>0.562430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056243</td>\n",
       "      <td>-0.025829</td>\n",
       "      <td>-0.279715</td>\n",
       "      <td>3.210981</td>\n",
       "      <td>0.048958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498059</th>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>26.42</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.287174</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.210000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>3.482210</td>\n",
       "      <td>0.302801</td>\n",
       "      <td>0.227101</td>\n",
       "      <td>0.113550</td>\n",
       "      <td>0.356206</td>\n",
       "      <td>-0.081713</td>\n",
       "      <td>0.458718</td>\n",
       "      <td>-0.467975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498060</th>\n",
       "      <td>1</td>\n",
       "      <td>513</td>\n",
       "      <td>88.74</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015595</td>\n",
       "      <td>0.172982</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.092500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.013645</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.007797</td>\n",
       "      <td>5.780933</td>\n",
       "      <td>0.078882</td>\n",
       "      <td>0.022538</td>\n",
       "      <td>0.045076</td>\n",
       "      <td>0.358400</td>\n",
       "      <td>-0.499654</td>\n",
       "      <td>1.207683</td>\n",
       "      <td>-1.849035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498061</th>\n",
       "      <td>5</td>\n",
       "      <td>1161</td>\n",
       "      <td>80.91</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.018949</td>\n",
       "      <td>0.069690</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.677727</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.008613</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>14.349277</td>\n",
       "      <td>0.123594</td>\n",
       "      <td>0.061797</td>\n",
       "      <td>0.049438</td>\n",
       "      <td>0.215484</td>\n",
       "      <td>0.289231</td>\n",
       "      <td>1.471976</td>\n",
       "      <td>1.844743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498062</th>\n",
       "      <td>6</td>\n",
       "      <td>1219</td>\n",
       "      <td>90.48</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009024</td>\n",
       "      <td>0.074225</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.225455</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>13.472591</td>\n",
       "      <td>0.088417</td>\n",
       "      <td>0.088417</td>\n",
       "      <td>0.044209</td>\n",
       "      <td>0.366828</td>\n",
       "      <td>0.503845</td>\n",
       "      <td>1.267551</td>\n",
       "      <td>1.988822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498063</th>\n",
       "      <td>5</td>\n",
       "      <td>1006</td>\n",
       "      <td>19.05</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015905</td>\n",
       "      <td>0.018936</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.190625</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.007952</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>52.808399</td>\n",
       "      <td>0.419948</td>\n",
       "      <td>0.524934</td>\n",
       "      <td>0.157480</td>\n",
       "      <td>0.108321</td>\n",
       "      <td>0.302500</td>\n",
       "      <td>0.356638</td>\n",
       "      <td>1.712943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498064</th>\n",
       "      <td>3</td>\n",
       "      <td>1311</td>\n",
       "      <td>9.85</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.012967</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.579412</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>133.096447</td>\n",
       "      <td>0.101523</td>\n",
       "      <td>1.015228</td>\n",
       "      <td>0.304569</td>\n",
       "      <td>0.056719</td>\n",
       "      <td>-0.100213</td>\n",
       "      <td>0.369381</td>\n",
       "      <td>-0.576603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498065</th>\n",
       "      <td>5</td>\n",
       "      <td>1345</td>\n",
       "      <td>61.57</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.045777</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.676957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>21.845054</td>\n",
       "      <td>0.064967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064967</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.280736</td>\n",
       "      <td>1.504133</td>\n",
       "      <td>1.936499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498066</th>\n",
       "      <td>6</td>\n",
       "      <td>80</td>\n",
       "      <td>64.53</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.806625</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.585000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.239733</td>\n",
       "      <td>0.061987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077483</td>\n",
       "      <td>0.372415</td>\n",
       "      <td>0.482587</td>\n",
       "      <td>1.873563</td>\n",
       "      <td>2.192983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498067</th>\n",
       "      <td>2</td>\n",
       "      <td>1623</td>\n",
       "      <td>12.69</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.007819</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.345000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>127.895981</td>\n",
       "      <td>0.630418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236407</td>\n",
       "      <td>0.273070</td>\n",
       "      <td>-0.290999</td>\n",
       "      <td>0.547632</td>\n",
       "      <td>-1.137739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498068</th>\n",
       "      <td>3</td>\n",
       "      <td>361</td>\n",
       "      <td>79.04</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022161</td>\n",
       "      <td>0.218947</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.880000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027701</td>\n",
       "      <td>0.008310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.567308</td>\n",
       "      <td>0.126518</td>\n",
       "      <td>0.037955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.366288</td>\n",
       "      <td>-0.087476</td>\n",
       "      <td>-2.002003</td>\n",
       "      <td>-0.397231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498069</th>\n",
       "      <td>2</td>\n",
       "      <td>1501</td>\n",
       "      <td>59.95</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.039940</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.987500</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>25.037531</td>\n",
       "      <td>0.133445</td>\n",
       "      <td>0.133445</td>\n",
       "      <td>0.083403</td>\n",
       "      <td>0.682855</td>\n",
       "      <td>-0.288454</td>\n",
       "      <td>1.605850</td>\n",
       "      <td>-1.109372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498070</th>\n",
       "      <td>6</td>\n",
       "      <td>1137</td>\n",
       "      <td>6.33</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023747</td>\n",
       "      <td>0.005567</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.234444</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179.620853</td>\n",
       "      <td>0.789889</td>\n",
       "      <td>0.631912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.629039</td>\n",
       "      <td>0.491641</td>\n",
       "      <td>-2.294293</td>\n",
       "      <td>2.207567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498071</th>\n",
       "      <td>2</td>\n",
       "      <td>948</td>\n",
       "      <td>38.69</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.040812</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.448333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.502455</td>\n",
       "      <td>0.103386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.418069</td>\n",
       "      <td>-0.291281</td>\n",
       "      <td>-2.071975</td>\n",
       "      <td>-1.265852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498072</th>\n",
       "      <td>5</td>\n",
       "      <td>1473</td>\n",
       "      <td>59.90</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0.040665</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.990000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>24.590985</td>\n",
       "      <td>0.033389</td>\n",
       "      <td>0.166945</td>\n",
       "      <td>0.083472</td>\n",
       "      <td>0.537267</td>\n",
       "      <td>0.302683</td>\n",
       "      <td>1.916720</td>\n",
       "      <td>1.386184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498073</th>\n",
       "      <td>2</td>\n",
       "      <td>165</td>\n",
       "      <td>95.42</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.157576</td>\n",
       "      <td>0.578303</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>1.729197</td>\n",
       "      <td>0.094320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031440</td>\n",
       "      <td>-0.048566</td>\n",
       "      <td>-0.317767</td>\n",
       "      <td>0.336128</td>\n",
       "      <td>-1.880551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498074</th>\n",
       "      <td>6</td>\n",
       "      <td>1539</td>\n",
       "      <td>42.31</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>0.027492</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.839565</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.374380</td>\n",
       "      <td>0.023635</td>\n",
       "      <td>0.070905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.601733</td>\n",
       "      <td>0.493902</td>\n",
       "      <td>-2.266427</td>\n",
       "      <td>2.160896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498075</th>\n",
       "      <td>6</td>\n",
       "      <td>1543</td>\n",
       "      <td>53.43</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016850</td>\n",
       "      <td>0.034627</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.055000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>28.878907</td>\n",
       "      <td>0.187161</td>\n",
       "      <td>0.149729</td>\n",
       "      <td>0.018716</td>\n",
       "      <td>-0.365887</td>\n",
       "      <td>0.496422</td>\n",
       "      <td>-1.670767</td>\n",
       "      <td>2.344311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498076</th>\n",
       "      <td>6</td>\n",
       "      <td>1196</td>\n",
       "      <td>78.10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.065301</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.620000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.003344</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>15.313700</td>\n",
       "      <td>0.051216</td>\n",
       "      <td>0.076825</td>\n",
       "      <td>0.025608</td>\n",
       "      <td>0.046391</td>\n",
       "      <td>0.512911</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>1.882704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498077</th>\n",
       "      <td>4</td>\n",
       "      <td>676</td>\n",
       "      <td>65.13</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013314</td>\n",
       "      <td>0.096346</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.236667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.008876</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>10.379242</td>\n",
       "      <td>0.122831</td>\n",
       "      <td>0.092123</td>\n",
       "      <td>0.015354</td>\n",
       "      <td>-0.186779</td>\n",
       "      <td>0.111434</td>\n",
       "      <td>-1.541044</td>\n",
       "      <td>0.441140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498078</th>\n",
       "      <td>5</td>\n",
       "      <td>1100</td>\n",
       "      <td>46.66</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.042418</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.332000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.006364</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>23.574796</td>\n",
       "      <td>0.150021</td>\n",
       "      <td>0.171453</td>\n",
       "      <td>0.042863</td>\n",
       "      <td>0.068788</td>\n",
       "      <td>0.315959</td>\n",
       "      <td>-0.455508</td>\n",
       "      <td>1.325383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498079</th>\n",
       "      <td>6</td>\n",
       "      <td>311</td>\n",
       "      <td>29.78</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.086817</td>\n",
       "      <td>0.095756</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.102963</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.025723</td>\n",
       "      <td>0.012862</td>\n",
       "      <td>10.443251</td>\n",
       "      <td>0.033580</td>\n",
       "      <td>0.268637</td>\n",
       "      <td>0.134318</td>\n",
       "      <td>0.119375</td>\n",
       "      <td>0.484379</td>\n",
       "      <td>1.382392</td>\n",
       "      <td>2.459363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498080</th>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "      <td>69.82</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.030357</td>\n",
       "      <td>0.124679</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.107059</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>8.020624</td>\n",
       "      <td>0.143225</td>\n",
       "      <td>0.042968</td>\n",
       "      <td>0.071613</td>\n",
       "      <td>0.439937</td>\n",
       "      <td>-0.510433</td>\n",
       "      <td>1.927774</td>\n",
       "      <td>-2.153347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498081</th>\n",
       "      <td>6</td>\n",
       "      <td>1733</td>\n",
       "      <td>43.08</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.024859</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.958182</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.004616</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>40.227484</td>\n",
       "      <td>0.046425</td>\n",
       "      <td>0.185701</td>\n",
       "      <td>0.092851</td>\n",
       "      <td>0.183670</td>\n",
       "      <td>0.490147</td>\n",
       "      <td>1.409201</td>\n",
       "      <td>2.442498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498082</th>\n",
       "      <td>3</td>\n",
       "      <td>1437</td>\n",
       "      <td>36.29</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016006</td>\n",
       "      <td>0.025254</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.577826</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>39.597685</td>\n",
       "      <td>0.248002</td>\n",
       "      <td>0.192891</td>\n",
       "      <td>0.055112</td>\n",
       "      <td>-0.167162</td>\n",
       "      <td>-0.104527</td>\n",
       "      <td>-0.600645</td>\n",
       "      <td>-0.548197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498083</th>\n",
       "      <td>5</td>\n",
       "      <td>101</td>\n",
       "      <td>46.25</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.457921</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>2.183784</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.043243</td>\n",
       "      <td>0.064865</td>\n",
       "      <td>0.341486</td>\n",
       "      <td>0.313902</td>\n",
       "      <td>0.400110</td>\n",
       "      <td>0.867679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498084</th>\n",
       "      <td>4</td>\n",
       "      <td>587</td>\n",
       "      <td>60.88</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.045997</td>\n",
       "      <td>0.103714</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.254815</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>9.641919</td>\n",
       "      <td>0.016426</td>\n",
       "      <td>0.131406</td>\n",
       "      <td>0.049277</td>\n",
       "      <td>-0.053252</td>\n",
       "      <td>0.087779</td>\n",
       "      <td>0.351923</td>\n",
       "      <td>0.526065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498085</th>\n",
       "      <td>1</td>\n",
       "      <td>1270</td>\n",
       "      <td>25.98</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.998462</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>48.883757</td>\n",
       "      <td>0.192456</td>\n",
       "      <td>0.038491</td>\n",
       "      <td>0.153965</td>\n",
       "      <td>0.246751</td>\n",
       "      <td>-0.507737</td>\n",
       "      <td>1.323659</td>\n",
       "      <td>-2.250099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498086</th>\n",
       "      <td>4</td>\n",
       "      <td>185</td>\n",
       "      <td>63.43</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.342865</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.686000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.048649</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>2.916601</td>\n",
       "      <td>0.157654</td>\n",
       "      <td>0.141889</td>\n",
       "      <td>0.031531</td>\n",
       "      <td>0.105984</td>\n",
       "      <td>0.118611</td>\n",
       "      <td>-0.462521</td>\n",
       "      <td>0.424089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498087</th>\n",
       "      <td>6</td>\n",
       "      <td>1480</td>\n",
       "      <td>19.87</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.013426</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.870000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>74.484147</td>\n",
       "      <td>0.352290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100654</td>\n",
       "      <td>0.160370</td>\n",
       "      <td>0.515485</td>\n",
       "      <td>3.165331</td>\n",
       "      <td>0.183863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498088</th>\n",
       "      <td>4</td>\n",
       "      <td>403</td>\n",
       "      <td>50.53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.125385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007444</td>\n",
       "      <td>7.975460</td>\n",
       "      <td>0.019790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059371</td>\n",
       "      <td>0.342761</td>\n",
       "      <td>0.110730</td>\n",
       "      <td>3.198606</td>\n",
       "      <td>0.121134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498089</th>\n",
       "      <td>6</td>\n",
       "      <td>154</td>\n",
       "      <td>84.63</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.549545</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.630000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.819686</td>\n",
       "      <td>0.070897</td>\n",
       "      <td>0.023632</td>\n",
       "      <td>0.047265</td>\n",
       "      <td>0.710191</td>\n",
       "      <td>0.519869</td>\n",
       "      <td>3.510299</td>\n",
       "      <td>0.123292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498090</th>\n",
       "      <td>3</td>\n",
       "      <td>285</td>\n",
       "      <td>74.97</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.263053</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74.970000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.014035</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>3.801521</td>\n",
       "      <td>0.053355</td>\n",
       "      <td>0.093371</td>\n",
       "      <td>0.026677</td>\n",
       "      <td>0.307022</td>\n",
       "      <td>-0.068999</td>\n",
       "      <td>3.409179</td>\n",
       "      <td>0.105333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498091</th>\n",
       "      <td>4</td>\n",
       "      <td>1691</td>\n",
       "      <td>22.89</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007688</td>\n",
       "      <td>0.013536</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.760769</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>73.875055</td>\n",
       "      <td>0.436872</td>\n",
       "      <td>0.262123</td>\n",
       "      <td>0.218436</td>\n",
       "      <td>0.494801</td>\n",
       "      <td>0.096569</td>\n",
       "      <td>1.999168</td>\n",
       "      <td>0.488506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498092</th>\n",
       "      <td>1</td>\n",
       "      <td>1685</td>\n",
       "      <td>61.99</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.036789</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.995000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>27.181804</td>\n",
       "      <td>0.096790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064527</td>\n",
       "      <td>0.495132</td>\n",
       "      <td>-0.492042</td>\n",
       "      <td>1.173011</td>\n",
       "      <td>-1.210670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498093</th>\n",
       "      <td>4</td>\n",
       "      <td>275</td>\n",
       "      <td>48.23</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.175382</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.384545</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.021818</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.010909</td>\n",
       "      <td>5.701845</td>\n",
       "      <td>0.124404</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>0.062202</td>\n",
       "      <td>0.101001</td>\n",
       "      <td>0.096966</td>\n",
       "      <td>0.374923</td>\n",
       "      <td>0.507155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498094</th>\n",
       "      <td>5</td>\n",
       "      <td>1085</td>\n",
       "      <td>50.15</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.046221</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.582143</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.005530</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>21.635095</td>\n",
       "      <td>0.059821</td>\n",
       "      <td>0.119641</td>\n",
       "      <td>0.039880</td>\n",
       "      <td>-0.097823</td>\n",
       "      <td>0.301640</td>\n",
       "      <td>-0.543317</td>\n",
       "      <td>1.711947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498095</th>\n",
       "      <td>5</td>\n",
       "      <td>687</td>\n",
       "      <td>33.47</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036390</td>\n",
       "      <td>0.048719</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.338800</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.010189</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>20.525844</td>\n",
       "      <td>0.209143</td>\n",
       "      <td>0.029878</td>\n",
       "      <td>0.029878</td>\n",
       "      <td>-0.427005</td>\n",
       "      <td>0.288301</td>\n",
       "      <td>-1.870431</td>\n",
       "      <td>1.752422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498096</th>\n",
       "      <td>6</td>\n",
       "      <td>425</td>\n",
       "      <td>18.91</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009412</td>\n",
       "      <td>0.044494</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.727500</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016471</td>\n",
       "      <td>0.021176</td>\n",
       "      <td>0.009412</td>\n",
       "      <td>22.474881</td>\n",
       "      <td>0.370175</td>\n",
       "      <td>0.475939</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.472386</td>\n",
       "      <td>0.513056</td>\n",
       "      <td>1.143937</td>\n",
       "      <td>1.454934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498097</th>\n",
       "      <td>2</td>\n",
       "      <td>671</td>\n",
       "      <td>86.03</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.038748</td>\n",
       "      <td>0.128212</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.308846</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>7.799605</td>\n",
       "      <td>0.127862</td>\n",
       "      <td>0.011624</td>\n",
       "      <td>0.058119</td>\n",
       "      <td>0.330910</td>\n",
       "      <td>-0.321494</td>\n",
       "      <td>1.989479</td>\n",
       "      <td>-1.814723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498098</th>\n",
       "      <td>4</td>\n",
       "      <td>1705</td>\n",
       "      <td>98.66</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.057865</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.665000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>17.281573</td>\n",
       "      <td>0.050679</td>\n",
       "      <td>0.030407</td>\n",
       "      <td>0.020272</td>\n",
       "      <td>0.058991</td>\n",
       "      <td>0.112030</td>\n",
       "      <td>-0.503091</td>\n",
       "      <td>0.457804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498099</th>\n",
       "      <td>1</td>\n",
       "      <td>1822</td>\n",
       "      <td>75.30</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016465</td>\n",
       "      <td>0.041328</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>24.196547</td>\n",
       "      <td>0.119522</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.053121</td>\n",
       "      <td>0.096394</td>\n",
       "      <td>-0.521803</td>\n",
       "      <td>1.458583</td>\n",
       "      <td>-2.411263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498100</th>\n",
       "      <td>3</td>\n",
       "      <td>1151</td>\n",
       "      <td>11.37</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021720</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.231310</td>\n",
       "      <td>0.263852</td>\n",
       "      <td>0.087951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.637499</td>\n",
       "      <td>-0.109895</td>\n",
       "      <td>-2.545646</td>\n",
       "      <td>-0.485906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498101</th>\n",
       "      <td>1</td>\n",
       "      <td>796</td>\n",
       "      <td>85.72</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.107688</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.720000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>9.286048</td>\n",
       "      <td>0.104993</td>\n",
       "      <td>0.034998</td>\n",
       "      <td>0.011666</td>\n",
       "      <td>0.095151</td>\n",
       "      <td>-0.469309</td>\n",
       "      <td>3.355964</td>\n",
       "      <td>0.082610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498102</th>\n",
       "      <td>2</td>\n",
       "      <td>1379</td>\n",
       "      <td>48.64</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014503</td>\n",
       "      <td>0.035272</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.432000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>28.351151</td>\n",
       "      <td>0.226151</td>\n",
       "      <td>0.143914</td>\n",
       "      <td>0.102796</td>\n",
       "      <td>0.431208</td>\n",
       "      <td>-0.308803</td>\n",
       "      <td>2.077825</td>\n",
       "      <td>-1.749740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498103</th>\n",
       "      <td>6</td>\n",
       "      <td>885</td>\n",
       "      <td>92.41</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.104418</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.160667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.576886</td>\n",
       "      <td>0.064928</td>\n",
       "      <td>0.054107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.454381</td>\n",
       "      <td>0.506017</td>\n",
       "      <td>-2.100359</td>\n",
       "      <td>2.010543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498104</th>\n",
       "      <td>3</td>\n",
       "      <td>441</td>\n",
       "      <td>36.91</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.083696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.455000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018141</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>11.947982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216743</td>\n",
       "      <td>0.108372</td>\n",
       "      <td>0.517835</td>\n",
       "      <td>-0.084884</td>\n",
       "      <td>1.092574</td>\n",
       "      <td>-0.370675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498105</th>\n",
       "      <td>2</td>\n",
       "      <td>319</td>\n",
       "      <td>57.74</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.094044</td>\n",
       "      <td>0.181003</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.924667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.524766</td>\n",
       "      <td>0.034638</td>\n",
       "      <td>0.017319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.680588</td>\n",
       "      <td>-0.314602</td>\n",
       "      <td>-2.465426</td>\n",
       "      <td>-1.618736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498106</th>\n",
       "      <td>4</td>\n",
       "      <td>1228</td>\n",
       "      <td>74.44</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.022801</td>\n",
       "      <td>0.060619</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.658571</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>16.496507</td>\n",
       "      <td>0.080602</td>\n",
       "      <td>0.067168</td>\n",
       "      <td>0.040301</td>\n",
       "      <td>-0.057299</td>\n",
       "      <td>0.084976</td>\n",
       "      <td>0.348254</td>\n",
       "      <td>0.540835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498107</th>\n",
       "      <td>4</td>\n",
       "      <td>708</td>\n",
       "      <td>19.56</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.027627</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.780000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>36.196319</td>\n",
       "      <td>0.408998</td>\n",
       "      <td>0.051125</td>\n",
       "      <td>0.153374</td>\n",
       "      <td>0.290756</td>\n",
       "      <td>0.110333</td>\n",
       "      <td>0.517168</td>\n",
       "      <td>0.377922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498108</th>\n",
       "      <td>2</td>\n",
       "      <td>1726</td>\n",
       "      <td>99.80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007532</td>\n",
       "      <td>0.057822</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.676923</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>17.294589</td>\n",
       "      <td>0.010020</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.010020</td>\n",
       "      <td>-0.232505</td>\n",
       "      <td>-0.289245</td>\n",
       "      <td>-1.525648</td>\n",
       "      <td>-1.405017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498109</th>\n",
       "      <td>4</td>\n",
       "      <td>961</td>\n",
       "      <td>18.62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.019376</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.206667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>51.611171</td>\n",
       "      <td>0.053706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053706</td>\n",
       "      <td>-0.213660</td>\n",
       "      <td>0.107843</td>\n",
       "      <td>-1.371567</td>\n",
       "      <td>0.448997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498110</th>\n",
       "      <td>3</td>\n",
       "      <td>585</td>\n",
       "      <td>26.17</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.013077</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>22.353840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343905</td>\n",
       "      <td>0.076423</td>\n",
       "      <td>-0.086967</td>\n",
       "      <td>-0.094858</td>\n",
       "      <td>-0.582623</td>\n",
       "      <td>-0.487519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498111</th>\n",
       "      <td>4</td>\n",
       "      <td>983</td>\n",
       "      <td>57.72</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030519</td>\n",
       "      <td>0.058718</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.924000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.004069</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.004069</td>\n",
       "      <td>17.030492</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.155925</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.117063</td>\n",
       "      <td>0.084078</td>\n",
       "      <td>1.641820</td>\n",
       "      <td>0.505170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498112</th>\n",
       "      <td>1</td>\n",
       "      <td>1342</td>\n",
       "      <td>69.89</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>0.052079</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.472500</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>19.201603</td>\n",
       "      <td>0.114466</td>\n",
       "      <td>0.143082</td>\n",
       "      <td>0.071541</td>\n",
       "      <td>0.707169</td>\n",
       "      <td>-0.485454</td>\n",
       "      <td>1.496246</td>\n",
       "      <td>-1.416488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498113</th>\n",
       "      <td>5</td>\n",
       "      <td>553</td>\n",
       "      <td>95.44</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.045208</td>\n",
       "      <td>0.172586</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.817600</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.016275</td>\n",
       "      <td>0.009042</td>\n",
       "      <td>0.009042</td>\n",
       "      <td>5.794216</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>0.052389</td>\n",
       "      <td>0.052389</td>\n",
       "      <td>0.364355</td>\n",
       "      <td>0.283246</td>\n",
       "      <td>2.064971</td>\n",
       "      <td>1.756742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498114</th>\n",
       "      <td>6</td>\n",
       "      <td>1767</td>\n",
       "      <td>65.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013016</td>\n",
       "      <td>0.037340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.868696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>26.780843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015156</td>\n",
       "      <td>-0.438262</td>\n",
       "      <td>0.487835</td>\n",
       "      <td>-1.685405</td>\n",
       "      <td>2.273375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498115</th>\n",
       "      <td>6</td>\n",
       "      <td>879</td>\n",
       "      <td>96.34</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>0.109602</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.352222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.010239</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>9.123936</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.093419</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>-0.294484</td>\n",
       "      <td>0.503851</td>\n",
       "      <td>-1.571458</td>\n",
       "      <td>2.208047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498116</th>\n",
       "      <td>4</td>\n",
       "      <td>783</td>\n",
       "      <td>59.10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012771</td>\n",
       "      <td>0.075479</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.910000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.248731</td>\n",
       "      <td>0.033841</td>\n",
       "      <td>0.033841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.453159</td>\n",
       "      <td>0.106284</td>\n",
       "      <td>-2.261474</td>\n",
       "      <td>0.454349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498117</th>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>98.90</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.050360</td>\n",
       "      <td>0.355755</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.064286</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.032374</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>2.810920</td>\n",
       "      <td>0.091001</td>\n",
       "      <td>0.050556</td>\n",
       "      <td>0.040445</td>\n",
       "      <td>0.313497</td>\n",
       "      <td>-0.502220</td>\n",
       "      <td>1.283820</td>\n",
       "      <td>-2.074560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498118</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>5.41</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.018033</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.601111</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>55.452865</td>\n",
       "      <td>1.109057</td>\n",
       "      <td>1.109057</td>\n",
       "      <td>0.739372</td>\n",
       "      <td>0.336591</td>\n",
       "      <td>-0.098049</td>\n",
       "      <td>1.371059</td>\n",
       "      <td>-0.498357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498119</th>\n",
       "      <td>2</td>\n",
       "      <td>1524</td>\n",
       "      <td>33.97</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005906</td>\n",
       "      <td>0.022290</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.774444</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>44.863115</td>\n",
       "      <td>0.058875</td>\n",
       "      <td>0.147189</td>\n",
       "      <td>0.088313</td>\n",
       "      <td>0.130232</td>\n",
       "      <td>-0.296911</td>\n",
       "      <td>0.386430</td>\n",
       "      <td>-1.575245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498120</th>\n",
       "      <td>3</td>\n",
       "      <td>1456</td>\n",
       "      <td>56.97</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.039128</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.034643</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.007555</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>25.557311</td>\n",
       "      <td>0.193084</td>\n",
       "      <td>0.122872</td>\n",
       "      <td>0.035106</td>\n",
       "      <td>-0.206077</td>\n",
       "      <td>-0.108752</td>\n",
       "      <td>-0.618502</td>\n",
       "      <td>-0.542440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498121 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  valuePerSecond  lineItemVoidsPerPosition  scannedLineItems  pricePerScannedLineItem  scansWithoutRegistrationPerScannedLineItem  quantityModificationsPerScannedLineItem  lineItemVoidsPerSecond  scansWithoutRegistrationPerSecond  quantityModificationsPerSecond  secondsPerEuro  lineItemVoidsPerEuro  scansWithoutRegistrationPerEuro  quantityModificationsPerEuro  pca_axis_1  pca_axis_2  tsne_axis_1  tsne_axis_2\n",
       "0                4                     467       88.48              4                         8                      4                   0.014989        0.189465                  0.571429               7.0                12.640000                                    1.142857                                 0.571429                0.008565                           0.017131                        0.008565        5.278029              0.045208                         0.090416                      0.045208    0.407059    0.107610     1.288056     0.439125\n",
       "1                3                    1004       58.99              7                         6                      1                   0.026892        0.058755                  0.259259              27.0                 2.184815                                    0.222222                                 0.037037                0.006972                           0.005976                        0.000996       17.019834              0.118664                         0.101712                      0.016952   -0.404056   -0.107318    -2.048066    -0.496164\n",
       "2                1                     162       14.00              4                         5                      4                   0.006173        0.086420                  4.000000               1.0                14.000000                                    5.000000                                 4.000000                0.024691                           0.030864                        0.024691       11.571429              0.285714                         0.357143                      0.285714    0.648069   -0.481068     3.447044     0.053117\n",
       "3                5                     532       84.79              9                         3                      4                   0.026316        0.159380                  0.642857              14.0                 6.056429                                    0.214286                                 0.285714                0.016917                           0.005639                        0.007519        6.274325              0.106145                         0.035382                      0.047175    0.292492    0.294786     1.344228     1.554838\n",
       "4                5                     890       42.16              4                         0                      0                   0.021348        0.047371                  0.210526              19.0                 2.218947                                    0.000000                                 0.000000                0.004494                           0.000000                        0.000000       21.110057              0.094877                         0.000000                      0.000000   -0.564485    0.295128    -2.414502     1.547169\n",
       "5                5                    1072       12.67              3                         4                      1                   0.019590        0.011819                  0.142857              21.0                 0.603333                                    0.190476                                 0.047619                0.002799                           0.003731                        0.000933       84.609313              0.236780                         0.315706                      0.078927   -0.388592    0.294494    -1.828019     1.732591\n",
       "6                3                     259       93.75              0                         7                      0                   0.100386        0.361969                  0.000000              26.0                 3.605769                                    0.269231                                 0.000000                0.000000                           0.027027                        0.000000        2.762667              0.000000                         0.074667                      0.000000   -0.591763   -0.104089    -2.445146    -0.517219\n",
       "7                2                    1528       47.35              2                         9                      5                   0.009817        0.030988                  0.133333              15.0                 3.156667                                    0.600000                                 0.333333                0.001309                           0.005890                        0.003272       32.270327              0.042239                         0.190074                      0.105597    0.458967   -0.303730     2.015723    -1.598574\n",
       "8                6                     816       80.89              9                         4                      0                   0.017157        0.099130                  0.642857              14.0                 5.777857                                    0.285714                                 0.000000                0.011029                           0.004902                        0.000000       10.087774              0.111262                         0.049450                      0.000000   -0.438512    0.506551    -2.074741     1.978520\n",
       "9                4                      16       31.91              7                         7                      4                   1.312500        1.994375                  0.333333              21.0                 1.519524                                    0.333333                                 0.190476                0.437500                           0.437500                        0.250000        0.501410              0.219367                         0.219367                      0.125353    0.211314    0.090980     1.474045     0.426568\n",
       "10               3                     714       94.29              8                         7                      0                   0.016807        0.132059                  0.666667              12.0                 7.857500                                    0.583333                                 0.000000                0.011204                           0.009804                        0.000000        7.572383              0.084845                         0.074239                      0.000000   -0.391550   -0.087511    -2.165339    -0.435252\n",
       "11               5                    1077       66.16              5                         8                      3                   0.015785        0.061430                  0.294118              17.0                 3.891765                                    0.470588                                 0.176471                0.004643                           0.007428                        0.002786       16.278718              0.075574                         0.120919                      0.045345    0.081986    0.298952     0.382123     1.827146\n",
       "12               4                    1301       84.35              3                        10                      5                   0.021522        0.064835                  0.107143              28.0                 3.012500                                    0.357143                                 0.178571                0.002306                           0.007686                        0.003843       15.423829              0.035566                         0.118554                      0.059277    0.332642    0.084617     2.101652     0.551876\n",
       "13               3                    1429       47.95              8                         1                      3                   0.003499        0.033555                  1.600000               5.0                 9.590000                                    0.200000                                 0.600000                0.005598                           0.000700                        0.002099       29.801877              0.166840                         0.020855                      0.062565    0.203086   -0.094804     0.419216    -0.478411\n",
       "14               3                    1196       83.77             11                        10                      0                   0.004181        0.070042                  2.200000               5.0                16.754000                                    2.000000                                 0.000000                0.009197                           0.008361                        0.000000       14.277188              0.131312                         0.119374                      0.000000   -0.249300   -0.073777    -1.691172    -0.394049\n",
       "15               3                    1567       75.53              7                        10                      1                   0.008296        0.048200                  0.538462              13.0                 5.810000                                    0.769231                                 0.076923                0.004467                           0.006382                        0.000638       20.746723              0.092678                         0.132398                      0.013240   -0.209044   -0.088058    -1.627112    -0.455931\n",
       "16               4                     289       18.66              8                         4                      0                   0.086505        0.064567                  0.320000              25.0                 0.746400                                    0.160000                                 0.000000                0.027682                           0.013841                        0.000000       15.487674              0.428725                         0.214362                      0.000000   -0.587221    0.094554    -2.528869     0.501003\n",
       "17               1                     335        1.19              0                         0                      1                   0.032836        0.003552                  0.000000              11.0                 0.108182                                    0.000000                                 0.090909                0.000000                           0.000000                        0.002985      281.512605              0.000000                         0.000000                      0.840336   -0.327371   -0.500976    -1.404831    -1.984003\n",
       "18               3                    1304       30.51              0                         7                      3                   0.012270        0.023397                  0.000000              16.0                 1.906875                                    0.437500                                 0.187500                0.000000                           0.005368                        0.002301       42.740085              0.000000                         0.229433                      0.098328    0.047629   -0.102692     0.390086    -0.537004\n",
       "19               5                    1353       82.02              0                         0                      2                   0.005174        0.060621                  0.000000               7.0                11.717143                                    0.000000                                 0.285714                0.000000                           0.000000                        0.001478       16.495977              0.000000                         0.000000                      0.024384   -0.061909    0.301631    -0.469931     1.453829\n",
       "20               6                    1749       46.56              5                         0                      5                   0.012007        0.026621                  0.238095              21.0                 2.217143                                    0.000000                                 0.238095                0.002859                           0.000000                        0.002859       37.564433              0.107388                         0.000000                      0.107388    0.335438    0.480083     1.940913     2.267286\n",
       "21               5                     757       91.02              9                         3                      0                   0.019815        0.120238                  0.600000              15.0                 6.068000                                    0.200000                                 0.000000                0.011889                           0.003963                        0.000000        8.316853              0.098879                         0.032960                      0.000000   -0.454010    0.304593    -2.222024     1.415683\n",
       "22               2                    1440       15.15              0                         6                      5                   0.016667        0.010521                  0.000000              24.0                 0.631250                                    0.250000                                 0.208333                0.000000                           0.004167                        0.003472       95.049505              0.000000                         0.396040                      0.330033    0.310644   -0.317246     2.062055    -1.834159\n",
       "23               4                    1101       22.55              8                         9                      0                   0.019982        0.020481                  0.363636              22.0                 1.025000                                    0.409091                                 0.000000                0.007266                           0.008174                        0.000000       48.824834              0.354767                         0.399113                      0.000000   -0.516836    0.103528    -2.477338     0.480600\n",
       "24               6                    1404       59.43              5                         8                      5                   0.015670        0.042329                  0.227273              22.0                 2.701364                                    0.363636                                 0.227273                0.003561                           0.005698                        0.003561       23.624432              0.084133                         0.134612                      0.084133    0.387792    0.488340     1.978658     2.308822\n",
       "25               6                    1396       13.16             11                         2                      0                   0.010745        0.009427                  0.733333              15.0                 0.877333                                    0.133333                                 0.000000                0.007880                           0.001433                        0.000000      106.079027              0.835866                         0.151976                      0.000000   -0.479203    0.502977    -2.042129     1.932095\n",
       "26               4                     248       14.00              9                         4                      2                   0.100806        0.056452                  0.360000              25.0                 0.560000                                    0.160000                                 0.080000                0.036290                           0.016129                        0.008065       17.714286              0.642857                         0.285714                      0.142857   -0.217940    0.089334    -0.608626     0.511476\n",
       "27               3                     359        6.40              0                         5                      3                   0.019499        0.017827                  0.000000               7.0                 0.914286                                    0.714286                                 0.428571                0.000000                           0.013928                        0.008357       56.093750              0.000000                         0.781250                      0.468750    0.136216   -0.096246     0.416303    -0.478445\n",
       "28               6                     915       36.79              9                         0                      0                   0.007650        0.040208                  1.285714               7.0                 5.255714                                    0.000000                                 0.000000                0.009836                           0.000000                        0.000000       24.870889              0.244632                         0.000000                      0.000000   -0.401927    0.508693    -1.884966     1.719997\n",
       "29               5                     617       49.57              0                         5                      1                   0.035656        0.080340                  0.000000              22.0                 2.253182                                    0.227273                                 0.045455                0.000000                           0.008104                        0.001621       12.447045              0.000000                         0.100867                      0.020173   -0.394770    0.294185    -1.889421     1.752990\n",
       "30               5                     785       69.87              2                         1                      3                   0.033121        0.089006                  0.076923              26.0                 2.687308                                    0.038462                                 0.115385                0.002548                           0.001274                        0.003822       11.235151              0.028625                         0.014312                      0.042937   -0.085711    0.281051     0.363176     2.021664\n",
       "31               6                    1576       94.01              6                         5                      5                   0.010152        0.059651                  0.375000              16.0                 5.875625                                    0.312500                                 0.312500                0.003807                           0.003173                        0.003173       16.764174              0.063823                         0.053186                      0.053186    0.453287    0.491772     1.920002     2.143842\n",
       "32               1                     128       44.10              3                         3                      1                   0.109375        0.344531                  0.214286              14.0                 3.150000                                    0.214286                                 0.071429                0.023438                           0.023438                        0.007812        2.902494              0.068027                         0.068027                      0.022676   -0.305911   -0.499015    -1.549833    -2.117539\n",
       "33               4                    1260       42.28              4                         7                      2                   0.003968        0.033556                  0.800000               5.0                 8.456000                                    1.400000                                 0.400000                0.003175                           0.005556                        0.001587       29.801325              0.094607                         0.165563                      0.047304    0.038380    0.113650    -0.500719     0.467817\n",
       "34               6                    1206       39.31              0                         0                      2                   0.001658        0.032595                  0.000000               2.0                19.655000                                    0.000000                                 1.000000                0.000000                           0.000000                        0.001658       30.679216              0.000000                         0.000000                      0.050878    0.017638    0.507226    -0.113821     1.689994\n",
       "35               4                     883       22.42              0                         7                      5                   0.007928        0.025391                  0.000000               7.0                 3.202857                                    1.000000                                 0.714286                0.000000                           0.007928                        0.005663       39.384478              0.000000                         0.312221                      0.223015    0.533834    0.101324     1.874397     0.447375\n",
       "36               1                    1554        4.12              4                        10                      1                   0.010296        0.002651                  0.250000              16.0                 0.257500                                    0.625000                                 0.062500                0.002574                           0.006435                        0.000644      377.184466              0.970874                         2.427184                      0.242718   -0.285590   -0.492750    -1.539333    -2.141390\n",
       "37               2                    1734        9.38             10                         6                      0                   0.014994        0.005409                  0.384615              26.0                 0.360769                                    0.230769                                 0.000000                0.005767                           0.003460                        0.000000      184.861407              1.066098                         0.639659                      0.000000   -0.577454   -0.303080    -2.396955    -1.499305\n",
       "38               3                    1182       94.67              4                         0                      4                   0.016074        0.080093                  0.210526              19.0                 4.982632                                    0.000000                                 0.210526                0.003384                           0.000000                        0.003384       12.485476              0.042252                         0.000000                      0.042252    0.186985   -0.114655     1.540088    -0.537563\n",
       "39               4                    1194       75.02              5                         8                      3                   0.010050        0.062831                  0.416667              12.0                 6.251667                                    0.666667                                 0.250000                0.004188                           0.006700                        0.002513       15.915756              0.066649                         0.106638                      0.039989    0.146536    0.104531     0.382479     0.465688\n",
       "40               5                     526       92.49             11                         1                      5                   0.053232        0.175837                  0.392857              28.0                 3.303214                                    0.035714                                 0.178571                0.020913                           0.001901                        0.009506        5.687101              0.118932                         0.010812                      0.054060    0.310278    0.276153     1.974766     1.768757\n",
       "41               4                    1662       70.34              0                        10                      0                   0.009627        0.042323                  0.000000              16.0                 4.396250                                    0.625000                                 0.000000                0.000000                           0.006017                        0.000000       23.628092              0.000000                         0.142167                      0.000000   -0.467701    0.109340    -2.210949     0.471721\n",
       "42               4                    1060       67.97              7                         6                      4                   0.028302        0.064123                  0.233333              30.0                 2.265667                                    0.200000                                 0.133333                0.006604                           0.005660                        0.003774       15.595115              0.102987                         0.088274                      0.058849    0.113458    0.081560     1.632863     0.532465\n",
       "43               5                    1276       59.30              1                         3                      0                   0.002351        0.046473                  0.333333               3.0                19.766667                                    1.000000                                 0.000000                0.000784                           0.002351                        0.000000       21.517707              0.016863                         0.050590                      0.000000   -0.354376    0.315715    -1.826596     1.206667\n",
       "44               4                    1075       80.75              9                         7                      3                   0.013953        0.075116                  0.600000              15.0                 5.383333                                    0.466667                                 0.200000                0.008372                           0.006512                        0.002791       13.312693              0.111455                         0.086687                      0.037152    0.124672    0.101301     0.358393     0.504236\n",
       "45               1                     665       67.99              2                         7                      3                   0.006015        0.102241                  0.500000               4.0                16.997500                                    1.750000                                 0.750000                0.003008                           0.010526                        0.004511        9.780850              0.029416                         0.102956                      0.044124    0.253154   -0.487076     0.424097    -1.681565\n",
       "46               5                    1768       81.84              9                         1                      1                   0.011878        0.046290                  0.428571              21.0                 3.897143                                    0.047619                                 0.047619                0.005090                           0.000566                        0.000566       21.603128              0.109971                         0.012219                      0.012219   -0.357523    0.293651    -1.751355     1.645385\n",
       "47               3                    1772       33.45              9                         5                      5                   0.006772        0.018877                  0.750000              12.0                 2.787500                                    0.416667                                 0.416667                0.005079                           0.002822                        0.002822       52.974589              0.269058                         0.149477                      0.149477    0.498718   -0.103505     1.981186    -0.538616\n",
       "48               2                     870       90.86              8                         8                      5                   0.031034        0.104437                  0.296296              27.0                 3.365185                                    0.296296                                 0.185185                0.009195                           0.009195                        0.005747        9.575171              0.088048                         0.088048                      0.055030    0.357115   -0.315162     2.108857    -1.824888\n",
       "49               4                    1323       40.34              7                         6                      4                   0.003779        0.030491                  1.400000               5.0                 8.068000                                    1.200000                                 0.800000                0.005291                           0.004535                        0.003023       32.796232              0.173525                         0.148736                      0.099157    0.427446    0.108353     1.351503     0.428718\n",
       "50               3                    1087       40.46              1                        10                      0                   0.017479        0.037222                  0.052632              19.0                 2.129474                                    0.526316                                 0.000000                0.000920                           0.009200                        0.000000       26.866041              0.024716                         0.247158                      0.000000   -0.505859   -0.093829    -2.444576    -0.487174\n",
       "51               3                     631       34.78              8                         0                      5                   0.045959        0.055119                  0.275862              29.0                 1.199310                                    0.000000                                 0.172414                0.012678                           0.000000                        0.007924       18.142611              0.230017                         0.000000                      0.143761    0.258254   -0.127098     2.137155    -0.610189\n",
       "52               1                     766       23.84              9                         6                      0                   0.026110        0.031123                  0.450000              20.0                 1.192000                                    0.300000                                 0.000000                0.011749                           0.007833                        0.000000       32.130872              0.377517                         0.251678                      0.000000   -0.510724   -0.497340    -2.242349    -2.108567\n",
       "53               4                    1243       89.45              4                         6                      0                   0.017699        0.071963                  0.181818              22.0                 4.065909                                    0.272727                                 0.000000                0.003218                           0.004827                        0.000000       13.896031              0.044718                         0.067077                      0.000000   -0.537213    0.099864    -2.521923     0.494900\n",
       "54               4                    1328       69.66              9                         1                      4                   0.005271        0.052455                  1.285714               7.0                 9.951429                                    0.142857                                 0.571429                0.006777                           0.000753                        0.003012       19.064025              0.129199                         0.014355                      0.057422    0.369560    0.100507     1.330060     0.432338\n",
       "55               5                     220       33.23              6                         5                      3                   0.077273        0.151045                  0.352941              17.0                 1.954706                                    0.294118                                 0.176471                0.027273                           0.022727                        0.013636        6.620524              0.180560                         0.150466                      0.090280    0.053972    0.295087     0.376573     1.801181\n",
       "56               5                     416        0.24              1                         7                      1                   0.060096        0.000577                  0.040000              25.0                 0.009600                                    0.280000                                 0.040000                0.002404                           0.016827                        0.002404     1733.333333              4.166667                        29.166667                      4.166667   -0.423952    0.293156    -1.823436     1.677843\n",
       "57               1                     330       16.71              6                         5                      3                   0.027273        0.050636                  0.666667               9.0                 1.856667                                    0.555556                                 0.333333                0.018182                           0.015152                        0.009091       19.748654              0.359066                         0.299222                      0.179533    0.147244   -0.496160     0.358784    -2.020848\n",
       "58               5                     266       58.63              8                         4                      0                   0.097744        0.220414                  0.307692              26.0                 2.255000                                    0.153846                                 0.000000                0.030075                           0.015038                        0.000000        4.536926              0.136449                         0.068224                      0.000000   -0.585148    0.293887    -2.444040     1.618795\n",
       "59               4                     916       18.89              4                         6                      0                   0.017467        0.020622                  0.250000              16.0                 1.180625                                    0.375000                                 0.000000                0.004367                           0.006550                        0.000000       48.491265              0.211752                         0.317628                      0.000000   -0.494299    0.104907    -2.445191     0.481664\n",
       "60               6                     492       29.45              1                         8                      3                   0.048780        0.059858                  0.041667              24.0                 1.227083                                    0.333333                                 0.125000                0.002033                           0.016260                        0.006098       16.706282              0.033956                         0.271647                      0.101868   -0.030359    0.490101     0.386263     2.530044\n",
       "61               6                     604       12.81              5                         3                      5                   0.036424        0.021209                  0.227273              22.0                 0.582273                                    0.136364                                 0.227273                0.008278                           0.004967                        0.008278       47.150664              0.390320                         0.234192                      0.390320    0.335972    0.481823     1.968992     2.293505\n",
       "62               4                     712       83.07             10                         0                      4                   0.028090        0.116671                  0.500000              20.0                 4.153500                                    0.000000                                 0.200000                0.014045                           0.000000                        0.005618        8.571085              0.120380                         0.000000                      0.048152    0.202694    0.085536     1.476217     0.521695\n",
       "63               4                     520       39.85              0                         4                      2                   0.017308        0.076635                  0.000000               9.0                 4.427778                                    0.444444                                 0.222222                0.000000                           0.007692                        0.003846       13.048934              0.000000                         0.100376                      0.050188   -0.072150    0.103467    -0.522323     0.490866\n",
       "64               6                     244       51.97              4                         6                      1                   0.069672        0.212992                  0.235294              17.0                 3.057059                                    0.352941                                 0.058824                0.016393                           0.024590                        0.004098        4.695016              0.076967                         0.115451                      0.019242   -0.309943    0.501196    -1.605085     2.290144\n",
       "65               1                     813       11.94              1                         5                      0                   0.027060        0.014686                  0.045455              22.0                 0.542727                                    0.227273                                 0.000000                0.001230                           0.006150                        0.000000       68.090452              0.083752                         0.418760                      0.000000   -0.585050   -0.502719    -2.282900    -2.140231\n",
       "66               5                     877       75.98              8                         3                      1                   0.023945        0.086636                  0.380952              21.0                 3.618095                                    0.142857                                 0.047619                0.009122                           0.003421                        0.001140       11.542511              0.105291                         0.039484                      0.013161   -0.349019    0.295377    -1.810546     1.734635\n",
       "67               5                     280       59.81              9                         5                      3                   0.075000        0.213607                  0.428571              21.0                 2.848095                                    0.238095                                 0.142857                0.032143                           0.017857                        0.010714        4.681491              0.150477                         0.083598                      0.050159    0.032398    0.292187     0.362073     1.920195\n",
       "68               4                    1354       21.72              2                         0                      2                   0.015510        0.016041                  0.095238              21.0                 1.034286                                    0.000000                                 0.095238                0.001477                           0.000000                        0.001477       62.338858              0.092081                         0.000000                      0.092081   -0.237265    0.087244    -0.585706     0.531649\n",
       "69               4                    1564       48.20              3                         2                      0                   0.001279        0.030818                  1.500000               2.0                24.100000                                    1.000000                                 0.000000                0.001918                           0.001279                        0.000000       32.448133              0.062241                         0.041494                      0.000000   -0.330304    0.117252    -1.736771     0.443097\n",
       "70               2                     131       66.48             11                        10                      1                   0.022901        0.507481                  3.666667               3.0                22.160000                                    3.333333                                 0.333333                0.083969                           0.076336                        0.007634        1.970517              0.165463                         0.150421                      0.015042   -0.005106   -0.272038    -0.993315    -0.934356\n",
       "71               2                    1184       35.34              5                         8                      2                   0.016892        0.029848                  0.250000              20.0                 1.767000                                    0.400000                                 0.100000                0.004223                           0.006757                        0.001689       33.503113              0.141483                         0.226372                      0.056593   -0.146389   -0.301370    -0.593195    -1.917251\n",
       "72               5                    1694       52.61              1                        10                      5                   0.001181        0.031057                  0.500000               2.0                26.305000                                    5.000000                                 2.500000                0.000590                           0.005903                        0.002952       32.199202              0.019008                         0.190078                      0.095039    0.764194    0.317292     1.221473     0.752527\n",
       "73               1                    1021       15.30              1                         3                      4                   0.018609        0.014985                  0.052632              19.0                 0.805263                                    0.157895                                 0.210526                0.000979                           0.002938                        0.003918       66.732026              0.065359                         0.196078                      0.261438    0.167253   -0.512760     1.357841    -2.423085\n",
       "74               1                    1309       40.16              8                         3                      5                   0.012987        0.030680                  0.470588              17.0                 2.362353                                    0.176471                                 0.294118                0.006112                           0.002292                        0.003820       32.594622              0.199203                         0.074701                      0.124502    0.418606   -0.511156     1.960953    -2.223192\n",
       "75               6                    1409       30.54              5                         8                      0                   0.009226        0.021675                  0.384615              13.0                 2.349231                                    0.615385                                 0.000000                0.003549                           0.005678                        0.000000       46.136215              0.163720                         0.261952                      0.000000   -0.436180    0.510618    -2.063818     1.988861\n",
       "76               4                     952       61.40             11                         0                      2                   0.001050        0.064496                 11.000000               1.0                61.400000                                    0.000000                                 2.000000                0.011555                           0.000000                        0.002101       15.504886              0.179153                         0.000000                      0.032573    0.265730    0.122574     3.404396     0.108386\n",
       "77               2                    1484        8.37              0                         7                      3                   0.018868        0.005640                  0.000000              28.0                 0.298929                                    0.250000                                 0.107143                0.000000                           0.004717                        0.002022      177.299881              0.000000                         0.836320                      0.358423   -0.094212   -0.314690     0.361347    -2.027979\n",
       "78               6                      33       81.30              1                         4                      0                   0.909091        2.463636                  0.033333              30.0                 2.710000                                    0.133333                                 0.000000                0.030303                           0.121212                        0.000000        0.405904              0.012300                         0.049200                      0.000000   -0.656401    0.488213    -2.255680     2.111732\n",
       "79               4                     555       19.91              4                         1                      3                   0.007207        0.035874                  1.000000               4.0                 4.977500                                    0.250000                                 0.750000                0.007207                           0.001802                        0.005405       27.875439              0.200904                         0.050226                      0.150678    0.184926    0.104084     0.473830     0.476910\n",
       "80               1                     953       86.81             10                         6                      1                   0.025184        0.091091                  0.416667              24.0                 3.617083                                    0.250000                                 0.041667                0.010493                           0.006296                        0.001049       10.977998              0.115194                         0.069116                      0.011519   -0.346283   -0.502986    -1.690420    -2.316562\n",
       "81               2                     437       56.71              2                         7                      1                   0.022883        0.129771                  0.200000              10.0                 5.671000                                    0.700000                                 0.100000                0.004577                           0.016018                        0.002288        7.705872              0.035267                         0.123435                      0.017634   -0.228421   -0.290254    -1.455960    -1.345126\n",
       "82               6                     675       14.47              2                         1                      2                   0.026667        0.021437                  0.111111              18.0                 0.803889                                    0.055556                                 0.111111                0.002963                           0.001481                        0.002963       46.648238              0.138217                         0.069109                      0.138217   -0.198784    0.490810    -0.567895     2.334896\n",
       "83               1                     581       32.72              0                         7                      5                   0.034423        0.056317                  0.000000              20.0                 1.636000                                    0.350000                                 0.250000                0.000000                           0.012048                        0.008606       17.756724              0.000000                         0.213936                      0.152812    0.370116   -0.512009     1.933192    -2.282469\n",
       "84               3                    1277       26.15              0                         8                      0                   0.008614        0.020478                  0.000000              11.0                 2.377273                                    0.727273                                 0.000000                0.000000                           0.006265                        0.000000       48.833652              0.000000                         0.305927                      0.000000   -0.441328   -0.088447    -2.182540    -0.449952\n",
       "85               4                     706       16.38              5                         5                      4                   0.004249        0.023201                  1.666667               3.0                 5.460000                                    1.666667                                 1.333333                0.007082                           0.007082                        0.005666       43.101343              0.305250                         0.305250                      0.244200    0.453798    0.109272     1.300240     0.426456\n",
       "86               3                      41       47.41              7                         2                      4                   0.365854        1.156341                  0.466667              15.0                 3.160667                                    0.133333                                 0.266667                0.170732                           0.048780                        0.097561        0.864796              0.147648                         0.042185                      0.084370    0.249387   -0.108368     1.411381    -0.540051\n",
       "87               4                     594       63.48              6                         6                      2                   0.043771        0.106869                  0.230769              26.0                 2.441538                                    0.230769                                 0.076923                0.010101                           0.010101                        0.003367        9.357278              0.094518                         0.094518                      0.031506   -0.213568    0.090515    -0.614394     0.516220\n",
       "88               1                    1044       51.44              9                         4                      4                   0.017241        0.049272                  0.500000              18.0                 2.857778                                    0.222222                                 0.222222                0.008621                           0.003831                        0.003831       20.295490              0.174961                         0.077760                      0.077760    0.239975   -0.507987     1.360582    -2.396240\n",
       "89               5                     650       59.73             11                         0                      1                   0.046154        0.091892                  0.366667              30.0                 1.991000                                    0.000000                                 0.033333                0.016923                           0.000000                        0.001538       10.882304              0.184162                         0.000000                      0.016742   -0.460335    0.283556    -1.834735     1.790446\n",
       "90               5                     338       43.91              6                         2                      4                   0.002959        0.129911                  6.000000               1.0                43.910000                                    2.000000                                 4.000000                0.017751                           0.005917                        0.011834        7.697563              0.136643                         0.045548                      0.091095    0.660309    0.316781     3.475505     0.123218\n",
       "91               3                     580       88.44             10                         6                      5                   0.001724        0.152483                 10.000000               1.0                88.440000                                    6.000000                                 5.000000                0.017241                           0.010345                        0.008621        6.558118              0.113071                         0.067843                      0.056536    1.060829   -0.069564     3.626117     0.108674\n",
       "92               6                     314       77.71              5                        10                      5                   0.070064        0.247484                  0.227273              22.0                 3.532273                                    0.454545                                 0.227273                0.015924                           0.031847                        0.015924        4.040664              0.064342                         0.128684                      0.064342    0.409609    0.490560     1.897657     2.218347\n",
       "93               2                    1625       72.28              9                         2                      2                   0.004923        0.044480                  1.125000               8.0                 9.035000                                    0.250000                                 0.250000                0.005538                           0.001231                        0.001231       22.482014              0.124516                         0.027670                      0.027670   -0.010886   -0.293898    -0.526151    -1.437430\n",
       "94               4                    1225       27.98              8                        10                      1                   0.013061        0.022841                  0.500000              16.0                 1.748750                                    0.625000                                 0.062500                0.006531                           0.008163                        0.000816       43.781272              0.285919                         0.357398                      0.035740   -0.256314    0.108177    -1.739169     0.482357\n",
       "95               1                     666       97.66              4                         0                      4                   0.034535        0.146637                  0.173913              23.0                 4.246087                                    0.000000                                 0.173913                0.006006                           0.000000                        0.006006        6.819578              0.040958                         0.000000                      0.040958    0.143085   -0.518497     1.288936    -2.446570\n",
       "96               1                     405       62.14              2                         8                      0                   0.037037        0.153432                  0.133333              15.0                 4.142667                                    0.533333                                 0.000000                0.004938                           0.019753                        0.000000        6.517541              0.032185                         0.128742                      0.000000   -0.462443   -0.491470    -2.103142    -1.925902\n",
       "97               3                     403       38.54              0                         8                      1                   0.027295        0.095633                  0.000000              11.0                 3.503636                                    0.727273                                 0.090909                0.000000                           0.019851                        0.002481       10.456668              0.000000                         0.207577                      0.025947   -0.250854   -0.091152    -1.569144    -0.455662\n",
       "98               5                     527       79.43             10                         6                      2                   0.003795        0.150721                  5.000000               2.0                39.715000                                    3.000000                                 1.000000                0.018975                           0.011385                        0.003795        6.634773              0.125897                         0.075538                      0.025179    0.211066    0.323883    -0.321653     0.723815\n",
       "99               1                     652       64.01              6                         2                      4                   0.007669        0.098175                  1.200000               5.0                12.802000                                    0.400000                                 0.800000                0.009202                           0.003067                        0.006135       10.185908              0.093735                         0.031245                      0.062490    0.397474   -0.496382     1.219413    -1.757077\n",
       "100              4                    1740       83.60              1                         7                      3                   0.004023        0.048046                  0.142857               7.0                11.942857                                    1.000000                                 0.428571                0.000575                           0.004023                        0.001724       20.813397              0.011962                         0.083732                      0.035885    0.190354    0.108247     0.396245     0.433069\n",
       "101              2                    1794       29.34              0                         9                      4                   0.011148        0.016355                  0.000000              20.0                 1.467000                                    0.450000                                 0.200000                0.000000                           0.005017                        0.002230       61.145194              0.000000                         0.306748                      0.136333    0.199407   -0.306850     1.385703    -1.713234\n",
       "102              3                    1077       86.33              6                        10                      0                   0.002786        0.080158                  2.000000               3.0                28.776667                                    3.333333                                 0.000000                0.005571                           0.009285                        0.000000       12.475385              0.069501                         0.115835                      0.000000   -0.227794   -0.071145    -1.494627    -0.318647\n",
       "103              2                    1258       36.61              8                         5                      3                   0.003180        0.029102                  2.000000               4.0                 9.152500                                    1.250000                                 0.750000                0.006359                           0.003975                        0.002385       34.362196              0.218520                         0.136575                      0.081945    0.258578   -0.288120     0.387946    -1.289043\n",
       "104              6                    1377       47.45              3                         6                      5                   0.000726        0.034459                  3.000000               1.0                47.450000                                    6.000000                                 5.000000                0.002179                           0.004357                        0.003631       29.020021              0.063224                         0.126449                      0.105374    0.915379    0.520606     3.539910     0.124749\n",
       "105              3                    1788       24.98              9                         2                      3                   0.012304        0.013971                  0.409091              22.0                 1.135455                                    0.090909                                 0.136364                0.005034                           0.001119                        0.001678       71.577262              0.360288                         0.080064                      0.120096   -0.013854   -0.112010     0.345096    -0.550858\n",
       "106              2                    1162       37.26             10                         6                      2                   0.004303        0.032065                  2.000000               5.0                 7.452000                                    1.200000                                 0.400000                0.008606                           0.005164                        0.001721       31.186259              0.268384                         0.161031                      0.053677    0.066070   -0.285305    -0.495923    -1.313870\n",
       "107              4                     979        8.86              8                         3                      2                   0.027579        0.009050                  0.296296              27.0                 0.328148                                    0.111111                                 0.074074                0.008172                           0.003064                        0.002043      110.496614              0.902935                         0.338600                      0.225734   -0.254634    0.086100    -0.588204     0.536142\n",
       "108              1                     587        3.83              0                         9                      0                   0.022147        0.006525                  0.000000              13.0                 0.294615                                    0.692308                                 0.000000                0.000000                           0.015332                        0.000000      153.263708              0.000000                         2.349869                      0.000000   -0.463786   -0.489571    -2.020708    -1.864661\n",
       "109              5                    1777       67.23              1                         1                      1                   0.001688        0.037833                  0.333333               3.0                22.410000                                    0.333333                                 0.333333                0.000563                           0.000563                        0.000563       26.431653              0.014874                         0.014874                      0.014874   -0.174948    0.310909    -1.237590     1.276763\n",
       "110              1                    1685       62.39              3                         7                      2                   0.017804        0.037027                  0.100000              30.0                 2.079667                                    0.233333                                 0.066667                0.001780                           0.004154                        0.001187       27.007533              0.048085                         0.112197                      0.032056   -0.266381   -0.512390    -0.639070    -2.522591\n",
       "111              2                    1246        2.33              9                         2                      1                   0.016854        0.001870                  0.428571              21.0                 0.110952                                    0.095238                                 0.047619                0.007223                           0.001605                        0.000803      534.763948              3.862661                         0.858369                      0.429185   -0.376011   -0.305903    -1.770405    -1.724881\n",
       "112              6                     303       54.13              2                         8                      0                   0.079208        0.178647                  0.083333              24.0                 2.255417                                    0.333333                                 0.000000                0.006601                           0.026403                        0.000000        5.597635              0.036948                         0.147792                      0.000000   -0.565796    0.498713    -2.246209     2.142398\n",
       "113              2                    1095        9.44             11                         6                      1                   0.015525        0.008621                  0.647059              17.0                 0.555294                                    0.352941                                 0.058824                0.010046                           0.005479                        0.000913      115.995763              1.165254                         0.635593                      0.105932   -0.288443   -0.296690    -1.678647    -1.623752\n",
       "114              1                       8       51.57              0                         5                      0                   0.250000        6.446250                  0.000000               2.0                25.785000                                    2.500000                                 0.000000                0.000000                           0.625000                        0.000000        0.155129              0.000000                         0.096956                      0.000000   -0.317583   -0.479721    -1.649683    -1.441282\n",
       "115              3                    1125       64.29              8                         9                      0                   0.002667        0.057147                  2.666667               3.0                21.430000                                    3.000000                                 0.000000                0.007111                           0.008000                        0.000000       17.498833              0.124436                         0.139991                      0.000000   -0.236115   -0.072329    -1.509923    -0.325212\n",
       "116              2                    1394       86.22              1                         2                      1                   0.012195        0.061851                  0.058824              17.0                 5.071765                                    0.117647                                 0.058824                0.000717                           0.001435                        0.000717       16.167942              0.011598                         0.023196                      0.011598   -0.344384   -0.302984    -1.705218    -1.615325\n",
       "117              3                     478        4.48             11                         8                      2                   0.039749        0.009372                  0.578947              19.0                 0.235789                                    0.421053                                 0.105263                0.023013                           0.016736                        0.004184      106.696429              2.455357                         1.785714                      0.446429   -0.114081   -0.099461    -0.564797    -0.530682\n",
       "118              6                    1746       68.99              2                         7                      2                   0.010309        0.039513                  0.111111              18.0                 3.832778                                    0.388889                                 0.111111                0.001145                           0.004009                        0.001145       25.308016              0.028990                         0.101464                      0.028990   -0.136049    0.498734    -0.591832     2.384673\n",
       "119              3                    1700       21.46              3                        10                      5                   0.004706        0.012624                  0.375000               8.0                 2.682500                                    1.250000                                 0.625000                0.001765                           0.005882                        0.002941       79.217148              0.139795                         0.465983                      0.232992    0.558587   -0.094934     1.831263    -0.519757\n",
       "120              6                     961       58.21              6                         0                      0                   0.019771        0.060572                  0.315789              19.0                 3.063684                                    0.000000                                 0.000000                0.006243                           0.000000                        0.000000       16.509191              0.103075                         0.000000                      0.000000   -0.548874    0.495751    -2.238045     2.114063\n",
       "121              3                     352       57.85              2                         4                      0                   0.008523        0.164347                  0.666667               3.0                19.283333                                    1.333333                                 0.000000                0.005682                           0.011364                        0.000000        6.084702              0.034572                         0.069144                      0.000000   -0.336361   -0.082467    -1.916044    -0.345205\n",
       "122              2                    1154       26.67              7                         9                      4                   0.017331        0.023111                  0.350000              20.0                 1.333500                                    0.450000                                 0.200000                0.006066                           0.007799                        0.003466       43.269591              0.262467                         0.337458                      0.149981    0.235012   -0.305194     1.454368    -1.824861\n",
       "123              3                     470        8.25              7                         7                      2                   0.010638        0.017553                  1.400000               5.0                 1.650000                                    1.400000                                 0.400000                0.014894                           0.014894                        0.004255       56.969697              0.848485                         0.848485                      0.242424    0.042272   -0.086112    -0.470571    -0.417043\n",
       "124              2                     618       90.66              2                        10                      5                   0.021036        0.146699                  0.153846              13.0                 6.973846                                    0.769231                                 0.384615                0.003236                           0.016181                        0.008091        6.816678              0.022060                         0.110302                      0.055151    0.508359   -0.300070     1.886692    -1.461049\n",
       "125              4                     301       43.08              4                         0                      1                   0.036545        0.143123                  0.363636              11.0                 3.916364                                    0.000000                                 0.090909                0.013289                           0.000000                        0.003322        6.987001              0.092851                         0.000000                      0.023213   -0.289943    0.100431    -1.631992     0.475209\n",
       "126              5                    1607       11.70              7                         6                      0                   0.002489        0.007281                  1.750000               4.0                 2.925000                                    1.500000                                 0.000000                0.004356                           0.003734                        0.000000      137.350427              0.598291                         0.512821                      0.000000   -0.328511    0.319326    -1.799030     1.136609\n",
       "127              1                     488       35.97              0                         3                      0                   0.014344        0.073709                  0.000000               7.0                 5.138571                                    0.428571                                 0.000000                0.000000                           0.006148                        0.000000       13.566861              0.000000                         0.083403                      0.000000   -0.429844   -0.490044    -1.923349    -1.716013\n",
       "128              3                    1466       18.47              1                         7                      2                   0.017735        0.012599                  0.038462              26.0                 0.710385                                    0.269231                                 0.076923                0.000682                           0.004775                        0.001364       79.371955              0.054142                         0.378993                      0.108284   -0.246775   -0.109779    -0.642605    -0.511483\n",
       "129              4                    1040       65.36             10                         3                      0                   0.020192        0.062846                  0.476190              21.0                 3.112381                                    0.142857                                 0.000000                0.009615                           0.002885                        0.000000       15.911873              0.152999                         0.045900                      0.000000   -0.525571    0.098653    -2.509734     0.494890\n",
       "130              6                     455       42.89              8                         0                      3                   0.028571        0.094264                  0.615385              13.0                 3.299231                                    0.000000                                 0.230769                0.017582                           0.000000                        0.006593       10.608533              0.186524                         0.000000                      0.069946    0.076987    0.493971     0.377488     2.139227\n",
       "131              4                    1695        8.71              9                         2                      2                   0.008850        0.005139                  0.600000              15.0                 0.580667                                    0.133333                                 0.133333                0.005310                           0.001180                        0.001180      194.603904              1.033295                         0.229621                      0.229621   -0.123339    0.097346    -0.585983     0.502683\n",
       "132              4                     518       35.77              0                         2                      2                   0.042471        0.069054                  0.000000              22.0                 1.625909                                    0.090909                                 0.090909                0.000000                           0.003861                        0.003861       14.481409              0.000000                         0.055913                      0.055913   -0.238255    0.087977    -0.624167     0.530130\n",
       "133              6                    1183       55.17              9                         9                      3                   0.015216        0.046636                  0.500000              18.0                 3.065000                                    0.500000                                 0.166667                0.007608                           0.007608                        0.002536       21.442813              0.163132                         0.163132                      0.054377    0.094516    0.499920     0.372490     2.355158\n",
       "134              3                    1230       80.65             10                         1                      3                   0.024390        0.065569                  0.333333              30.0                 2.688333                                    0.033333                                 0.100000                0.008130                           0.000813                        0.002439       15.251085              0.123993                         0.012399                      0.037198   -0.086448   -0.120337     0.342721    -0.582871\n",
       "135              6                     571       14.13              6                         8                      5                   0.026270        0.024746                  0.400000              15.0                 0.942000                                    0.533333                                 0.333333                0.010508                           0.014011                        0.008757       40.410474              0.424628                         0.566171                      0.353857    0.461146    0.494981     1.952633     2.082048\n",
       "136              2                     252       26.07              5                         9                      2                   0.059524        0.103452                  0.333333              15.0                 1.738000                                    0.600000                                 0.133333                0.019841                           0.035714                        0.007937        9.666283              0.191791                         0.345224                      0.076717   -0.083666   -0.295489    -0.549774    -1.630075\n",
       "137              1                     806       37.38              5                         5                      3                   0.008685        0.046377                  0.714286               7.0                 5.340000                                    0.714286                                 0.428571                0.006203                           0.006203                        0.003722       21.562333              0.133761                         0.133761                      0.080257    0.178951   -0.493633     0.390586    -1.946326\n",
       "138              1                    1798        4.14              4                         8                      3                   0.006674        0.002303                  0.333333              12.0                 0.345000                                    0.666667                                 0.250000                0.002225                           0.004449                        0.001669      434.299517              0.966184                         1.932367                      0.724638    0.114780   -0.496235     0.379510    -2.147009\n",
       "139              1                     583       86.07              9                         9                      5                   0.029160        0.147633                  0.529412              17.0                 5.062941                                    0.529412                                 0.294118                0.015437                           0.015437                        0.008576        6.773556              0.104566                         0.104566                      0.058092    0.485475   -0.503528     1.881517    -2.084089\n",
       "140              5                     735       48.40              2                         7                      4                   0.010884        0.065850                  0.250000               8.0                 6.050000                                    0.875000                                 0.500000                0.002721                           0.009524                        0.005442       15.185950              0.041322                         0.144628                      0.082645    0.353275    0.303670     1.336312     1.440252\n",
       "141              6                     926        4.05              0                         6                      5                   0.026998        0.004374                  0.000000              25.0                 0.162000                                    0.240000                                 0.200000                0.000000                           0.006479                        0.005400      228.641975              0.000000                         1.481481                      1.234568    0.295811    0.480973     1.946658     2.329079\n",
       "142              3                    1789       46.50              7                         0                      3                   0.001118        0.025992                  3.500000               2.0                23.250000                                    0.000000                                 1.500000                0.003913                           0.000000                        0.001677       38.473118              0.150538                         0.000000                      0.064516    0.289340   -0.090152     0.546109    -0.407515\n",
       "143              5                    1387       58.57              3                         7                      1                   0.000721        0.042228                  3.000000               1.0                58.570000                                    7.000000                                 1.000000                0.002163                           0.005047                        0.000721       23.681065              0.051221                         0.119515                      0.017074    0.047770    0.330549     3.250104     0.146613\n",
       "144              3                     252       72.77             11                         1                      5                   0.023810        0.288770                  1.833333               6.0                12.128333                                    0.166667                                 0.833333                0.043651                           0.003968                        0.019841        3.462966              0.151161                         0.013742                      0.068710    0.591178   -0.100129     1.660981    -0.425891\n",
       "145              6                     133       13.79              0                         2                      0                   0.157895        0.103684                  0.000000              21.0                 0.656667                                    0.095238                                 0.000000                0.000000                           0.015038                        0.000000        9.644670              0.000000                         0.145033                      0.000000   -0.600899    0.493726    -2.185368     2.021419\n",
       "146              5                    1784       11.42              4                         5                      4                   0.012892        0.006401                  0.173913              23.0                 0.496522                                    0.217391                                 0.173913                0.002242                           0.002803                        0.002242      156.217163              0.350263                         0.437828                      0.350263    0.149957    0.286002     1.517198     1.945273\n",
       "147              2                     392       20.86              9                         5                      2                   0.030612        0.053214                  0.750000              12.0                 1.738333                                    0.416667                                 0.166667                0.022959                           0.012755                        0.005102       18.791946              0.431448                         0.239693                      0.095877   -0.058865   -0.295941    -0.519896    -1.583911\n",
       "148              6                    1400       61.12              0                         8                      0                   0.001429        0.043657                  0.000000               2.0                30.560000                                    4.000000                                 0.000000                0.000000                           0.005714                        0.000000       22.905759              0.000000                         0.130890                      0.000000   -0.276335    0.525029    -1.310742     1.230293\n",
       "149              4                     486       94.09             11                         9                      5                   0.026749        0.193601                  0.846154              13.0                 7.237692                                    0.692308                                 0.384615                0.022634                           0.018519                        0.010288        5.165267              0.116909                         0.095653                      0.053141    0.550154    0.101063     1.911370     0.481390\n",
       "150              1                     354       82.36              4                         3                      4                   0.056497        0.232655                  0.200000              20.0                 4.118000                                    0.150000                                 0.200000                0.011299                           0.008475                        0.011299        4.298203              0.048567                         0.036425                      0.048567    0.194579   -0.512313     1.333194    -2.401191\n",
       "151              2                    1077       85.23              1                         8                      0                   0.022284        0.079136                  0.041667              24.0                 3.551250                                    0.333333                                 0.000000                0.000929                           0.007428                        0.000000       12.636396              0.011733                         0.093864                      0.000000   -0.560821   -0.300450    -2.395571    -1.588696\n",
       "152              6                     986       38.30              2                         4                      2                   0.011156        0.038844                  0.181818              11.0                 3.481818                                    0.363636                                 0.181818                0.002028                           0.004057                        0.002028       25.744125              0.052219                         0.104439                      0.052219   -0.087083    0.501789    -0.515210     2.188878\n",
       "153              4                     883       71.85             11                         9                      2                   0.010193        0.081370                  1.222222               9.0                 7.983333                                    1.000000                                 0.222222                0.012458                           0.010193                        0.002265       12.289492              0.153097                         0.125261                      0.027836    0.042803    0.113504    -0.527827     0.491065\n",
       "154              3                    1064       13.03             11                         6                      0                   0.022556        0.012246                  0.458333              24.0                 0.542917                                    0.250000                                 0.000000                0.010338                           0.005639                        0.000000       81.657713              0.844206                         0.460476                      0.000000   -0.548807   -0.101111    -2.508782    -0.502444\n",
       "155              4                    1387       45.70              4                         7                      4                   0.019466        0.032949                  0.148148              27.0                 1.692593                                    0.259259                                 0.148148                0.002884                           0.005047                        0.002884       30.350109              0.087527                         0.153173                      0.087527    0.131617    0.084744     1.619102     0.531306\n",
       "156              6                     701       97.76              4                         9                      3                   0.037090        0.139458                  0.153846              26.0                 3.760000                                    0.346154                                 0.115385                0.005706                           0.012839                        0.004280        7.170622              0.040917                         0.092062                      0.030687   -0.007980    0.490875     0.332471     2.474133\n",
       "157              4                    1649       88.15              1                         7                      3                   0.011522        0.053457                  0.052632              19.0                 4.639474                                    0.368421                                 0.157895                0.000606                           0.004245                        0.001819       18.706750              0.011344                         0.079410                      0.034033    0.037613    0.095235     0.363114     0.500594\n",
       "158              6                    1074       24.42              9                         6                      0                   0.023277        0.022737                  0.360000              25.0                 0.976800                                    0.240000                                 0.000000                0.008380                           0.005587                        0.000000       43.980344              0.368550                         0.245700                      0.000000   -0.566490    0.497118    -2.260485     2.198636\n",
       "159              1                      77       68.76             11                        10                      4                   0.233766        0.892987                  0.611111              18.0                 3.820000                                    0.555556                                 0.222222                0.142857                           0.129870                        0.051948        1.119837              0.159977                         0.145433                      0.058173    0.302100   -0.500545     1.288634    -2.168150\n",
       "160              1                    1598       16.47              6                         1                      0                   0.018773        0.010307                  0.200000              30.0                 0.549000                                    0.033333                                 0.000000                0.003755                           0.000626                        0.000000       97.024894              0.364299                         0.060716                      0.000000   -0.674898   -0.513560    -2.257268    -2.174318\n",
       "161              1                      16       29.44              9                         1                      5                   0.625000        1.840000                  0.900000              10.0                 2.944000                                    0.100000                                 0.500000                0.562500                           0.062500                        0.312500        0.543478              0.305707                         0.033967                      0.169837    0.495087   -0.506332     1.784030    -1.787131\n",
       "162              1                    1307       36.94              4                         1                      4                   0.005356        0.028263                  0.571429               7.0                 5.277143                                    0.142857                                 0.571429                0.003060                           0.000765                        0.003060       35.381700              0.108284                         0.027071                      0.108284    0.326265   -0.501366     1.263510    -1.938844\n",
       "163              2                    1437       49.61              2                         0                      4                   0.013222        0.034523                  0.105263              19.0                 2.611053                                    0.000000                                 0.210526                0.001392                           0.000000                        0.002784       28.965934              0.040314                         0.000000                      0.080629    0.161229   -0.315550     1.420020    -1.908916\n",
       "164              5                     872       18.26              3                         7                      2                   0.010321        0.020940                  0.333333               9.0                 2.028889                                    0.777778                                 0.222222                0.003440                           0.008028                        0.002294       47.754655              0.164294                         0.383352                      0.109529   -0.039924    0.307610    -0.502431     1.561197\n",
       "165              6                     498       56.83              1                         0                      0                   0.014056        0.114116                  0.142857               7.0                 8.118571                                    0.000000                                 0.000000                0.002008                           0.000000                        0.000000        8.762977              0.017596                         0.000000                      0.000000   -0.440682    0.506328    -1.885966     1.746130\n",
       "166              5                     174       21.04              6                         6                      5                   0.051724        0.120920                  0.666667               9.0                 2.337778                                    0.666667                                 0.555556                0.034483                           0.034483                        0.028736        8.269962              0.285171                         0.285171                      0.237643    0.528609    0.299278     1.921933     1.361451\n",
       "167              3                    1273        4.72              7                         0                      5                   0.013354        0.003708                  0.411765              17.0                 0.277647                                    0.000000                                 0.294118                0.005499                           0.000000                        0.003928      269.703390              1.483051                         0.000000                      1.059322    0.378371   -0.115637     2.115145    -0.579228\n",
       "168              6                    1019       63.42              0                         8                      1                   0.012758        0.062237                  0.000000              13.0                 4.878462                                    0.615385                                 0.076923                0.000000                           0.007851                        0.000981       16.067487              0.000000                         0.126143                      0.015768   -0.266321    0.506886    -1.476022     2.055631\n",
       "169              5                    1706       49.16             11                         5                      1                   0.016999        0.028816                  0.379310              29.0                 1.695172                                    0.172414                                 0.034483                0.006448                           0.002931                        0.000586       34.703011              0.223759                         0.101709                      0.020342   -0.417305    0.290406    -1.838035     1.807709\n",
       "170              2                     475       34.30              8                         6                      0                   0.056842        0.072211                  0.296296              27.0                 1.270370                                    0.222222                                 0.000000                0.016842                           0.012632                        0.000000       13.848397              0.233236                         0.174927                      0.000000   -0.589245   -0.304641    -2.434409    -1.585167\n",
       "171              2                     934       56.45              9                        10                      2                   0.016060        0.060439                  0.600000              15.0                 3.763333                                    0.666667                                 0.133333                0.009636                           0.010707                        0.002141       16.545616              0.159433                         0.177148                      0.035430   -0.044783   -0.292619    -0.550653    -1.667654\n",
       "172              6                     605       91.49              0                         4                      0                   0.039669        0.151223                  0.000000              24.0                 3.812083                                    0.166667                                 0.000000                0.000000                           0.006612                        0.000000        6.612745              0.000000                         0.043721                      0.000000   -0.593271    0.494126    -2.242729     2.116446\n",
       "173              5                    1001       17.92              5                         4                      3                   0.018981        0.017902                  0.263158              19.0                 0.943158                                    0.210526                                 0.157895                0.004995                           0.003996                        0.002997       55.859375              0.279018                         0.223214                      0.167411    0.012420    0.291662     0.363158     1.954131\n",
       "174              2                    1621       33.91              8                         0                      0                   0.017273        0.020919                  0.285714              28.0                 1.211071                                    0.000000                                 0.000000                0.004935                           0.000000                        0.000000       47.803008              0.235919                         0.000000                      0.000000   -0.645039   -0.312165    -2.441574    -1.590580\n",
       "175              1                    1197       11.15              1                         9                      3                   0.010860        0.009315                  0.076923              13.0                 0.857692                                    0.692308                                 0.230769                0.000835                           0.007519                        0.002506      107.354260              0.089686                         0.807175                      0.269058    0.097118   -0.497028     0.380270    -2.195638\n",
       "176              2                     150       33.51              8                         3                      1                   0.193333        0.223400                  0.275862              29.0                 1.155517                                    0.103448                                 0.034483                0.053333                           0.020000                        0.006667        4.476276              0.238735                         0.089526                      0.029842   -0.450280   -0.312841    -1.811161    -1.732991\n",
       "177              6                    1500       36.88              1                         6                      4                   0.012000        0.024587                  0.055556              18.0                 2.048889                                    0.333333                                 0.222222                0.000667                           0.004000                        0.002667       40.672451              0.027115                         0.162690                      0.108460    0.207546    0.491452     1.395871     2.374583\n",
       "178              2                     817       24.61             11                        10                      2                   0.012240        0.030122                  1.100000              10.0                 2.461000                                    1.000000                                 0.200000                0.013464                           0.012240                        0.002448       33.197887              0.446973                         0.406339                      0.081268    0.018372   -0.287020    -0.559148    -1.401861\n",
       "179              2                    1525        1.98              7                         3                      0                   0.001311        0.001298                  3.500000               2.0                 0.990000                                    1.500000                                 0.000000                0.004590                           0.001967                        0.000000      770.202020              3.535354                         1.515152                      0.000000   -0.316800   -0.280733    -1.658054    -1.095683\n",
       "180              5                     478       84.37              6                         5                      5                   0.039749        0.176506                  0.315789              19.0                 4.440526                                    0.263158                                 0.263158                0.012552                           0.010460                        0.010460        5.665521              0.071115                         0.059263                      0.059263    0.414915    0.288380     2.070851     1.650042\n",
       "181              5                     972       71.76              6                         8                      3                   0.005144        0.073827                  1.200000               5.0                14.352000                                    1.600000                                 0.600000                0.006173                           0.008230                        0.003086       13.545151              0.083612                         0.111483                      0.041806    0.264242    0.313588     0.379195     1.312437\n",
       "182              3                     509       21.03              1                         3                      0                   0.035363        0.041316                  0.055556              18.0                 1.168333                                    0.166667                                 0.000000                0.001965                           0.005894                        0.000000       24.203519              0.047551                         0.142653                      0.000000   -0.553342   -0.101330    -2.473010    -0.486504\n",
       "183              5                      84        5.11              0                         4                      1                   0.142857        0.060833                  0.000000              12.0                 0.425833                                    0.333333                                 0.083333                0.000000                           0.047619                        0.011905       16.438356              0.000000                         0.782779                      0.195695   -0.306115    0.302235    -1.567058     1.470711\n",
       "184              4                     373       36.81              4                         5                      0                   0.072386        0.098686                  0.148148              27.0                 1.363333                                    0.185185                                 0.000000                0.010724                           0.013405                        0.000000       10.133116              0.108666                         0.135833                      0.000000   -0.615962    0.092926    -2.577707     0.498381\n",
       "185              3                    1315       64.80              9                         7                      4                   0.004563        0.049278                  1.500000               6.0                10.800000                                    1.166667                                 0.666667                0.006844                           0.005323                        0.003042       20.293210              0.138889                         0.108025                      0.061728    0.437612   -0.090601     1.287018    -0.483106\n",
       "186              3                     760       32.90              0                         0                      4                   0.011842        0.043289                  0.000000               9.0                 3.655556                                    0.000000                                 0.444444                0.000000                           0.000000                        0.005263       23.100304              0.000000                         0.000000                      0.121581    0.264727   -0.106542     1.449901    -0.521575\n",
       "187              5                    1808       44.36              5                         0                      3                   0.013827        0.024535                  0.200000              25.0                 1.774400                                    0.000000                                 0.120000                0.002765                           0.000000                        0.001659       40.757439              0.112714                         0.000000                      0.067628   -0.076099    0.281653     0.394807     1.997327\n",
       "188              2                    1689       23.54             10                         7                      5                   0.004144        0.013937                  1.428571               7.0                 3.362857                                    1.000000                                 0.714286                0.005921                           0.004144                        0.002960       71.750212              0.424809                         0.297366                      0.212404    0.593354   -0.294885     1.767289    -1.242308\n",
       "189              6                      64       73.73              0                         4                      3                   0.218750        1.152031                  0.000000              14.0                 5.266429                                    0.285714                                 0.214286                0.000000                           0.062500                        0.046875        0.868032              0.000000                         0.054252                      0.040689    0.064350    0.495696     0.348556     2.204473\n",
       "190              5                    1005       45.66              4                         2                      4                   0.026866        0.045433                  0.148148              27.0                 1.691111                                    0.074074                                 0.148148                0.003980                           0.001990                        0.003980       22.010512              0.087604                         0.043802                      0.087604    0.095280    0.278800     1.511743     1.986378\n",
       "191              3                     993       57.11             10                        10                      0                   0.020141        0.057513                  0.500000              20.0                 2.855500                                    0.500000                                 0.000000                0.010070                           0.010070                        0.000000       17.387498              0.175101                         0.175101                      0.000000   -0.465014   -0.092257    -2.405918    -0.470611\n",
       "192              2                      60       35.59             10                         2                      4                   0.433333        0.593167                  0.384615              26.0                 1.368846                                    0.076923                                 0.153846                0.166667                           0.033333                        0.066667        1.685867              0.280978                         0.056196                      0.112391    0.134467   -0.318643     1.377448    -1.868006\n",
       "193              6                     679       86.06              8                         8                      3                   0.019146        0.126745                  0.615385              13.0                 6.620000                                    0.615385                                 0.230769                0.011782                           0.011782                        0.004418        7.889844              0.092958                         0.092958                      0.034859    0.154100    0.504049     0.374560     2.175920\n",
       "194              2                    1717       45.44              4                         9                      2                   0.009901        0.026465                  0.235294              17.0                 2.672941                                    0.529412                                 0.117647                0.002330                           0.005242                        0.001165       37.786092              0.088028                         0.198063                      0.044014   -0.106768   -0.297170    -0.578729    -1.813681\n",
       "195              1                     175       97.26             10                         7                      1                   0.097143        0.555771                  0.588235              17.0                 5.721176                                    0.411765                                 0.058824                0.057143                           0.040000                        0.005714        1.799301              0.102817                         0.071972                      0.010282   -0.254858   -0.494790    -1.508955    -2.094675\n",
       "196              2                     888       96.19              6                         5                      0                   0.010135        0.108322                  0.666667               9.0                10.687778                                    0.555556                                 0.000000                0.006757                           0.005631                        0.000000        9.231729              0.062377                         0.051980                      0.000000   -0.379825   -0.286914    -2.101203    -1.326087\n",
       "197              4                     167       30.81             10                         2                      3                   0.113772        0.184491                  0.526316              19.0                 1.621579                                    0.105263                                 0.157895                0.059880                           0.011976                        0.017964        5.420318              0.324570                         0.064914                      0.097371    0.028507    0.090757     0.384752     0.510863\n",
       "198              2                    1314       17.78              2                         1                      0                   0.000761        0.013531                  2.000000               1.0                17.780000                                    1.000000                                 0.000000                0.001522                           0.000761                        0.000000       73.903262              0.112486                         0.056243                      0.000000   -0.341165   -0.283304    -1.684899    -1.202937\n",
       "199              4                     849       92.81             10                         3                      2                   0.002356        0.109317                  5.000000               2.0                46.405000                                    1.500000                                 1.000000                0.011779                           0.003534                        0.002356        9.147721              0.107747                         0.032324                      0.021549    0.185274    0.120086    -0.295869     0.278374\n",
       "200              5                     303        2.72             10                         8                      1                   0.039604        0.008977                  0.833333              12.0                 0.226667                                    0.666667                                 0.083333                0.033003                           0.026403                        0.003300      111.397059              3.676471                         2.941176                      0.367647   -0.221813    0.309872    -1.446125     1.335933\n",
       "201              6                     186       42.15              7                         3                      4                   0.048387        0.226613                  0.777778               9.0                 4.683333                                    0.333333                                 0.444444                0.037634                           0.016129                        0.021505        4.412811              0.166074                         0.071174                      0.094899    0.332402    0.498828     1.264839     1.934676\n",
       "202              2                    1816       72.32              2                         6                      5                   0.014317        0.039824                  0.076923              26.0                 2.781538                                    0.230769                                 0.192308                0.001101                           0.003304                        0.002753       25.110619              0.027655                         0.082965                      0.069137    0.316737   -0.317950     2.086976    -1.886836\n",
       "203              5                     471       60.59              8                         2                      4                   0.048832        0.128641                  0.347826              23.0                 2.634348                                    0.086957                                 0.173913                0.016985                           0.004246                        0.008493        7.773560              0.132035                         0.033009                      0.066017    0.165588    0.283829     1.430841     1.856731\n",
       "204              5                     591       11.09              7                         1                      4                   0.001692        0.018765                  7.000000               1.0                11.090000                                    1.000000                                 4.000000                0.011844                           0.001692                        0.006768       53.291253              0.631199                         0.090171                      0.360685    0.616504    0.313159     3.442124     0.126928\n",
       "205              4                     401       84.09              3                         2                      5                   0.049875        0.209701                  0.150000              20.0                 4.204500                                    0.100000                                 0.250000                0.007481                           0.004988                        0.012469        4.768700              0.035676                         0.023784                      0.059460    0.365517    0.083210     2.087890     0.531684\n",
       "206              2                     574        0.07             10                         0                      0                   0.015679        0.000122                  1.111111               9.0                 0.007778                                    0.000000                                 0.000000                0.017422                           0.000000                        0.000000     8200.000000            142.857143                         0.000000                      0.000000   -0.435042   -0.293384    -1.985836    -1.258672\n",
       "207              6                    1227        7.76              0                         8                      3                   0.012225        0.006324                  0.000000              15.0                 0.517333                                    0.533333                                 0.200000                0.000000                           0.006520                        0.002445      158.118557              0.000000                         1.030928                      0.386598    0.058595    0.498770     0.351063     2.302593\n",
       "208              4                     605       73.28              2                         0                      1                   0.016529        0.121124                  0.200000              10.0                 7.328000                                    0.000000                                 0.100000                0.003306                           0.000000                        0.001653        8.256004              0.027293                         0.000000                      0.013646   -0.277961    0.101422    -1.562473     0.449867\n",
       "209              6                     877       14.63              2                         6                      3                   0.030787        0.016682                  0.074074              27.0                 0.541852                                    0.222222                                 0.111111                0.002281                           0.006842                        0.003421       59.945318              0.136705                         0.410116                      0.205058   -0.078370    0.485045     0.389452     2.560760\n",
       "210              5                    1149       19.94              7                         1                      4                   0.013925        0.017354                  0.437500              16.0                 1.246250                                    0.062500                                 0.250000                0.006092                           0.000870                        0.003481       57.622869              0.351053                         0.050150                      0.200602    0.219046    0.289120     1.397904     1.686173\n",
       "211              2                      51       76.96             11                         8                      3                   0.549020        1.509020                  0.392857              28.0                 2.748571                                    0.285714                                 0.107143                0.215686                           0.156863                        0.058824        0.662682              0.142931                         0.103950                      0.038981   -0.008287   -0.310349     0.346922    -1.870742\n",
       "212              6                    1570       81.60              8                         3                      4                   0.015924        0.051975                  0.320000              25.0                 3.264000                                    0.120000                                 0.160000                0.005096                           0.001911                        0.002548       19.240196              0.098039                         0.036765                      0.049020    0.156245    0.483390     1.401493     2.479143\n",
       "213              2                     148       55.66              2                         0                      4                   0.162162        0.376081                  0.083333              24.0                 2.319167                                    0.000000                                 0.166667                0.013514                           0.000000                        0.027027        2.659001              0.035932                         0.000000                      0.071865    0.108193   -0.320773     1.394666    -1.952027\n",
       "214              1                    1037       33.26             10                         6                      4                   0.005786        0.032073                  1.666667               6.0                 5.543333                                    1.000000                                 0.666667                0.009643                           0.005786                        0.003857       31.178593              0.300661                         0.180397                      0.120265    0.421547   -0.491970     1.209323    -1.778674\n",
       "215              6                     698       15.93              4                        10                      5                   0.034384        0.022822                  0.166667              24.0                 0.663750                                    0.416667                                 0.208333                0.005731                           0.014327                        0.007163       43.816698              0.251099                         0.627746                      0.313873    0.360773    0.487661     1.959383     2.308124\n",
       "216              1                    1332        9.19              5                        10                      4                   0.010511        0.006899                  0.357143              14.0                 0.656429                                    0.714286                                 0.285714                0.003754                           0.007508                        0.003003      144.940152              0.544070                         1.088139                      0.435256    0.297457   -0.498436     1.259410    -2.106750\n",
       "217              2                     658       58.00              2                         8                      2                   0.022796        0.088146                  0.133333              15.0                 3.866667                                    0.533333                                 0.133333                0.003040                           0.012158                        0.003040       11.344828              0.034483                         0.137931                      0.034483   -0.096190   -0.296949    -0.555048    -1.738005\n",
       "218              3                     723       31.00             11                         9                      2                   0.016598        0.042877                  0.916667              12.0                 2.583333                                    0.750000                                 0.166667                0.015214                           0.012448                        0.002766       23.322581              0.354839                         0.290323                      0.064516   -0.013583   -0.090532    -0.528755    -0.525878\n",
       "219              6                    1391       69.14             11                         2                      2                   0.003595        0.049705                  2.200000               5.0                13.828000                                    0.400000                                 0.400000                0.007908                           0.001438                        0.001438       20.118600              0.159097                         0.028927                      0.028927    0.051391    0.510208    -0.505678     1.634006\n",
       "220              4                    1643       73.39              6                         1                      1                   0.017651        0.044668                  0.206897              29.0                 2.530690                                    0.034483                                 0.034483                0.003652                           0.000609                        0.000609       22.387246              0.081755                         0.013626                      0.013626   -0.463446    0.084954    -2.024617     0.533434\n",
       "221              6                     852       42.93              3                         9                      2                   0.034038        0.050387                  0.103448              29.0                 1.480345                                    0.310345                                 0.068966                0.003521                           0.010563                        0.002347       19.846261              0.069881                         0.209644                      0.046587   -0.246956    0.489775    -0.644835     2.516904\n",
       "222              1                     868       62.30              5                         6                      0                   0.010369        0.071774                  0.555556               9.0                 6.922222                                    0.666667                                 0.000000                0.005760                           0.006912                        0.000000       13.932584              0.080257                         0.096308                      0.000000   -0.390702   -0.486453    -2.012085    -1.829113\n",
       "223              1                    1186       91.64              4                        10                      0                   0.007589        0.077268                  0.444444               9.0                10.182222                                    1.111111                                 0.000000                0.003373                           0.008432                        0.000000       12.941947              0.043649                         0.109123                      0.000000   -0.352883   -0.481367    -1.802917    -1.667604\n",
       "224              3                     867       61.82              5                        10                      3                   0.012687        0.071303                  0.454545              11.0                 5.620000                                    0.909091                                 0.272727                0.005767                           0.011534                        0.003460       14.024588              0.080880                         0.161760                      0.048528    0.170813   -0.092140     0.380211    -0.502559\n",
       "225              5                    1489       93.17              3                         7                      1                   0.003358        0.062572                  0.600000               5.0                18.634000                                    1.400000                                 0.200000                0.002015                           0.004701                        0.000672       15.981539              0.032199                         0.075131                      0.010733   -0.133884    0.316928    -1.280134     1.250465\n",
       "226              1                     309       54.72              1                         9                      2                   0.067961        0.177087                  0.047619              21.0                 2.605714                                    0.428571                                 0.095238                0.003236                           0.029126                        0.006472        5.646930              0.018275                         0.164474                      0.036550   -0.163369   -0.502154    -0.640747    -2.371208\n",
       "227              3                    1709       12.78              4                         1                      3                   0.006437        0.007478                  0.363636              11.0                 1.161818                                    0.090909                                 0.272727                0.002341                           0.000585                        0.001755      133.724570              0.312989                         0.078247                      0.234742    0.075566   -0.103639     0.428342    -0.515022\n",
       "228              3                     161       39.01              8                         2                      0                   0.093168        0.242298                  0.533333              15.0                 2.600667                                    0.133333                                 0.000000                0.049689                           0.012422                        0.000000        4.127147              0.205076                         0.051269                      0.000000   -0.484544   -0.097448    -2.346547    -0.439370\n",
       "229              2                     208        2.23              1                        10                      2                   0.091346        0.010721                  0.052632              19.0                 0.117368                                    0.526316                                 0.105263                0.004808                           0.048077                        0.009615       93.273543              0.448430                         4.484305                      0.896861   -0.151180   -0.299799    -0.623690    -1.723073\n",
       "230              2                    1572       83.24              6                         3                      5                   0.013995        0.052952                  0.272727              22.0                 3.783636                                    0.136364                                 0.227273                0.003817                           0.001908                        0.003181       18.885151              0.072081                         0.036040                      0.060067    0.364162   -0.316287     2.119008    -1.840066\n",
       "231              6                    1299       54.84              5                         0                      1                   0.002309        0.042217                  1.666667               3.0                18.280000                                    0.000000                                 0.333333                0.003849                           0.000000                        0.000770       23.687090              0.091174                         0.000000                      0.018235   -0.161877    0.510681    -1.261282     1.653437\n",
       "232              1                    1701       94.49              3                         7                      1                   0.008818        0.055550                  0.200000              15.0                 6.299333                                    0.466667                                 0.066667                0.001764                           0.004115                        0.000588       18.001905              0.031749                         0.074082                      0.010583   -0.270420   -0.494255    -1.523670    -2.134762\n",
       "233              4                     410       11.13              7                         2                      2                   0.073171        0.027146                  0.233333              30.0                 0.371000                                    0.066667                                 0.066667                0.017073                           0.004878                        0.004878       36.837376              0.628931                         0.179695                      0.179695   -0.298526    0.081630    -0.602282     0.557966\n",
       "234              2                     859       55.80             11                        10                      4                   0.019790        0.064959                  0.647059              17.0                 3.282353                                    0.588235                                 0.235294                0.012806                           0.011641                        0.004657       15.394265              0.197133                         0.179211                      0.071685    0.308933   -0.299581     1.388300    -1.639994\n",
       "235              4                    1106       35.40              9                         3                      0                   0.011754        0.032007                  0.692308              13.0                 2.723077                                    0.230769                                 0.000000                0.008137                           0.002712                        0.000000       31.242938              0.254237                         0.084746                      0.000000   -0.450975    0.106126    -2.329400     0.458421\n",
       "236              6                    1073       64.26              0                         3                      1                   0.022367        0.059888                  0.000000              24.0                 2.677500                                    0.125000                                 0.041667                0.000000                           0.002796                        0.000932       16.697790              0.000000                         0.046685                      0.015562   -0.427074    0.490085    -1.721572     2.358052\n",
       "237              1                     489       67.27              0                         9                      0                   0.014315        0.137566                  0.000000               7.0                 9.610000                                    1.285714                                 0.000000                0.000000                           0.018405                        0.000000        7.269214              0.000000                         0.133789                      0.000000   -0.366393   -0.482060    -1.730990    -1.520159\n",
       "238              4                      22       60.15              8                         9                      3                   1.136364        2.734091                  0.320000              25.0                 2.406000                                    0.360000                                 0.120000                0.363636                           0.409091                        0.136364        0.365752              0.133001                         0.149626                      0.049875    0.011980    0.092557     0.359428     0.520287\n",
       "239              2                       4       58.11              7                         7                      4                   5.000000       14.527500                  0.350000              20.0                 2.905500                                    0.350000                                 0.200000                1.750000                           1.750000                        1.000000        0.068835              0.120461                         0.120461                      0.068835    0.232398   -0.307440     1.266045    -1.291368\n",
       "240              3                       7       28.58              3                         1                      0                   1.857143        4.082857                  0.230769              13.0                 2.198462                                    0.076923                                 0.000000                0.428571                           0.142857                        0.000000        0.244927              0.104969                         0.034990                      0.000000   -0.500124   -0.098199    -2.258512    -0.437479\n",
       "241              4                     208       77.53              2                         5                      2                   0.048077        0.372740                  0.200000              10.0                 7.753000                                    0.500000                                 0.200000                0.009615                           0.024038                        0.009615        2.682832              0.025796                         0.064491                      0.025796   -0.050841    0.104713    -0.561977     0.473652\n",
       "242              4                    1331       62.88              7                         9                      1                   0.003757        0.047243                  1.400000               5.0                12.576000                                    1.800000                                 0.200000                0.005259                           0.006762                        0.000751       21.167303              0.111323                         0.143130                      0.015903   -0.104432    0.120352    -1.396567     0.415872\n",
       "243              5                     619       32.10              3                         6                      2                   0.033926        0.051858                  0.142857              21.0                 1.528571                                    0.285714                                 0.095238                0.004847                           0.009693                        0.003231       19.283489              0.093458                         0.186916                      0.062305   -0.183667    0.294208    -0.576585     1.917542\n",
       "244              6                      45       51.88              8                         7                      2                   0.577778        1.152889                  0.307692              26.0                 1.995385                                    0.269231                                 0.076923                0.177778                           0.155556                        0.044444        0.867386              0.154202                         0.134927                      0.038551   -0.199700    0.491629    -0.620596     2.438824\n",
       "245              6                     291       87.69              7                         1                      5                   0.079038        0.301340                  0.304348              23.0                 3.812609                                    0.043478                                 0.217391                0.024055                           0.003436                        0.017182        3.318508              0.079827                         0.011404                      0.057019    0.345184    0.479855     1.864071     2.233412\n",
       "246              5                    1077       98.03              3                         2                      1                   0.013928        0.091021                  0.200000              15.0                 6.535333                                    0.133333                                 0.066667                0.002786                           0.001857                        0.000929       10.986433              0.030603                         0.020402                      0.010201   -0.307090    0.299263    -1.685744     1.584835\n",
       "247              6                    1216        1.76              6                         7                      2                   0.022204        0.001447                  0.222222              27.0                 0.065185                                    0.259259                                 0.074074                0.004934                           0.005757                        0.001645      690.909091              3.409091                         3.977273                      1.136364   -0.238180    0.489873    -0.638376     2.504639\n",
       "248              5                     877       12.59              5                         0                      5                   0.023945        0.014356                  0.238095              21.0                 0.599524                                    0.000000                                 0.238095                0.005701                           0.000000                        0.005701       69.658459              0.397141                         0.000000                      0.397141    0.324896    0.279568     2.101542     1.746174\n",
       "249              6                    1698       86.47              6                         5                      4                   0.006478        0.050925                  0.545455              11.0                 7.860909                                    0.454545                                 0.363636                0.003534                           0.002945                        0.002356       19.636868              0.069388                         0.057824                      0.046259    0.330386    0.499763     1.305311     2.059500\n",
       "...            ...                     ...         ...            ...                       ...                    ...                        ...             ...                       ...               ...                      ...                                         ...                                      ...                     ...                                ...                             ...             ...                   ...                              ...                           ...         ...         ...          ...          ...\n",
       "497871           3                     214       42.32              8                        10                      1                   0.102804        0.197757                  0.363636              22.0                 1.923636                                    0.454545                                 0.045455                0.037383                           0.046729                        0.004673        5.056711              0.189036                         0.236295                      0.023629   -0.318887   -0.097898    -1.847975    -0.463108\n",
       "497872           1                      40       63.03              4                         2                      2                   0.300000        1.575750                  0.333333              12.0                 5.252500                                    0.166667                                 0.166667                0.100000                           0.050000                        0.050000        0.634618              0.063462                         0.031731                      0.031731   -0.093333   -0.500276    -0.563991    -2.141233\n",
       "497873           4                     822        8.40              8                         9                      3                   0.035280        0.010219                  0.275862              29.0                 0.289655                                    0.310345                                 0.103448                0.009732                           0.010949                        0.003650       97.857143              0.952381                         1.071429                      0.357143   -0.050245    0.088211     0.382395     0.540968\n",
       "497874           3                     277       80.70             10                         4                      5                   0.064982        0.291336                  0.555556              18.0                 4.483333                                    0.222222                                 0.277778                0.036101                           0.014440                        0.018051        3.432466              0.123916                         0.049566                      0.061958    0.439175   -0.110510     2.066726    -0.575874\n",
       "497875           3                    1291       10.97              3                        10                      1                   0.019365        0.008497                  0.120000              25.0                 0.438800                                    0.400000                                 0.040000                0.002324                           0.007746                        0.000775      117.684594              0.273473                         0.911577                      0.091158   -0.388849   -0.102270    -1.958143    -0.480060\n",
       "497876           6                    1723       99.46              8                         5                      5                   0.012768        0.057725                  0.363636              22.0                 4.520909                                    0.227273                                 0.227273                0.004643                           0.002902                        0.002902       17.323547              0.080434                         0.050271                      0.050271    0.393984    0.486245     1.960557     2.257914\n",
       "497877           4                     889       43.05             10                         6                      4                   0.033746        0.048425                  0.333333              30.0                 1.435000                                    0.200000                                 0.133333                0.011249                           0.006749                        0.004499       20.650407              0.232288                         0.139373                      0.092915    0.120549    0.082003     1.558144     0.521085\n",
       "497878           3                    1657       26.99             10                        10                      5                   0.015691        0.016288                  0.384615              26.0                 1.038077                                    0.384615                                 0.192308                0.006035                           0.006035                        0.003018       61.393109              0.370508                         0.370508                      0.185254    0.371521   -0.112003     2.102036    -0.596868\n",
       "497879           6                     239       39.45              4                         2                      3                   0.071130        0.165063                  0.235294              17.0                 2.320588                                    0.117647                                 0.176471                0.016736                           0.008368                        0.012552        6.058302              0.101394                         0.050697                      0.076046    0.022993    0.491016     0.382550     2.412438\n",
       "497880           5                     988       22.33              2                         8                      4                   0.004049        0.022601                  0.500000               4.0                 5.582500                                    2.000000                                 1.000000                0.002024                           0.008097                        0.004049       44.245410              0.089566                         0.358262                      0.179131    0.429917    0.310101     1.256478     1.248799\n",
       "497881           5                     683       56.40             11                         3                      2                   0.038067        0.082577                  0.423077              26.0                 2.169231                                    0.115385                                 0.076923                0.016105                           0.004392                        0.002928       12.109929              0.195035                         0.053191                      0.035461   -0.212855    0.288189    -0.590204     1.968852\n",
       "497882           5                    1736       23.54              6                         3                      0                   0.001728        0.013560                  2.000000               3.0                 7.846667                                    1.000000                                 0.000000                0.003456                           0.001728                        0.000000       73.746814              0.254885                         0.127443                      0.000000   -0.338180    0.316965    -1.869084     1.200675\n",
       "497883           1                    1637       55.99              1                         0                      3                   0.015883        0.034203                  0.038462              26.0                 2.153462                                    0.000000                                 0.115385                0.000611                           0.000000                        0.001833       29.237364              0.017860                         0.000000                      0.053581   -0.102898   -0.519752     0.340993    -2.515509\n",
       "497884           6                    1456        4.47              7                         5                      0                   0.008242        0.003070                  0.583333              12.0                 0.372500                                    0.416667                                 0.000000                0.004808                           0.003434                        0.000000      325.727069              1.565996                         1.118568                      0.000000   -0.446601    0.508332    -2.032781     1.927589\n",
       "497885           1                     711       60.04              1                         2                      5                   0.016878        0.084444                  0.083333              12.0                 5.003333                                    0.166667                                 0.416667                0.001406                           0.002813                        0.007032       11.842105              0.016656                         0.033311                      0.083278    0.442906   -0.508993     1.862295    -1.996423\n",
       "497886           6                      65       89.17              4                         0                      0                   0.030769        1.371846                  2.000000               2.0                44.585000                                    0.000000                                 0.000000                0.061538                           0.000000                        0.000000        0.728945              0.044858                         0.000000                      0.000000   -0.312971    0.515902    -1.350984     1.242417\n",
       "497887           5                    1277        3.81              9                         6                      4                   0.010180        0.002984                  0.692308              13.0                 0.293077                                    0.461538                                 0.307692                0.007048                           0.004699                        0.003132      335.170604              2.362205                         1.574803                      1.049869    0.298286    0.298441     1.390683     1.576313\n",
       "497888           5                     690       26.73              6                         4                      1                   0.010145        0.038739                  0.857143               7.0                 3.818571                                    0.571429                                 0.142857                0.008696                           0.005797                        0.001449       25.813692              0.224467                         0.149645                      0.037411   -0.202973    0.309912    -1.421479     1.358110\n",
       "497889           3                    1221       71.01              3                         5                      0                   0.019656        0.058157                  0.125000              24.0                 2.958750                                    0.208333                                 0.000000                0.002457                           0.004095                        0.000000       17.194761              0.042248                         0.070413                      0.000000   -0.577679   -0.103624    -2.590283    -0.505800\n",
       "497890           1                    1810       53.38              5                         1                      2                   0.009392        0.029492                  0.294118              17.0                 3.140000                                    0.058824                                 0.117647                0.002762                           0.000552                        0.001105       33.907831              0.093668                         0.018734                      0.037467   -0.158978   -0.505922    -0.612788    -2.348500\n",
       "497891           3                    1710       39.55              5                         3                      0                   0.005848        0.023129                  0.500000              10.0                 3.955000                                    0.300000                                 0.000000                0.002924                           0.001754                        0.000000       43.236410              0.126422                         0.075853                      0.000000   -0.436691   -0.091548    -2.252739    -0.443313\n",
       "497892           3                    1181       50.24              4                         8                      3                   0.025402        0.042540                  0.133333              30.0                 1.674667                                    0.266667                                 0.100000                0.003387                           0.006774                        0.002540       23.507166              0.079618                         0.159236                      0.059713   -0.075250   -0.114245     0.359917    -0.577615\n",
       "497893           6                     768       51.22              0                         8                      1                   0.029948        0.066693                  0.000000              23.0                 2.226957                                    0.347826                                 0.043478                0.000000                           0.010417                        0.001302       14.994143              0.000000                         0.156189                      0.019524   -0.383327    0.496569    -1.712215     2.338566\n",
       "497894           4                    1695        3.12              0                         2                      1                   0.005900        0.001841                  0.000000              10.0                 0.312000                                    0.200000                                 0.100000                0.000000                           0.001180                        0.000590      543.269231              0.000000                         0.641026                      0.320513   -0.301135    0.102399    -1.564669     0.485726\n",
       "497895           6                     902       96.04              1                         9                      0                   0.011086        0.106475                  0.100000              10.0                 9.604000                                    0.900000                                 0.000000                0.001109                           0.009978                        0.000000        9.391920              0.010412                         0.093711                      0.000000   -0.389311    0.514706    -1.863031     1.772185\n",
       "497896           1                     902       16.64              3                         5                      2                   0.008869        0.018448                  0.375000               8.0                 2.080000                                    0.625000                                 0.250000                0.003326                           0.005543                        0.002217       54.206731              0.180288                         0.300481                      0.120192   -0.043057   -0.493192    -0.497411    -2.050189\n",
       "497897           3                     630        7.77             10                         2                      1                   0.009524        0.012333                  1.666667               6.0                 1.295000                                    0.333333                                 0.166667                0.015873                           0.003175                        0.001587       81.081081              1.287001                         0.257400                      0.128700   -0.189188   -0.090140    -1.430743    -0.415446\n",
       "497898           5                    1140       38.15              8                        10                      1                   0.007895        0.033465                  0.888889               9.0                 4.238889                                    1.111111                                 0.111111                0.007018                           0.008772                        0.000877       29.882045              0.209699                         0.262123                      0.026212   -0.165749    0.315800    -1.402647     1.371706\n",
       "497899           3                     492       29.87              0                         6                      1                   0.030488        0.060711                  0.000000              15.0                 1.991333                                    0.400000                                 0.066667                0.000000                           0.012195                        0.002033       16.471376              0.000000                         0.200870                      0.033478   -0.315887   -0.097712    -1.750163    -0.497995\n",
       "497900           1                    1078       15.34              2                         2                      4                   0.001855        0.014230                  1.000000               2.0                 7.670000                                    1.000000                                 2.000000                0.001855                           0.001855                        0.003711       70.273794              0.130378                         0.130378                      0.260756    0.447825   -0.493488     1.373389    -1.405849\n",
       "497901           6                     965       28.15              8                         6                      3                   0.010363        0.029171                  0.800000              10.0                 2.815000                                    0.600000                                 0.300000                0.008290                           0.006218                        0.003109       34.280639              0.284192                         0.213144                      0.106572    0.156169    0.504222     0.378608     2.115143\n",
       "497902           1                    1151       49.09              0                         8                      1                   0.025195        0.042650                  0.000000              29.0                 1.692759                                    0.275862                                 0.034483                0.000000                           0.006950                        0.000869       23.446730              0.000000                         0.162966                      0.020371   -0.449582   -0.508612    -1.650975    -2.349825\n",
       "497903           3                     542       69.85              4                         2                      4                   0.036900        0.128875                  0.200000              20.0                 3.492500                                    0.100000                                 0.200000                0.007380                           0.003690                        0.007380        7.759485              0.057266                         0.028633                      0.057266    0.182536   -0.113831     1.563627    -0.576309\n",
       "497904           2                     314       31.63              9                         9                      3                   0.022293        0.100732                  1.285714               7.0                 4.518571                                    1.285714                                 0.428571                0.028662                           0.028662                        0.009554        9.927284              0.284540                         0.284540                      0.094847    0.234081   -0.287762     0.338303    -1.328888\n",
       "497905           5                     149       26.28              0                         4                      1                   0.093960        0.176376                  0.000000              14.0                 1.877143                                    0.285714                                 0.071429                0.000000                           0.026846                        0.006711        5.669711              0.000000                         0.152207                      0.038052   -0.321065    0.300549    -1.616944     1.539526\n",
       "497906           5                     374       40.41              8                         2                      2                   0.069519        0.108048                  0.307692              26.0                 1.554231                                    0.076923                                 0.076923                0.021390                           0.005348                        0.005348        9.255135              0.197971                         0.049493                      0.049493   -0.240248    0.286005    -0.575605     1.961350\n",
       "497907           3                     765       53.54             11                         4                      5                   0.022222        0.069987                  0.647059              17.0                 3.149412                                    0.235294                                 0.294118                0.014379                           0.005229                        0.006536       14.288383              0.205454                         0.074710                      0.093388    0.446651   -0.109417     2.143117    -0.575129\n",
       "497908           2                    1338       38.12             10                         7                      1                   0.011211        0.028490                  0.666667              15.0                 2.541333                                    0.466667                                 0.066667                0.007474                           0.005232                        0.000747       35.099685              0.262329                         0.183631                      0.026233   -0.253148   -0.293300    -1.606304    -1.544726\n",
       "497909           6                    1050       76.05             10                         1                      4                   0.017143        0.072429                  0.555556              18.0                 4.225000                                    0.055556                                 0.222222                0.009524                           0.000952                        0.003810       13.806706              0.131492                         0.013149                      0.052597    0.230571    0.488465     1.348774     2.320577\n",
       "497910           1                     656        8.07              2                         7                      4                   0.033537        0.012302                  0.090909              22.0                 0.366818                                    0.318182                                 0.181818                0.003049                           0.010671                        0.006098       81.288724              0.247831                         0.867410                      0.495663    0.166070   -0.511053     1.370506    -2.410851\n",
       "497911           4                     693       45.34              9                         2                      5                   0.004329        0.065426                  3.000000               3.0                15.113333                                    0.666667                                 1.666667                0.012987                           0.002886                        0.007215       15.284517              0.198500                         0.044111                      0.110278    0.666552    0.105446     1.575251     0.342943\n",
       "497912           2                    1225       29.08              9                         0                      4                   0.017143        0.023739                  0.428571              21.0                 1.384762                                    0.000000                                 0.190476                0.007347                           0.000000                        0.003265       42.125172              0.309491                         0.000000                      0.137552    0.167730   -0.315987     1.417449    -1.883173\n",
       "497913           6                     976       89.50              1                         3                      5                   0.008197        0.091701                  0.125000               8.0                11.187500                                    0.375000                                 0.625000                0.001025                           0.003074                        0.005123       10.905028              0.011173                         0.033520                      0.055866    0.517734    0.496522     1.771479     1.781639\n",
       "497914           5                     451       36.47              6                         9                      5                   0.050998        0.080865                  0.260870              23.0                 1.585652                                    0.391304                                 0.217391                0.013304                           0.019956                        0.011086       12.366328              0.164519                         0.246778                      0.137099    0.382066    0.288344     2.117811     1.728118\n",
       "497915           1                     925       97.29              8                         9                      3                   0.020541        0.105178                  0.421053              19.0                 5.120526                                    0.473684                                 0.157895                0.008649                           0.009730                        0.003243        9.507658              0.082228                         0.092507                      0.030836    0.092716   -0.500268     0.336133    -2.353075\n",
       "497916           1                    1646       57.60              7                         1                      0                   0.015188        0.034994                  0.280000              25.0                 2.304000                                    0.040000                                 0.000000                0.004253                           0.000608                        0.000000       28.576389              0.121528                         0.017361                      0.000000   -0.602324   -0.507928    -2.269138    -2.158515\n",
       "497917           3                     566       46.97              2                         6                      0                   0.017668        0.082986                  0.200000              10.0                 4.697000                                    0.600000                                 0.000000                0.003534                           0.010601                        0.000000       12.050245              0.042580                         0.127741                      0.000000   -0.425786   -0.089053    -2.216361    -0.442334\n",
       "497918           1                    1002       11.63              6                         9                      2                   0.011976        0.011607                  0.500000              12.0                 0.969167                                    0.750000                                 0.166667                0.005988                           0.008982                        0.001996       86.156492              0.515907                         0.773861                      0.171969   -0.047894   -0.491920    -0.520650    -2.125407\n",
       "497919           2                     629       53.94              1                         2                      4                   0.033386        0.085755                  0.047619              21.0                 2.568571                                    0.095238                                 0.190476                0.001590                           0.003180                        0.006359       11.661105              0.018539                         0.037078                      0.074156    0.150477   -0.315663     1.427899    -1.941239\n",
       "497920           2                     611       44.62              0                         7                      5                   0.049100        0.073028                  0.000000              30.0                 1.487333                                    0.233333                                 0.166667                0.000000                           0.011457                        0.008183       13.693411              0.000000                         0.156880                      0.112057    0.261445   -0.321915     2.089285    -1.877363\n",
       "497921           4                     950       17.65              4                         2                      5                   0.002105        0.018579                  2.000000               2.0                 8.825000                                    1.000000                                 2.500000                0.004211                           0.002105                        0.005263       53.824363              0.226629                         0.113314                      0.283286    0.670363    0.105083     1.627837     0.256227\n",
       "497922           5                     101       57.19              4                         7                      1                   0.237624        0.566238                  0.166667              24.0                 2.382917                                    0.291667                                 0.041667                0.039604                           0.069307                        0.009901        1.766043              0.069942                         0.122399                      0.017486   -0.378796    0.295495    -1.843209     1.743744\n",
       "497923           2                    1699       21.95             11                        10                      4                   0.013537        0.012919                  0.478261              23.0                 0.954348                                    0.434783                                 0.173913                0.006474                           0.005886                        0.002354       77.403189              0.501139                         0.455581                      0.182232    0.226269   -0.305935     1.415527    -1.808570\n",
       "497924           1                    1076       55.47              5                         4                      5                   0.000929        0.051552                  5.000000               1.0                55.470000                                    4.000000                                 5.000000                0.004647                           0.003717                        0.004647       19.397873              0.090139                         0.072111                      0.090139    0.919136   -0.480284     3.563014     0.086820\n",
       "497925           6                     232        7.20              1                         8                      5                   0.012931        0.031034                  0.333333               3.0                 2.400000                                    2.666667                                 1.666667                0.004310                           0.034483                        0.021552       32.222222              0.138889                         1.111111                      0.694444    0.644360    0.508524     1.554195     1.392249\n",
       "497926           5                     453       96.32              9                         2                      0                   0.017660        0.212627                  1.125000               8.0                12.040000                                    0.250000                                 0.000000                0.019868                           0.004415                        0.000000        4.703073              0.093439                         0.020764                      0.000000   -0.373520    0.311056    -1.889049     1.237284\n",
       "497927           5                     972       45.90              4                        10                      4                   0.028807        0.047222                  0.142857              28.0                 1.639286                                    0.357143                                 0.142857                0.004115                           0.010288                        0.004115       21.176471              0.087146                         0.217865                      0.087146    0.142679    0.286934     1.460048     2.007905\n",
       "497928           4                    1750       76.78              6                         1                      1                   0.000571        0.043874                  6.000000               1.0                76.780000                                    1.000000                                 1.000000                0.003429                           0.000571                        0.000571       22.792394              0.078145                         0.013024                      0.013024    0.008865    0.123184     3.183962     0.101032\n",
       "497929           1                    1111        5.52              2                         8                      1                   0.027003        0.004968                  0.066667              30.0                 0.184000                                    0.266667                                 0.033333                0.001800                           0.007201                        0.000900      201.268116              0.362319                         1.449275                      0.181159   -0.464504   -0.509576    -1.759948    -2.285219\n",
       "497930           5                      13       41.17              4                         1                      0                   1.076923        3.166923                  0.285714              14.0                 2.940714                                    0.071429                                 0.000000                0.307692                           0.076923                        0.000000        0.315764              0.097158                         0.024290                      0.000000   -0.501541    0.301000    -2.266945     1.432820\n",
       "497931           4                     955       94.26             11                         2                      0                   0.009424        0.098702                  1.222222               9.0                10.473333                                    0.222222                                 0.000000                0.011518                           0.002094                        0.000000       10.131551              0.116698                         0.021218                      0.000000   -0.376634    0.110731    -2.025526     0.445616\n",
       "497932           1                     990       66.95              7                        10                      4                   0.021212        0.067626                  0.333333              21.0                 3.188095                                    0.476190                                 0.190476                0.007071                           0.010101                        0.004040       14.787155              0.104556                         0.149365                      0.059746    0.244647   -0.504483     1.356587    -2.414823\n",
       "497933           1                    1549       24.13              1                         0                      3                   0.001937        0.015578                  0.333333               3.0                 8.043333                                    0.000000                                 1.000000                0.000646                           0.000000                        0.001937       64.193949              0.041442                         0.000000                      0.124327    0.180683   -0.496246     0.476644    -1.811791\n",
       "497934           2                    1660       67.63             10                         1                      5                   0.004217        0.040741                  1.428571               7.0                 9.661429                                    0.142857                                 0.714286                0.006024                           0.000602                        0.003012       24.545320              0.147863                         0.014786                      0.073932    0.561913   -0.301452     1.735964    -1.260534\n",
       "497935           1                    1337       87.25             10                        10                      3                   0.010471        0.065258                  0.714286              14.0                 6.232143                                    0.714286                                 0.214286                0.007479                           0.007479                        0.002244       15.323782              0.114613                         0.114613                      0.034384    0.167772   -0.493282     0.349151    -2.056843\n",
       "497936           2                    1310       46.16              7                         3                      1                   0.003053        0.035237                  1.750000               4.0                11.540000                                    0.750000                                 0.250000                0.005344                           0.002290                        0.000763       28.379549              0.151646                         0.064991                      0.021664   -0.146173   -0.285910    -1.329197    -1.262713\n",
       "497937           6                     473       42.82              6                         1                      4                   0.019027        0.090529                  0.666667               9.0                 4.757778                                    0.111111                                 0.444444                0.012685                           0.002114                        0.008457       11.046240              0.140121                         0.023354                      0.093414    0.310647    0.496200     1.274024     2.005695\n",
       "497938           5                     944       21.03              4                         6                      2                   0.029661        0.022278                  0.142857              28.0                 0.751071                                    0.214286                                 0.071429                0.004237                           0.006356                        0.002119       44.888255              0.190204                         0.285307                      0.095102   -0.259895    0.287508    -0.609184     2.023527\n",
       "497939           6                     595       83.51              2                         3                      2                   0.010084        0.140353                  0.333333               6.0                13.918333                                    0.500000                                 0.333333                0.003361                           0.005042                        0.003361        7.124895              0.023949                         0.035924                      0.023949   -0.007990    0.506929    -0.482164     1.881562\n",
       "497940           2                    1620       98.43              2                         3                      5                   0.012346        0.060759                  0.100000              20.0                 4.921500                                    0.150000                                 0.250000                0.001235                           0.001852                        0.003086       16.458397              0.020319                         0.030479                      0.050798    0.371833   -0.315134     2.034848    -1.807346\n",
       "497941           3                     310       49.27              4                        10                      2                   0.019355        0.158935                  0.666667               6.0                 8.211667                                    1.666667                                 0.333333                0.012903                           0.032258                        0.006452        6.291861              0.081185                         0.202963                      0.040593    0.049810   -0.084002    -0.522279    -0.458336\n",
       "497942           1                    1390       60.46              4                         6                      0                   0.010791        0.043496                  0.266667              15.0                 4.030667                                    0.400000                                 0.000000                0.002878                           0.004317                        0.000000       22.990407              0.066159                         0.099239                      0.000000   -0.468515   -0.493005    -2.171754    -2.018435\n",
       "497943           3                     109       37.82             11                         9                      4                   0.238532        0.346972                  0.423077              26.0                 1.454615                                    0.346154                                 0.153846                0.100917                           0.082569                        0.036697        2.882073              0.290851                         0.237969                      0.105764    0.191321   -0.110475     1.533154    -0.592224\n",
       "497944           3                    1726       93.51              4                        10                      0                   0.003476        0.054177                  0.666667               6.0                15.585000                                    1.666667                                 0.000000                0.002317                           0.005794                        0.000000       18.457919              0.042776                         0.106940                      0.000000   -0.308373   -0.077549    -1.825610    -0.478387\n",
       "497945           4                     717       51.97              0                         8                      4                   0.009763        0.072483                  0.000000               7.0                 7.424286                                    1.142857                                 0.571429                0.000000                           0.011158                        0.005579       13.796421              0.000000                         0.153935                      0.076967    0.367547    0.105673     1.383200     0.455537\n",
       "497946           1                    1385       90.09              8                         1                      0                   0.009386        0.065047                  0.615385              13.0                 6.930000                                    0.076923                                 0.000000                0.005776                           0.000722                        0.000000       15.373515              0.088800                         0.011100                      0.000000   -0.451828   -0.495266    -2.047025    -1.886567\n",
       "497947           2                     614       37.52              9                         9                      2                   0.048860        0.061107                  0.300000              30.0                 1.250667                                    0.300000                                 0.066667                0.014658                           0.014658                        0.003257       16.364606              0.239872                         0.239872                      0.053305   -0.229100   -0.309281    -0.634213    -1.971938\n",
       "497948           1                     379        6.76              8                         5                      2                   0.034301        0.017836                  0.615385              13.0                 0.520000                                    0.384615                                 0.153846                0.021108                           0.013193                        0.005277       56.065089              1.183432                         0.739645                      0.295858   -0.081448   -0.497355    -0.558030    -2.194152\n",
       "497949           6                      33       72.47              8                         6                      3                   0.151515        2.196061                  1.600000               5.0                14.494000                                    1.200000                                 0.600000                0.242424                           0.181818                        0.090909        0.455361              0.110391                         0.082793                      0.041396    0.260391    0.511392     0.384186     1.738853\n",
       "497950           3                    1709       64.66              7                         8                      0                   0.009362        0.037835                  0.437500              16.0                 4.041250                                    0.500000                                 0.000000                0.004096                           0.004681                        0.000000       26.430560              0.108259                         0.123724                      0.000000   -0.448012   -0.090996    -2.335179    -0.467485\n",
       "497951           1                    1411       36.17              8                        10                      4                   0.010631        0.025634                  0.533333              15.0                 2.411333                                    0.666667                                 0.266667                0.005670                           0.007087                        0.002835       39.010229              0.221178                         0.276472                      0.110589    0.310510   -0.498269     1.306725    -2.195733\n",
       "497952           1                    1323       38.93             10                         8                      3                   0.009826        0.029426                  0.769231              13.0                 2.994615                                    0.615385                                 0.230769                0.007559                           0.006047                        0.002268       33.984074              0.256871                         0.205497                      0.077061    0.147743   -0.495217     0.356603    -2.157650\n",
       "497953           4                     505       61.61              8                         4                      5                   0.043564        0.122000                  0.363636              22.0                 2.800455                                    0.181818                                 0.227273                0.015842                           0.007921                        0.009901        8.196721              0.129849                         0.064925                      0.081156    0.375232    0.084578     2.143034     0.525180\n",
       "497954           6                    1792       98.21              8                         2                      3                   0.009487        0.054805                  0.470588              17.0                 5.777059                                    0.117647                                 0.176471                0.004464                           0.001116                        0.001674       18.246614              0.081458                         0.020365                      0.030547    0.062847    0.493280     0.401105     2.330642\n",
       "497955           5                    1603       33.78              2                        10                      3                   0.001871        0.021073                  0.666667               3.0                11.260000                                    3.333333                                 1.000000                0.001248                           0.006238                        0.001871       47.454115              0.059207                         0.296033                      0.088810    0.296223    0.317961     0.387246     1.120883\n",
       "497956           2                     927        5.17              2                         3                      0                   0.006472        0.005577                  0.333333               6.0                 0.861667                                    0.500000                                 0.000000                0.002157                           0.003236                        0.000000      179.303675              0.386847                         0.580271                      0.000000   -0.419540   -0.288909    -2.062783    -1.302834\n",
       "497957           5                    1009       63.24             11                         7                      4                   0.010902        0.062676                  1.000000              11.0                 5.749091                                    0.636364                                 0.363636                0.010902                           0.006938                        0.003964       15.955092              0.173941                         0.110689                      0.063251    0.365522    0.303200     1.334533     1.481709\n",
       "497958           2                     257       90.49              0                         8                      0                   0.058366        0.352101                  0.000000              15.0                 6.032667                                    0.533333                                 0.000000                0.000000                           0.031128                        0.000000        2.840093              0.000000                         0.088408                      0.000000   -0.462979   -0.291816    -2.142370    -1.380672\n",
       "497959           3                     105       53.68              5                         1                      4                   0.266667        0.511238                  0.178571              28.0                 1.917143                                    0.035714                                 0.142857                0.047619                           0.009524                        0.038095        1.956036              0.093145                         0.018629                      0.074516    0.085535   -0.122962     1.508674    -0.579206\n",
       "497960           2                    1553        6.14             10                         2                      4                   0.017386        0.003954                  0.370370              27.0                 0.227407                                    0.074074                                 0.148148                0.006439                           0.001288                        0.002576      252.931596              1.628664                         0.325733                      0.651466    0.112413   -0.319560     1.427250    -1.957337\n",
       "497961           1                     157       71.89              5                         3                      0                   0.133758        0.457898                  0.238095              21.0                 3.423333                                    0.142857                                 0.000000                0.031847                           0.019108                        0.000000        2.183892              0.069551                         0.041730                      0.000000   -0.547886   -0.502436    -2.263123    -2.100250\n",
       "497962           4                     380       17.38              6                         6                      2                   0.044737        0.045737                  0.352941              17.0                 1.022353                                    0.352941                                 0.117647                0.015789                           0.015789                        0.005263       21.864212              0.345224                         0.345224                      0.115075   -0.127697    0.098907    -0.579296     0.504211\n",
       "497963           2                    1203       82.83              1                         1                      5                   0.004988        0.068853                  0.166667               6.0                13.805000                                    0.166667                                 0.833333                0.000831                           0.000831                        0.004156       14.523723              0.012073                         0.012073                      0.060365    0.533351   -0.302982     1.737878    -1.260504\n",
       "497964           3                    1598       31.89              5                         7                      1                   0.017522        0.019956                  0.178571              28.0                 1.138929                                    0.250000                                 0.035714                0.003129                           0.004380                        0.000626       50.109752              0.156789                         0.219505                      0.031358   -0.427080   -0.107822    -2.040401    -0.491006\n",
       "497965           3                     858       26.06              5                         0                      5                   0.005828        0.030373                  1.000000               5.0                 5.212000                                    0.000000                                 1.000000                0.005828                           0.000000                        0.005828       32.924021              0.191865                         0.000000                      0.191865    0.544451   -0.102936     1.894366    -0.498546\n",
       "497966           6                     449       47.71              4                         8                      0                   0.004454        0.106258                  2.000000               2.0                23.855000                                    4.000000                                 0.000000                0.008909                           0.017817                        0.000000        9.411025              0.083840                         0.167680                      0.000000   -0.249344    0.526457    -1.385854     1.224655\n",
       "497967           4                     848       61.01              6                         7                      4                   0.003538        0.071946                  2.000000               3.0                20.336667                                    2.333333                                 1.333333                0.007075                           0.008255                        0.004717       13.899361              0.098345                         0.114735                      0.065563    0.508678    0.114009     1.209460     0.461805\n",
       "497968           3                    1191       69.48              4                         6                      0                   0.001679        0.058338                  2.000000               2.0                34.740000                                    3.000000                                 0.000000                0.003359                           0.005038                        0.000000       17.141623              0.057571                         0.086356                      0.000000   -0.257001   -0.075054    -1.558675    -0.304257\n",
       "497969           6                     583       70.73              8                         1                      0                   0.013722        0.121321                  1.000000               8.0                 8.841250                                    0.125000                                 0.000000                0.013722                           0.001715                        0.000000        8.242613              0.113106                         0.014138                      0.000000   -0.398151    0.508976    -1.887037     1.748286\n",
       "497970           6                     229       64.02              7                         3                      0                   0.074236        0.279563                  0.411765              17.0                 3.765882                                    0.176471                                 0.000000                0.030568                           0.013100                        0.000000        3.577007              0.109341                         0.046860                      0.000000   -0.496375    0.501370    -2.147514     2.030763\n",
       "497971           6                     411       62.33             11                         2                      1                   0.065693        0.151655                  0.407407              27.0                 2.308519                                    0.074074                                 0.037037                0.026764                           0.004866                        0.002433        6.593936              0.176480                         0.032087                      0.016044   -0.411929    0.488627    -1.692066     2.316603\n",
       "497972           1                     685       29.13              1                         9                      5                   0.030657        0.042526                  0.047619              21.0                 1.387143                                    0.428571                                 0.238095                0.001460                           0.013139                        0.007299       23.515276              0.034329                         0.308960                      0.171644    0.377302   -0.510459     1.913296    -2.275492\n",
       "497973           1                    1646       13.33             11                        10                      1                   0.004860        0.008098                  1.375000               8.0                 1.666250                                    1.250000                                 0.125000                0.006683                           0.006075                        0.000608      123.480870              0.825206                         0.750188                      0.075019   -0.144314   -0.481809    -1.318567    -1.700728\n",
       "497974           3                    1549       23.01              0                         5                      1                   0.012266        0.014855                  0.000000              19.0                 1.211053                                    0.263158                                 0.052632                0.000000                           0.003228                        0.000646       67.318557              0.000000                         0.217297                      0.043459   -0.371256   -0.102658    -1.896519    -0.482931\n",
       "497975           6                    1331       35.55              8                         4                      2                   0.015778        0.026709                  0.380952              21.0                 1.692857                                    0.190476                                 0.095238                0.006011                           0.003005                        0.001503       37.440225              0.225035                         0.112518                      0.056259   -0.172472    0.493332    -0.591919     2.473187\n",
       "497976           4                      20       60.86              3                         9                      1                   0.550000        3.043000                  0.272727              11.0                 5.532727                                    0.818182                                 0.090909                0.150000                           0.450000                        0.050000        0.328623              0.049293                         0.147880                      0.016431   -0.217885    0.111022    -1.527506     0.472024\n",
       "497977           3                    1584       76.14              6                         0                      1                   0.011995        0.048068                  0.315789              19.0                 4.007368                                    0.000000                                 0.052632                0.003788                           0.000000                        0.000631       20.803783              0.078802                         0.000000                      0.013134   -0.359569   -0.106157    -1.883287    -0.455039\n",
       "497978           6                    1508       46.81              3                         7                      0                   0.001326        0.031041                  1.500000               2.0                23.405000                                    3.500000                                 0.000000                0.001989                           0.004642                        0.000000       32.215339              0.064089                         0.149541                      0.000000   -0.271931    0.524596    -1.409193     1.295561\n",
       "497979           6                     906       55.48              9                         2                      5                   0.025386        0.061236                  0.391304              23.0                 2.412174                                    0.086957                                 0.217391                0.009934                           0.002208                        0.005519       16.330209              0.162221                         0.036049                      0.090123    0.351504    0.481314     1.957192     2.290627\n",
       "497980           6                    1125       66.48              7                         0                      4                   0.001778        0.059093                  3.500000               2.0                33.240000                                    0.000000                                 2.000000                0.006222                           0.000000                        0.003556       16.922383              0.105295                         0.000000                      0.060168    0.507549    0.507919     1.079857     1.089849\n",
       "497981           2                    1561       86.14             10                         4                      2                   0.017937        0.055183                  0.357143              28.0                 3.076429                                    0.142857                                 0.071429                0.006406                           0.002562                        0.001281       18.121662              0.116090                         0.046436                      0.023218   -0.223493   -0.311944    -0.612456    -1.974560\n",
       "497982           6                    1562       48.35              0                        10                      0                   0.011524        0.030954                  0.000000              18.0                 2.686111                                    0.555556                                 0.000000                0.000000                           0.006402                        0.000000       32.306101              0.000000                         0.206825                      0.000000   -0.497936    0.506739    -2.083967     2.013481\n",
       "497983           5                    1070       92.80             11                         1                      4                   0.021495        0.086729                  0.478261              23.0                 4.034783                                    0.043478                                 0.173913                0.010280                           0.000935                        0.003738       11.530172              0.118534                         0.010776                      0.043103    0.183623    0.284008     1.379803     1.823357\n",
       "497984           4                    1128       39.32              4                         1                      3                   0.021277        0.034858                  0.166667              24.0                 1.638333                                    0.041667                                 0.125000                0.003546                           0.000887                        0.002660       28.687691              0.101729                         0.025432                      0.076297   -0.063716    0.083412     0.352569     0.543517\n",
       "497985           6                     172       61.32              5                         2                      1                   0.098837        0.356512                  0.294118              17.0                 3.607059                                    0.117647                                 0.058824                0.029070                           0.011628                        0.005814        2.804958              0.081539                         0.032616                      0.016308   -0.331533    0.496927    -1.555691     2.213828\n",
       "497986           6                     369       82.69              7                         2                      2                   0.032520        0.224092                  0.583333              12.0                 6.890833                                    0.166667                                 0.166667                0.018970                           0.005420                        0.005420        4.462450              0.084654                         0.024187                      0.024187   -0.070648    0.500310    -0.527367     2.119238\n",
       "497987           5                     161       88.57              7                         5                      0                   0.111801        0.550124                  0.388889              18.0                 4.920556                                    0.277778                                 0.000000                0.043478                           0.031056                        0.000000        1.817771              0.079034                         0.056453                      0.000000   -0.484007    0.303100    -2.262990     1.453666\n",
       "497988           3                      12       50.17              2                        10                      5                   1.416667        4.180833                  0.117647              17.0                 2.951176                                    0.588235                                 0.294118                0.166667                           0.833333                        0.416667        0.239187              0.039864                         0.199322                      0.099661    0.444640   -0.105156     1.934557    -0.590354\n",
       "497989           1                      13       87.02              6                         8                      1                   1.230769        6.693846                  0.375000              16.0                 5.438750                                    0.500000                                 0.062500                0.461538                           0.615385                        0.076923        0.149391              0.068950                         0.091933                      0.011492   -0.259910   -0.493856    -1.526429    -2.059335\n",
       "497990           5                    1312       29.28              6                         0                      5                   0.013720        0.022317                  0.333333              18.0                 1.626667                                    0.000000                                 0.277778                0.004573                           0.000000                        0.003811       44.808743              0.204918                         0.000000                      0.170765    0.369721    0.283148     2.120246     1.692070\n",
       "497991           6                    1768       45.98              6                         4                      3                   0.002828        0.026007                  1.200000               5.0                 9.196000                                    0.800000                                 0.600000                0.003394                           0.002262                        0.001697       38.451501              0.130492                         0.086994                      0.065246    0.215417    0.507956     0.365629     1.862777\n",
       "497992           6                    1331       86.17              3                         1                      0                   0.015026        0.064741                  0.150000              20.0                 4.308500                                    0.050000                                 0.000000                0.002254                           0.000751                        0.000000       15.446211              0.034815                         0.011605                      0.000000   -0.558711    0.495557    -2.205069     2.095035\n",
       "497993           6                      64       75.16              3                         6                      3                   0.437500        1.174375                  0.107143              28.0                 2.684286                                    0.214286                                 0.107143                0.046875                           0.093750                        0.046875        0.851517              0.039915                         0.079830                      0.039915   -0.064012    0.484775     0.356794     2.512816\n",
       "497994           2                     217       30.89              3                         3                      1                   0.027650        0.142350                  0.500000               6.0                 5.148333                                    0.500000                                 0.166667                0.013825                           0.013825                        0.004608        7.024927              0.097119                         0.097119                      0.032373   -0.212926   -0.290807    -1.399179    -1.320059\n",
       "497995           3                    1123       98.64              9                         9                      1                   0.013357        0.087836                  0.600000              15.0                 6.576000                                    0.600000                                 0.066667                0.008014                           0.008014                        0.000890       11.384834              0.091241                         0.091241                      0.010138   -0.221773   -0.090620    -1.666022    -0.446184\n",
       "497996           2                     404       39.61             10                         7                      4                   0.074257        0.098045                  0.333333              30.0                 1.320333                                    0.233333                                 0.133333                0.024752                           0.017327                        0.009901       10.199445              0.252461                         0.176723                      0.100985    0.127347   -0.316765     1.455288    -1.925460\n",
       "497997           6                    1724       61.73              6                         1                      3                   0.008701        0.035806                  0.400000              15.0                 4.115333                                    0.066667                                 0.200000                0.003480                           0.000580                        0.001740       27.928074              0.097197                         0.016200                      0.048599    0.055712    0.493124     0.396777     2.330573\n",
       "497998           4                    1072       48.65              2                         8                      5                   0.018657        0.045382                  0.100000              20.0                 2.432500                                    0.400000                                 0.250000                0.001866                           0.007463                        0.004664       22.034943              0.041110                         0.164440                      0.102775    0.392459    0.089610     2.167140     0.520323\n",
       "497999           4                    1146       83.19             10                         1                      3                   0.020942        0.072592                  0.416667              24.0                 3.466250                                    0.041667                                 0.125000                0.008726                           0.000873                        0.002618       13.775694              0.120207                         0.012021                      0.036062   -0.018873    0.085490     0.346169     0.530277\n",
       "498000           4                     618        3.98              2                         1                      5                   0.012945        0.006440                  0.250000               8.0                 0.497500                                    0.125000                                 0.625000                0.003236                           0.001618                        0.008091      155.276382              0.502513                         0.251256                      1.256281    0.472828    0.093066     1.900436     0.471335\n",
       "498001           4                    1120       58.73             10                         7                      5                   0.018750        0.052437                  0.476190              21.0                 2.796667                                    0.333333                                 0.238095                0.008929                           0.006250                        0.004464       19.070322              0.170271                         0.119190                      0.085135    0.417751    0.089700     2.159581     0.548878\n",
       "498002           2                     878       82.84              4                         0                      1                   0.012528        0.094351                  0.363636              11.0                 7.530909                                    0.000000                                 0.090909                0.004556                           0.000000                        0.001139       10.598745              0.048286                         0.000000                      0.012071   -0.275274   -0.298547    -1.514400    -1.433863\n",
       "498003           2                     881       35.51              5                         5                      1                   0.030647        0.040306                  0.185185              27.0                 1.315185                                    0.185185                                 0.037037                0.005675                           0.005675                        0.001135       24.809913              0.140805                         0.140805                      0.028161   -0.428842   -0.309141    -1.906032    -1.846047\n",
       "498004           1                     892       95.82             10                         6                      3                   0.023543        0.107422                  0.476190              21.0                 4.562857                                    0.285714                                 0.142857                0.011211                           0.006726                        0.003363        9.309121              0.104362                         0.062617                      0.031309    0.056918   -0.505272     0.323255    -2.369564\n",
       "498005           4                    1154        9.15             11                         9                      1                   0.018198        0.007929                  0.523810              21.0                 0.435714                                    0.428571                                 0.047619                0.009532                           0.007799                        0.000867      126.120219              1.202186                         0.983607                      0.109290   -0.311736    0.102476    -1.858715     0.462043\n",
       "498006           2                     500       50.93              7                         5                      2                   0.032000        0.101860                  0.437500              16.0                 3.183125                                    0.312500                                 0.125000                0.014000                           0.010000                        0.004000        9.817396              0.137444                         0.098174                      0.039270   -0.106676   -0.300231    -0.552954    -1.736231\n",
       "498007           6                     438       23.23              4                         9                      2                   0.025114        0.053037                  0.363636              11.0                 2.111818                                    0.818182                                 0.181818                0.009132                           0.020548                        0.004566       18.854929              0.172191                         0.387430                      0.086096   -0.042063    0.507938    -0.523065     2.124035\n",
       "498008           5                     580       10.63              8                         3                      3                   0.048276        0.018328                  0.285714              28.0                 0.379643                                    0.107143                                 0.107143                0.013793                           0.005172                        0.005172       54.562559              0.752587                         0.282220                      0.282220   -0.082119    0.282185     0.356861     1.975282\n",
       "498009           5                    1607       71.42              8                         6                      3                   0.001867        0.044443                  2.666667               3.0                23.806667                                    2.000000                                 1.000000                0.004978                           0.003734                        0.001867       22.500700              0.112013                         0.084010                      0.042005    0.323571    0.316354     0.399155     1.137945\n",
       "498010           5                     182       56.29              7                        10                      4                   0.043956        0.309286                  0.875000               8.0                 7.036250                                    1.250000                                 0.500000                0.038462                           0.054945                        0.021978        3.233256              0.124356                         0.177651                      0.071061    0.410503    0.308894     1.298575     1.347872\n",
       "498011           2                     873       56.69              5                         5                      0                   0.028637        0.064937                  0.200000              25.0                 2.267600                                    0.200000                                 0.000000                0.005727                           0.005727                        0.000000       15.399541              0.088199                         0.088199                      0.000000   -0.582822   -0.304221    -2.523409    -1.617010\n",
       "498012           5                    1398       27.71             10                         5                      2                   0.018598        0.019821                  0.384615              26.0                 1.065769                                    0.192308                                 0.076923                0.007153                           0.003577                        0.001431       50.451101              0.360881                         0.180440                      0.072176   -0.213236    0.290083    -0.611810     1.999489\n",
       "498013           5                    1014       63.96              2                         6                      2                   0.007890        0.063077                  0.250000               8.0                 7.995000                                    0.750000                                 0.250000                0.001972                           0.005917                        0.001972       15.853659              0.031270                         0.093809                      0.031270   -0.021911    0.308046    -0.489122     1.506339\n",
       "498014           5                     775       40.67              1                        10                      4                   0.016774        0.052477                  0.076923              13.0                 3.128462                                    0.769231                                 0.307692                0.001290                           0.012903                        0.005161       19.055815              0.024588                         0.245881                      0.098353    0.299922    0.301256     1.405477     1.577039\n",
       "498015           4                    1286       58.18              0                         3                      1                   0.006998        0.045241                  0.000000               9.0                 6.464444                                    0.333333                                 0.111111                0.000000                           0.002333                        0.000778       22.103816              0.000000                         0.051564                      0.017188   -0.259565    0.105410    -1.556376     0.435602\n",
       "498016           1                     603       71.47             11                         2                      3                   0.003317        0.118524                  5.500000               2.0                35.735000                                    1.000000                                 1.500000                0.018242                           0.003317                        0.004975        8.437106              0.153911                         0.027984                      0.041976    0.369656   -0.483745     0.332684    -0.963786\n",
       "498017           2                     209       91.97              9                         3                      0                   0.086124        0.440048                  0.500000              18.0                 5.109444                                    0.166667                                 0.000000                0.043062                           0.014354                        0.000000        2.272480              0.097858                         0.032619                      0.000000   -0.487168   -0.298225    -2.261619    -1.476540\n",
       "498018           5                    1001       74.24              7                         6                      0                   0.008991        0.074166                  0.777778               9.0                 8.248889                                    0.666667                                 0.000000                0.006993                           0.005994                        0.000000       13.483297              0.094289                         0.080819                      0.000000   -0.375193    0.313870    -2.112742     1.316592\n",
       "498019           5                    1324       28.98              2                         3                      4                   0.000755        0.021888                  2.000000               1.0                28.980000                                    3.000000                                 4.000000                0.001511                           0.002266                        0.003021       45.686680              0.069013                         0.103520                      0.138026    0.604215    0.314179     3.388991     0.134462\n",
       "498020           5                     951       10.74              9                         2                      5                   0.005258        0.011293                  1.800000               5.0                 2.148000                                    0.400000                                 1.000000                0.009464                           0.002103                        0.005258       88.547486              0.837989                         0.186220                      0.465549    0.580511    0.300506     1.736223     1.153623\n",
       "498021           4                       4       41.16              6                         7                      4                   6.250000       10.290000                  0.240000              25.0                 1.646400                                    0.280000                                 0.160000                1.500000                           1.750000                        1.000000        0.097182              0.145773                         0.170068                      0.097182    0.164367    0.086800     1.208001     0.006072\n",
       "498022           6                    1341       86.28              6                        10                      4                   0.011931        0.064340                  0.375000              16.0                 5.392500                                    0.625000                                 0.250000                0.004474                           0.007457                        0.002983       15.542420              0.069541                         0.115902                      0.046361    0.304757    0.500109     1.370524     2.253173\n",
       "498023           2                      14       56.76              5                         1                      0                   0.357143        4.054286                  1.000000               5.0                11.352000                                    0.200000                                 0.000000                0.357143                           0.071429                        0.000000        0.246653              0.088090                         0.017618                      0.000000   -0.380183   -0.288317    -1.854480    -1.200103\n",
       "498024           4                     588       40.94              1                         4                      5                   0.030612        0.069626                  0.055556              18.0                 2.274444                                    0.222222                                 0.277778                0.001701                           0.006803                        0.008503       14.362482              0.024426                         0.097704                      0.122130    0.378489    0.086512     2.127613     0.537959\n",
       "498025           3                    1580        5.96              0                         4                      5                   0.010759        0.003772                  0.000000              17.0                 0.350588                                    0.235294                                 0.294118                0.000000                           0.002532                        0.003165      265.100671              0.000000                         0.671141                      0.838926    0.372136   -0.112801     2.034317    -0.552250\n",
       "498026           1                    1814       12.55              4                         9                      4                   0.006064        0.006918                  0.363636              11.0                 1.140909                                    0.818182                                 0.363636                0.002205                           0.004961                        0.002205      144.541833              0.318725                         0.717131                      0.318725    0.323437   -0.496505     1.259555    -2.058402\n",
       "498027           1                     847       54.17              0                        10                      2                   0.028335        0.063955                  0.000000              24.0                 2.257083                                    0.416667                                 0.083333                0.000000                           0.011806                        0.002361       15.635961              0.000000                         0.184604                      0.036921   -0.195251   -0.504122    -0.658638    -2.439608\n",
       "498028           2                     856       42.39              0                         5                      5                   0.016355        0.049521                  0.000000              14.0                 3.027857                                    0.357143                                 0.357143                0.000000                           0.005841                        0.005841       20.193442              0.000000                         0.117952                      0.117952    0.428977   -0.308174     2.082868    -1.612717\n",
       "498029           3                    1380       92.66              4                         3                      5                   0.013768        0.067145                  0.210526              19.0                 4.876842                                    0.157895                                 0.263158                0.002899                           0.002174                        0.003623       14.893158              0.043169                         0.032376                      0.053961    0.391932   -0.113856     2.113660    -0.578362\n",
       "498030           2                     560       51.69             10                         5                      1                   0.030357        0.092304                  0.588235              17.0                 3.040588                                    0.294118                                 0.058824                0.017857                           0.008929                        0.001786       10.833817              0.193461                         0.096731                      0.019346   -0.286111   -0.297724    -1.692565    -1.584580\n",
       "498031           3                     865        1.37             11                         7                      2                   0.017341        0.001584                  0.733333              15.0                 0.091333                                    0.466667                                 0.133333                0.012717                           0.008092                        0.002312      631.386861              8.029197                         5.109489                      1.459854   -0.076531   -0.096455    -0.581668    -0.510481\n",
       "498032           1                    1377       44.34              0                         2                      2                   0.021060        0.032200                  0.000000              29.0                 1.528966                                    0.068966                                 0.068966                0.000000                           0.001452                        0.001452       31.055480              0.000000                         0.045106                      0.045106   -0.312384   -0.518153    -0.633360    -2.528799\n",
       "498033           4                    1162       50.89              8                         2                      3                   0.010327        0.043795                  0.666667              12.0                 4.240833                                    0.166667                                 0.250000                0.006885                           0.001721                        0.002582       22.833563              0.157202                         0.039300                      0.058951    0.107338    0.097948     0.396946     0.495607\n",
       "498034           4                     442       93.62             10                        10                      0                   0.042986        0.211810                  0.526316              19.0                 4.927368                                    0.526316                                 0.000000                0.022624                           0.022624                        0.000000        4.721213              0.106815                         0.106815                      0.000000   -0.440895    0.108936    -2.268001     0.498674\n",
       "498035           6                     426       86.54              6                         3                      2                   0.018779        0.203146                  0.750000               8.0                10.817500                                    0.375000                                 0.250000                0.014085                           0.007042                        0.004695        4.922579              0.069332                         0.034666                      0.023111   -0.013686    0.505769    -0.504177     1.956118\n",
       "498036           3                    1115       12.44              2                         7                      0                   0.008072        0.011157                  0.222222               9.0                 1.382222                                    0.777778                                 0.000000                0.001794                           0.006278                        0.000000       89.630225              0.160772                         0.562701                      0.000000   -0.419783   -0.087195    -2.167825    -0.411364\n",
       "498037           2                     994       51.18              9                        10                      4                   0.004024        0.051489                  2.250000               4.0                12.795000                                    2.500000                                 1.000000                0.009054                           0.010060                        0.004024       19.421649              0.175850                         0.195389                      0.078156    0.512460   -0.283281     1.147365    -1.182094\n",
       "498038           6                    1006       76.66              2                        10                      4                   0.020875        0.076203                  0.095238              21.0                 3.650476                                    0.476190                                 0.190476                0.001988                           0.009940                        0.003976       13.122880              0.026089                         0.130446                      0.052178    0.221764    0.493659     1.374972     2.395316\n",
       "498039           3                    1377       87.61              6                         2                      0                   0.007988        0.063624                  0.545455              11.0                 7.964545                                    0.181818                                 0.000000                0.004357                           0.001452                        0.000000       15.717384              0.068485                         0.022828                      0.000000   -0.432407   -0.092852    -2.211316    -0.455448\n",
       "498040           5                     695        3.77              2                         2                      5                   0.011511        0.005424                  0.250000               8.0                 0.471250                                    0.250000                                 0.625000                0.002878                           0.002878                        0.007194      184.350133              0.530504                         0.530504                      1.326260    0.480751    0.294168     1.935878     1.365138\n",
       "498041           4                     235       58.68             10                         9                      3                   0.046809        0.249702                  0.909091              11.0                 5.334545                                    0.818182                                 0.272727                0.042553                           0.038298                        0.012766        4.004772              0.170416                         0.153374                      0.051125    0.189705    0.107790     0.357380     0.473245\n",
       "498042           5                    1580       59.92              5                         9                      4                   0.001266        0.037924                  2.500000               2.0                29.960000                                    4.500000                                 2.000000                0.003165                           0.005696                        0.002532       26.368491              0.083445                         0.150200                      0.066756    0.591405    0.320574     0.931662     0.800498\n",
       "498043           3                    1467       86.97              7                         6                      1                   0.006135        0.059284                  0.777778               9.0                 9.663333                                    0.666667                                 0.111111                0.004772                           0.004090                        0.000682       16.867885              0.080488                         0.068989                      0.011498   -0.184398   -0.088165    -1.532502    -0.446634\n",
       "498044           2                     436       56.54              9                         2                      5                   0.043578        0.129679                  0.473684              19.0                 2.975789                                    0.105263                                 0.263158                0.020642                           0.004587                        0.011468        7.711355              0.159179                         0.035373                      0.088433    0.398903   -0.314244     2.098449    -1.709724\n",
       "498045           1                     276       55.27              0                         9                      5                   0.097826        0.200254                  0.000000              27.0                 2.047037                                    0.333333                                 0.185185                0.000000                           0.032609                        0.018116        4.993667              0.000000                         0.162837                      0.090465    0.313270   -0.516511     1.872921    -2.332967\n",
       "498046           3                    1150       31.81              2                         8                      2                   0.011304        0.027661                  0.153846              13.0                 2.446923                                    0.615385                                 0.153846                0.001739                           0.006957                        0.001739       36.152153              0.062873                         0.251493                      0.062873   -0.082362   -0.095233    -0.550952    -0.499150\n",
       "498047           3                    1257       68.08              2                         4                      1                   0.012729        0.054161                  0.125000              16.0                 4.255000                                    0.250000                                 0.062500                0.001591                           0.003182                        0.000796       18.463572              0.029377                         0.058754                      0.014689   -0.319074   -0.099784    -1.836191    -0.450929\n",
       "498048           6                    1329       90.91              1                        10                      0                   0.020316        0.068405                  0.037037              27.0                 3.367037                                    0.370370                                 0.000000                0.000752                           0.007524                        0.000000       14.618854              0.011000                         0.109999                      0.000000   -0.577908    0.498470    -2.215459     2.116667\n",
       "498049           6                      74       39.14              9                         1                      3                   0.121622        0.528919                  1.000000               9.0                 4.348889                                    0.111111                                 0.333333                0.121622                           0.013514                        0.040541        1.890649              0.229944                         0.025549                      0.076648    0.140629    0.499588     0.382790     1.905114\n",
       "498050           5                     296       90.15              3                         4                      0                   0.101351        0.304561                  0.100000              30.0                 3.005000                                    0.133333                                 0.000000                0.010135                           0.013514                        0.000000        3.283417              0.033278                         0.044370                      0.000000   -0.643651    0.289025    -2.446935     1.607977\n",
       "498051           5                     550       39.44              1                         2                      1                   0.001818        0.071709                  1.000000               1.0                39.440000                                    2.000000                                 1.000000                0.001818                           0.003636                        0.001818       13.945233              0.025355                         0.050710                      0.025355   -0.086355    0.317151    -0.224669     0.960902\n",
       "498052           1                     154       97.15              7                         2                      5                   0.012987        0.630844                  3.500000               2.0                48.575000                                    1.000000                                 2.500000                0.045455                           0.012987                        0.032468        1.585178              0.072054                         0.020587                      0.051467    0.758849   -0.489491     1.198422    -0.940836\n",
       "498053           4                    1674       22.70              7                         4                      5                   0.002389        0.013560                  1.750000               4.0                 5.675000                                    1.000000                                 1.250000                0.004182                           0.002389                        0.002987       73.744493              0.308370                         0.176211                      0.220264    0.618356    0.104645     1.677158     0.414093\n",
       "498054           5                    1542       79.01              6                         4                      5                   0.019455        0.051239                  0.200000              30.0                 2.633667                                    0.133333                                 0.166667                0.003891                           0.002594                        0.003243       19.516517              0.075940                         0.050627                      0.063283    0.279533    0.276435     2.125745     1.806038\n",
       "498055           2                    1210       83.79              9                         6                      0                   0.021488        0.069248                  0.346154              26.0                 3.222692                                    0.230769                                 0.000000                0.007438                           0.004959                        0.000000       14.440864              0.107411                         0.071608                      0.000000   -0.557716   -0.302632    -2.440468    -1.617686\n",
       "498056           5                     289       80.54              3                         0                      0                   0.103806        0.278685                  0.100000              30.0                 2.684667                                    0.000000                                 0.000000                0.010381                           0.000000                        0.000000        3.588279              0.037249                         0.000000                      0.000000   -0.675826    0.284363    -2.426243     1.566113\n",
       "498057           6                    1208       44.88             10                         9                      2                   0.020695        0.037152                  0.400000              25.0                 1.795200                                    0.360000                                 0.080000                0.008278                           0.007450                        0.001656       26.916221              0.222816                         0.200535                      0.044563   -0.167101    0.495667    -0.624764     2.493826\n",
       "498058           2                    1401       17.78             10                         0                      1                   0.000714        0.012691                 10.000000               1.0                17.780000                                    0.000000                                 1.000000                0.007138                           0.000000                        0.000714       78.796400              0.562430                         0.000000                      0.056243   -0.025829   -0.279715     3.210981     0.048958\n",
       "498059           3                      92       26.42              8                         6                      3                   0.021739        0.287174                  4.000000               2.0                13.210000                                    3.000000                                 1.500000                0.086957                           0.065217                        0.032609        3.482210              0.302801                         0.227101                      0.113550    0.356206   -0.081713     0.458718    -0.467975\n",
       "498060           1                     513       88.74              7                         2                      4                   0.015595        0.172982                  0.875000               8.0                11.092500                                    0.250000                                 0.500000                0.013645                           0.003899                        0.007797        5.780933              0.078882                         0.022538                      0.045076    0.358400   -0.499654     1.207683    -1.849035\n",
       "498061           5                    1161       80.91             10                         5                      4                   0.018949        0.069690                  0.454545              22.0                 3.677727                                    0.227273                                 0.181818                0.008613                           0.004307                        0.003445       14.349277              0.123594                         0.061797                      0.049438    0.215484    0.289231     1.471976     1.844743\n",
       "498062           6                    1219       90.48              8                         8                      4                   0.009024        0.074225                  0.727273              11.0                 8.225455                                    0.727273                                 0.363636                0.006563                           0.006563                        0.003281       13.472591              0.088417                         0.088417                      0.044209    0.366828    0.503845     1.267551     1.988822\n",
       "498063           5                    1006       19.05              8                        10                      3                   0.015905        0.018936                  0.500000              16.0                 1.190625                                    0.625000                                 0.187500                0.007952                           0.009940                        0.002982       52.808399              0.419948                         0.524934                      0.157480    0.108321    0.302500     0.356638     1.712943\n",
       "498064           3                    1311        9.85              1                        10                      3                   0.012967        0.007513                  0.058824              17.0                 0.579412                                    0.588235                                 0.176471                0.000763                           0.007628                        0.002288      133.096447              0.101523                         1.015228                      0.304569    0.056719   -0.100213     0.369381    -0.576603\n",
       "498065           5                    1345       61.57              4                         0                      4                   0.017100        0.045777                  0.173913              23.0                 2.676957                                    0.000000                                 0.173913                0.002974                           0.000000                        0.002974       21.845054              0.064967                         0.000000                      0.064967    0.130100    0.280736     1.504133     1.936499\n",
       "498066           6                      80       64.53              4                         0                      5                   0.225000        0.806625                  0.222222              18.0                 3.585000                                    0.000000                                 0.277778                0.050000                           0.000000                        0.062500        1.239733              0.061987                         0.000000                      0.077483    0.372415    0.482587     1.873563     2.192983\n",
       "498067           2                    1623       12.69              8                         0                      3                   0.001232        0.007819                  4.000000               2.0                 6.345000                                    0.000000                                 1.500000                0.004929                           0.000000                        0.001848      127.895981              0.630418                         0.000000                      0.236407    0.273070   -0.290999     0.547632    -1.137739\n",
       "498068           3                     361       79.04             10                         3                      0                   0.022161        0.218947                  1.250000               8.0                 9.880000                                    0.375000                                 0.000000                0.027701                           0.008310                        0.000000        4.567308              0.126518                         0.037955                      0.000000   -0.366288   -0.087476    -2.002003    -0.397231\n",
       "498069           2                    1501       59.95              8                         8                      5                   0.002665        0.039940                  2.000000               4.0                14.987500                                    2.000000                                 1.250000                0.005330                           0.005330                        0.003331       25.037531              0.133445                         0.133445                      0.083403    0.682855   -0.288454     1.605850    -1.109372\n",
       "498070           6                    1137        6.33              5                         4                      0                   0.023747        0.005567                  0.185185              27.0                 0.234444                                    0.148148                                 0.000000                0.004398                           0.003518                        0.000000      179.620853              0.789889                         0.631912                      0.000000   -0.629039    0.491641    -2.294293     2.207567\n",
       "498071           2                     948       38.69              4                         0                      0                   0.006329        0.040812                  0.666667               6.0                 6.448333                                    0.000000                                 0.000000                0.004219                           0.000000                        0.000000       24.502455              0.103386                         0.000000                      0.000000   -0.418069   -0.291281    -2.071975    -1.265852\n",
       "498072           5                    1473       59.90              2                        10                      5                   0.006789        0.040665                  0.200000              10.0                 5.990000                                    1.000000                                 0.500000                0.001358                           0.006789                        0.003394       24.590985              0.033389                         0.166945                      0.083472    0.537267    0.302683     1.916720     1.386184\n",
       "498073           2                     165       95.42              9                         0                      3                   0.157576        0.578303                  0.346154              26.0                 3.670000                                    0.000000                                 0.115385                0.054545                           0.000000                        0.018182        1.729197              0.094320                         0.000000                      0.031440   -0.048566   -0.317767     0.336128    -1.880551\n",
       "498074           6                    1539       42.31              1                         3                      0                   0.014945        0.027492                  0.043478              23.0                 1.839565                                    0.130435                                 0.000000                0.000650                           0.001949                        0.000000       36.374380              0.023635                         0.070905                      0.000000   -0.601733    0.493902    -2.266427     2.160896\n",
       "498075           6                    1543       53.43             10                         8                      1                   0.016850        0.034627                  0.384615              26.0                 2.055000                                    0.307692                                 0.038462                0.006481                           0.005185                        0.000648       28.878907              0.187161                         0.149729                      0.018716   -0.365887    0.496422    -1.670767     2.344311\n",
       "498076           6                    1196       78.10              4                         6                      2                   0.004181        0.065301                  0.800000               5.0                15.620000                                    1.200000                                 0.400000                0.003344                           0.005017                        0.001672       15.313700              0.051216                         0.076825                      0.025608    0.046391    0.512911    -0.487240     1.882704\n",
       "498077           4                     676       65.13              8                         6                      1                   0.013314        0.096346                  0.888889               9.0                 7.236667                                    0.666667                                 0.111111                0.011834                           0.008876                        0.001479       10.379242              0.122831                         0.092123                      0.015354   -0.186779    0.111434    -1.541044     0.441140\n",
       "498078           5                    1100       46.66              7                         8                      2                   0.004545        0.042418                  1.400000               5.0                 9.332000                                    1.600000                                 0.400000                0.006364                           0.007273                        0.001818       23.574796              0.150021                         0.171453                      0.042863    0.068788    0.315959    -0.455508     1.325383\n",
       "498079           6                     311       29.78              1                         8                      4                   0.086817        0.095756                  0.037037              27.0                 1.102963                                    0.296296                                 0.148148                0.003215                           0.025723                        0.012862       10.443251              0.033580                         0.268637                      0.134318    0.119375    0.484379     1.382392     2.459363\n",
       "498080           1                     560       69.82             10                         3                      5                   0.030357        0.124679                  0.588235              17.0                 4.107059                                    0.176471                                 0.294118                0.017857                           0.005357                        0.008929        8.020624              0.143225                         0.042968                      0.071613    0.439937   -0.510433     1.927774    -2.153347\n",
       "498081           6                    1733       43.08              2                         8                      4                   0.012695        0.024859                  0.090909              22.0                 1.958182                                    0.363636                                 0.181818                0.001154                           0.004616                        0.002308       40.227484              0.046425                         0.185701                      0.092851    0.183670    0.490147     1.409201     2.442498\n",
       "498082           3                    1437       36.29              9                         7                      2                   0.016006        0.025254                  0.391304              23.0                 1.577826                                    0.304348                                 0.086957                0.006263                           0.004871                        0.001392       39.597685              0.248002                         0.192891                      0.055112   -0.167162   -0.104527    -0.600645    -0.548197\n",
       "498083           5                     101       46.25             10                         2                      3                   0.019802        0.457921                  5.000000               2.0                23.125000                                    1.000000                                 1.500000                0.099010                           0.019802                        0.029703        2.183784              0.216216                         0.043243                      0.064865    0.341486    0.313902     0.400110     0.867679\n",
       "498084           4                     587       60.88              1                         8                      3                   0.045997        0.103714                  0.037037              27.0                 2.254815                                    0.296296                                 0.111111                0.001704                           0.013629                        0.005111        9.641919              0.016426                         0.131406                      0.049277   -0.053252    0.087779     0.351923     0.526065\n",
       "498085           1                    1270       25.98              5                         1                      4                   0.010236        0.020457                  0.384615              13.0                 1.998462                                    0.076923                                 0.307692                0.003937                           0.000787                        0.003150       48.883757              0.192456                         0.038491                      0.153965    0.246751   -0.507737     1.323659    -2.250099\n",
       "498086           4                     185       63.43             10                         9                      2                   0.027027        0.342865                  2.000000               5.0                12.686000                                    1.800000                                 0.400000                0.054054                           0.048649                        0.010811        2.916601              0.157654                         0.141889                      0.031531    0.105984    0.118611    -0.462521     0.424089\n",
       "498087           6                    1480       19.87              7                         0                      2                   0.000676        0.013426                  7.000000               1.0                19.870000                                    0.000000                                 2.000000                0.004730                           0.000000                        0.001351       74.484147              0.352290                         0.000000                      0.100654    0.160370    0.515485     3.165331     0.183863\n",
       "498088           4                     403       50.53              1                         0                      3                   0.002481        0.125385                  1.000000               1.0                50.530000                                    0.000000                                 3.000000                0.002481                           0.000000                        0.007444        7.975460              0.019790                         0.000000                      0.059371    0.342761    0.110730     3.198606     0.121134\n",
       "498089           6                     154       84.63              6                         2                      4                   0.006494        0.549545                  6.000000               1.0                84.630000                                    2.000000                                 4.000000                0.038961                           0.012987                        0.025974        1.819686              0.070897                         0.023632                      0.047265    0.710191    0.519869     3.510299     0.123292\n",
       "498090           3                     285       74.97              4                         7                      2                   0.003509        0.263053                  4.000000               1.0                74.970000                                    7.000000                                 2.000000                0.014035                           0.024561                        0.007018        3.801521              0.053355                         0.093371                      0.026677    0.307022   -0.068999     3.409179     0.105333\n",
       "498091           4                    1691       22.89             10                         6                      5                   0.007688        0.013536                  0.769231              13.0                 1.760769                                    0.461538                                 0.384615                0.005914                           0.003548                        0.002957       73.875055              0.436872                         0.262123                      0.218436    0.494801    0.096569     1.999168     0.488506\n",
       "498092           1                    1685       61.99              6                         0                      4                   0.001187        0.036789                  3.000000               2.0                30.995000                                    0.000000                                 2.000000                0.003561                           0.000000                        0.002374       27.181804              0.096790                         0.000000                      0.064527    0.495132   -0.492042     1.173011    -1.210670\n",
       "498093           4                     275       48.23              6                         1                      3                   0.040000        0.175382                  0.545455              11.0                 4.384545                                    0.090909                                 0.272727                0.021818                           0.003636                        0.010909        5.701845              0.124404                         0.020734                      0.062202    0.101001    0.096966     0.374923     0.507155\n",
       "498094           5                    1085       50.15              3                         6                      2                   0.012903        0.046221                  0.214286              14.0                 3.582143                                    0.428571                                 0.142857                0.002765                           0.005530                        0.001843       21.635095              0.059821                         0.119641                      0.039880   -0.097823    0.301640    -0.543317     1.711947\n",
       "498095           5                     687       33.47              7                         1                      1                   0.036390        0.048719                  0.280000              25.0                 1.338800                                    0.040000                                 0.040000                0.010189                           0.001456                        0.001456       20.525844              0.209143                         0.029878                      0.029878   -0.427005    0.288301    -1.870431     1.752422\n",
       "498096           6                     425       18.91              7                         9                      4                   0.009412        0.044494                  1.750000               4.0                 4.727500                                    2.250000                                 1.000000                0.016471                           0.021176                        0.009412       22.474881              0.370175                         0.475939                      0.211528    0.472386    0.513056     1.143937     1.454934\n",
       "498097           2                     671       86.03             11                         1                      5                   0.038748        0.128212                  0.423077              26.0                 3.308846                                    0.038462                                 0.192308                0.016393                           0.001490                        0.007452        7.799605              0.127862                         0.011624                      0.058119    0.330910   -0.321494     1.989479    -1.814723\n",
       "498098           4                    1705       98.66              5                         3                      2                   0.002346        0.057865                  1.250000               4.0                24.665000                                    0.750000                                 0.500000                0.002933                           0.001760                        0.001173       17.281573              0.050679                         0.030407                      0.020272    0.058991    0.112030    -0.503091     0.457804\n",
       "498099           1                    1822       75.30              9                         2                      4                   0.016465        0.041328                  0.300000              30.0                 2.510000                                    0.066667                                 0.133333                0.004940                           0.001098                        0.002195       24.196547              0.119522                         0.026560                      0.053121    0.096394   -0.521803     1.458583    -2.411263\n",
       "498100           3                    1151       11.37              3                         1                      0                   0.021720        0.009878                  0.120000              25.0                 0.454800                                    0.040000                                 0.000000                0.002606                           0.000869                        0.000000      101.231310              0.263852                         0.087951                      0.000000   -0.637499   -0.109895    -2.545646    -0.485906\n",
       "498101           1                     796       85.72              9                         3                      1                   0.001256        0.107688                  9.000000               1.0                85.720000                                    3.000000                                 1.000000                0.011307                           0.003769                        0.001256        9.286048              0.104993                         0.034998                      0.011666    0.095151   -0.469309     3.355964     0.082610\n",
       "498102           2                    1379       48.64             11                         7                      5                   0.014503        0.035272                  0.550000              20.0                 2.432000                                    0.350000                                 0.250000                0.007977                           0.005076                        0.003626       28.351151              0.226151                         0.143914                      0.102796    0.431208   -0.308803     2.077825    -1.749740\n",
       "498103           6                     885       92.41              6                         5                      0                   0.016949        0.104418                  0.400000              15.0                 6.160667                                    0.333333                                 0.000000                0.006780                           0.005650                        0.000000        9.576886              0.064928                         0.054107                      0.000000   -0.454381    0.506017    -2.100359     2.010543\n",
       "498104           3                     441       36.91              0                         8                      4                   0.004535        0.083696                  0.000000               2.0                18.455000                                    4.000000                                 2.000000                0.000000                           0.018141                        0.009070       11.947982              0.000000                         0.216743                      0.108372    0.517835   -0.084884     1.092574    -0.370675\n",
       "498105           2                     319       57.74              2                         1                      0                   0.094044        0.181003                  0.066667              30.0                 1.924667                                    0.033333                                 0.000000                0.006270                           0.003135                        0.000000        5.524766              0.034638                         0.017319                      0.000000   -0.680588   -0.314602    -2.465426    -1.618736\n",
       "498106           4                    1228       74.44              6                         5                      3                   0.022801        0.060619                  0.214286              28.0                 2.658571                                    0.178571                                 0.107143                0.004886                           0.004072                        0.002443       16.496507              0.080602                         0.067168                      0.040301   -0.057299    0.084976     0.348254     0.540835\n",
       "498107           4                     708       19.56              8                         1                      3                   0.002825        0.027627                  4.000000               2.0                 9.780000                                    0.500000                                 1.500000                0.011299                           0.001412                        0.004237       36.196319              0.408998                         0.051125                      0.153374    0.290756    0.110333     0.517168     0.377922\n",
       "498108           2                    1726       99.80              1                        10                      1                   0.007532        0.057822                  0.076923              13.0                 7.676923                                    0.769231                                 0.076923                0.000579                           0.005794                        0.000579       17.294589              0.010020                         0.100200                      0.010020   -0.232505   -0.289245    -1.525648    -1.405017\n",
       "498109           4                     961       18.62              1                         0                      1                   0.003122        0.019376                  0.333333               3.0                 6.206667                                    0.000000                                 0.333333                0.001041                           0.000000                        0.001041       51.611171              0.053706                         0.000000                      0.053706   -0.213660    0.107843    -1.371567     0.448997\n",
       "498110           3                     585       26.17              0                         9                      2                   0.022222        0.044735                  0.000000              13.0                 2.013077                                    0.692308                                 0.153846                0.000000                           0.015385                        0.003419       22.353840              0.000000                         0.343905                      0.076423   -0.086967   -0.094858    -0.582623    -0.487519\n",
       "498111           4                     983       57.72              4                         9                      4                   0.030519        0.058718                  0.133333              30.0                 1.924000                                    0.300000                                 0.133333                0.004069                           0.009156                        0.004069       17.030492              0.069300                         0.155925                      0.069300    0.117063    0.084078     1.641820     0.505170\n",
       "498112           1                    1342       69.89              8                        10                      5                   0.002981        0.052079                  2.000000               4.0                17.472500                                    2.500000                                 1.250000                0.005961                           0.007452                        0.003726       19.201603              0.114466                         0.143082                      0.071541    0.707169   -0.485454     1.496246    -1.416488\n",
       "498113           5                     553       95.44              9                         5                      5                   0.045208        0.172586                  0.360000              25.0                 3.817600                                    0.200000                                 0.200000                0.016275                           0.009042                        0.009042        5.794216              0.094300                         0.052389                      0.052389    0.364355    0.283246     2.064971     1.756742\n",
       "498114           6                    1767       65.98              0                         0                      1                   0.013016        0.037340                  0.000000              23.0                 2.868696                                    0.000000                                 0.043478                0.000000                           0.000000                        0.000566       26.780843              0.000000                         0.000000                      0.015156   -0.438262    0.487835    -1.685405     2.273375\n",
       "498115           6                     879       96.34              2                         9                      1                   0.020478        0.109602                  0.111111              18.0                 5.352222                                    0.500000                                 0.055556                0.002275                           0.010239                        0.001138        9.123936              0.020760                         0.093419                      0.010380   -0.294484    0.503851    -1.571458     2.208047\n",
       "498116           4                     783       59.10              2                         2                      0                   0.012771        0.075479                  0.200000              10.0                 5.910000                                    0.200000                                 0.000000                0.002554                           0.002554                        0.000000       13.248731              0.033841                         0.033841                      0.000000   -0.453159    0.106284    -2.261474     0.454349\n",
       "498117           1                     278       98.90              9                         5                      4                   0.050360        0.355755                  0.642857              14.0                 7.064286                                    0.357143                                 0.285714                0.032374                           0.017986                        0.014388        2.810920              0.091001                         0.050556                      0.040445    0.313497   -0.502220     1.283820    -2.074560\n",
       "498118           3                     300        5.41              6                         6                      4                   0.030000        0.018033                  0.666667               9.0                 0.601111                                    0.666667                                 0.444444                0.020000                           0.020000                        0.013333       55.452865              1.109057                         1.109057                      0.739372    0.336591   -0.098049     1.371059    -0.498357\n",
       "498119           2                    1524       33.97              2                         5                      3                   0.005906        0.022290                  0.222222               9.0                 3.774444                                    0.555556                                 0.333333                0.001312                           0.003281                        0.001969       44.863115              0.058875                         0.147189                      0.088313    0.130232   -0.296911     0.386430    -1.575245\n",
       "498120           3                    1456       56.97             11                         7                      2                   0.019231        0.039128                  0.392857              28.0                 2.034643                                    0.250000                                 0.071429                0.007555                           0.004808                        0.001374       25.557311              0.193084                         0.122872                      0.035106   -0.206077   -0.108752    -0.618502    -0.542440\n",
       "\n",
       "[498121 rows x 24 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(test[best_model_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498121"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497871</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497872</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497873</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497874</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497875</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497876</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497877</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497878</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497879</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497880</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497881</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497882</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497883</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497884</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497885</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497886</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497887</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497888</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497889</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497890</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497891</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497892</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497893</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497894</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497895</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497896</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497897</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497898</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497899</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497900</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497901</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497902</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497903</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497904</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497905</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497906</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497907</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497908</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497909</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497910</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497911</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497912</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497913</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497914</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497915</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497916</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497917</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497918</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497919</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497920</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497921</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497922</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497923</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497924</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497925</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497926</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497927</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497928</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497929</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497930</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497931</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497932</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497933</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497934</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497935</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497936</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497937</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497938</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497939</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497940</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497941</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497942</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497943</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497944</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497945</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497946</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497947</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497948</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497949</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497950</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497951</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497952</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497953</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497954</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497955</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497956</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497957</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497958</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497959</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497960</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497961</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497962</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497963</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497964</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497965</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497966</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497967</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497968</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497969</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497970</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497971</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497972</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497973</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497974</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497975</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497976</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497977</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497978</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497979</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497980</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497981</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497982</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497983</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497984</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497985</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497986</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497987</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497988</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497989</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497990</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497991</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497992</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497993</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497994</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497995</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497996</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497997</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497998</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497999</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498000</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498001</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498002</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498003</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498004</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498005</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498006</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498007</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498008</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498009</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498010</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498011</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498012</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498013</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498014</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498015</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498016</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498017</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498018</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498019</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498020</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498021</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498022</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498023</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498024</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498025</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498026</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498027</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498028</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498029</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498030</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498031</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498032</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498033</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498034</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498035</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498036</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498037</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498038</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498039</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498040</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498041</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498042</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498043</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498044</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498045</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498046</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498047</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498048</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498049</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498050</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498051</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498052</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498053</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498054</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498055</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498056</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498057</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498058</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498059</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498060</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498061</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498062</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498063</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498064</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498065</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498066</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498067</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498068</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498069</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498070</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498071</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498072</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498073</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498074</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498075</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498076</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498077</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498078</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498079</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498080</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498081</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498082</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498083</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498084</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498085</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498086</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498087</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498088</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498089</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498090</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498091</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498092</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498093</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498094</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498095</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498096</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498097</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498098</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498099</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498100</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498101</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498102</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498103</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498104</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498105</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498106</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498107</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498108</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498109</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498110</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498111</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498112</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498113</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498114</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498115</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498116</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498117</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498118</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498119</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498120</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498121 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fraud\n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           0\n",
       "5           0\n",
       "6           0\n",
       "7           0\n",
       "8           0\n",
       "9           0\n",
       "10          0\n",
       "11          0\n",
       "12          0\n",
       "13          0\n",
       "14          0\n",
       "15          0\n",
       "16          0\n",
       "17          0\n",
       "18          0\n",
       "19          0\n",
       "20          0\n",
       "21          0\n",
       "22          0\n",
       "23          0\n",
       "24          0\n",
       "25          0\n",
       "26          0\n",
       "27          0\n",
       "28          0\n",
       "29          0\n",
       "30          0\n",
       "31          0\n",
       "32          0\n",
       "33          0\n",
       "34          0\n",
       "35          0\n",
       "36          0\n",
       "37          0\n",
       "38          0\n",
       "39          0\n",
       "40          0\n",
       "41          0\n",
       "42          0\n",
       "43          0\n",
       "44          0\n",
       "45          0\n",
       "46          0\n",
       "47          0\n",
       "48          0\n",
       "49          0\n",
       "50          0\n",
       "51          0\n",
       "52          0\n",
       "53          0\n",
       "54          0\n",
       "55          0\n",
       "56          0\n",
       "57          0\n",
       "58          0\n",
       "59          0\n",
       "60          0\n",
       "61          0\n",
       "62          0\n",
       "63          0\n",
       "64          0\n",
       "65          0\n",
       "66          0\n",
       "67          0\n",
       "68          0\n",
       "69          0\n",
       "70          0\n",
       "71          0\n",
       "72          0\n",
       "73          0\n",
       "74          0\n",
       "75          0\n",
       "76          0\n",
       "77          1\n",
       "78          0\n",
       "79          0\n",
       "80          1\n",
       "81          0\n",
       "82          0\n",
       "83          0\n",
       "84          0\n",
       "85          0\n",
       "86          0\n",
       "87          0\n",
       "88          0\n",
       "89          0\n",
       "90          0\n",
       "91          0\n",
       "92          0\n",
       "93          0\n",
       "94          0\n",
       "95          0\n",
       "96          0\n",
       "97          0\n",
       "98          0\n",
       "99          0\n",
       "100         0\n",
       "101         0\n",
       "102         0\n",
       "103         0\n",
       "104         0\n",
       "105         0\n",
       "106         0\n",
       "107         0\n",
       "108         0\n",
       "109         0\n",
       "110         1\n",
       "111         0\n",
       "112         0\n",
       "113         0\n",
       "114         0\n",
       "115         0\n",
       "116         0\n",
       "117         0\n",
       "118         0\n",
       "119         0\n",
       "120         0\n",
       "121         0\n",
       "122         0\n",
       "123         0\n",
       "124         0\n",
       "125         0\n",
       "126         0\n",
       "127         0\n",
       "128         0\n",
       "129         0\n",
       "130         0\n",
       "131         0\n",
       "132         0\n",
       "133         0\n",
       "134         0\n",
       "135         0\n",
       "136         0\n",
       "137         0\n",
       "138         0\n",
       "139         0\n",
       "140         0\n",
       "141         0\n",
       "142         0\n",
       "143         0\n",
       "144         0\n",
       "145         0\n",
       "146         0\n",
       "147         0\n",
       "148         0\n",
       "149         0\n",
       "150         0\n",
       "151         0\n",
       "152         0\n",
       "153         0\n",
       "154         0\n",
       "155         0\n",
       "156         0\n",
       "157         0\n",
       "158         0\n",
       "159         0\n",
       "160         1\n",
       "161         0\n",
       "162         0\n",
       "163         0\n",
       "164         0\n",
       "165         0\n",
       "166         0\n",
       "167         0\n",
       "168         0\n",
       "169         0\n",
       "170         0\n",
       "171         0\n",
       "172         0\n",
       "173         0\n",
       "174         0\n",
       "175         0\n",
       "176         0\n",
       "177         0\n",
       "178         0\n",
       "179         0\n",
       "180         0\n",
       "181         0\n",
       "182         0\n",
       "183         0\n",
       "184         0\n",
       "185         0\n",
       "186         0\n",
       "187         0\n",
       "188         0\n",
       "189         0\n",
       "190         0\n",
       "191         0\n",
       "192         0\n",
       "193         0\n",
       "194         0\n",
       "195         0\n",
       "196         0\n",
       "197         0\n",
       "198         0\n",
       "199         0\n",
       "200         0\n",
       "201         0\n",
       "202         0\n",
       "203         0\n",
       "204         0\n",
       "205         0\n",
       "206         0\n",
       "207         0\n",
       "208         0\n",
       "209         0\n",
       "210         0\n",
       "211         0\n",
       "212         0\n",
       "213         0\n",
       "214         0\n",
       "215         0\n",
       "216         0\n",
       "217         0\n",
       "218         0\n",
       "219         0\n",
       "220         0\n",
       "221         0\n",
       "222         0\n",
       "223         0\n",
       "224         0\n",
       "225         0\n",
       "226         0\n",
       "227         0\n",
       "228         0\n",
       "229         0\n",
       "230         0\n",
       "231         0\n",
       "232         0\n",
       "233         0\n",
       "234         0\n",
       "235         0\n",
       "236         0\n",
       "237         0\n",
       "238         0\n",
       "239         0\n",
       "240         0\n",
       "241         0\n",
       "242         0\n",
       "243         0\n",
       "244         0\n",
       "245         0\n",
       "246         0\n",
       "247         0\n",
       "248         0\n",
       "249         0\n",
       "...       ...\n",
       "497871      0\n",
       "497872      0\n",
       "497873      0\n",
       "497874      0\n",
       "497875      0\n",
       "497876      0\n",
       "497877      0\n",
       "497878      0\n",
       "497879      0\n",
       "497880      0\n",
       "497881      0\n",
       "497882      0\n",
       "497883      1\n",
       "497884      0\n",
       "497885      0\n",
       "497886      0\n",
       "497887      0\n",
       "497888      0\n",
       "497889      0\n",
       "497890      0\n",
       "497891      0\n",
       "497892      0\n",
       "497893      0\n",
       "497894      0\n",
       "497895      0\n",
       "497896      0\n",
       "497897      0\n",
       "497898      0\n",
       "497899      0\n",
       "497900      0\n",
       "497901      0\n",
       "497902      1\n",
       "497903      0\n",
       "497904      0\n",
       "497905      0\n",
       "497906      0\n",
       "497907      0\n",
       "497908      0\n",
       "497909      0\n",
       "497910      0\n",
       "497911      0\n",
       "497912      0\n",
       "497913      0\n",
       "497914      0\n",
       "497915      0\n",
       "497916      1\n",
       "497917      0\n",
       "497918      0\n",
       "497919      0\n",
       "497920      0\n",
       "497921      0\n",
       "497922      0\n",
       "497923      1\n",
       "497924      0\n",
       "497925      0\n",
       "497926      0\n",
       "497927      0\n",
       "497928      0\n",
       "497929      1\n",
       "497930      0\n",
       "497931      0\n",
       "497932      0\n",
       "497933      0\n",
       "497934      0\n",
       "497935      0\n",
       "497936      0\n",
       "497937      0\n",
       "497938      0\n",
       "497939      0\n",
       "497940      0\n",
       "497941      0\n",
       "497942      0\n",
       "497943      0\n",
       "497944      0\n",
       "497945      0\n",
       "497946      0\n",
       "497947      1\n",
       "497948      0\n",
       "497949      0\n",
       "497950      0\n",
       "497951      0\n",
       "497952      0\n",
       "497953      0\n",
       "497954      0\n",
       "497955      0\n",
       "497956      0\n",
       "497957      0\n",
       "497958      0\n",
       "497959      0\n",
       "497960      1\n",
       "497961      0\n",
       "497962      0\n",
       "497963      0\n",
       "497964      0\n",
       "497965      0\n",
       "497966      0\n",
       "497967      0\n",
       "497968      0\n",
       "497969      0\n",
       "497970      0\n",
       "497971      0\n",
       "497972      0\n",
       "497973      0\n",
       "497974      0\n",
       "497975      0\n",
       "497976      0\n",
       "497977      0\n",
       "497978      0\n",
       "497979      0\n",
       "497980      0\n",
       "497981      1\n",
       "497982      0\n",
       "497983      0\n",
       "497984      0\n",
       "497985      0\n",
       "497986      0\n",
       "497987      0\n",
       "497988      0\n",
       "497989      0\n",
       "497990      0\n",
       "497991      0\n",
       "497992      0\n",
       "497993      0\n",
       "497994      0\n",
       "497995      0\n",
       "497996      1\n",
       "497997      0\n",
       "497998      0\n",
       "497999      0\n",
       "498000      0\n",
       "498001      0\n",
       "498002      0\n",
       "498003      0\n",
       "498004      0\n",
       "498005      0\n",
       "498006      0\n",
       "498007      0\n",
       "498008      0\n",
       "498009      0\n",
       "498010      0\n",
       "498011      0\n",
       "498012      0\n",
       "498013      0\n",
       "498014      0\n",
       "498015      0\n",
       "498016      0\n",
       "498017      0\n",
       "498018      0\n",
       "498019      0\n",
       "498020      0\n",
       "498021      0\n",
       "498022      0\n",
       "498023      0\n",
       "498024      0\n",
       "498025      0\n",
       "498026      0\n",
       "498027      1\n",
       "498028      0\n",
       "498029      0\n",
       "498030      0\n",
       "498031      0\n",
       "498032      1\n",
       "498033      0\n",
       "498034      0\n",
       "498035      0\n",
       "498036      0\n",
       "498037      0\n",
       "498038      0\n",
       "498039      0\n",
       "498040      0\n",
       "498041      0\n",
       "498042      0\n",
       "498043      0\n",
       "498044      0\n",
       "498045      1\n",
       "498046      0\n",
       "498047      0\n",
       "498048      0\n",
       "498049      0\n",
       "498050      0\n",
       "498051      0\n",
       "498052      0\n",
       "498053      0\n",
       "498054      0\n",
       "498055      0\n",
       "498056      0\n",
       "498057      0\n",
       "498058      0\n",
       "498059      0\n",
       "498060      0\n",
       "498061      0\n",
       "498062      0\n",
       "498063      0\n",
       "498064      0\n",
       "498065      0\n",
       "498066      0\n",
       "498067      0\n",
       "498068      0\n",
       "498069      0\n",
       "498070      0\n",
       "498071      0\n",
       "498072      0\n",
       "498073      0\n",
       "498074      0\n",
       "498075      0\n",
       "498076      0\n",
       "498077      0\n",
       "498078      0\n",
       "498079      0\n",
       "498080      0\n",
       "498081      0\n",
       "498082      0\n",
       "498083      0\n",
       "498084      0\n",
       "498085      0\n",
       "498086      0\n",
       "498087      0\n",
       "498088      0\n",
       "498089      0\n",
       "498090      0\n",
       "498091      0\n",
       "498092      0\n",
       "498093      0\n",
       "498094      0\n",
       "498095      0\n",
       "498096      0\n",
       "498097      0\n",
       "498098      0\n",
       "498099      1\n",
       "498100      0\n",
       "498101      0\n",
       "498102      0\n",
       "498103      0\n",
       "498104      0\n",
       "498105      0\n",
       "498106      0\n",
       "498107      0\n",
       "498108      0\n",
       "498109      0\n",
       "498110      0\n",
       "498111      0\n",
       "498112      0\n",
       "498113      0\n",
       "498114      0\n",
       "498115      0\n",
       "498116      0\n",
       "498117      0\n",
       "498118      0\n",
       "498119      0\n",
       "498120      0\n",
       "\n",
       "[498121 rows x 1 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.columns = ['fraud']\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    475226\n",
      "1     22895\n",
      "Name: fraud, dtype: int64\n",
      "0    0.954037\n",
      "1    0.045963\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(predictions_df.fraud.value_counts())\n",
    "print(predictions_df.fraud.value_counts() / len(predictions_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv('TU_Muenchen_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
