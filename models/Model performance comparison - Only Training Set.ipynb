{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "njobs = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv',sep='|')\n",
    "test=pd.read_csv('test.csv',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1775\n",
      "1     104\n",
      "Name: fraud, dtype: int64\n",
      "0    0.944651\n",
      "1    0.055349\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train.fraud.value_counts())\n",
    "print(train.fraud.value_counts() / len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training set\n",
    "\n",
    "#train['totalScanTimeInMinutes'] = train['totalScanTimeInSeconds'] / 60.0\n",
    "train['scannedLineItems'] = train['scannedLineItemsPerSecond'] * train['totalScanTimeInSeconds']\n",
    "train['pricePerScannedLineItem'] = train['grandTotal'] / train['scannedLineItems']\n",
    "train['scansWithoutRegistrationPerScannedLineItem'] = train['scansWithoutRegistration'] / train['scannedLineItems']\n",
    "train['quantityModificationsPerScannedLineItem'] = train['quantityModifications'] / train['scannedLineItems']\n",
    "train['lineItemVoidsPerSecond'] = train['lineItemVoids'] / train['totalScanTimeInSeconds']\n",
    "train['scansWithoutRegistrationPerSecond'] = train['scansWithoutRegistration'] / train['totalScanTimeInSeconds']\n",
    "train['quantityModificationsPerSecond'] = train['quantityModifications'] / train['totalScanTimeInSeconds']\n",
    "train['secondsPerEuro'] = train['totalScanTimeInSeconds'] / train['grandTotal']\n",
    "train['lineItemVoidsPerEuro'] = train['lineItemVoids'] / train['grandTotal']\n",
    "train['scansWithoutRegistrationPerEuro'] = train['scansWithoutRegistration'] / train['grandTotal']\n",
    "train['quantityModificationsPerEuro'] = train['quantityModifications'] / train['grandTotal']\n",
    "\n",
    "\n",
    "# for test set\n",
    "\n",
    "#train['totalScanTimeInMinutes'] = train['totalScanTimeInSeconds'] / 60.0\n",
    "test['scannedLineItems'] = test['scannedLineItemsPerSecond'] * test['totalScanTimeInSeconds']\n",
    "test['pricePerScannedLineItem'] = test['grandTotal'] / test['scannedLineItems']\n",
    "test['scansWithoutRegistrationPerScannedLineItem'] = test['scansWithoutRegistration'] / test['scannedLineItems']\n",
    "test['quantityModificationsPerScannedLineItem'] = test['quantityModifications'] / test['scannedLineItems']\n",
    "test['lineItemVoidsPerSecond'] = test['lineItemVoids'] / test['totalScanTimeInSeconds']\n",
    "test['scansWithoutRegistrationPerSecond'] = train['scansWithoutRegistration'] / train['totalScanTimeInSeconds']\n",
    "test['quantityModificationsPerSecond'] = train['quantityModifications'] / train['totalScanTimeInSeconds']\n",
    "test['secondsPerEuro'] = train['totalScanTimeInSeconds'] / train['grandTotal']\n",
    "test['lineItemVoidsPerEuro'] = train['lineItemVoids'] / train['grandTotal']\n",
    "test['scansWithoutRegistrationPerEuro'] = train['scansWithoutRegistration'] / train['grandTotal']\n",
    "test['quantityModificationsPerEuro'] = train['quantityModifications'] / train['grandTotal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm.classes import OneClassSVM\n",
    "from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
    "from sklearn.neighbors.classification import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from sklearn.linear_model.ridge import RidgeClassifierCV\n",
    "from sklearn.linear_model.ridge import RidgeClassifier\n",
    "from sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier\n",
    "from sklearn.gaussian_process.gpc import GaussianProcessClassifier\n",
    "from sklearn.ensemble.voting_classifier import VotingClassifier\n",
    "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.mixture import DPGMM\n",
    "#from sklearn.mixture import GMM\n",
    "#from sklearn.mixture import GaussianMixture\n",
    "#from sklearn.mixture import VBGMM\n",
    "\n",
    "\n",
    "\n",
    "model_factory = [\n",
    "# RandomForestRegressor(),\n",
    "# XGBRegressor(nthread=1),\n",
    " #MLPRegressor(),\n",
    "# Ridge(),\n",
    "# BayesianRidge(),\n",
    "# ExtraTreesRegressor(),\n",
    "# ElasticNet(),\n",
    "# KNeighborsRegressor(),\n",
    "# GradientBoostingRegressor()\n",
    "  ExtraTreeClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    " #   OneClassSVM(),  # doesn't work instantly\n",
    "    \n",
    "                  # MLPClassifier(),# takes very long for a larger training set\n",
    "    \n",
    " #   RadiusNeighborsClassifier(), # doesn't work instantly\n",
    "    KNeighborsClassifier(),\n",
    "#    ClassifierChain(),     # ensemble method\n",
    "#    MultiOutputClassifier(), # ensemble method\n",
    "#    OutputCodeClassifier(), # ensemble method\n",
    "#    OneVsOneClassifier(), # ensemble method\n",
    "#    OneVsRestClassifier(), # ensemble method\n",
    "    SGDClassifier(),\n",
    "    RidgeClassifierCV(),\n",
    "    RidgeClassifier(),\n",
    "    PassiveAggressiveClassifier(),\n",
    "    \n",
    "                   # GaussianProcessClassifier(),  # takes very long for a larger training set\n",
    "    \n",
    "#    VotingClassifier(), # ensemble method\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    BernoulliNB(),\n",
    "  #  CalibratedClassifierCV(), # doesn't work instantly\n",
    "    GaussianNB(), # causing some problems\n",
    " #   LabelPropagation(), # doesn't work instantly\n",
    " #   LabelSpreading(),  # doesn't work instantly\n",
    "    LinearDiscriminantAnalysis(), # causing some problems\n",
    " #   LinearSVC(max_iter = 100000), # causing some problems\n",
    "    LogisticRegression(max_iter = 10000),\n",
    "    LogisticRegressionCV(max_iter = 10000),\n",
    "    MultinomialNB(),\n",
    "    NearestCentroid(),\n",
    "  #  NuSVC(),   # doesn't work instantly\n",
    "    Perceptron(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    # SVC(),\n",
    "#    DPGMM(),\n",
    "#    GMM(),\n",
    "#    GaussianMixture(),\n",
    "#    VBGMM()\n",
    "]\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    score = ((-25)*fp + (-5)*fn + 5*tp) / len(y_true)\n",
    "    return (score)\n",
    "\n",
    "my_custom_score = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "model_tuning_factory = [\n",
    "    GridSearchCV(LogisticRegression(max_iter = 10000), \n",
    "                 dict(# penalty = ['l1','l2'],  # automatic regularization  -> option 'l1' doesnt work with all solvers and leads to errors\n",
    "                      fit_intercept = [True, False]),\n",
    "                    #  solver = ['lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score),\n",
    "    GridSearchCV(LogisticRegressionCV(max_iter = 10000), \n",
    "                 dict(# penalty = ['l1','l2'],  # automatic regularization  -> option 'l1' doesnt work with all solvers and leads to errors\n",
    "                      fit_intercept = [True, False]),\n",
    "                    #  solver = ['lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score),\n",
    "    GridSearchCV(KNeighborsClassifier(), \n",
    "                 dict(n_neighbors = range(1,4),\n",
    "                      weights = ['uniform', 'distance']),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score),\n",
    "    GridSearchCV(DecisionTreeClassifier(),\n",
    "                 dict(criterion = ['entropy', 'gini']),\n",
    "                     # max_depth = range(1,100)),\n",
    "                 #     max_leaf_nodes = range(2,100)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score),\n",
    "    GridSearchCV(ExtraTreeClassifier(),\n",
    "                 dict(criterion = ['entropy', 'gini']),\n",
    "                     # max_depth = range(1,100)),\n",
    "                 #     max_leaf_nodes = range(2,100)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score),\n",
    "    GridSearchCV(AdaBoostClassifier(),\n",
    "                 dict(n_estimators = range(1,100)),\n",
    "                      #max_depth = range(1,100)),\n",
    "                 #     max_leaf_nodes = range(2,100)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score),    \n",
    "    GridSearchCV(BernoulliNB(),\n",
    "                 dict(binarize  = np.arange(0.0, 1.0, 0.1)),\n",
    "                     # max_depth = range(1,100)),\n",
    "                 #     max_leaf_nodes = range(2,100)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score), \n",
    "    GridSearchCV(GaussianNB(),\n",
    "                 dict(),\n",
    "                     #var_smoothing  = np.arange(0.0, 1.0, 0.1),\n",
    "                     # max_depth = range(1,100)),\n",
    "                 #     max_leaf_nodes = range(2,100)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score), \n",
    "                 \n",
    "    GridSearchCV(GradientBoostingClassifier(),\n",
    "                 dict(learning_rate = np.arange(0.01, 1.0, 0.01),\n",
    "                      n_estimators = range(50,150)),\n",
    "                     # max_depth = range(1,100)),\n",
    "                 #     max_leaf_nodes = range(2,100)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score),                  \n",
    "                 \n",
    "    GridSearchCV(BaggingClassifier(), \n",
    "                 dict(n_estimators = range(5,50)),\n",
    "                     # max_depth = range(1,100)),\n",
    "                 #     max_leaf_nodes = range(2,100)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score), \n",
    "    GridSearchCV(NearestCentroid(),\n",
    "                 dict(),\n",
    "                 # dict(n_estimators = range(5,50)),\n",
    "                     # max_depth = range(1,100)),\n",
    "                 #     max_leaf_nodes = range(2,100)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score), \n",
    "    GridSearchCV(Perceptron(),\n",
    "                 dict(),\n",
    "                 # dict(n_estimators = range(5,50)),\n",
    "                     # max_depth = range(1,100)),\n",
    "                 #     max_leaf_nodes = range(2,100)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score), \n",
    "    GridSearchCV(LinearDiscriminantAnalysis(),\n",
    "                 dict(),\n",
    "                 #dict(n_components = range(5,50)),\n",
    "                     # max_depth = range(1,100)),\n",
    "                 #     max_leaf_nodes = range(2,100)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score),\n",
    "    GridSearchCV(QuadraticDiscriminantAnalysis(),\n",
    "                 dict(),\n",
    "                 # dict(n_estimators = range(5,50)),\n",
    "                     # max_depth = range(1,100)),\n",
    "                 #     max_leaf_nodes = range(2,100)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score)                 \n",
    "]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model_tuning_factory_temp = [\n",
    "    GridSearchCV(DecisionTreeClassifier(),\n",
    "                 dict(criterion = ['entropy', 'gini'],\n",
    "                     # max_depth = range(1,100)),\n",
    "                     #  max_leaf_nodes = range(2,100)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score),\n",
    "    GridSearchCV(LogisticRegression(max_iter = 10000), \n",
    "                 dict(# penalty = ['l1','l2'],  # automatic regularization  -> option 'l1' doesnt work with all solvers and leads to errors\n",
    "                      fit_intercept = [True, False]),\n",
    "                    #  solver = ['lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "model_tuning_factory2 = [\n",
    "    RandomizedSearchCV(KNeighborsClassifier(), \n",
    "                 dict(n_neighbors = range(1,4),\n",
    "                      weights = ['uniform', 'distance']),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 n_iter = 10),\n",
    "    RandomizedSearchCV(LogisticRegression(max_iter = 10000), \n",
    "                 dict(#penalty = ['l2'],  # automatic regularization  -> option 'l1' doesnt work with all solvers and leads to errors\n",
    "                      fit_intercept = [True, False]),\n",
    "                      #solver = ['lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 n_iter = 10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    score = ((-25)*fp + (-5)*fn + 5*tp) / len(y_true)\n",
    "    return (score)\n",
    "\n",
    "my_custom_score = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "loo = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train['fraud']\n",
    "X = train.drop('fraud',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished DecisionTreeClassifier with None and 1 features\n",
      "Finished LogisticRegression with None and 1 features\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.DataFrame(feature_scaler.fit_transform(train.values), columns=train.columns, index=train.index)\n",
    "\n",
    "result_table = pd.DataFrame(columns=[\"Model\", \"Data Preparation\", \"Feature Count\", \"Features\", \"Optimal Parameters\", \"Monetary Value Per Instance - Mean\", \"Monetary Value Per Instance - Standard Deviation\", \"Raw Model\"])\n",
    "\n",
    "\n",
    "# three types of data preparation: No preparation, MaxMinScaler, StandardScaler\n",
    "for data_preparation_step in range(1,4):\n",
    "    if (data_preparation_step == 1):  \n",
    "        X_scaled = X\n",
    "        data_preparation = \"None\"\n",
    "    elif (data_preparation_step == 2):\n",
    "        feature_scaler = MinMaxScaler()  \n",
    "        feature_scaler.fit_transform(X.values) \n",
    "        X_scaled = pd.DataFrame(feature_scaler.fit_transform(X.values), columns=X.columns, index=X.index) \n",
    "        data_preparation = \"MinMaxScaler\"\n",
    "    elif (data_preparation_step == 3):\n",
    "        feature_scaler = StandardScaler()  \n",
    "        feature_scaler.fit_transform(X.values) \n",
    "        X_scaled = pd.DataFrame(feature_scaler.fit_transform(X.values), columns=X.columns, index=X.index)\n",
    "        data_preparation = \"StandardScaler\"\n",
    "\n",
    "    for feature_count in range(1,len(list(X))+1):\n",
    "\n",
    "        for model in model_tuning_factory_temp:   # replace with GridSearch for greater accuracy\n",
    "                \n",
    "            best_features = SelectKBest(f_classif, k=feature_count).fit(X_scaled,Y)\n",
    "            best_feature_list = X.columns[best_features.get_support()]\n",
    "            X_selected_features = X[best_feature_list]\n",
    "            \n",
    "            model.seed = 42\n",
    "            model.fit(X_selected_features,Y)  \n",
    "            model_name = model.best_estimator_.__class__.__name__\n",
    "            score_mean = model.cv_results_['mean_test_score'][model.best_index_]\n",
    "            score_std = model.cv_results_['std_test_score'][model.best_index_]\n",
    "            \n",
    "            print(\"Finished \" + model.best_estimator_.__class__.__name__ + \" with \" + data_preparation + \" and \" + str(feature_count) + \" features\")\n",
    "\n",
    "    #  print(\"Finished \" + model.__class__.__name__ + \" with \" + str(feature_count) + \" features:\" + \" after \" + str(time.time() - t) + \" seconds\")\n",
    "    #  print(\"Monetary Value Per Instance - Mean \" + str(score_mean))\n",
    "    #  print(\"Monetary Value Per Instance - Standard Deviation \" + str(score_std))  \n",
    "    #  print(\"\")  \n",
    "\n",
    "            result_table = result_table.append({\n",
    "             \"Model\": model_name,\n",
    "             \"Data Preparation\": data_preparation,\n",
    "             \"Feature Count\": feature_count,\n",
    "             \"Features\": best_feature_list.values,\n",
    "             \"Optimal Parameters\": model.best_params_,\n",
    "             \"Monetary Value Per Instance - Mean\":  score_mean,\n",
    "             \"Monetary Value Per Instance - Standard Deviation\": score_std,\n",
    "             \"Raw Model\": model.best_estimator_\n",
    "              }, ignore_index=True)\n",
    "    \n",
    "result_table.sort_values(by = \"Monetary Value Per Instance - Mean\", ascending = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
