{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning) \n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "njobs = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv',sep='|')\n",
    "test=pd.read_csv('test.csv',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1775\n",
      "1     104\n",
      "Name: fraud, dtype: int64\n",
      "0    0.944651\n",
      "1    0.055349\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train.fraud.value_counts())\n",
    "print(train.fraud.value_counts() / len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derviable directly from given features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['scannedLineItems'] = train['scannedLineItemsPerSecond'] * train['totalScanTimeInSeconds']\n",
    "train['pricePerScannedLineItem'] = train['grandTotal'] / train['scannedLineItems']\n",
    "train['scansWithoutRegistrationPerScannedLineItem'] = train['scansWithoutRegistration'] / train['scannedLineItems']\n",
    "train['quantityModificationsPerScannedLineItem'] = train['quantityModifications'] / train['scannedLineItems']\n",
    "train['lineItemVoidsPerSecond'] = train['lineItemVoids'] / train['totalScanTimeInSeconds']\n",
    "train['scansWithoutRegistrationPerSecond'] = train['scansWithoutRegistration'] / train['totalScanTimeInSeconds']\n",
    "train['quantityModificationsPerSecond'] = train['quantityModifications'] / train['totalScanTimeInSeconds']\n",
    "train['secondsPerEuro'] = train['totalScanTimeInSeconds'] / train['grandTotal']\n",
    "train['lineItemVoidsPerEuro'] = train['lineItemVoids'] / train['grandTotal']\n",
    "train['scansWithoutRegistrationPerEuro'] = train['scansWithoutRegistration'] / train['grandTotal']\n",
    "train['quantityModificationsPerEuro'] = train['quantityModifications'] / train['grandTotal']\n",
    "\n",
    "test['scannedLineItems'] = test['scannedLineItemsPerSecond'] * test['totalScanTimeInSeconds']\n",
    "test['pricePerScannedLineItem'] = test['grandTotal'] / test['scannedLineItems']\n",
    "test['scansWithoutRegistrationPerScannedLineItem'] = test['scansWithoutRegistration'] / test['scannedLineItems']\n",
    "test['quantityModificationsPerScannedLineItem'] = test['quantityModifications'] / test['scannedLineItems']\n",
    "test['lineItemVoidsPerSecond'] = test['lineItemVoids'] / test['totalScanTimeInSeconds']\n",
    "test['scansWithoutRegistrationPerSecond'] = test['scansWithoutRegistration'] / test['totalScanTimeInSeconds']\n",
    "test['quantityModificationsPerSecond'] = test['quantityModifications'] / test['totalScanTimeInSeconds']\n",
    "test['secondsPerEuro'] = test['totalScanTimeInSeconds'] / test['grandTotal']\n",
    "test['lineItemVoidsPerEuro'] = test['lineItemVoids'] / test['grandTotal']\n",
    "test['scansWithoutRegistrationPerEuro'] = test['scansWithoutRegistration'] / test['grandTotal']\n",
    "test['quantityModificationsPerEuro'] = test['quantityModifications'] / test['grandTotal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test['grandTotal'] == 0.00, 'secondsPerEuro'] = test['totalScanTimeInSeconds'] / 0.01\n",
    "test.loc[test['grandTotal'] == 0.00, 'lineItemVoidsPerEuro'] = test['lineItemVoids'] / 0.01\n",
    "test.loc[test['grandTotal'] == 0.00, 'scansWithoutRegistrationPerEuro'] = test['scansWithoutRegistration'] / 0.01\n",
    "test.loc[test['grandTotal'] == 0.00, 'quantityModificationsPerEuro'] = test['quantityModifications'] / 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derivable from PCA / tSNE Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['fraud'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_combined = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 20) (500000,)\n"
     ]
    }
   ],
   "source": [
    "X_train_test_combined = train_test_combined.drop('fraud',axis=1)\n",
    "Y_train_test_combined = train_test_combined['fraud']\n",
    "print(X_train_test_combined.shape, Y_train_test_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer \n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "X_train_test_combined = pd.DataFrame(feature_scaler.fit_transform(X_train_test_combined.values), columns=X_train_test_combined.columns, index=X_train_test_combined.index)\n",
    "\n",
    "#feature_scaler = StandardScaler()\n",
    "#X = pd.DataFrame(feature_scaler.fit_transform(X.values), columns=X.columns, index=X.index)\n",
    "#data_preparation = \"StandardScaler\"\n",
    "\n",
    "#transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "#X_scaled = pd.DataFrame(transformer.transform(X), columns=X.columns, index=X.index)\n",
    "#data_preparation = \"LogScaler\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derive features from PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation per principal component: [0.16124041 0.1559011  0.14737624 0.13509126 0.13197732 0.11312753\n",
      " 0.11161208]\n",
      "Cumulative explained variation for 7 principal components: 0.9563259397521094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train_test_combined_PCA = X_train_test_combined.copy()\n",
    "\n",
    "pca = PCA(n_components=7)\n",
    "pca_result = pca.fit_transform(X_train_test_combined_PCA)\n",
    "\n",
    "X_train_test_combined_PCA['pca-one'] = pca_result[:,0]\n",
    "X_train_test_combined_PCA['pca-two'] = pca_result[:,1] \n",
    "X_train_test_combined_PCA['pca-three'] = pca_result[:,2]\n",
    "\n",
    "X_train_test_combined_PCA['pca-one'] = pca_result[:,0]\n",
    "X_train_test_combined_PCA['pca-two'] = pca_result[:,1] \n",
    "X_train_test_combined_PCA['pca-three'] = pca_result[:,2]\n",
    "\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "print('Cumulative explained variation for 7 principal components: {}'.format(np.sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,20)\n",
    "\n",
    "plt.title('Grau Unlabeled, Blau: Fraud, Rot: No fraud')\n",
    "\n",
    "plt.scatter(X_train_test_combined_PCA[\"pca-one\"][Y_train_test_combined == -1].values, X_train_test_combined_PCA[\"pca-two\"][Y_train_test_combined == -1].values, color='grey', alpha = 0.1)\n",
    "plt.scatter(X_train_test_combined_PCA[\"pca-one\"][Y_train_test_combined == 0], X_train_test_combined_PCA[\"pca-two\"][Y_train_test_combined == 0], color='r')\n",
    "plt.scatter(X_train_test_combined_PCA[\"pca-one\"][Y_train_test_combined == 1], X_train_test_combined_PCA[\"pca-two\"][Y_train_test_combined == 1], color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add PCA axes as additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pca_axis_1'] = X_train_test_combined_PCA['pca-one'].head(len(train))\n",
    "train['pca_axis_2'] = X_train_test_combined_PCA['pca-two'].head(len(train))\n",
    "\n",
    "test['pca_axis_1'] = X_train_test_combined_PCA['pca-one'].tail(len(test)).reset_index(drop = True)\n",
    "test['pca_axis_2'] = X_train_test_combined_PCA['pca-two'].tail(len(test)).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derive features from tSNE -> computation takes a long time -> use the precomputed value three cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_test_combined_tSNE = X_train_test_combined.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 500000 samples in 3.783s...\n",
      "[t-SNE] Computed neighbors for 500000 samples in 470.343s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 24000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 25000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 26000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 27000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 28000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 29000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 30000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 31000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 32000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 33000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 34000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 35000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 36000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 37000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 38000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 39000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 40000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 41000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 42000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 43000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 44000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 45000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 46000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 47000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 48000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 49000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 50000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 51000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 52000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 53000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 54000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 55000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 56000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 57000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 58000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 59000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 60000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 61000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 62000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 63000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 64000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 65000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 66000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 67000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 68000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 69000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 70000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 71000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 72000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 73000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 74000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 75000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 76000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 77000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 78000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 79000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 80000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 81000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 82000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 83000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 84000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 85000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 86000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 87000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 88000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 89000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 90000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 91000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 92000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 93000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 94000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 95000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 96000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 97000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 98000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 99000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 100000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 101000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 102000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 103000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 104000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 105000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 106000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 107000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 108000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 109000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 110000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 111000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 112000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 113000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 114000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 115000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 116000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 117000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 118000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 119000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 120000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 121000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 122000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 123000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 124000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 125000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 126000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 127000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 128000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 129000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 130000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 131000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 132000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 133000 / 500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computed conditional probabilities for sample 134000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 135000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 136000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 137000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 138000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 139000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 140000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 141000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 142000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 143000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 144000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 145000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 146000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 147000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 148000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 149000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 150000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 151000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 152000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 153000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 154000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 155000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 156000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 157000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 158000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 159000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 160000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 161000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 162000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 163000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 164000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 165000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 166000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 167000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 168000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 169000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 170000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 171000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 172000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 173000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 174000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 175000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 176000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 177000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 178000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 179000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 180000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 181000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 182000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 183000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 184000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 185000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 186000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 187000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 188000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 189000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 190000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 191000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 192000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 193000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 194000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 195000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 196000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 197000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 198000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 199000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 200000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 201000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 202000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 203000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 204000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 205000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 206000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 207000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 208000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 209000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 210000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 211000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 212000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 213000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 214000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 215000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 216000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 217000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 218000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 219000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 220000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 221000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 222000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 223000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 224000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 225000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 226000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 227000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 228000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 229000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 230000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 231000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 232000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 233000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 234000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 235000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 236000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 237000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 238000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 239000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 240000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 241000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 242000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 243000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 244000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 245000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 246000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 247000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 248000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 249000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 250000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 251000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 252000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 253000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 254000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 255000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 256000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 257000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 258000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 259000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 260000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 261000 / 500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computed conditional probabilities for sample 262000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 263000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 264000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 265000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 266000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 267000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 268000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 269000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 270000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 271000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 272000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 273000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 274000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 275000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 276000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 277000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 278000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 279000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 280000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 281000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 282000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 283000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 284000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 285000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 286000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 287000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 288000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 289000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 290000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 291000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 292000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 293000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 294000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 295000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 296000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 297000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 298000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 299000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 300000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 301000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 302000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 303000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 304000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 305000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 306000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 307000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 308000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 309000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 310000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 311000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 312000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 313000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 314000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 315000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 316000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 317000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 318000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 319000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 320000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 321000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 322000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 323000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 324000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 325000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 326000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 327000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 328000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 329000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 330000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 331000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 332000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 333000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 334000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 335000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 336000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 337000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 338000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 339000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 340000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 341000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 342000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 343000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 344000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 345000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 346000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 347000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 348000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 349000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 350000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 351000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 352000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 353000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 354000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 355000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 356000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 357000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 358000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 359000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 360000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 361000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 362000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 363000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 364000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 365000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 366000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 367000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 368000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 369000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 370000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 371000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 372000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 373000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 374000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 375000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 376000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 377000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 378000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 379000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 380000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 381000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 382000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 383000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 384000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 385000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 386000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 387000 / 500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computed conditional probabilities for sample 388000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 389000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 390000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 391000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 392000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 393000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 394000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 395000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 396000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 397000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 398000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 399000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 400000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 401000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 402000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 403000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 404000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 405000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 406000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 407000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 408000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 409000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 410000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 411000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 412000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 413000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 414000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 415000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 416000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 417000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 418000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 419000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 420000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 421000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 422000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 423000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 424000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 425000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 426000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 427000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 428000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 429000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 430000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 431000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 432000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 433000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 434000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 435000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 436000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 437000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 438000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 439000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 440000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 441000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 442000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 443000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 444000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 445000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 446000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 447000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 448000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 449000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 450000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 451000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 452000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 453000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 454000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 455000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 456000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 457000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 458000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 459000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 460000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 461000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 462000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 463000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 464000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 465000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 466000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 467000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 468000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 469000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 470000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 471000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 472000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 473000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 474000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 475000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 476000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 477000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 478000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 479000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 480000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 481000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 482000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 483000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 484000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 485000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 486000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 487000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 488000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 489000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 490000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 491000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 492000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 493000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 494000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 495000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 496000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 497000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 498000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 499000 / 500000\n",
      "[t-SNE] Computed conditional probabilities for sample 500000 / 500000\n",
      "[t-SNE] Mean sigma: 0.113243\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 110.984383\n",
      "[t-SNE] KL divergence after 300 iterations: 7.044293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=30, n_iter=300, init = 'pca')\n",
    "tsne_results = tsne.fit_transform(X_train_test_combined_tSNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tsne_results_df = pd.DataFrame(tsne_results)\n",
    "# tsne_results_df.to_pickle(\"tsneResult.pkl\")\n",
    "tsne_results = pd.read_pickle(\"tsneResult.pkl\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_test_combined_tSNE['tsne-one'] = tsne_results[:,0]\n",
    "X_train_test_combined_tSNE['tsne-two'] = tsne_results[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,20)\n",
    "\n",
    "plt.title('Grau Unlabeled, Blau: Fraud, Rot: No fraud')\n",
    "\n",
    "plt.scatter(X_train_test_combined_tSNE[\"tsne-one\"][Y_train_test_combined == -1].values, X_train_test_combined_tSNE[\"tsne-two\"][Y_train_test_combined == -1].values, color='grey', alpha = 0.1)\n",
    "plt.scatter(X_train_test_combined_tSNE[\"tsne-one\"][Y_train_test_combined == 0], X_train_test_combined_tSNE[\"tsne-two\"][Y_train_test_combined == 0], color='r')\n",
    "plt.scatter(X_train_test_combined_tSNE[\"tsne-one\"][Y_train_test_combined == 1], X_train_test_combined_tSNE[\"tsne-two\"][Y_train_test_combined == 1], color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tsne_axis_1'] = X_train_test_combined_tSNE['tsne-one'].head(len(train))\n",
    "train['tsne_axis_2'] = X_train_test_combined_tSNE['tsne-two'].head(len(train))\n",
    "\n",
    "test['tsne_axis_1'] = X_train_test_combined_tSNE['tsne-one'].tail(len(test)).reset_index(drop = True)\n",
    "test['tsne_axis_2'] = X_train_test_combined_tSNE['tsne-two'].tail(len(test)).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out center of clusters -> *this doesnt apply to the current solution yet -> dont run it*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1dd4006cb70>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "plt.scatter(train['tsne_axis_1'][train['fraud'] == 1], train['tsne_axis_2'][train['fraud'] == 1], color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x17617fcafd0>"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 9)\n",
    "\n",
    "combined_data = pd.DataFrame(train['tsne_axis_1'][train['fraud'] == 1])\n",
    "combined_data['tsne_axis_2'] = train['tsne_axis_2'][train['fraud'] == 1]\n",
    "\n",
    "kmeans.fit_predict(combined_data)\n",
    "\n",
    "plt.scatter(train['tsne_axis_1'][train['fraud'] == 1], train['tsne_axis_2'][train['fraud'] == 1], color='blue')\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "plt.scatter(cluster_centers[:,0], cluster_centers[:,1], color='red', marker = \"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance to cluster centers as additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dist(points1, points2):\n",
    "    result = []\n",
    "    for i in range(0, len(points1)):\n",
    "        result.append(np.sqrt( (points1['tsne_axis_1'][i] - points2['tsne_axis_1'][i])**2\n",
    "                              +(points1['tsne_axis_2'][i] - points2['tsne_axis_2'][i])**2))\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = pd.DataFrame(\n",
    "            {'tsne_axis_1': train['tsne_axis_1'],\n",
    "             'tsne_axis_2': train['tsne_axis_2'],\n",
    "            })\n",
    "coordinates['tsne_axis_1']\n",
    "\n",
    "cluster_center_1 = pd.DataFrame([cluster_centers[0]] * len(train))\n",
    "cluster_center_2 = pd.DataFrame([cluster_centers[1]] * len(train))\n",
    "cluster_center_3 = pd.DataFrame([cluster_centers[2]] * len(train))\n",
    "cluster_center_4 = pd.DataFrame([cluster_centers[0]] * len(train))\n",
    "cluster_center_5 = pd.DataFrame([cluster_centers[1]] * len(train))\n",
    "cluster_center_6 = pd.DataFrame([cluster_centers[2]] * len(train))\n",
    "cluster_center_7 = pd.DataFrame([cluster_centers[0]] * len(train))\n",
    "cluster_center_8 = pd.DataFrame([cluster_centers[1]] * len(train))\n",
    "cluster_center_9 = pd.DataFrame([cluster_centers[2]] * len(train))\n",
    "\n",
    "cluster_center_1.columns = ['tsne_axis_1', 'tsne_axis_2']\n",
    "cluster_center_2.columns = ['tsne_axis_1', 'tsne_axis_2']\n",
    "cluster_center_3.columns = ['tsne_axis_1', 'tsne_axis_2']\n",
    "cluster_center_4.columns = ['tsne_axis_1', 'tsne_axis_2']\n",
    "cluster_center_5.columns = ['tsne_axis_1', 'tsne_axis_2']\n",
    "cluster_center_6.columns = ['tsne_axis_1', 'tsne_axis_2']\n",
    "cluster_center_7.columns = ['tsne_axis_1', 'tsne_axis_2']\n",
    "cluster_center_8.columns = ['tsne_axis_1', 'tsne_axis_2']\n",
    "cluster_center_9.columns = ['tsne_axis_1', 'tsne_axis_2']\n",
    "\n",
    "# don't use distance to cluster centers because it 'does not preserve distances nor density' (https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne)\n",
    "\n",
    "#train['distance_cluster_center_1'] = dist(cluster_center_1, coordinates)\n",
    "#train['distance_cluster_center_2'] = dist(cluster_center_2, coordinates)\n",
    "#train['distance_cluster_center_3'] = dist(cluster_center_3, coordinates)\n",
    "#train['distance_cluster_center_4'] = dist(cluster_center_4, coordinates)\n",
    "#train['distance_cluster_center_5'] = dist(cluster_center_5, coordinates)\n",
    "#train['distance_cluster_center_6'] = dist(cluster_center_6, coordinates)\n",
    "#train['distance_cluster_center_7'] = dist(cluster_center_7, coordinates)\n",
    "#train['distance_cluster_center_8'] = dist(cluster_center_8, coordinates)\n",
    "#train['distance_cluster_center_9'] = dist(cluster_center_9, coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the fraud label from the test data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop('fraud',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tsne_axis_2</th>\n",
       "      <td>0.178372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca_axis_2</th>\n",
       "      <td>0.166534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scannedLineItems</th>\n",
       "      <td>0.148536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trustLevel</th>\n",
       "      <td>0.122207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <td>0.044537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <td>0.033547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <td>0.033313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scansWithoutRegistrationPerScannedLineItem</th>\n",
       "      <td>0.029109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantityModificationsPerScannedLineItem</th>\n",
       "      <td>0.028409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineItemVoids</th>\n",
       "      <td>0.022987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <td>0.019729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valuePerSecond</th>\n",
       "      <td>0.018961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsne_axis_1</th>\n",
       "      <td>0.018688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pricePerScannedLineItem</th>\n",
       "      <td>0.017403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca_axis_1</th>\n",
       "      <td>0.016765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secondsPerEuro</th>\n",
       "      <td>0.015621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grandTotal</th>\n",
       "      <td>0.014028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantityModifications</th>\n",
       "      <td>0.012599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scansWithoutRegistrationPerSecond</th>\n",
       "      <td>0.012489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineItemVoidsPerEuro</th>\n",
       "      <td>0.011725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineItemVoidsPerSecond</th>\n",
       "      <td>0.010217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantityModificationsPerSecond</th>\n",
       "      <td>0.008567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantityModificationsPerEuro</th>\n",
       "      <td>0.008194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scansWithoutRegistrationPerEuro</th>\n",
       "      <td>0.007461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Importance\n",
       "tsne_axis_2                                   0.178372\n",
       "pca_axis_2                                    0.166534\n",
       "scannedLineItems                              0.148536\n",
       "trustLevel                                    0.122207\n",
       "totalScanTimeInSeconds                        0.044537\n",
       "scannedLineItemsPerSecond                     0.033547\n",
       "scansWithoutRegistration                      0.033313\n",
       "scansWithoutRegistrationPerScannedLineItem    0.029109\n",
       "quantityModificationsPerScannedLineItem       0.028409\n",
       "lineItemVoids                                 0.022987\n",
       "lineItemVoidsPerPosition                      0.019729\n",
       "valuePerSecond                                0.018961\n",
       "tsne_axis_1                                   0.018688\n",
       "pricePerScannedLineItem                       0.017403\n",
       "pca_axis_1                                    0.016765\n",
       "secondsPerEuro                                0.015621\n",
       "grandTotal                                    0.014028\n",
       "quantityModifications                         0.012599\n",
       "scansWithoutRegistrationPerSecond             0.012489\n",
       "lineItemVoidsPerEuro                          0.011725\n",
       "lineItemVoidsPerSecond                        0.010217\n",
       "quantityModificationsPerSecond                0.008567\n",
       "quantityModificationsPerEuro                  0.008194\n",
       "scansWithoutRegistrationPerEuro               0.007461"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "y = train['fraud']\n",
    "x = train.drop('fraud',axis=1)\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier(criterion = 'entropy')\n",
    "model.fit(x, y)\n",
    "\n",
    "pd.DataFrame(model.feature_importances_, list(x), columns =['Importance']).sort_values(by='Importance', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying oversampling to dataset -> works but we will probably won't use it\n",
    "- Classical Oversampling\n",
    "- SMOTE Technique\n",
    "- ADASYN Technique\n",
    "\n",
    "Each one has a slightly different approach for generating synthetic instances\n",
    "- Simply duplicated fraud instances\n",
    "- ADASYN focuses on generating samples next to the original samples which are wrongly classified using a k-Nearest Neighbors classifier\n",
    "- SMOTE will not make any distinction between easy and hard samples to be classified using the nearest neighbors rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1775\n",
      "0    1775\n",
      "Name: fraud, dtype: int64\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# !pip install imblearn  # might be necessary for installation\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def randomOverSampling(train):\n",
    "    ros = RandomOverSampler(random_state=42, ratio = 1)\n",
    "    X_train_extended, Y_train_extended = ros.fit_resample(train.drop('fraud',axis=1), train['fraud'])\n",
    "\n",
    "    new_train = pd.DataFrame(X_train_extended, columns=train.drop('fraud',axis=1).columns) \n",
    "    new_train[\"fraud\"] = Y_train_extended\n",
    "    return new_train\n",
    "\n",
    "print(randomOverSampling(train).fraud.value_counts())\n",
    "print(randomOverSampling(train).fraud.value_counts() / len(randomOverSampling(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1775\n",
      "0    1775\n",
      "Name: fraud, dtype: int64\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#!pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def smoteOverSamling(train):\n",
    "    sm = SMOTE(random_state=42, k_neighbors = 3, ratio = 1)\n",
    "    X_train_extended, Y_train_extended = sm.fit_sample(train.drop('fraud',axis=1), train['fraud'])\n",
    "\n",
    "    new_train = pd.DataFrame(X_train_extended, columns=train.drop('fraud',axis=1).columns) \n",
    "    new_train [\"fraud\"] = Y_train_extended\n",
    "    return new_train\n",
    "    \n",
    "print(smoteOverSamling(train).fraud.value_counts())\n",
    "print(smoteOverSamling(train).fraud.value_counts() / len(smoteOverSamling(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1820\n",
      "0    1775\n",
      "Name: fraud, dtype: int64\n",
      "1    0.506259\n",
      "0    0.493741\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "def adasynOverSamling(train):\n",
    "    ada = ADASYN(random_state=42, n_neighbors = 3, ratio = 1)\n",
    "    X_train_extended, Y_train_extended = ada.fit_resample(train.drop('fraud',axis=1), train['fraud'])\n",
    "\n",
    "    new_train = pd.DataFrame(X_train_extended, columns=train.drop('fraud',axis=1).columns) \n",
    "    new_train[\"fraud\"] = Y_train_extended\n",
    "    return new_train\n",
    "    \n",
    "print(adasynOverSamling(train).fraud.value_counts())\n",
    "print(adasynOverSamling(train).fraud.value_counts() / len(adasynOverSamling(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually apply one of these techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = smoteOverSamling(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Custom Score Function based on the given cost matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    score = ((-25)*fp + (-5)*fn + 5*tp) / len(y_true)\n",
    "    return (score)\n",
    "\n",
    "my_custom_score = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "loo = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building model factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "#from sklearn.multioutput import ClassifierChain\n",
    "#from sklearn.multioutput import MultiOutputClassifier\n",
    "#from sklearn.multiclass import OutputCodeClassifier\n",
    "#from sklearn.multiclass import OneVsOneClassifier\n",
    "#from sklearn.multiclass import OneVsRestClassifier\n",
    "#from sklearn.ensemble.voting_classifier import VotingClassifier\n",
    "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.bagging import BaggingClassifier\n",
    "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "model_tuning_factory = [   \n",
    "    GridSearchCV(LogisticRegression(max_iter = 10000), \n",
    "                 dict(solver = ['newton-cg', 'lbfgs', 'liblinear'], \n",
    "                      fit_intercept = [True, False],\n",
    "                      C = np.arange(0.1, 2.0, 0.1)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(KNeighborsClassifier(), \n",
    "                 dict(\n",
    "                     n_neighbors = [1, 3, 5, 10, 15],\n",
    "                     weights = ['uniform', 'distance'],\n",
    "                     p = [1, 2, 3]\n",
    "                 ),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "#    GridSearchCV(NearestCentroid(),     # cause some problems\n",
    "#                 dict(metric = ['euclidean', 'manhattan', 'chebyshev', 'minkowski', 'mahalanobis']),\n",
    "#                 cv = skf,\n",
    "#                 scoring = my_custom_score,\n",
    "#                 refit = True,\n",
    "#                  n_jobs = -1),\n",
    "    GridSearchCV(DecisionTreeClassifier(),\n",
    "                 dict(criterion = ['entropy', 'gini'],\n",
    "                      splitter = ['best', 'random'],\n",
    "                      max_depth = range(1,50,2),\n",
    "                      min_samples_split = range(2,10,2)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(ExtraTreeClassifier(),\n",
    "                 dict(criterion = ['entropy', 'gini'],\n",
    "                      splitter = ['best', 'random'],\n",
    "                      max_depth = range(1,50,2),\n",
    "                      min_samples_split = range(2,10,2)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(ExtraTreesClassifier(),\n",
    "                 dict(n_estimators = range(5,200,5),\n",
    "                      criterion = ['entropy', 'gini']),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),    \n",
    "    GridSearchCV(RandomForestClassifier(),\n",
    "                 dict(n_estimators = range(5,200,5),\n",
    "                      criterion = ['entropy', 'gini']),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),    \n",
    "    GridSearchCV(BernoulliNB(),\n",
    "                 dict(binarize  = [0.0, 0.2, 0.5, 1, 2],\n",
    "                      alpha = [0, 0.2, 0.5, 1, 2]),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),     \n",
    "    GridSearchCV(GaussianNB(),\n",
    "                 dict(),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),     \n",
    "    GridSearchCV(GradientBoostingClassifier(),\n",
    "                 dict(loss = ['deviance', 'exponential'],\n",
    "                      n_estimators = [20, 50,100, 150, 200]),\n",
    "                 #dict(n_estimators = range(1,150)),\n",
    "                 #     learning_rate = np.arange(0.01, 1.0, 0.01)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(AdaBoostClassifier(),\n",
    "                 dict(n_estimators = [20, 50,100, 150, 200],\n",
    "                      algorithm = ['SAMME', 'SAMME.R'],\n",
    "                      base_estimator = [DecisionTreeClassifier(max_depth=1), # default setting\n",
    "                                        LogisticRegression(),\n",
    "                                        ExtraTreesClassifier(),\n",
    "                              #          GradientBoostingClassifier(), # takes a long time (-> + 30 sec)\n",
    "                              #          RandomForestClassifier(),  # takes a long time (-> + 30 sec)\n",
    "                                        DecisionTreeClassifier(),\n",
    "                                        ExtraTreeClassifier(),\n",
    "                                        None] # default\n",
    "                      ),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1), \n",
    "    GridSearchCV(XGBClassifier(),\n",
    "                 dict(objective = ['binary:logistic'],\n",
    "                      eval_metric = ['error'],\n",
    "                      base_score = [0.3, 0.5, 0.7],\n",
    "                      learning_rate = [0.01, 0.1, 0.2, 0.3],\n",
    "                      n_estimators = [10, 20, 50, 100]),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(XGBClassifier(),\n",
    "                 dict(objective = ['binary:logistic'],\n",
    "                      eval_metric = ['error'],\n",
    "                   #   base_score = [0.3, 0.5, 0.7],\n",
    "                   #   learning_rate = [0.01, 0.1, 0.2, 0.3],\n",
    "                      n_estimators = [10, 20, 50, 100],\n",
    "                      base_estimator = [DecisionTreeClassifier(max_depth=1), # default setting for AdaBoost\n",
    "                                        LogisticRegression(),\n",
    "                                        ExtraTreesClassifier(),\n",
    "                              #          GradientBoostingClassifier(), # takes a long time (-> + 30 sec)\n",
    "                              #          RandomForestClassifier(),  # takes a long time (-> + 30 sec)\n",
    "                                        DecisionTreeClassifier(),\n",
    "                                        ExtraTreeClassifier(),\n",
    "                                        None]),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(BaggingClassifier(), \n",
    "                 dict(n_estimators = [5, 10, 20, 50, 100, 150, 200],\n",
    "                      base_estimator = [DecisionTreeClassifier(max_depth=1), # default setting for AdaBoost\n",
    "                                        LogisticRegression(),\n",
    "                                        ExtraTreesClassifier(),\n",
    "                              #          GradientBoostingClassifier(), # takes a long time (-> + 30 sec)\n",
    "                              #          RandomForestClassifier(),  # takes a long time (-> + 30 sec)\n",
    "                                        DecisionTreeClassifier(),\n",
    "                                        ExtraTreeClassifier(),\n",
    "                                        None]),   # standard -> 'Decision Tree'\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1), \n",
    "    GridSearchCV(Perceptron(),\n",
    "                 dict(penalty = ['l2', 'l1', 'elasticnet', None],\n",
    "                      alpha = np.arange(0.0005, 0.001, 0.0005),\n",
    "                      fit_intercept = [True, False],\n",
    "                      max_iter = range(5,100,5)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1), \n",
    "    GridSearchCV(MLPClassifier(),\n",
    "                 dict(solver = ['lbfgs', 'sgd', 'adam'],\n",
    "                      activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "                      learning_rate = ['constant', 'invscaling', 'adaptive']),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1), \n",
    "#    GridSearchCV(LinearDiscriminantAnalysis(),                       # tsne dimensions cause problems\n",
    "#                 dict(solver  = ['svd', 'lsqr', 'eigen'],\n",
    "#                      n_components = range(1,20)),\n",
    "#                 cv = skf,\n",
    "#                 scoring = my_custom_score,\n",
    "#                 refit = True,\n",
    "#                 n_jobs = -1),\n",
    "    GridSearchCV(QuadraticDiscriminantAnalysis(),\n",
    "                 dict(reg_param = np.arange(0.1, 1.0, 0.1)),\n",
    "                 cv = skf,\n",
    "                 scoring = my_custom_score,\n",
    "                 refit = True,\n",
    "                 n_jobs = -1),\n",
    "    GridSearchCV(SVC(),                                           # gets very slow at some point\n",
    "                 dict(C = [0.01, 0.1, 0.5, 1, 2]),\n",
    "                  cv = skf,\n",
    "                  scoring = my_custom_score,\n",
    "                  refit = True,\n",
    "                  n_jobs = -1)\n",
    "]       \n",
    "\n",
    "\n",
    "model_tuning_factory_temp = [\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished LogisticRegression with No Oversampling and No Scaling and 1 features after 3.52 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 2 features after 1.87 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 3 features after 2.22 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 4 features after 2.59 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 5 features after 13.36 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 6 features after 11.94 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 7 features after 10.26 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 8 features after 12.3 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 9 features after 11.12 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 10 features after 16.52 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 11 features after 18.02 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 12 features after 19.78 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 13 features after 24.92 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 14 features after 16.89 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 15 features after 25.63 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 16 features after 17.87 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 17 features after 25.04 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 18 features after 26.9 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 19 features after 29.29 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 20 features after 38.66 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 21 features after 49.25 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 22 features after 59.49 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 23 features after 108.85 seconds\n",
      "Finished LogisticRegression with No Oversampling and No Scaling and 24 features after 61.88 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 1 features after 1.74 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 2 features after 1.96 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 3 features after 2.17 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 4 features after 2.82 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 5 features after 2.37 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 6 features after 2.59 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 7 features after 2.37 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 8 features after 2.64 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 9 features after 2.86 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 10 features after 2.97 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 11 features after 2.91 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 12 features after 3.12 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 13 features after 3.04 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 14 features after 3.18 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 15 features after 3.22 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 16 features after 3.29 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 17 features after 3.73 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 18 features after 3.24 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 19 features after 3.54 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 20 features after 5.13 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 21 features after 5.09 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 22 features after 5.61 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 23 features after 5.47 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and No Scaling and 24 features after 5.25 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 1 features after 8.57 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 2 features after 7.03 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 3 features after 8.52 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 4 features after 8.93 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 5 features after 17.96 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 6 features after 9.28 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 7 features after 10.23 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 8 features after 7.53 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 9 features after 5.51 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 10 features after 5.78 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 11 features after 5.78 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 12 features after 5.92 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 13 features after 8.68 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 14 features after 9.47 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 15 features after 10.49 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 16 features after 11.37 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 17 features after 11.55 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 18 features after 11.04 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 19 features after 19.96 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 20 features after 13.92 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 21 features after 11.57 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 22 features after 24.79 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 23 features after 14.62 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and No Scaling and 24 features after 12.65 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 1 features after 8.88 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 2 features after 7.88 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 3 features after 8.44 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 4 features after 8.94 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 5 features after 8.69 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 6 features after 8.71 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 7 features after 8.59 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 8 features after 8.22 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 9 features after 8.69 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 10 features after 9.15 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 11 features after 9.1 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 12 features after 9.62 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 13 features after 6.95 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 14 features after 9.2 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 15 features after 10.04 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 16 features after 9.64 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 17 features after 10.37 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 18 features after 9.46 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 19 features after 9.83 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 20 features after 9.89 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 21 features after 9.84 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 22 features after 9.52 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 23 features after 10.72 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and No Scaling and 24 features after 10.45 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 1 features after 29.0 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 2 features after 22.44 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 3 features after 27.61 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 4 features after 28.69 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 5 features after 25.05 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 6 features after 22.19 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 7 features after 22.94 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 8 features after 26.26 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 9 features after 30.02 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 10 features after 29.52 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 11 features after 30.17 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 12 features after 30.07 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 13 features after 35.7 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 14 features after 29.04 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 15 features after 31.98 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 16 features after 32.02 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 17 features after 30.12 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 18 features after 31.76 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 19 features after 27.32 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 20 features after 31.86 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 21 features after 23.3 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 22 features after 31.18 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 23 features after 32.14 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and No Scaling and 24 features after 32.38 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 1 features after 35.56 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 2 features after 35.46 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 3 features after 32.77 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 4 features after 40.75 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 5 features after 36.64 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 6 features after 35.66 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 7 features after 38.5 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 8 features after 38.53 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 9 features after 42.37 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 10 features after 44.14 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 11 features after 43.17 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 12 features after 42.49 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 13 features after 43.48 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 14 features after 42.98 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 15 features after 43.49 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 16 features after 50.84 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 17 features after 50.77 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 18 features after 51.89 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 19 features after 52.53 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 20 features after 52.96 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 21 features after 53.44 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 22 features after 53.61 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 23 features after 54.36 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and No Scaling and 24 features after 56.91 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 1 features after 2.37 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 2 features after 0.61 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 3 features after 1.65 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 4 features after 2.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 5 features after 1.59 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 6 features after 1.92 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 7 features after 2.03 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 8 features after 1.97 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 9 features after 1.69 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 10 features after 1.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 11 features after 0.8 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 12 features after 0.72 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 13 features after 0.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 14 features after 0.56 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 15 features after 0.92 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 16 features after 1.9 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 17 features after 1.25 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 18 features after 1.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 19 features after 1.45 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 20 features after 2.13 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 21 features after 2.02 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished BernoulliNB with No Oversampling and No Scaling and 22 features after 1.43 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 23 features after 1.84 seconds\n",
      "Finished BernoulliNB with No Oversampling and No Scaling and 24 features after 1.77 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 1 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 2 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 3 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 4 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 5 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 6 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 7 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 8 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 9 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 10 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 11 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 12 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 13 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 14 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 15 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 16 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 17 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 18 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 19 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 20 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 21 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 22 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 23 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and No Scaling and 24 features after 0.07 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 1 features after 9.5 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 2 features after 10.04 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 3 features after 10.08 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 4 features after 10.62 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 5 features after 9.97 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 6 features after 12.46 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 7 features after 10.03 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 8 features after 12.18 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 9 features after 11.83 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 10 features after 12.84 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 11 features after 12.13 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 12 features after 10.68 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 13 features after 10.47 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 14 features after 9.39 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 15 features after 8.74 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 16 features after 8.1 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 17 features after 7.33 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 18 features after 7.58 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 19 features after 7.94 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 20 features after 8.35 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 21 features after 7.58 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 22 features after 7.43 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 23 features after 7.58 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and No Scaling and 24 features after 8.31 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 1 features after 17.71 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 2 features after 19.33 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 3 features after 19.78 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 4 features after 23.89 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 5 features after 24.78 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 6 features after 23.56 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 7 features after 23.86 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 8 features after 19.68 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 9 features after 28.33 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 10 features after 23.69 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 11 features after 22.25 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 12 features after 29.67 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 13 features after 25.54 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 14 features after 29.42 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 15 features after 28.92 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 16 features after 40.84 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 17 features after 37.71 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 18 features after 44.04 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 19 features after 31.49 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 20 features after 44.26 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 21 features after 45.82 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 22 features after 36.59 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 23 features after 47.65 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and No Scaling and 24 features after 49.01 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 1 features after 42.49 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 2 features after 35.16 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 3 features after 34.25 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 4 features after 36.5 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 5 features after 39.18 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 6 features after 38.09 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 7 features after 37.82 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 8 features after 37.42 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 9 features after 34.91 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished XGBClassifier with No Oversampling and No Scaling and 10 features after 34.43 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 11 features after 38.53 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 12 features after 37.21 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 13 features after 37.26 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 14 features after 35.07 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 15 features after 35.91 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 16 features after 37.64 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 17 features after 26.01 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 18 features after 33.01 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 19 features after 27.82 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 20 features after 31.36 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 21 features after 28.82 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 22 features after 32.77 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 23 features after 27.31 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 24 features after 33.29 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 1 features after 20.29 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 2 features after 17.97 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 3 features after 21.92 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 4 features after 22.5 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 5 features after 21.55 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 6 features after 15.37 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 7 features after 11.69 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 8 features after 11.55 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 9 features after 11.38 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 10 features after 13.07 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 11 features after 11.35 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 12 features after 11.25 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 13 features after 11.27 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 14 features after 13.39 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 15 features after 11.33 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 16 features after 11.9 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 17 features after 11.1 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 18 features after 10.78 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 19 features after 10.5 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 20 features after 10.32 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 21 features after 11.63 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 22 features after 10.42 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 23 features after 10.36 seconds\n",
      "Finished XGBClassifier with No Oversampling and No Scaling and 24 features after 10.05 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 1 features after 28.17 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 2 features after 28.62 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 3 features after 26.45 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 4 features after 30.59 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 5 features after 27.85 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 6 features after 21.05 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 7 features after 31.24 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 8 features after 32.05 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 9 features after 34.34 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 10 features after 37.66 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 11 features after 23.62 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 12 features after 34.95 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 13 features after 36.07 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 14 features after 36.76 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 15 features after 36.61 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 16 features after 36.89 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 17 features after 31.14 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 18 features after 45.33 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 19 features after 38.78 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 20 features after 88.03 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 21 features after 118.04 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 22 features after 99.65 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 23 features after 122.78 seconds\n",
      "Finished BaggingClassifier with No Oversampling and No Scaling and 24 features after 134.02 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 1 features after 7.39 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 2 features after 7.37 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 3 features after 7.96 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 4 features after 7.66 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 5 features after 13.25 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 6 features after 11.92 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 7 features after 16.47 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 8 features after 19.09 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 9 features after 20.24 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 10 features after 21.72 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 11 features after 31.33 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 12 features after 9.94 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 13 features after 21.47 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 14 features after 35.18 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 15 features after 28.9 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 16 features after 48.07 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 17 features after 28.57 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 18 features after 7.58 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 19 features after 8.25 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 20 features after 8.92 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 21 features after 9.51 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 22 features after 6.33 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 23 features after 10.19 seconds\n",
      "Finished Perceptron with No Oversampling and No Scaling and 24 features after 12.43 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished MLPClassifier with No Oversampling and No Scaling and 1 features after 37.09 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 2 features after 45.17 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 3 features after 48.2 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 4 features after 45.98 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 5 features after 32.88 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 6 features after 32.47 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 7 features after 35.18 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 8 features after 35.31 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 9 features after 38.72 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 10 features after 39.35 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 11 features after 30.56 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 12 features after 37.67 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 13 features after 38.96 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 14 features after 33.11 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 15 features after 39.86 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 16 features after 42.75 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 17 features after 42.45 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 18 features after 25.74 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 19 features after 37.96 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 20 features after 43.77 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 21 features after 43.95 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 22 features after 45.69 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 23 features after 46.1 seconds\n",
      "Finished MLPClassifier with No Oversampling and No Scaling and 24 features after 45.65 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 1 features after 0.21 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 2 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 3 features after 0.67 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 4 features after 0.92 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 5 features after 1.13 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 6 features after 1.17 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 7 features after 1.37 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 8 features after 1.94 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 9 features after 1.76 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 10 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 11 features after 0.3 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 12 features after 2.81 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 13 features after 1.2 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 14 features after 1.93 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 15 features after 0.31 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 16 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 17 features after 0.31 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 18 features after 0.28 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 19 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 20 features after 0.29 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 21 features after 1.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 22 features after 1.04 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 23 features after 1.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and No Scaling and 24 features after 2.38 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 1 features after 0.33 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 2 features after 0.37 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 3 features after 0.33 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 4 features after 0.31 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 5 features after 4.16 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 6 features after 4.23 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 7 features after 2.52 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 8 features after 3.96 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 9 features after 4.06 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 10 features after 4.32 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 11 features after 3.43 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 12 features after 4.52 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 13 features after 4.13 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 14 features after 3.52 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 15 features after 4.62 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 16 features after 4.0 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 17 features after 4.15 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 18 features after 4.55 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 19 features after 4.19 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 20 features after 2.82 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 21 features after 5.03 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 22 features after 4.75 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 23 features after 4.66 seconds\n",
      "Finished SVC with No Oversampling and No Scaling and 24 features after 4.85 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 1 features after 3.49 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 2 features after 4.14 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 3 features after 16.22 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 4 features after 12.82 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 5 features after 17.52 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 6 features after 17.99 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 7 features after 15.49 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 8 features after 16.89 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 9 features after 15.53 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 10 features after 24.78 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 11 features after 21.64 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 12 features after 22.82 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 13 features after 33.06 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 14 features after 26.05 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 15 features after 33.91 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 16 features after 27.43 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 17 features after 24.18 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 18 features after 25.47 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 19 features after 28.29 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 20 features after 45.29 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 21 features after 64.23 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 22 features after 68.7 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 23 features after 101.24 seconds\n",
      "Finished LogisticRegression with No Oversampling and MinMaxScaler and 24 features after 61.0 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 1 features after 1.05 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 2 features after 1.16 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 3 features after 1.85 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 4 features after 1.67 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 5 features after 1.68 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 6 features after 1.32 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 7 features after 1.34 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 8 features after 2.05 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 9 features after 1.72 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 10 features after 1.96 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 11 features after 1.97 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 12 features after 1.7 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 13 features after 2.04 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 14 features after 2.08 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 15 features after 2.1 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 16 features after 1.92 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 17 features after 2.62 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 18 features after 2.7 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 19 features after 2.31 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 20 features after 4.21 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 21 features after 3.36 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 22 features after 4.2 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 23 features after 4.13 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and MinMaxScaler and 24 features after 3.97 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 1 features after 5.62 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 2 features after 6.35 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 3 features after 5.95 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 4 features after 7.74 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 5 features after 6.53 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 6 features after 6.86 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 7 features after 7.74 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 8 features after 7.69 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 9 features after 7.37 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 10 features after 9.4 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 11 features after 8.66 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 12 features after 8.62 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 13 features after 7.12 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 14 features after 6.94 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 15 features after 7.13 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 16 features after 7.53 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 17 features after 7.46 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 18 features after 7.68 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 19 features after 8.18 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 20 features after 8.57 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 21 features after 8.3 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 22 features after 8.06 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 23 features after 7.95 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and MinMaxScaler and 24 features after 7.84 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 1 features after 6.92 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 2 features after 6.3 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 3 features after 6.29 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 4 features after 7.15 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 5 features after 6.21 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 6 features after 6.2 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 7 features after 6.69 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 8 features after 6.4 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 9 features after 5.18 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 10 features after 5.38 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 11 features after 6.83 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 12 features after 7.34 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 13 features after 6.47 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 14 features after 6.56 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 15 features after 6.68 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 16 features after 6.71 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 17 features after 6.84 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 18 features after 6.78 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 19 features after 5.71 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 20 features after 5.52 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 21 features after 5.51 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 22 features after 8.08 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 23 features after 7.07 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and MinMaxScaler and 24 features after 6.71 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 1 features after 16.67 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 2 features after 16.72 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 3 features after 16.72 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 4 features after 19.05 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 5 features after 16.74 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 6 features after 17.84 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 7 features after 16.88 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 8 features after 19.51 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 9 features after 19.25 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 10 features after 18.41 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 11 features after 19.74 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 12 features after 19.26 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 13 features after 17.78 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 14 features after 18.23 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 15 features after 21.5 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 16 features after 19.12 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 17 features after 17.78 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 18 features after 21.16 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 19 features after 19.71 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 20 features after 18.13 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 21 features after 20.59 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 22 features after 19.68 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 23 features after 18.76 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and MinMaxScaler and 24 features after 20.46 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 1 features after 22.1 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 2 features after 20.85 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 3 features after 22.13 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 4 features after 23.45 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 5 features after 21.42 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 6 features after 25.39 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 7 features after 24.11 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 8 features after 22.68 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 9 features after 28.59 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 10 features after 27.91 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 11 features after 25.07 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 12 features after 27.32 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 13 features after 28.05 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 14 features after 26.65 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 15 features after 24.18 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 16 features after 32.3 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 17 features after 28.6 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 18 features after 32.48 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 19 features after 28.53 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 20 features after 33.71 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 21 features after 29.64 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 22 features after 31.61 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 23 features after 30.95 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and MinMaxScaler and 24 features after 32.52 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 1 features after 0.9 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 2 features after 0.97 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 3 features after 0.69 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 4 features after 0.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 5 features after 0.66 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 6 features after 0.66 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 7 features after 0.63 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 8 features after 0.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 9 features after 0.58 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 10 features after 0.65 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 11 features after 0.63 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 12 features after 0.61 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 13 features after 0.68 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 14 features after 0.59 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 15 features after 1.22 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 16 features after 0.97 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 17 features after 0.91 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 18 features after 1.1 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 19 features after 1.26 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 20 features after 1.44 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 21 features after 0.95 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 22 features after 0.87 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 23 features after 0.85 seconds\n",
      "Finished BernoulliNB with No Oversampling and MinMaxScaler and 24 features after 0.94 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 1 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 2 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 3 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 4 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 5 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 6 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 7 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 8 features after 0.05 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 9 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 10 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 11 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 12 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 13 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 14 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 15 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 16 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 17 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 18 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 19 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 20 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 21 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 22 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 23 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and MinMaxScaler and 24 features after 0.03 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 1 features after 6.6 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 2 features after 8.53 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 3 features after 9.54 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 4 features after 5.93 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 5 features after 10.13 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 6 features after 7.31 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 7 features after 9.55 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 8 features after 9.98 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 9 features after 8.97 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 10 features after 10.06 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 11 features after 10.76 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 12 features after 9.6 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 13 features after 10.77 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 14 features after 11.91 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 15 features after 11.71 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 16 features after 13.91 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 17 features after 10.68 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 18 features after 11.23 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 19 features after 13.7 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 20 features after 11.94 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 21 features after 11.76 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 22 features after 11.55 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 23 features after 10.47 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and MinMaxScaler and 24 features after 13.05 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 1 features after 41.16 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 2 features after 41.81 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 3 features after 37.2 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 4 features after 43.91 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 5 features after 55.55 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 6 features after 30.02 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 7 features after 23.4 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 8 features after 28.29 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 9 features after 25.56 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 10 features after 38.97 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 11 features after 35.35 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 12 features after 39.94 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 13 features after 37.09 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 14 features after 23.39 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 15 features after 23.84 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 16 features after 23.72 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 17 features after 25.43 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 18 features after 24.17 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 19 features after 31.02 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 20 features after 30.1 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 21 features after 31.72 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 22 features after 26.3 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 23 features after 33.24 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and MinMaxScaler and 24 features after 33.5 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 1 features after 20.05 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 2 features after 22.99 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 3 features after 20.05 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 4 features after 20.81 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 5 features after 20.27 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 6 features after 19.9 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 7 features after 19.63 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 8 features after 19.66 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 9 features after 22.36 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 10 features after 19.83 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 11 features after 19.6 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 12 features after 21.95 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 13 features after 19.49 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 14 features after 19.18 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 15 features after 21.34 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 16 features after 19.96 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 17 features after 18.67 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 18 features after 19.59 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 19 features after 18.85 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 20 features after 18.24 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 21 features after 18.06 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 22 features after 18.02 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 23 features after 17.77 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 24 features after 17.46 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 1 features after 15.35 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 2 features after 12.43 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 3 features after 11.83 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 4 features after 15.22 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 5 features after 12.45 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 6 features after 11.83 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 7 features after 14.71 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 8 features after 12.0 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 9 features after 11.65 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 10 features after 11.55 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 11 features after 11.81 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 12 features after 14.14 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 13 features after 11.53 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 14 features after 11.18 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 15 features after 11.17 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 16 features after 10.93 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 17 features after 10.63 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 18 features after 12.7 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 19 features after 12.84 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 20 features after 11.0 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 21 features after 10.75 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 22 features after 10.93 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 23 features after 10.2 seconds\n",
      "Finished XGBClassifier with No Oversampling and MinMaxScaler and 24 features after 11.03 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 1 features after 41.2 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 2 features after 28.29 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 3 features after 74.84 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 4 features after 85.67 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 5 features after 39.35 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 6 features after 104.29 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 7 features after 81.24 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 8 features after 90.4 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 9 features after 43.89 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 10 features after 62.32 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 11 features after 120.49 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 12 features after 67.35 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 13 features after 113.86 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 14 features after 45.55 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 15 features after 70.03 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 16 features after 77.93 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 17 features after 92.31 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 18 features after 96.55 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 19 features after 79.76 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 20 features after 118.97 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 21 features after 116.31 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 22 features after 150.58 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 23 features after 138.46 seconds\n",
      "Finished BaggingClassifier with No Oversampling and MinMaxScaler and 24 features after 188.68 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 1 features after 16.78 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 2 features after 4.55 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 3 features after 4.78 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 4 features after 4.91 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 5 features after 3.64 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 6 features after 4.16 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 7 features after 3.5 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 8 features after 4.63 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 9 features after 4.18 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 10 features after 4.23 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 11 features after 5.15 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 12 features after 4.74 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 13 features after 4.47 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 14 features after 5.57 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 15 features after 5.35 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 16 features after 4.65 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 17 features after 5.99 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 18 features after 6.02 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 19 features after 5.2 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 20 features after 5.66 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 21 features after 6.79 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 22 features after 7.02 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 23 features after 7.0 seconds\n",
      "Finished Perceptron with No Oversampling and MinMaxScaler and 24 features after 7.51 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 1 features after 23.61 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 2 features after 28.31 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 3 features after 35.87 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 4 features after 31.91 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 5 features after 27.42 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 6 features after 29.47 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 7 features after 21.31 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 8 features after 31.77 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 9 features after 25.88 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 10 features after 30.19 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 11 features after 28.44 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 12 features after 27.49 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 13 features after 26.89 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 14 features after 27.09 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 15 features after 28.02 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 16 features after 27.39 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 17 features after 26.85 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 18 features after 27.66 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 19 features after 27.02 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 20 features after 25.5 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 21 features after 29.74 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 22 features after 29.48 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 23 features after 29.6 seconds\n",
      "Finished MLPClassifier with No Oversampling and MinMaxScaler and 24 features after 28.1 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 1 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 2 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 3 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 4 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 5 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 6 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 7 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 8 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 9 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 10 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 11 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 12 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 13 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 14 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 15 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 16 features after 0.28 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 17 features after 0.28 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 18 features after 0.28 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 19 features after 0.29 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 20 features after 0.28 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 21 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 22 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 23 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and MinMaxScaler and 24 features after 0.26 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 1 features after 0.27 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 2 features after 0.29 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 3 features after 0.28 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 4 features after 0.32 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 5 features after 1.83 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 6 features after 2.77 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 7 features after 2.49 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 8 features after 2.29 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 9 features after 2.03 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 10 features after 1.78 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 11 features after 1.85 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 12 features after 1.87 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 13 features after 1.84 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 14 features after 1.86 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 15 features after 2.2 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 16 features after 2.89 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 17 features after 3.04 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 18 features after 2.84 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 19 features after 2.12 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 20 features after 2.07 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 21 features after 2.11 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 22 features after 2.99 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 23 features after 3.53 seconds\n",
      "Finished SVC with No Oversampling and MinMaxScaler and 24 features after 3.07 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 1 features after 2.17 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 2 features after 2.23 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 3 features after 2.56 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 4 features after 4.93 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 5 features after 7.88 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 6 features after 6.66 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 7 features after 6.61 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 8 features after 9.96 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 9 features after 9.92 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 10 features after 10.0 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 11 features after 11.64 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 12 features after 17.87 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 13 features after 17.53 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 14 features after 13.68 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 15 features after 12.93 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 16 features after 18.05 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 17 features after 14.04 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 18 features after 14.56 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished LogisticRegression with No Oversampling and StandardScaler and 19 features after 18.84 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 20 features after 28.87 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 21 features after 26.47 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 22 features after 31.65 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 23 features after 46.8 seconds\n",
      "Finished LogisticRegression with No Oversampling and StandardScaler and 24 features after 55.95 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 1 features after 1.06 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 2 features after 1.07 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 3 features after 1.61 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 4 features after 1.88 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 5 features after 1.91 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 6 features after 2.25 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 7 features after 2.0 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 8 features after 2.12 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 9 features after 2.16 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 10 features after 1.59 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 11 features after 1.63 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 12 features after 2.23 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 13 features after 2.62 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 14 features after 2.66 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 15 features after 2.41 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 16 features after 2.93 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 17 features after 2.61 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 18 features after 2.48 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 19 features after 2.53 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 20 features after 3.77 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 21 features after 4.19 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 22 features after 4.16 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 23 features after 3.45 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and StandardScaler and 24 features after 3.24 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 1 features after 4.46 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 2 features after 5.74 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 3 features after 6.92 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 4 features after 7.01 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 5 features after 6.85 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 6 features after 6.94 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 7 features after 7.41 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 8 features after 7.16 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 9 features after 6.91 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 10 features after 7.8 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 11 features after 6.93 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 12 features after 8.77 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 13 features after 6.96 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 14 features after 6.82 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 15 features after 6.71 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 16 features after 6.88 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 17 features after 7.07 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 18 features after 8.76 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 19 features after 9.09 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 20 features after 8.87 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 21 features after 8.19 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 22 features after 7.85 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 23 features after 9.1 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and StandardScaler and 24 features after 8.43 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 1 features after 5.7 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 2 features after 5.2 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 3 features after 5.11 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 4 features after 4.8 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 5 features after 4.75 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 6 features after 4.85 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 7 features after 4.97 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 8 features after 6.79 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 9 features after 7.52 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 10 features after 7.19 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 11 features after 5.74 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 12 features after 5.57 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 13 features after 5.2 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 14 features after 5.36 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 15 features after 6.49 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 16 features after 6.7 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 17 features after 7.38 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 18 features after 7.26 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 19 features after 5.74 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 20 features after 5.59 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 21 features after 5.72 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 22 features after 6.83 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 23 features after 6.89 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and StandardScaler and 24 features after 8.06 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 1 features after 16.2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 2 features after 18.06 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 3 features after 15.85 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 4 features after 17.5 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 5 features after 16.59 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 6 features after 21.02 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 7 features after 16.76 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 8 features after 19.92 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 9 features after 23.59 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 10 features after 24.68 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 11 features after 25.68 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 12 features after 28.35 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 13 features after 25.68 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 14 features after 24.53 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 15 features after 28.15 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 16 features after 29.39 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 17 features after 26.72 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 18 features after 28.45 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 19 features after 27.1 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 20 features after 30.5 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 21 features after 27.95 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 22 features after 27.39 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 23 features after 29.34 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and StandardScaler and 24 features after 28.4 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 1 features after 25.42 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 2 features after 27.17 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 3 features after 24.0 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 4 features after 28.86 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 5 features after 29.27 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 6 features after 29.18 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 7 features after 30.83 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 8 features after 30.16 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 9 features after 34.43 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 10 features after 34.28 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 11 features after 23.18 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 12 features after 24.35 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 13 features after 25.18 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 14 features after 26.32 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 15 features after 24.86 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 16 features after 27.78 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 17 features after 28.26 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 18 features after 32.99 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 19 features after 28.68 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 20 features after 29.72 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 21 features after 30.18 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 22 features after 29.21 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 23 features after 29.57 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and StandardScaler and 24 features after 29.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 1 features after 0.51 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 2 features after 0.98 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 3 features after 1.0 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 4 features after 0.97 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 5 features after 0.89 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 6 features after 0.88 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 7 features after 0.8 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 8 features after 0.77 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 9 features after 0.76 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 10 features after 0.72 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 11 features after 0.7 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 12 features after 0.7 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 13 features after 0.69 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 14 features after 0.7 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 15 features after 0.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 16 features after 0.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 17 features after 0.69 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 18 features after 0.72 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 19 features after 0.74 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 20 features after 0.72 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 21 features after 0.71 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 22 features after 0.73 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 23 features after 0.74 seconds\n",
      "Finished BernoulliNB with No Oversampling and StandardScaler and 24 features after 0.7 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 1 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 2 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 3 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 4 features after 0.04 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 5 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 6 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 7 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 8 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 9 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 10 features after 0.03 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished GaussianNB with No Oversampling and StandardScaler and 11 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 12 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 13 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 14 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 15 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 16 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 17 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 18 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 19 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 20 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 21 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 22 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 23 features after 0.03 seconds\n",
      "Finished GaussianNB with No Oversampling and StandardScaler and 24 features after 0.03 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 1 features after 5.13 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 2 features after 5.19 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 3 features after 6.01 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 4 features after 6.9 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 5 features after 6.07 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 6 features after 5.62 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 7 features after 6.01 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 8 features after 7.19 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 9 features after 7.36 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 10 features after 6.77 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 11 features after 6.43 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 12 features after 6.33 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 13 features after 8.11 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 14 features after 8.19 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 15 features after 7.62 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 16 features after 7.09 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 17 features after 9.35 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 18 features after 8.14 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 19 features after 7.38 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 20 features after 6.73 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 21 features after 8.02 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 22 features after 8.94 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 23 features after 7.32 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and StandardScaler and 24 features after 6.86 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 1 features after 13.0 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 2 features after 18.7 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 3 features after 14.4 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 4 features after 18.24 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 5 features after 18.2 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 6 features after 20.65 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 7 features after 18.03 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 8 features after 16.44 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 9 features after 24.19 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 10 features after 17.68 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 11 features after 22.82 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 12 features after 20.41 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 13 features after 24.52 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 14 features after 26.53 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 15 features after 30.92 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 16 features after 22.49 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 17 features after 32.57 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 18 features after 37.24 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 19 features after 31.82 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 20 features after 35.2 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 21 features after 27.21 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 22 features after 40.59 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 23 features after 57.47 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and StandardScaler and 24 features after 160.9 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 1 features after 28.78 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 2 features after 24.37 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 3 features after 23.79 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 4 features after 25.03 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 5 features after 25.06 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 6 features after 25.19 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 7 features after 28.37 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 8 features after 29.56 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 9 features after 26.26 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 10 features after 27.15 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 11 features after 27.71 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 12 features after 28.26 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 13 features after 27.86 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 14 features after 24.17 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 15 features after 21.25 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 16 features after 21.89 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 17 features after 22.31 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 18 features after 23.33 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished XGBClassifier with No Oversampling and StandardScaler and 19 features after 25.11 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 20 features after 28.56 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 21 features after 29.64 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 22 features after 26.64 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 23 features after 27.42 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 24 features after 29.75 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 1 features after 15.65 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 2 features after 15.41 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 3 features after 15.67 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 4 features after 14.73 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 5 features after 16.02 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 6 features after 16.1 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 7 features after 15.88 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 8 features after 15.79 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 9 features after 16.24 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 10 features after 16.67 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 11 features after 16.56 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 12 features after 17.31 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 13 features after 15.61 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 14 features after 16.69 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 15 features after 17.51 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 16 features after 16.68 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 17 features after 14.07 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 18 features after 13.78 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 19 features after 13.91 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 20 features after 13.15 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 21 features after 13.35 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 22 features after 14.29 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 23 features after 15.4 seconds\n",
      "Finished XGBClassifier with No Oversampling and StandardScaler and 24 features after 15.52 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 1 features after 46.96 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 2 features after 62.88 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 3 features after 68.81 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 4 features after 58.36 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 5 features after 73.79 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 6 features after 66.57 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 7 features after 71.11 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 8 features after 48.1 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 9 features after 105.2 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 10 features after 78.06 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 11 features after 117.21 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 12 features after 117.77 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 13 features after 116.02 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 14 features after 93.21 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 15 features after 32.44 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 16 features after 34.58 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 17 features after 37.55 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 18 features after 47.67 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 19 features after 42.6 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 20 features after 56.11 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 21 features after 47.81 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 22 features after 39.5 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 23 features after 45.1 seconds\n",
      "Finished BaggingClassifier with No Oversampling and StandardScaler and 24 features after 41.22 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 1 features after 4.88 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 2 features after 3.15 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 3 features after 2.95 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 4 features after 6.93 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 5 features after 4.59 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 6 features after 3.43 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 7 features after 5.71 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 8 features after 5.76 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 9 features after 3.72 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 10 features after 5.13 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 11 features after 7.38 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 12 features after 4.19 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 13 features after 7.82 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 14 features after 5.79 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 15 features after 7.24 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 16 features after 6.14 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 17 features after 4.32 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 18 features after 8.01 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 19 features after 6.49 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 20 features after 5.69 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 21 features after 8.81 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 22 features after 5.01 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 23 features after 7.7 seconds\n",
      "Finished Perceptron with No Oversampling and StandardScaler and 24 features after 7.53 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 1 features after 23.81 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 2 features after 20.48 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 3 features after 28.42 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 4 features after 26.96 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 5 features after 20.66 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 6 features after 27.39 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished MLPClassifier with No Oversampling and StandardScaler and 7 features after 21.81 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 8 features after 24.73 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 9 features after 23.09 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 10 features after 22.52 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 11 features after 21.85 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 12 features after 29.23 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 13 features after 22.93 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 14 features after 23.92 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 15 features after 29.03 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 16 features after 25.26 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 17 features after 24.62 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 18 features after 28.89 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 19 features after 29.84 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 20 features after 25.8 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 21 features after 25.45 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 22 features after 25.54 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 23 features after 28.18 seconds\n",
      "Finished MLPClassifier with No Oversampling and StandardScaler and 24 features after 27.67 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 1 features after 0.22 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 2 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 3 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 4 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 5 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 6 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 7 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 8 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 9 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 10 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 11 features after 0.28 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 12 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 13 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 14 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 15 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 16 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 17 features after 0.21 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 18 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 19 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 20 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 21 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 22 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 23 features after 0.29 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and StandardScaler and 24 features after 0.26 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 1 features after 0.34 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 2 features after 0.27 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 3 features after 0.3 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 4 features after 0.35 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 5 features after 1.61 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 6 features after 1.69 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 7 features after 1.72 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 8 features after 3.24 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 9 features after 2.27 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 10 features after 3.31 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 11 features after 2.97 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 12 features after 2.31 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 13 features after 2.01 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 14 features after 2.01 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 15 features after 2.02 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 16 features after 1.99 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 17 features after 1.95 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 18 features after 1.94 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 19 features after 1.94 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 20 features after 2.0 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 21 features after 2.01 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 22 features after 2.06 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 23 features after 2.1 seconds\n",
      "Finished SVC with No Oversampling and StandardScaler and 24 features after 2.1 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 1 features after 2.73 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 2 features after 3.16 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 3 features after 3.72 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 4 features after 5.22 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 5 features after 4.29 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 6 features after 4.19 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 7 features after 5.04 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 8 features after 5.4 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 9 features after 11.62 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 10 features after 10.61 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 11 features after 13.53 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 12 features after 13.42 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 13 features after 18.18 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 14 features after 24.09 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 15 features after 22.72 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 16 features after 24.78 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 17 features after 43.8 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished LogisticRegression with No Oversampling and LogScaler and 18 features after 44.14 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 19 features after 44.49 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 20 features after 49.3 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 21 features after 51.2 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 22 features after 44.86 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 23 features after 80.77 seconds\n",
      "Finished LogisticRegression with No Oversampling and LogScaler and 24 features after 95.06 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 1 features after 1.12 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 2 features after 1.15 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 3 features after 2.06 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 4 features after 2.45 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 5 features after 2.94 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 6 features after 2.06 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 7 features after 1.74 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 8 features after 3.05 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 9 features after 2.07 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 10 features after 3.06 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 11 features after 2.55 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 12 features after 2.44 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 13 features after 3.65 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 14 features after 4.34 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 15 features after 2.57 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 16 features after 4.76 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 17 features after 4.45 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 18 features after 3.3 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 19 features after 4.99 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 20 features after 3.89 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 21 features after 4.51 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 22 features after 4.14 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 23 features after 3.77 seconds\n",
      "Finished KNeighborsClassifier with No Oversampling and LogScaler and 24 features after 3.91 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 1 features after 7.12 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 2 features after 5.51 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 3 features after 8.4 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 4 features after 6.8 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 5 features after 7.86 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 6 features after 7.26 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 7 features after 7.38 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 8 features after 7.72 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 9 features after 8.17 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 10 features after 9.89 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 11 features after 8.8 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 12 features after 8.9 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 13 features after 10.79 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 14 features after 9.19 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 15 features after 9.64 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 16 features after 11.14 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 17 features after 10.42 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 18 features after 11.6 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 19 features after 9.89 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 20 features after 8.78 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 21 features after 8.0 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 22 features after 7.96 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 23 features after 8.05 seconds\n",
      "Finished DecisionTreeClassifier with No Oversampling and LogScaler and 24 features after 8.06 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 1 features after 4.56 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 2 features after 4.66 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 3 features after 4.83 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 4 features after 6.32 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 5 features after 6.43 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 6 features after 5.58 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 7 features after 5.26 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 8 features after 5.53 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 9 features after 5.2 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 10 features after 5.24 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 11 features after 5.22 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 12 features after 5.73 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 13 features after 6.87 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 14 features after 6.29 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 15 features after 5.8 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 16 features after 6.85 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 17 features after 5.57 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 18 features after 5.69 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 19 features after 5.47 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 20 features after 5.79 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 21 features after 6.62 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 22 features after 7.18 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 23 features after 6.43 seconds\n",
      "Finished ExtraTreeClassifier with No Oversampling and LogScaler and 24 features after 6.11 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 1 features after 15.42 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 2 features after 16.27 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 3 features after 19.44 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 4 features after 15.63 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 5 features after 16.98 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 6 features after 18.7 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 7 features after 17.12 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 8 features after 18.4 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 9 features after 19.26 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 10 features after 17.74 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 11 features after 18.39 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 12 features after 17.67 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 13 features after 18.82 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 14 features after 18.35 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 15 features after 18.21 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 16 features after 18.8 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 17 features after 18.55 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 18 features after 20.08 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 19 features after 20.12 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 20 features after 17.13 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 21 features after 18.2 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 22 features after 18.59 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 23 features after 21.51 seconds\n",
      "Finished ExtraTreesClassifier with No Oversampling and LogScaler and 24 features after 18.48 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 1 features after 17.45 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 2 features after 23.43 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 3 features after 20.2 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 4 features after 20.61 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 5 features after 21.46 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 6 features after 21.19 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 7 features after 25.84 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 8 features after 21.67 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 9 features after 24.65 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 10 features after 27.52 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 11 features after 27.85 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 12 features after 28.26 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 13 features after 26.16 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 14 features after 24.55 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 15 features after 26.3 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 16 features after 32.85 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 17 features after 28.75 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 18 features after 28.09 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 19 features after 28.29 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 20 features after 29.84 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 21 features after 30.07 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 22 features after 32.75 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 23 features after 30.08 seconds\n",
      "Finished RandomForestClassifier with No Oversampling and LogScaler and 24 features after 31.01 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 1 features after 0.6 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 2 features after 0.68 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 3 features after 0.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 4 features after 0.63 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 5 features after 0.64 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 6 features after 0.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 7 features after 0.68 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 8 features after 0.68 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 9 features after 0.66 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 10 features after 0.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 11 features after 0.7 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 12 features after 0.7 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 13 features after 0.67 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 14 features after 0.63 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 15 features after 0.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 16 features after 0.61 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 17 features after 0.62 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 18 features after 0.82 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 19 features after 1.02 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 20 features after 0.96 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 21 features after 0.92 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 22 features after 0.89 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 23 features after 0.96 seconds\n",
      "Finished BernoulliNB with No Oversampling and LogScaler and 24 features after 1.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 1 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 2 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 3 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 4 features after 0.05 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 5 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 6 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 7 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 8 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 9 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 10 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 11 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 12 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 13 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 14 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 15 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 16 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 17 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 18 features after 0.06 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished GaussianNB with No Oversampling and LogScaler and 19 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 20 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 21 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 22 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 23 features after 0.06 seconds\n",
      "Finished GaussianNB with No Oversampling and LogScaler and 24 features after 0.06 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 1 features after 8.1 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 2 features after 6.23 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 3 features after 5.51 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 4 features after 10.07 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 5 features after 7.49 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 6 features after 6.58 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 7 features after 6.39 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 8 features after 6.12 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 9 features after 10.63 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 10 features after 8.22 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 11 features after 7.15 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 12 features after 6.92 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 13 features after 9.37 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 14 features after 8.8 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 15 features after 7.86 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 16 features after 7.46 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 17 features after 7.13 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 18 features after 6.98 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 19 features after 10.63 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 20 features after 9.75 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 21 features after 7.99 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 22 features after 8.1 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 23 features after 7.51 seconds\n",
      "Finished GradientBoostingClassifier with No Oversampling and LogScaler and 24 features after 8.8 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 1 features after 56.0 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 2 features after 26.66 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 3 features after 47.73 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 4 features after 83.3 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 5 features after 60.2 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 6 features after 115.14 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 7 features after 59.19 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 8 features after 56.67 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 9 features after 57.28 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 10 features after 57.21 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 11 features after 30.67 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 12 features after 24.55 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 13 features after 18.74 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 14 features after 18.18 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 15 features after 19.0 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 16 features after 20.66 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 17 features after 20.9 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 18 features after 19.76 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 19 features after 20.9 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 20 features after 22.61 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 21 features after 24.37 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 22 features after 23.6 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 23 features after 25.14 seconds\n",
      "Finished AdaBoostClassifier with No Oversampling and LogScaler and 24 features after 27.39 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 1 features after 20.16 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 2 features after 19.5 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 3 features after 19.42 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 4 features after 19.55 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 5 features after 19.55 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 6 features after 19.4 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 7 features after 19.52 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 8 features after 19.36 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 9 features after 19.27 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 10 features after 19.21 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 11 features after 19.1 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 12 features after 18.88 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 13 features after 18.81 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 14 features after 18.66 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 15 features after 18.64 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 16 features after 18.65 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 17 features after 18.24 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 18 features after 18.13 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 19 features after 18.1 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 20 features after 17.75 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 21 features after 17.59 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 22 features after 17.55 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 23 features after 17.27 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 24 features after 17.13 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 1 features after 11.48 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 2 features after 11.82 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 3 features after 11.44 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 4 features after 11.48 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 5 features after 11.46 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 6 features after 11.57 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished XGBClassifier with No Oversampling and LogScaler and 7 features after 11.58 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 8 features after 11.32 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 9 features after 11.39 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 10 features after 11.41 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 11 features after 11.27 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 12 features after 11.07 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 13 features after 11.06 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 14 features after 10.99 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 15 features after 10.85 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 16 features after 10.6 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 17 features after 10.54 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 18 features after 10.62 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 19 features after 10.5 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 20 features after 10.39 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 21 features after 10.32 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 22 features after 10.02 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 23 features after 9.89 seconds\n",
      "Finished XGBClassifier with No Oversampling and LogScaler and 24 features after 9.78 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 1 features after 21.5 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 2 features after 24.84 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 3 features after 27.36 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 4 features after 24.21 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 5 features after 26.96 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 6 features after 25.3 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 7 features after 26.16 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 8 features after 26.01 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 9 features after 23.87 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 10 features after 27.28 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 11 features after 30.87 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 12 features after 15.33 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 13 features after 25.54 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 14 features after 27.92 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 15 features after 39.39 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 16 features after 38.32 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 17 features after 30.38 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 18 features after 31.91 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 19 features after 31.93 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 20 features after 35.41 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 21 features after 31.85 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 22 features after 33.12 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 23 features after 38.96 seconds\n",
      "Finished BaggingClassifier with No Oversampling and LogScaler and 24 features after 34.05 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 1 features after 2.37 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 2 features after 2.5 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 3 features after 2.63 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 4 features after 2.97 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 5 features after 3.01 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 6 features after 3.11 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 7 features after 3.21 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 8 features after 3.31 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 9 features after 3.35 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 10 features after 3.5 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 11 features after 3.57 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 12 features after 3.52 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 13 features after 3.65 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 14 features after 3.82 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 15 features after 3.77 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 16 features after 3.86 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 17 features after 4.0 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 18 features after 4.12 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 19 features after 4.11 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 20 features after 4.15 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 21 features after 4.26 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 22 features after 4.37 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 23 features after 4.42 seconds\n",
      "Finished Perceptron with No Oversampling and LogScaler and 24 features after 4.39 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 1 features after 10.25 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 2 features after 20.01 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 3 features after 21.51 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 4 features after 25.83 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 5 features after 27.66 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 6 features after 28.38 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 7 features after 29.36 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 8 features after 31.04 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 9 features after 21.53 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 10 features after 21.74 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 11 features after 21.65 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 12 features after 22.06 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 13 features after 23.26 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 14 features after 23.57 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 15 features after 23.85 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 16 features after 23.6 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 17 features after 23.78 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 18 features after 23.95 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 19 features after 25.55 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 20 features after 24.7 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 21 features after 24.58 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 22 features after 25.22 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished MLPClassifier with No Oversampling and LogScaler and 23 features after 24.84 seconds\n",
      "Finished MLPClassifier with No Oversampling and LogScaler and 24 features after 25.65 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 1 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 2 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 3 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 4 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 5 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 6 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 7 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 8 features after 0.28 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 9 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 10 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 11 features after 0.27 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 12 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 13 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 14 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 15 features after 0.21 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 16 features after 0.26 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 17 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 18 features after 0.23 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 19 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 20 features after 0.22 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 21 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 22 features after 0.25 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 23 features after 0.24 seconds\n",
      "Finished QuadraticDiscriminantAnalysis with No Oversampling and LogScaler and 24 features after 0.21 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 1 features after 0.3 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 2 features after 0.27 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 3 features after 0.27 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 4 features after 0.35 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 5 features after 0.42 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 6 features after 0.4 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 7 features after 0.47 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 8 features after 0.47 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 9 features after 1.5 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 10 features after 1.61 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 11 features after 1.8 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 12 features after 1.76 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 13 features after 1.83 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 14 features after 1.86 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 15 features after 1.89 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 16 features after 1.88 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 17 features after 1.89 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 18 features after 1.94 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 19 features after 1.94 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 20 features after 1.94 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 21 features after 2.01 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 22 features after 2.03 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 23 features after 2.05 seconds\n",
      "Finished SVC with No Oversampling and LogScaler and 24 features after 2.05 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Oversampling Strategy</th>\n",
       "      <th>Data Preparation</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Feature Selection Technique</th>\n",
       "      <th>Features</th>\n",
       "      <th>Optimal Parameters</th>\n",
       "      <th>Monetary Value Per Instance - Mean</th>\n",
       "      <th>Monetary Value Per Instance - Standard Deviation</th>\n",
       "      <th>Raw Model</th>\n",
       "      <th>Cross Validation Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05056438446044922, 0.1769...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05425493717193604, 0.2058...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0544543981552124, 0.20874...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05784509181976318, 0.1759...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05764601230621338, 0.2370...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04896423816680908, 0.1266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05176126956939697, 0.1792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04527671337127685, 0.1219...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09464631080627442, 0.2716...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07219569683074951, 0.1997...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06940767765045167, 0.1830...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07019481658935547, 0.1837...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0765883445739746, 0.20574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08726654052734376, 0.2142...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07240362167358398, 0.1908...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09723961353302002, 0.2663...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.067877</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04886937141418457, 0.2208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.067877</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.042185854911804196, 0.113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.067877</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06083579063415527, 0.2342...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.067877</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.054952597618103026, 0.169...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.048834</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06512563228607178, 0.2223...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.048834</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05804469585418701, 0.1643...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.048834</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06412808895111084, 0.2182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05774281024932861, 0.2250...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0528484582901001, 0.13783...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.057942438125610354, 0.167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.050159239768981935, 0.137...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06233291625976563, 0.2334...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>0.065235</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06372709274291992, 0.1700...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.178286</td>\n",
       "      <td>0.037837</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05504775047302246, 0.1483...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.178286</td>\n",
       "      <td>0.037837</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0602384090423584, 0.16525...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.178286</td>\n",
       "      <td>0.037837</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05754311084747314, 0.1559...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.178286</td>\n",
       "      <td>0.037837</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08517186641693116, 0.2419...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05365612506866455, 0.1235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.055957</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10382180213928223, 0.2846...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0863689661026001, 0.22998...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06043775081634521, 0.1801...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.055957</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0884629487991333, 0.23247...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.055957</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08497223854064942, 0.2405...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.085051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.047373270988464354, 0.227...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.085051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.054852294921875, 0.197471...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07329485416412354, 0.1973...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.085051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04617421627044678, 0.1861...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.055957</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10900802612304687, 0.2985...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.055957</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08357594013214112, 0.2111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.085051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04158902168273926, 0.1112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05434770584106445, 0.1255...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0904578685760498, 0.23537...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07040562629699706, 0.1862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05115077495574951, 0.1144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.055957</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08427395820617675, 0.2062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10053102970123291, 0.2586...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.14730463027954102, 0.4388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.16774826049804686, 0.4257...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10761196613311767, 0.3731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07768809795379639, 0.2032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07998497486114502, 0.2098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10192716121673584, 0.3012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10950686931610107, 0.3880...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09693987369537353, 0.2672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.076385760307312, 0.215919...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10561714172363282, 0.2862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08666703701019288, 0.2775...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09614214897155762, 0.2436...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07319338321685791, 0.1937...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07369229793548585, 0.1929...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.11190094947814941, 0.3706...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09414799213409424, 0.2254...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07559263706207275, 0.1951...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07579183578491211, 0.1952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07398192882537842, 0.1952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.078339</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07119827270507813, 0.1813...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.078339</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08337123394012451, 0.2124...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.047732</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05874302387237549, 0.1603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.047732</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0631256103515625, 0.17283...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.091369</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04487769603729248, 0.2045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.091369</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0493680477142334, 0.18689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.078339</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06620879173278808, 0.1773...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.091369</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03450796604156494, 0.0948...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.091369</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03640217781066894, 0.0924...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.047732</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.054049324989318845, 0.151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.047732</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05196094512939453, 0.1405...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.078339</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07130908966064453, 0.1967...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.075290</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07280433177947998, 0.1994...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.090441</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06432745456695557, 0.2056...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.090441</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.047373604774475095, 0.127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.053056859970092775, 0.146...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.090441</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.050658655166625974, 0.134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.075290</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06900987625122071, 0.1769...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.075290</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06621336936950684, 0.1708...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07031164169311524, 0.1879...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06103696823120117, 0.1693...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.075290</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06394476890563965, 0.1676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.079998</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04228670597076416, 0.1146...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.099022</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06622262001037597, 0.1593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.099022</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.10741264820098877, 0.2868...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.079998</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.061435699462890625, 0.167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.083711</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04746236801147461, 0.1296...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.079998</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04567661285400391, 0.1585...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.083711</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.056648421287536624, 0.216...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.077572</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06681082248687745, 0.1788...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.084592</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09494562149047851, 0.3041...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.083711</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.055950379371643065, 0.174...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.099022</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07051081657409668, 0.1447...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.084592</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06971023082733155, 0.1898...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.111650</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04616866111755371, 0.1168...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.099022</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.061125540733337404, 0.167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.084592</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07420058250427246, 0.2036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.105269</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0828782081604004, 0.24354...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.104804</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06861603260040283, 0.1392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.104804</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05385591983795166, 0.1222...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.104804</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.11539115905761718, 0.2542...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.105269</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05683636665344238, 0.1550...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.28064892292022703, 0.4285...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2594059944152832, 0.43603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.28743069171905516, 0.4544...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.33161239624023436, 0.5098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.18031723499298097, 0.2596...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.16236634254455568, 0.2576...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.105269</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05575084686279297, 0.1612...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.104804</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04736342430114746, 0.1138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2527</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.103828</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04557158946990967, 0.1204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.096857</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0503582239151001, 0.11379...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.129985</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.053855514526367186, 0.184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.129985</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.038297486305236814, 0.116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.093202</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07340354919433593, 0.1800...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.132513</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.10751242637634277, 0.2949...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.129985</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.031216812133789063, 0.086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.132513</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05465312004089355, 0.1188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.129985</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03919310569763183, 0.1301...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.132513</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06941385269165039, 0.1625...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.093202</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07908811569213867, 0.2218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.093202</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08766539096832275, 0.2657...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2595245122909546, 0.45370...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.079820</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05744645595550537, 0.1536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.102953</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2513269424438477, 0.41508...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.102953</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.14659833908081055, 0.2325...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.16226651668548583, 0.2745...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.19009184837341309, 0.2466...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.165457820892334, 0.256612...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.22858889102935792, 0.3346...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.067006</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04776921272277832, 0.1276...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.25202579498291017, 0.4675...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.13912887573242189, 0.2344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.1782235860824585, 0.26489...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.102953</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2847379922866821, 0.40661...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2579099893569946, 0.44530...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.20944011211395264, 0.2536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2762607574462891, 0.41070...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.19019110202789308, 0.2361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.051450514793396, 0.113895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.08128278255462647, 0.1861...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05295817852020264, 0.1214...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.110174</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.08995909690856933, 0.1844...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.128010</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0595362663269043, 0.12007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05005559921264648, 0.1152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05155096054077148, 0.1190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05224721431732178, 0.1147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.086018</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.049364900588989256, 0.141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05914161205291748, 0.2110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.08377583026885986, 0.1908...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.048862767219543454, 0.120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.128010</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.047164511680603025, 0.122...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.083513</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0834766149520874, 0.25092...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.110174</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04756455421447754, 0.1249...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.112153</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04636678695678711, 0.1173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07030153274536133, 0.1540...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07439258098602294, 0.1579...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07638888359069824, 0.1446...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.103006</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0485623836517334, 0.11369...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.103006</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.044971132278442384, 0.109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.087476</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0974393367767334, 0.27576...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06621546745300293, 0.1588...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05305461883544922, 0.1191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05415375232696533, 0.1261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05315177440643311, 0.1204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06203348636627197, 0.1835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07469966411590576, 0.1797...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07100958824157715, 0.1970...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.054151725769042966, 0.122...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05644643306732178, 0.1194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06731901168823243, 0.1786...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06662108898162841, 0.2061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07539486885070801, 0.1741...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.128010</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07120904922485352, 0.1920...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.110174</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06283140182495117, 0.1496...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.110174</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06881554126739502, 0.1420...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05724701881408691, 0.1261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.049456214904785155, 0.150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06273086071014404, 0.2555...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05076322555541992, 0.1785...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0557497501373291, 0.14301...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.046674704551696776, 0.105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0661158800125122, 0.14271...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06821739673614502, 0.1353...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06492578983306885, 0.1444...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06212446689605713, 0.1486...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.087476</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.062330222129821776, 0.169...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.083513</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05764613151550293, 0.1691...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.086018</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.059441137313842776, 0.184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.086018</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04487917423248291, 0.1244...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.083513</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05772840976715088, 0.1551...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.087476</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.061433267593383786, 0.169...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05495185852050781, 0.1201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.128010</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05285842418670654, 0.1210...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0576446533203125, 0.11748...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.083513</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05744495391845703, 0.1520...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.11349625587463379, 0.2605...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.11429400444030761, 0.2532...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.10870921611785889, 0.2765...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.055741071701049805, 0.127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05374746322631836, 0.1195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05255935192108154, 0.1206...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.087476</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.059335947036743164, 0.158...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.049658918380737306, 0.117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.086018</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.045376038551330565, 0.123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.072739</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.048470354080200194, 0.131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.128764</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06232442855834961, 0.1646...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.128764</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05575051307678223, 0.1297...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.102588</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2517263650894165, 0.32652...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.102588</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.3012934684753418, 0.46006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.128764</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0739020824432373, 0.20126...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.128764</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05554981231689453, 0.1282...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2529</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.138101</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04955921173095703, 0.1181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.102588</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2949108839035034, 0.43982...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.071731</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05425503253936768, 0.1473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.119093</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07219626903533935, 0.1636...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.119093</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06751933097839355, 0.1985...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.119093</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.056249046325683595, 0.141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.117310</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05226066112518311, 0.1454...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.119093</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05714693069458008, 0.1358...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.124806</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.039194798469543456, 0.161...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.124806</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06960606575012207, 0.1556...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.114216</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.046375465393066403, 0.110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.043768787384033205, 0.113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.121461</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05384728908538818, 0.1218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.11289780139923096, 0.2836...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04537820816040039, 0.1156...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.121461</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04865508079528809, 0.1162...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05445399284362793, 0.1674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04448013305664063, 0.1406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.114216</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.042585015296936035, 0.132...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07380204200744629, 0.1564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.121461</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.068715500831604, 0.163762...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05086121559143066, 0.1235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05166172981262207, 0.1113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05385534763336182, 0.1165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.124806</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06731965541839599, 0.2016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.114216</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07699387073516846, 0.2085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07749254703521728, 0.1738...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.04158830642700195, 0.1239...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.030617785453796387, 0.097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.040684986114501956, 0.109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.024733757972717284, 0.099...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.08577024936676025, 0.1671...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.202372</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002000260353088379, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.06791837215423584, 0.1641...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.202372</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0021890878677368166, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.202372</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0029921531677246094, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.202372</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0370006799697876, 0.02184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.05415475368499756, 0.1199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.06810693740844727, 0.1593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.486961</td>\n",
       "      <td>0.174131</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.005185842514038086, 0.008...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020973682403564453, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019947290420532227, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.226355</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.0025930881500244142, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.226355</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.0024930715560913088, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002094268798828125, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.226355</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.0024931907653808595, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.176007</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.004784727096557617, 0.008...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001698613166809082, 0.001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 20}</td>\n",
       "      <td>-0.494944</td>\n",
       "      <td>0.208084</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.00997333526611328, 0.0172...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 65}</td>\n",
       "      <td>-0.497605</td>\n",
       "      <td>0.252656</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.00827794075012207, 0.0131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.038889741897583006, 0.104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.03330719470977783, 0.1074...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.028323936462402343, 0.090...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.022838282585144042, 0.083...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.034804034233093264, 0.126...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.237846</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.006083846092224121, 0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.03789839744567871, 0.0878...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.04736628532409668, 0.1230...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, tsne_axis_2]</td>\n",
       "      <td>{'n_neighbors': 15, 'p': 1, 'weights': 'distan...</td>\n",
       "      <td>-0.508249</td>\n",
       "      <td>0.278805</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.0019944429397583006, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.508249</td>\n",
       "      <td>0.226676</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010073065757751465, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 50}</td>\n",
       "      <td>-0.510910</td>\n",
       "      <td>0.206680</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.009773635864257812, 0.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 20}</td>\n",
       "      <td>-0.513571</td>\n",
       "      <td>0.214829</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.01047201156616211, 0.0183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.513571</td>\n",
       "      <td>0.176453</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.007878780364990234, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 50}</td>\n",
       "      <td>-0.518893</td>\n",
       "      <td>0.286750</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010471916198730469, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 20}</td>\n",
       "      <td>-0.518893</td>\n",
       "      <td>0.264937</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.0059841394424438475, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 110}</td>\n",
       "      <td>-0.518893</td>\n",
       "      <td>0.226578</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010073018074035645, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 35}</td>\n",
       "      <td>-0.524215</td>\n",
       "      <td>0.268706</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.00607759952545166, 0.0106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-0.524215</td>\n",
       "      <td>0.230935</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.0074798583984375, 0.01296...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 55}</td>\n",
       "      <td>-0.526876</td>\n",
       "      <td>0.279849</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.005688071250915527, 0.009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 70}</td>\n",
       "      <td>-0.526876</td>\n",
       "      <td>0.244553</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.007480120658874512, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.529537</td>\n",
       "      <td>0.244084</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010073137283325196, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 130}</td>\n",
       "      <td>-0.534859</td>\n",
       "      <td>0.215134</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.007779240608215332, 0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 40}</td>\n",
       "      <td>-0.540181</td>\n",
       "      <td>0.215757</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.008078479766845703, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-0.558808</td>\n",
       "      <td>0.375726</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010074925422668458, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.585418</td>\n",
       "      <td>0.426356</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004488039016723633, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.585418</td>\n",
       "      <td>0.426356</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020936250686645506, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.585418</td>\n",
       "      <td>0.426356</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004587554931640625, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.585418</td>\n",
       "      <td>0.426356</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0018947839736938477, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 20}</td>\n",
       "      <td>-0.588079</td>\n",
       "      <td>0.330010</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.009574294090270996, 0.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.598723</td>\n",
       "      <td>0.272556</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.005787014961242676, 0.010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 80}</td>\n",
       "      <td>-0.609367</td>\n",
       "      <td>0.316002</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.0068816661834716795, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 5}</td>\n",
       "      <td>-0.612028</td>\n",
       "      <td>0.276676</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.009873700141906739, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 5}</td>\n",
       "      <td>-0.612028</td>\n",
       "      <td>0.341373</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.007280588150024414, 0.014...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.612028</td>\n",
       "      <td>0.282696</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010372328758239745, 0.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.633316</td>\n",
       "      <td>0.314175</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0027924537658691405, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.633316</td>\n",
       "      <td>0.314175</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028923749923706055, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.633316</td>\n",
       "      <td>0.314175</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0026931047439575197, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.633316</td>\n",
       "      <td>0.314175</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028923749923706055, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 15}</td>\n",
       "      <td>-0.635977</td>\n",
       "      <td>0.253680</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0098738431930542, 0.01755...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2620</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.643960</td>\n",
       "      <td>0.338754</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002004098892211914, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.643960</td>\n",
       "      <td>0.338754</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0028924942016601562, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.643960</td>\n",
       "      <td>0.338754</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.011967825889587402, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.643960</td>\n",
       "      <td>0.338754</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.00208892822265625, 0.0020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 15}</td>\n",
       "      <td>-0.646621</td>\n",
       "      <td>0.344109</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.007480049133300781, 0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 40}</td>\n",
       "      <td>-0.646621</td>\n",
       "      <td>0.318148</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.00747983455657959, 0.0127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 70}</td>\n",
       "      <td>-0.659925</td>\n",
       "      <td>0.342170</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.006582307815551758, 0.011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 35}</td>\n",
       "      <td>-0.665247</td>\n",
       "      <td>0.342096</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.007380127906799316, 0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 195}</td>\n",
       "      <td>-0.665247</td>\n",
       "      <td>0.343630</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.005485415458679199, 0.009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 45}</td>\n",
       "      <td>-0.665247</td>\n",
       "      <td>0.338414</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.0077789068222045895, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.699840</td>\n",
       "      <td>0.231817</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003091740608215332, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.699840</td>\n",
       "      <td>0.231817</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0032912492752075195, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.699840</td>\n",
       "      <td>0.231817</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028923511505126952, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-0.718467</td>\n",
       "      <td>0.363213</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.005485129356384277, 0.009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro, tsne_axis_2]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.758382</td>\n",
       "      <td>0.601923</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0016941308975219726, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro, tsne_axis_2]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.758382</td>\n",
       "      <td>0.601923</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0023941993713378906, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro, tsne_axis_2]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.758382</td>\n",
       "      <td>0.601923</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002094602584838867, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro, tsne_axis_2]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.758382</td>\n",
       "      <td>0.601923</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001989459991455078, 0.001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.250215</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002992057800292969, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.250215</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003091716766357422, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.250215</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0037897586822509765, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.250215</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0032914400100708006, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.250215</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002692770957946777, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.250215</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0036898136138916017, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.409595</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010372304916381836, 0.018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003690195083618164, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.251640</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028921127319335937, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003889608383178711, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.251640</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029918909072875976, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.251640</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002992129325866699, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003693246841430664, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0033852338790893556, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.251640</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002992057800292969, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.251640</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029918670654296873, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034906864166259766, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0033908843994140624, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.251640</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0030918121337890625, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.806280</td>\n",
       "      <td>0.262763</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002895641326904297, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.806280</td>\n",
       "      <td>0.262763</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003889608383178711, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.806280</td>\n",
       "      <td>0.262763</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003789973258972168, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.819585</td>\n",
       "      <td>0.257036</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0035903453826904297, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.819585</td>\n",
       "      <td>0.257036</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003091859817504883, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.819585</td>\n",
       "      <td>0.257036</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034908294677734376, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.838212</td>\n",
       "      <td>0.256809</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003789854049682617, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.838212</td>\n",
       "      <td>0.256809</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003789663314819336, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.838212</td>\n",
       "      <td>0.256809</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034906148910522463, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.848856</td>\n",
       "      <td>0.266595</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00299220085144043, 0.0030...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.848856</td>\n",
       "      <td>0.266595</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0032911300659179688, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.848856</td>\n",
       "      <td>0.266595</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00299220085144043, 0.0032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.854178</td>\n",
       "      <td>0.233818</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003390955924987793, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.854178</td>\n",
       "      <td>0.233818</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0030917882919311523, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.854178</td>\n",
       "      <td>0.233818</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031913995742797853, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.001894998550415039, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025927543640136717, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029918432235717775, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0026930809020996095, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003490638732910156, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034904003143310545, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003091716766357422, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031916141510009766, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.968600</td>\n",
       "      <td>0.301185</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0028923511505126952], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.968600</td>\n",
       "      <td>0.301185</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002892279624938965], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.968600</td>\n",
       "      <td>0.301185</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0029920339584350586], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.968600</td>\n",
       "      <td>0.301185</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002992129325866699], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.992549</td>\n",
       "      <td>0.314330</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002194046974182129], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.992549</td>\n",
       "      <td>0.314330</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002194046974182129], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.992549</td>\n",
       "      <td>0.314330</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.00239255428314209], 'std_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.008515</td>\n",
       "      <td>0.316076</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0019947052001953124], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.008515</td>\n",
       "      <td>0.316076</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002992105484008789], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.008515</td>\n",
       "      <td>0.316076</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0023935317993164064], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.128260</td>\n",
       "      <td>0.362905</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028921127319335937, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-1.141565</td>\n",
       "      <td>0.375864</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0032910823822021483, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.168175</td>\n",
       "      <td>0.384575</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002692413330078125, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.216072</td>\n",
       "      <td>0.365313</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0021939516067504884, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.303885</td>\n",
       "      <td>0.336085</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002294039726257324], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.303885</td>\n",
       "      <td>0.336085</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091740608215332], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.303885</td>\n",
       "      <td>0.336085</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018947839736938477], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.335817</td>\n",
       "      <td>0.347962</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091764450073242], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.575306</td>\n",
       "      <td>0.407667</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003391265869140625, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.575306</td>\n",
       "      <td>0.407667</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003690147399902344, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.575306</td>\n",
       "      <td>0.407667</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003490710258483887, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.615221</td>\n",
       "      <td>0.437013</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0037897348403930662, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.641831</td>\n",
       "      <td>0.451592</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00458831787109375, 0.0038...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.641831</td>\n",
       "      <td>0.451592</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034904241561889648, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.641831</td>\n",
       "      <td>0.451592</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0039894342422485355, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.655136</td>\n",
       "      <td>0.460204</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0039947509765625, 0.00378...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.655136</td>\n",
       "      <td>0.460204</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002792501449584961, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.655136</td>\n",
       "      <td>0.482530</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003191542625427246, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.668441</td>\n",
       "      <td>0.483029</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031916618347167967, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.668441</td>\n",
       "      <td>0.483029</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.004288268089294433, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.668441</td>\n",
       "      <td>0.483029</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003633713722229004, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.668441</td>\n",
       "      <td>0.483029</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0038915395736694334, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.469873</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.004288601875305176, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.469873</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.004088997840881348, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.354357</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994633674621582], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.469873</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003390979766845703, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.354357</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994490623474121], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.469873</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0040891170501708984, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.354357</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091764450073242], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.753592</td>\n",
       "      <td>0.336484</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002422666549682617, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.753592</td>\n",
       "      <td>0.341291</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032912254333496093], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-1.830761</td>\n",
       "      <td>0.418668</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025930404663085938, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.355518</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0035905361175537108, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.351686</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.004498291015625, 0.003986...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.355518</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.004487967491149903, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.351686</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.004587340354919434, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.351686</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.06293184757232666, 0.0369...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.355518</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.04019250869750977, 0.1039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.355518</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0042888402938842775, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.351686</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0038893938064575194, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.910591</td>\n",
       "      <td>0.392880</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028924465179443358, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.038318</td>\n",
       "      <td>0.383335</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002692842483520508], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.038318</td>\n",
       "      <td>0.365848</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091883659362793], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.051623</td>\n",
       "      <td>0.377972</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0029920339584350586], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.051623</td>\n",
       "      <td>0.377972</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0031911611557006838], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.051623</td>\n",
       "      <td>0.377972</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0026927947998046874], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.064928</td>\n",
       "      <td>0.480548</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.00299227237701416], 'std_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.064928</td>\n",
       "      <td>0.480548</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017951726913452148], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.064928</td>\n",
       "      <td>0.480548</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017953634262084961], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.481187</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017951488494873046], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.481187</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016982316970825194], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0030918121337890625], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018950939178466798], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032912731170654298], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032912731170654298], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018949031829833985], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0022939443588256836], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017982006072998046], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.481187</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0030918359756469727], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.331027</td>\n",
       "      <td>0.437288</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091859817504883], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.428792</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003191637992858887], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.428792</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002293896675109863], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.422121</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091859817504883], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.422121</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018949508666992188], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.428792</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032914400100708006], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.422121</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018948078155517577], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.428792</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017952442169189454], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.410857</td>\n",
       "      <td>0.491129</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032910823822021483], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.410857</td>\n",
       "      <td>0.491129</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002991914749145508], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.410857</td>\n",
       "      <td>0.491129</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0030918359756469727], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.410857</td>\n",
       "      <td>0.491129</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0028922319412231444], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.424162</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032912492752075195], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.424162</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994800567626953], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.424162</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0015930891036987304], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.437467</td>\n",
       "      <td>0.521442</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0028917789459228516], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.437467</td>\n",
       "      <td>0.521442</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0019947052001953124], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.437467</td>\n",
       "      <td>0.521442</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017978906631469726], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.623736</td>\n",
       "      <td>0.579755</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018949508666992188], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.623736</td>\n",
       "      <td>0.579755</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003490638732910156], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.623736</td>\n",
       "      <td>0.579755</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018887519836425781], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.668973</td>\n",
       "      <td>0.567880</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994633674621582], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.668973</td>\n",
       "      <td>0.567880</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0024932384490966796], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.668973</td>\n",
       "      <td>0.567880</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032913684844970703], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.868547</td>\n",
       "      <td>0.374236</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032912731170654298], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.868547</td>\n",
       "      <td>0.607780</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003191351890563965], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.868547</td>\n",
       "      <td>0.607780</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994681358337402], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.868547</td>\n",
       "      <td>0.455081</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003191494941711426], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.868547</td>\n",
       "      <td>0.607780</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0024959564208984373], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.881852</td>\n",
       "      <td>0.449491</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0035904645919799805], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.895157</td>\n",
       "      <td>0.447384</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.00329129695892334], 'std_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.908462</td>\n",
       "      <td>0.618653</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003191685676574707], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.908462</td>\n",
       "      <td>0.618653</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002094554901123047], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.908462</td>\n",
       "      <td>0.618653</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002892422676086426], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.113358</td>\n",
       "      <td>0.525436</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003590512275695801], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.153273</td>\n",
       "      <td>0.592028</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018949508666992188], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.153273</td>\n",
       "      <td>0.592028</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091716766357422], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.153273</td>\n",
       "      <td>0.592028</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0030918359756469727], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.299627</td>\n",
       "      <td>0.557549</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994681358337402], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.299627</td>\n",
       "      <td>0.557549</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002992081642150879], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.299627</td>\n",
       "      <td>0.557549</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003191709518432617], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.326237</td>\n",
       "      <td>0.546788</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003590297698974609], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.339542</td>\n",
       "      <td>0.564224</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003391003608703613], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.366152</td>\n",
       "      <td>0.555916</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0033910512924194337], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.432677</td>\n",
       "      <td>0.536953</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032912492752075195], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002293968200683594], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0020971298217773438], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003490638732910156], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0021943092346191407], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.534650</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0036902666091918946], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003690338134765625], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0037902355194091796], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0031914710998535156], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003191637992858887], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0030918359756469727], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0022940635681152344], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002592611312866211], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002991914749145508], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.472592</td>\n",
       "      <td>0.553080</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0033912181854248045], 'st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2732 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Model Oversampling Strategy Data Preparation Feature Count Feature Selection Technique                                           Features                                 Optimal Parameters  Monetary Value Per Instance - Mean  Monetary Value Per Instance - Standard Deviation                                          Raw Model                           Cross Validation Results\n",
       "414              AdaBoostClassifier       No Oversampling       No Scaling            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05056438446044922, 0.1769...\n",
       "1095             AdaBoostClassifier       No Oversampling     MinMaxScaler            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05425493717193604, 0.2058...\n",
       "2461             AdaBoostClassifier       No Oversampling        LogScaler            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0544543981552124, 0.20874...\n",
       "1778             AdaBoostClassifier       No Oversampling   StandardScaler            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05784509181976318, 0.1759...\n",
       "1097             AdaBoostClassifier       No Oversampling     MinMaxScaler            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05764601230621338, 0.2370...\n",
       "1780             AdaBoostClassifier       No Oversampling   StandardScaler            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04896423816680908, 0.1266...\n",
       "412              AdaBoostClassifier       No Oversampling       No Scaling            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05176126956939697, 0.1792...\n",
       "2463             AdaBoostClassifier       No Oversampling        LogScaler            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04527671337127685, 0.1219...\n",
       "1115             AdaBoostClassifier       No Oversampling     MinMaxScaler            21                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09464631080627442, 0.2716...\n",
       "1117             AdaBoostClassifier       No Oversampling     MinMaxScaler            22                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07219569683074951, 0.1997...\n",
       "2481             AdaBoostClassifier       No Oversampling        LogScaler            21                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06940767765045167, 0.1830...\n",
       "2483             AdaBoostClassifier       No Oversampling        LogScaler            22                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07019481658935547, 0.1837...\n",
       "1798             AdaBoostClassifier       No Oversampling   StandardScaler            21                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0765883445739746, 0.20574...\n",
       "1800             AdaBoostClassifier       No Oversampling   StandardScaler            22                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08726654052734376, 0.2142...\n",
       "434              AdaBoostClassifier       No Oversampling       No Scaling            22                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07240362167358398, 0.1908...\n",
       "432              AdaBoostClassifier       No Oversampling       No Scaling            21                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09723961353302002, 0.2663...\n",
       "1093             AdaBoostClassifier       No Oversampling     MinMaxScaler            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.067877  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04886937141418457, 0.2208...\n",
       "1776             AdaBoostClassifier       No Oversampling   StandardScaler            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.067877  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.042185854911804196, 0.113...\n",
       "2459             AdaBoostClassifier       No Oversampling        LogScaler            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.067877  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06083579063415527, 0.2342...\n",
       "410              AdaBoostClassifier       No Oversampling       No Scaling            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.067877  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.054952597618103026, 0.169...\n",
       "419              AdaBoostClassifier       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.048834  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06512563228607178, 0.2223...\n",
       "1102             AdaBoostClassifier       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.048834  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05804469585418701, 0.1643...\n",
       "1785             AdaBoostClassifier       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.048834  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06412808895111084, 0.2182...\n",
       "1099             AdaBoostClassifier       No Oversampling     MinMaxScaler            13                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.180947                                          0.063034  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05774281024932861, 0.2250...\n",
       "1101             AdaBoostClassifier       No Oversampling     MinMaxScaler            14                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.180947                                          0.063034  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0528484582901001, 0.13783...\n",
       "1784             AdaBoostClassifier       No Oversampling   StandardScaler            14                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.180947                                          0.063034  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.057942438125610354, 0.167...\n",
       "2467             AdaBoostClassifier       No Oversampling        LogScaler            14                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.180947                                          0.063034  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.050159239768981935, 0.137...\n",
       "418              AdaBoostClassifier       No Oversampling       No Scaling            14                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.180947                                          0.063034  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06233291625976563, 0.2334...\n",
       "2478             AdaBoostClassifier       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.180947                                          0.065235  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06372709274291992, 0.1700...\n",
       "2471             AdaBoostClassifier       No Oversampling        LogScaler            16                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.178286                                          0.037837  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05504775047302246, 0.1483...\n",
       "1105             AdaBoostClassifier       No Oversampling     MinMaxScaler            16                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.178286                                          0.037837  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0602384090423584, 0.16525...\n",
       "1788             AdaBoostClassifier       No Oversampling   StandardScaler            16                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.178286                                          0.037837  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05754311084747314, 0.1559...\n",
       "422              AdaBoostClassifier       No Oversampling       No Scaling            16                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.178286                                          0.037837  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08517186641693116, 0.2419...\n",
       "1165                  XGBClassifier       No Oversampling     MinMaxScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.175625                                          0.108912  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05365612506866455, 0.1235...\n",
       "1112             AdaBoostClassifier       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.055957  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10382180213928223, 0.2846...\n",
       "1116             AdaBoostClassifier       No Oversampling     MinMaxScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.089419  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0863689661026001, 0.22998...\n",
       "1848                  XGBClassifier       No Oversampling   StandardScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.175625                                          0.108912  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06043775081634521, 0.1801...\n",
       "427              AdaBoostClassifier       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.055957  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0884629487991333, 0.23247...\n",
       "1110             AdaBoostClassifier       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.055957  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08497223854064942, 0.2405...\n",
       "1091             AdaBoostClassifier       No Oversampling     MinMaxScaler             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.175625                                          0.085051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.047373270988464354, 0.227...\n",
       "2457             AdaBoostClassifier       No Oversampling        LogScaler             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.175625                                          0.085051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.054852294921875, 0.197471...\n",
       "1799             AdaBoostClassifier       No Oversampling   StandardScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.089419  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07329485416412354, 0.1973...\n",
       "408              AdaBoostClassifier       No Oversampling       No Scaling             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.175625                                          0.085051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04617421627044678, 0.1861...\n",
       "1795             AdaBoostClassifier       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.055957  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10900802612304687, 0.2985...\n",
       "1793             AdaBoostClassifier       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.055957  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08357594013214112, 0.2111...\n",
       "1774             AdaBoostClassifier       No Oversampling   StandardScaler             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.175625                                          0.085051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04158902168273926, 0.1112...\n",
       "482                   XGBClassifier       No Oversampling       No Scaling            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.175625                                          0.108912  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05434770584106445, 0.1255...\n",
       "433              AdaBoostClassifier       No Oversampling       No Scaling            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.089419  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0904578685760498, 0.23537...\n",
       "2482             AdaBoostClassifier       No Oversampling        LogScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.089419  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07040562629699706, 0.1862...\n",
       "2531                  XGBClassifier       No Oversampling        LogScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.175625                                          0.108912  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05115077495574951, 0.1144...\n",
       "429              AdaBoostClassifier       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.055957  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08427395820617675, 0.2062...\n",
       "1122             AdaBoostClassifier       No Oversampling     MinMaxScaler            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10053102970123291, 0.2586...\n",
       "1804             AdaBoostClassifier       No Oversampling   StandardScaler            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.14730463027954102, 0.4388...\n",
       "1805             AdaBoostClassifier       No Oversampling   StandardScaler            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.16774826049804686, 0.4257...\n",
       "437              AdaBoostClassifier       No Oversampling       No Scaling            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10761196613311767, 0.3731...\n",
       "1121             AdaBoostClassifier       No Oversampling     MinMaxScaler            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07768809795379639, 0.2032...\n",
       "1120             AdaBoostClassifier       No Oversampling     MinMaxScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07998497486114502, 0.2098...\n",
       "1119             AdaBoostClassifier       No Oversampling     MinMaxScaler            23                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10192716121673584, 0.3012...\n",
       "436              AdaBoostClassifier       No Oversampling       No Scaling            23                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10950686931610107, 0.3880...\n",
       "438              AdaBoostClassifier       No Oversampling       No Scaling            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09693987369537353, 0.2672...\n",
       "1118             AdaBoostClassifier       No Oversampling     MinMaxScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.076385760307312, 0.215919...\n",
       "435              AdaBoostClassifier       No Oversampling       No Scaling            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10561714172363282, 0.2862...\n",
       "1801             AdaBoostClassifier       No Oversampling   StandardScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08666703701019288, 0.2775...\n",
       "1803             AdaBoostClassifier       No Oversampling   StandardScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09614214897155762, 0.2436...\n",
       "2484             AdaBoostClassifier       No Oversampling        LogScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07319338321685791, 0.1937...\n",
       "2485             AdaBoostClassifier       No Oversampling        LogScaler            23                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07369229793548585, 0.1929...\n",
       "439              AdaBoostClassifier       No Oversampling       No Scaling            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.11190094947814941, 0.3706...\n",
       "1802             AdaBoostClassifier       No Oversampling   StandardScaler            23                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09414799213409424, 0.2254...\n",
       "2488             AdaBoostClassifier       No Oversampling        LogScaler            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07559263706207275, 0.1951...\n",
       "2487             AdaBoostClassifier       No Oversampling        LogScaler            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07579183578491211, 0.1952...\n",
       "2486             AdaBoostClassifier       No Oversampling        LogScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07398192882537842, 0.1952...\n",
       "430              AdaBoostClassifier       No Oversampling       No Scaling            20                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.078339  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07119827270507813, 0.1813...\n",
       "1796             AdaBoostClassifier       No Oversampling   StandardScaler            20                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.078339  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08337123394012451, 0.2124...\n",
       "420              AdaBoostClassifier       No Oversampling       No Scaling            15                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.047732  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05874302387237549, 0.1603...\n",
       "1786             AdaBoostClassifier       No Oversampling   StandardScaler            15                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.047732  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0631256103515625, 0.17283...\n",
       "2455             AdaBoostClassifier       No Oversampling        LogScaler             8                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.170303                                          0.091369  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04487769603729248, 0.2045...\n",
       "1089             AdaBoostClassifier       No Oversampling     MinMaxScaler             8                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.170303                                          0.091369  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0493680477142334, 0.18689...\n",
       "2479             AdaBoostClassifier       No Oversampling        LogScaler            20                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.078339  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06620879173278808, 0.1773...\n",
       "1772             AdaBoostClassifier       No Oversampling   StandardScaler             8                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.170303                                          0.091369  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03450796604156494, 0.0948...\n",
       "406              AdaBoostClassifier       No Oversampling       No Scaling             8                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.170303                                          0.091369  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03640217781066894, 0.0924...\n",
       "1103             AdaBoostClassifier       No Oversampling     MinMaxScaler            15                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.047732  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.054049324989318845, 0.151...\n",
       "2469             AdaBoostClassifier       No Oversampling        LogScaler            15                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.047732  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05196094512939453, 0.1405...\n",
       "1113             AdaBoostClassifier       No Oversampling     MinMaxScaler            20                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.078339  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07130908966064453, 0.1967...\n",
       "428              AdaBoostClassifier       No Oversampling       No Scaling            19                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.075290  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07280433177947998, 0.1994...\n",
       "416              AdaBoostClassifier       No Oversampling       No Scaling            13                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.164981                                          0.090441  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06432745456695557, 0.2056...\n",
       "2465             AdaBoostClassifier       No Oversampling        LogScaler            13                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.164981                                          0.090441  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.047373604774475095, 0.127...\n",
       "421              AdaBoostClassifier       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.164981                                          0.105500  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.053056859970092775, 0.146...\n",
       "1782             AdaBoostClassifier       No Oversampling   StandardScaler            13                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.164981                                          0.090441  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.050658655166625974, 0.134...\n",
       "1794             AdaBoostClassifier       No Oversampling   StandardScaler            19                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.075290  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06900987625122071, 0.1769...\n",
       "1111             AdaBoostClassifier       No Oversampling     MinMaxScaler            19                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.075290  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06621336936950684, 0.1708...\n",
       "1104             AdaBoostClassifier       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.164981                                          0.105500  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07031164169311524, 0.1879...\n",
       "1787             AdaBoostClassifier       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.164981                                          0.105500  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06103696823120117, 0.1693...\n",
       "2477             AdaBoostClassifier       No Oversampling        LogScaler            19                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.075290  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06394476890563965, 0.1676...\n",
       "1149                  XGBClassifier       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.079998  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04228670597076416, 0.1146...\n",
       "1839                  XGBClassifier       No Oversampling   StandardScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.162320                                          0.099022  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06622262001037597, 0.1593...\n",
       "473                   XGBClassifier       No Oversampling       No Scaling            17                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.162320                                          0.099022  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.10741264820098877, 0.2868...\n",
       "466                   XGBClassifier       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.079998  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.061435699462890625, 0.167...\n",
       "417              AdaBoostClassifier       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.162320                                          0.083711  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04746236801147461, 0.1296...\n",
       "1832                  XGBClassifier       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.079998  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04567661285400391, 0.1585...\n",
       "1783             AdaBoostClassifier       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.162320                                          0.083711  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.056648421287536624, 0.216...\n",
       "2480             AdaBoostClassifier       No Oversampling        LogScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.162320                                          0.077572  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06681082248687745, 0.1788...\n",
       "431              AdaBoostClassifier       No Oversampling       No Scaling            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.162320                                          0.084592  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09494562149047851, 0.3041...\n",
       "1100             AdaBoostClassifier       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.162320                                          0.083711  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.055950379371643065, 0.174...\n",
       "1156                  XGBClassifier       No Oversampling     MinMaxScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.162320                                          0.099022  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.07051081657409668, 0.1447...\n",
       "1114             AdaBoostClassifier       No Oversampling     MinMaxScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.162320                                          0.084592  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06971023082733155, 0.1898...\n",
       "2523                  XGBClassifier       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.162320                                          0.111650  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04616866111755371, 0.1168...\n",
       "2522                  XGBClassifier       No Oversampling        LogScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.162320                                          0.099022  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.061125540733337404, 0.167...\n",
       "1797             AdaBoostClassifier       No Oversampling   StandardScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.162320                                          0.084592  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07420058250427246, 0.2036...\n",
       "423              AdaBoostClassifier       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.159659                                          0.105269  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0828782081604004, 0.24354...\n",
       "1847                  XGBClassifier       No Oversampling   StandardScaler            21                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.104804  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06861603260040283, 0.1392...\n",
       "1164                  XGBClassifier       No Oversampling     MinMaxScaler            21                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.104804  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05385591983795166, 0.1222...\n",
       "481                   XGBClassifier       No Oversampling       No Scaling            21                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.104804  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.11539115905761718, 0.2542...\n",
       "1789             AdaBoostClassifier       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.159659                                          0.105269  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05683636665344238, 0.1550...\n",
       "717              LogisticRegression       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.6, 'fit_intercept': True, 'solver': 'l...                            0.159659                                          0.094489  LogisticRegression(C=1.6, class_weight=None, d...  {'mean_fit_time': [0.28064892292022703, 0.4285...\n",
       "715              LogisticRegression       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.094489  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.2594059944152832, 0.43603...\n",
       "32               LogisticRegression       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.094489  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.28743069171905516, 0.4544...\n",
       "34               LogisticRegression       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.6, 'fit_intercept': True, 'solver': 'l...                            0.159659                                          0.094489  LogisticRegression(C=1.6, class_weight=None, d...  {'mean_fit_time': [0.33161239624023436, 0.5098...\n",
       "1398             LogisticRegression       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.094489  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.18031723499298097, 0.2596...\n",
       "1400             LogisticRegression       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.6, 'fit_intercept': True, 'solver': 'l...                            0.159659                                          0.094489  LogisticRegression(C=1.6, class_weight=None, d...  {'mean_fit_time': [0.16236634254455568, 0.2576...\n",
       "1106             AdaBoostClassifier       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.159659                                          0.105269  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05575084686279297, 0.1612...\n",
       "2530                  XGBClassifier       No Oversampling        LogScaler            21                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.104804  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04736342430114746, 0.1138...\n",
       "2527                  XGBClassifier       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.156998                                          0.103828  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04557158946990967, 0.1204...\n",
       "2525                  XGBClassifier       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.156998                                          0.096857  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.0503582239151001, 0.11379...\n",
       "2453             AdaBoostClassifier       No Oversampling        LogScaler             7                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.156998                                          0.129985  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.053855514526367186, 0.184...\n",
       "1087             AdaBoostClassifier       No Oversampling     MinMaxScaler             7                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.156998                                          0.129985  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.038297486305236814, 0.116...\n",
       "1108             AdaBoostClassifier       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.093202  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07340354919433593, 0.1800...\n",
       "480                   XGBClassifier       No Oversampling       No Scaling            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.156998                                          0.132513  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.10751242637634277, 0.2949...\n",
       "1770             AdaBoostClassifier       No Oversampling   StandardScaler             7                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.156998                                          0.129985  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.031216812133789063, 0.086...\n",
       "1163                  XGBClassifier       No Oversampling     MinMaxScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.156998                                          0.132513  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05465312004089355, 0.1188...\n",
       "404              AdaBoostClassifier       No Oversampling       No Scaling             7                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.156998                                          0.129985  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03919310569763183, 0.1301...\n",
       "1846                  XGBClassifier       No Oversampling   StandardScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.156998                                          0.132513  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06941385269165039, 0.1625...\n",
       "425              AdaBoostClassifier       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.093202  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07908811569213867, 0.2218...\n",
       "1791             AdaBoostClassifier       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.093202  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08766539096832275, 0.2657...\n",
       "24               LogisticRegression       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.2595245122909546, 0.45370...\n",
       "2474             AdaBoostClassifier       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.154337                                          0.079820  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05744645595550537, 0.1536...\n",
       "22               LogisticRegression       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.102953  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.2513269424438477, 0.41508...\n",
       "1388             LogisticRegression       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.102953  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.14659833908081055, 0.2325...\n",
       "1390             LogisticRegression       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.16226651668548583, 0.2745...\n",
       "1392             LogisticRegression       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.19009184837341309, 0.2466...\n",
       "30               LogisticRegression       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.165457820892334, 0.256612...\n",
       "713              LogisticRegression       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.22858889102935792, 0.3346...\n",
       "2464             AdaBoostClassifier       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.154337                                          0.067006  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04776921272277832, 0.1276...\n",
       "707              LogisticRegression       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.25202579498291017, 0.4675...\n",
       "1394             LogisticRegression       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.13912887573242189, 0.2344...\n",
       "1396             LogisticRegression       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.1782235860824585, 0.26489...\n",
       "705              LogisticRegression       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.102953  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.2847379922866821, 0.40661...\n",
       "28               LogisticRegression       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.2579099893569946, 0.44530...\n",
       "709              LogisticRegression       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.20944011211395264, 0.2536...\n",
       "711              LogisticRegression       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.2762607574462891, 0.41070...\n",
       "26               LogisticRegression       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.19019110202789308, 0.2361...\n",
       "2533                  XGBClassifier       No Oversampling        LogScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.051450514793396, 0.113895...\n",
       "467                   XGBClassifier       No Oversampling       No Scaling            14                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.08128278255462647, 0.1861...\n",
       "470                   XGBClassifier       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05295817852020264, 0.1214...\n",
       "477                   XGBClassifier       No Oversampling       No Scaling            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.110174  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.08995909690856933, 0.1844...\n",
       "479                   XGBClassifier       No Oversampling       No Scaling            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.128010  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.0595362663269043, 0.12007...\n",
       "2532                  XGBClassifier       No Oversampling        LogScaler            22                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05005559921264648, 0.1152...\n",
       "2535                  XGBClassifier       No Oversampling        LogScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05155096054077148, 0.1190...\n",
       "2534                  XGBClassifier       No Oversampling        LogScaler            23                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05224721431732178, 0.1147...\n",
       "415              AdaBoostClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.086018  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.049364900588989256, 0.141...\n",
       "465                   XGBClassifier       No Oversampling       No Scaling            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05914161205291748, 0.2110...\n",
       "463                   XGBClassifier       No Oversampling       No Scaling            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.08377583026885986, 0.1908...\n",
       "461                   XGBClassifier       No Oversampling       No Scaling            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.048862767219543454, 0.120...\n",
       "2528                  XGBClassifier       No Oversampling        LogScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.128010  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.047164511680603025, 0.122...\n",
       "424              AdaBoostClassifier       No Oversampling       No Scaling            17                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.083513  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0834766149520874, 0.25092...\n",
       "2526                  XGBClassifier       No Oversampling        LogScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.110174  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04756455421447754, 0.1249...\n",
       "2521                  XGBClassifier       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.112153  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04636678695678711, 0.1173...\n",
       "2516                  XGBClassifier       No Oversampling        LogScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07030153274536133, 0.1540...\n",
       "2514                  XGBClassifier       No Oversampling        LogScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.07439258098602294, 0.1579...\n",
       "2512                  XGBClassifier       No Oversampling        LogScaler            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.07638888359069824, 0.1446...\n",
       "2511                  XGBClassifier       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.103006  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0485623836517334, 0.11369...\n",
       "2509                  XGBClassifier       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.103006  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.044971132278442384, 0.109...\n",
       "426              AdaBoostClassifier       No Oversampling       No Scaling            18                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.087476  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0974393367767334, 0.27576...\n",
       "2510                  XGBClassifier       No Oversampling        LogScaler            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06621546745300293, 0.1588...\n",
       "1170                  XGBClassifier       No Oversampling     MinMaxScaler            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05305461883544922, 0.1191...\n",
       "483                   XGBClassifier       No Oversampling       No Scaling            22                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05415375232696533, 0.1261...\n",
       "1169                  XGBClassifier       No Oversampling     MinMaxScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05315177440643311, 0.1204...\n",
       "1854                  XGBClassifier       No Oversampling   StandardScaler            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06203348636627197, 0.1835...\n",
       "1853                  XGBClassifier       No Oversampling   StandardScaler            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07469966411590576, 0.1797...\n",
       "1852                  XGBClassifier       No Oversampling   StandardScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07100958824157715, 0.1970...\n",
       "1167                  XGBClassifier       No Oversampling     MinMaxScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.054151725769042966, 0.122...\n",
       "1166                  XGBClassifier       No Oversampling     MinMaxScaler            22                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05644643306732178, 0.1194...\n",
       "1851                  XGBClassifier       No Oversampling   StandardScaler            23                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06731901168823243, 0.1786...\n",
       "1850                  XGBClassifier       No Oversampling   StandardScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06662108898162841, 0.2061...\n",
       "1849                  XGBClassifier       No Oversampling   StandardScaler            22                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07539486885070801, 0.1741...\n",
       "1845                  XGBClassifier       No Oversampling   StandardScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.128010  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.07120904922485352, 0.1920...\n",
       "1843                  XGBClassifier       No Oversampling   StandardScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.110174  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06283140182495117, 0.1496...\n",
       "1160                  XGBClassifier       No Oversampling     MinMaxScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.110174  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06881554126739502, 0.1420...\n",
       "1836                  XGBClassifier       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05724701881408691, 0.1261...\n",
       "1833                  XGBClassifier       No Oversampling   StandardScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.049456214904785155, 0.150...\n",
       "1831                  XGBClassifier       No Oversampling   StandardScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06273086071014404, 0.2555...\n",
       "1829                  XGBClassifier       No Oversampling   StandardScaler            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05076322555541992, 0.1785...\n",
       "1827                  XGBClassifier       No Oversampling   StandardScaler            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0557497501373291, 0.14301...\n",
       "1153                  XGBClassifier       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.046674704551696776, 0.105...\n",
       "1150                  XGBClassifier       No Oversampling     MinMaxScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0661158800125122, 0.14271...\n",
       "1148                  XGBClassifier       No Oversampling     MinMaxScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06821739673614502, 0.1353...\n",
       "1146                  XGBClassifier       No Oversampling     MinMaxScaler            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06492578983306885, 0.1444...\n",
       "1144                  XGBClassifier       No Oversampling     MinMaxScaler            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06212446689605713, 0.1486...\n",
       "1109             AdaBoostClassifier       No Oversampling     MinMaxScaler            18                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.087476  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.062330222129821776, 0.169...\n",
       "1107             AdaBoostClassifier       No Oversampling     MinMaxScaler            17                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.083513  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05764613151550293, 0.1691...\n",
       "1098             AdaBoostClassifier       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.086018  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.059441137313842776, 0.184...\n",
       "1781             AdaBoostClassifier       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.086018  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04487917423248291, 0.1244...\n",
       "1790             AdaBoostClassifier       No Oversampling   StandardScaler            17                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.083513  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05772840976715088, 0.1551...\n",
       "1792             AdaBoostClassifier       No Oversampling   StandardScaler            18                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.087476  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.061433267593383786, 0.169...\n",
       "1168                  XGBClassifier       No Oversampling     MinMaxScaler            23                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05495185852050781, 0.1201...\n",
       "1162                  XGBClassifier       No Oversampling     MinMaxScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.128010  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05285842418670654, 0.1210...\n",
       "2537                  XGBClassifier       No Oversampling        LogScaler            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0576446533203125, 0.11748...\n",
       "2473             AdaBoostClassifier       No Oversampling        LogScaler            17                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.083513  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05744495391845703, 0.1520...\n",
       "484                   XGBClassifier       No Oversampling       No Scaling            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.11349625587463379, 0.2605...\n",
       "485                   XGBClassifier       No Oversampling       No Scaling            23                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.11429400444030761, 0.2532...\n",
       "486                   XGBClassifier       No Oversampling       No Scaling            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.10870921611785889, 0.2765...\n",
       "487                   XGBClassifier       No Oversampling       No Scaling            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.055741071701049805, 0.127...\n",
       "488                   XGBClassifier       No Oversampling       No Scaling            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05374746322631836, 0.1195...\n",
       "1171                  XGBClassifier       No Oversampling     MinMaxScaler            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05255935192108154, 0.1206...\n",
       "2475             AdaBoostClassifier       No Oversampling        LogScaler            18                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.087476  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.059335947036743164, 0.158...\n",
       "2536                  XGBClassifier       No Oversampling        LogScaler            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.049658918380737306, 0.117...\n",
       "2466             AdaBoostClassifier       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.086018  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.045376038551330565, 0.123...\n",
       "2468             AdaBoostClassifier       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.072739  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.048470354080200194, 0.131...\n",
       "2506                  XGBClassifier       No Oversampling        LogScaler             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.149015                                          0.128764  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.06232442855834961, 0.1646...\n",
       "1140                  XGBClassifier       No Oversampling     MinMaxScaler             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.149015                                          0.128764  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.05575051307678223, 0.1297...\n",
       "1402             LogisticRegression       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.149015                                          0.102588  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.2517263650894165, 0.32652...\n",
       "36               LogisticRegression       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.149015                                          0.102588  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.3012934684753418, 0.46006...\n",
       "457                   XGBClassifier       No Oversampling       No Scaling             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.149015                                          0.128764  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.0739020824432373, 0.20126...\n",
       "1823                  XGBClassifier       No Oversampling   StandardScaler             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.149015                                          0.128764  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.05554981231689453, 0.1282...\n",
       "2529                  XGBClassifier       No Oversampling        LogScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.149015                                          0.138101  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.04955921173095703, 0.1181...\n",
       "719              LogisticRegression       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.149015                                          0.102588  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.2949108839035034, 0.43982...\n",
       "2472             AdaBoostClassifier       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.146354                                          0.071731  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05425503253936768, 0.1473...\n",
       "2518                  XGBClassifier       No Oversampling        LogScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.119093  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07219626903533935, 0.1636...\n",
       "469                   XGBClassifier       No Oversampling       No Scaling            15                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.119093  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06751933097839355, 0.1985...\n",
       "1152                  XGBClassifier       No Oversampling     MinMaxScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.119093  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.056249046325683595, 0.141...\n",
       "2470             AdaBoostClassifier       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.146354                                          0.117310  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05226066112518311, 0.1454...\n",
       "1835                  XGBClassifier       No Oversampling   StandardScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.119093  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05714693069458008, 0.1358...\n",
       "1142                  XGBClassifier       No Oversampling     MinMaxScaler            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.124806  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.039194798469543456, 0.161...\n",
       "2508                  XGBClassifier       No Oversampling        LogScaler            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.124806  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06960606575012207, 0.1556...\n",
       "1145                  XGBClassifier       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.114216  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.046375465393066403, 0.110...\n",
       "2515                  XGBClassifier       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.043768787384033205, 0.113...\n",
       "475                   XGBClassifier       No Oversampling       No Scaling            18                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.143693                                          0.121461  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.05384728908538818, 0.1218...\n",
       "476                   XGBClassifier       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.11289780139923096, 0.2836...\n",
       "1147                  XGBClassifier       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04537820816040039, 0.1156...\n",
       "2524                  XGBClassifier       No Oversampling        LogScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.143693                                          0.121461  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.04865508079528809, 0.1162...\n",
       "1844                  XGBClassifier       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05445399284362793, 0.1674...\n",
       "1830                  XGBClassifier       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04448013305664063, 0.1406...\n",
       "1828                  XGBClassifier       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.114216  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.042585015296936035, 0.132...\n",
       "1842                  XGBClassifier       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.07380204200744629, 0.1564...\n",
       "1841                  XGBClassifier       No Oversampling   StandardScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.143693                                          0.121461  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.068715500831604, 0.163762...\n",
       "478                   XGBClassifier       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05086121559143066, 0.1235...\n",
       "1161                  XGBClassifier       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05166172981262207, 0.1113...\n",
       "1159                  XGBClassifier       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05385534763336182, 0.1165...\n",
       "459                   XGBClassifier       No Oversampling       No Scaling            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.124806  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06731965541839599, 0.2016...\n",
       "462                   XGBClassifier       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.114216  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07699387073516846, 0.2085...\n",
       "464                   XGBClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.07749254703521728, 0.1738...\n",
       "...                             ...                   ...              ...           ...                         ...                                                ...                                                ...                                 ...                                               ...                                                ...                                                ...\n",
       "1856                  XGBClassifier       No Oversampling   StandardScaler             1                         RFE                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.04158830642700195, 0.1239...\n",
       "1172                  XGBClassifier       No Oversampling     MinMaxScaler             1                 SelectKBest                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.030617785453796387, 0.097...\n",
       "2540                  XGBClassifier       No Oversampling        LogScaler             2                 SelectKBest                          [trustLevel, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.040684986114501956, 0.109...\n",
       "1855                  XGBClassifier       No Oversampling   StandardScaler             1                 SelectKBest                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.024733757972717284, 0.099...\n",
       "489                   XGBClassifier       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.08577024936676025, 0.1671...\n",
       "2622                     Perceptron       No Oversampling        LogScaler             6                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.476317                                          0.202372  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002000260353088379, 0.002...\n",
       "490                   XGBClassifier       No Oversampling       No Scaling             1                         RFE                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.06791837215423584, 0.1641...\n",
       "1939                     Perceptron       No Oversampling   StandardScaler             6                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.476317                                          0.202372  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0021890878677368166, 0.00...\n",
       "1256                     Perceptron       No Oversampling     MinMaxScaler             6                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.476317                                          0.202372  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0029921531677246094, 0.00...\n",
       "573                      Perceptron       No Oversampling       No Scaling             6                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.476317                                          0.202372  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0370006799697876, 0.02184...\n",
       "1173                  XGBClassifier       No Oversampling     MinMaxScaler             1                         RFE                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.05415475368499756, 0.1199...\n",
       "2539                  XGBClassifier       No Oversampling        LogScaler             1                         RFE                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.06810693740844727, 0.1593...\n",
       "2223           ExtraTreesClassifier       No Oversampling        LogScaler             2                         RFE                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.486961                                          0.174131  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.005185842514038086, 0.008...\n",
       "1931                     Perceptron       No Oversampling   StandardScaler             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020973682403564453, 0.00...\n",
       "1248                     Perceptron       No Oversampling     MinMaxScaler             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019947290420532227, 0.00...\n",
       "1415           KNeighborsClassifier       No Oversampling   StandardScaler             1                 SelectKBest                                      [tsne_axis_2]   {'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}                           -0.489622                                          0.226355  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.0025930881500244142, 0.00...\n",
       "49             KNeighborsClassifier       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]   {'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}                           -0.489622                                          0.226355  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.0024930715560913088, 0.00...\n",
       "565                      Perceptron       No Oversampling       No Scaling             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002094268798828125, 0.002...\n",
       "732            KNeighborsClassifier       No Oversampling     MinMaxScaler             1                 SelectKBest                                      [tsne_axis_2]   {'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}                           -0.489622                                          0.226355  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.0024931907653808595, 0.00...\n",
       "1539           ExtraTreesClassifier       No Oversampling   StandardScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.489622                                          0.176007  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.004784727096557617, 0.008...\n",
       "2614                     Perceptron       No Oversampling        LogScaler             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001698613166809082, 0.001...\n",
       "2273         RandomForestClassifier       No Oversampling        LogScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 20}                           -0.494944                                          0.208084  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.00997333526611328, 0.0172...\n",
       "175            ExtraTreesClassifier       No Oversampling       No Scaling             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 65}                           -0.497605                                          0.252656  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.00827794075012207, 0.0131...\n",
       "1176                  XGBClassifier       No Oversampling     MinMaxScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.038889741897583006, 0.104...\n",
       "493                   XGBClassifier       No Oversampling       No Scaling             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.03330719470977783, 0.1074...\n",
       "1857                  XGBClassifier       No Oversampling   StandardScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.028323936462402343, 0.090...\n",
       "1859                  XGBClassifier       No Oversampling   StandardScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.022838282585144042, 0.083...\n",
       "2542                  XGBClassifier       No Oversampling        LogScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.034804034233093264, 0.126...\n",
       "173            ExtraTreesClassifier       No Oversampling       No Scaling             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.505588                                          0.237846  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.006083846092224121, 0.012...\n",
       "1174                  XGBClassifier       No Oversampling     MinMaxScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.03789839744567871, 0.0878...\n",
       "491                   XGBClassifier       No Oversampling       No Scaling             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.04736628532409668, 0.1230...\n",
       "2099           KNeighborsClassifier       No Oversampling        LogScaler             2                 SelectKBest                          [trustLevel, tsne_axis_2]  {'n_neighbors': 15, 'p': 1, 'weights': 'distan...                           -0.508249                                          0.278805  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.0019944429397583006, 0.00...\n",
       "905          RandomForestClassifier       No Oversampling     MinMaxScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.508249                                          0.226676  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010073065757751465, 0.017...\n",
       "224          RandomForestClassifier       No Oversampling       No Scaling             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 50}                           -0.510910                                          0.206680  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.009773635864257812, 0.016...\n",
       "1589         RandomForestClassifier       No Oversampling   StandardScaler             2                         RFE                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 20}                           -0.513571                                          0.214829  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.01047201156616211, 0.0183...\n",
       "857            ExtraTreesClassifier       No Oversampling     MinMaxScaler             2                         RFE                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.513571                                          0.176453  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.007878780364990234, 0.013...\n",
       "1588         RandomForestClassifier       No Oversampling   StandardScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 50}                           -0.518893                                          0.286750  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010471916198730469, 0.017...\n",
       "1540           ExtraTreesClassifier       No Oversampling   StandardScaler             2                         RFE                          [pca_axis_2, tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 20}                           -0.518893                                          0.264937  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.0059841394424438475, 0.01...\n",
       "1590         RandomForestClassifier       No Oversampling   StandardScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]      {'criterion': 'entropy', 'n_estimators': 110}                           -0.518893                                          0.226578  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010073018074035645, 0.017...\n",
       "907          RandomForestClassifier       No Oversampling     MinMaxScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 35}                           -0.524215                                          0.268706  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.00607759952545166, 0.0106...\n",
       "858            ExtraTreesClassifier       No Oversampling     MinMaxScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 10}                           -0.524215                                          0.230935  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.0074798583984375, 0.01296...\n",
       "2224           ExtraTreesClassifier       No Oversampling        LogScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 55}                           -0.526876                                          0.279849  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.005688071250915527, 0.009...\n",
       "2272         RandomForestClassifier       No Oversampling        LogScaler             2                         RFE                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 70}                           -0.526876                                          0.244553  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.007480120658874512, 0.013...\n",
       "223          RandomForestClassifier       No Oversampling       No Scaling             2                         RFE                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.529537                                          0.244084  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010073137283325196, 0.017...\n",
       "856            ExtraTreesClassifier       No Oversampling     MinMaxScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]         {'criterion': 'gini', 'n_estimators': 130}                           -0.534859                                          0.215134  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.007779240608215332, 0.012...\n",
       "1541           ExtraTreesClassifier       No Oversampling   StandardScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 40}                           -0.540181                                          0.215757  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.008078479766845703, 0.013...\n",
       "220          RandomForestClassifier       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 10}                           -0.558808                                          0.375726  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010074925422668458, 0.017...\n",
       "1935                     Perceptron       No Oversampling   StandardScaler             4                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.585418                                          0.426356  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004488039016723633, 0.005...\n",
       "2618                     Perceptron       No Oversampling        LogScaler             4                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.585418                                          0.426356  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020936250686645506, 0.00...\n",
       "1252                     Perceptron       No Oversampling     MinMaxScaler             4                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.585418                                          0.426356  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004587554931640625, 0.003...\n",
       "569                      Perceptron       No Oversampling       No Scaling             4                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.585418                                          0.426356  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0018947839736938477, 0.00...\n",
       "2271         RandomForestClassifier       No Oversampling        LogScaler             2                 SelectKBest                          [trustLevel, tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 20}                           -0.588079                                          0.330010  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.009574294090270996, 0.016...\n",
       "904          RandomForestClassifier       No Oversampling     MinMaxScaler             1                         RFE                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.598723                                          0.272556  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.005787014961242676, 0.010...\n",
       "2222           ExtraTreesClassifier       No Oversampling        LogScaler             2                 SelectKBest                          [trustLevel, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 80}                           -0.609367                                          0.316002  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.0068816661834716795, 0.01...\n",
       "1587         RandomForestClassifier       No Oversampling   StandardScaler             1                         RFE                                      [tsne_axis_2]           {'criterion': 'gini', 'n_estimators': 5}                           -0.612028                                          0.276676  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.009873700141906739, 0.017...\n",
       "903          RandomForestClassifier       No Oversampling     MinMaxScaler             1                 SelectKBest                                      [tsne_axis_2]        {'criterion': 'entropy', 'n_estimators': 5}                           -0.612028                                          0.341373  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.007280588150024414, 0.014...\n",
       "1586         RandomForestClassifier       No Oversampling   StandardScaler             1                 SelectKBest                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.612028                                          0.282696  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010372328758239745, 0.016...\n",
       "2004  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                 {'reg_param': 0.1}                           -0.633316                                          0.314175  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0027924537658691405, 0.00...\n",
       "638   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                 {'reg_param': 0.1}                           -0.633316                                          0.314175  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028923749923706055, 0.00...\n",
       "2687  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                 {'reg_param': 0.1}                           -0.633316                                          0.314175  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0026931047439575197, 0.00...\n",
       "1321  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                 {'reg_param': 0.1}                           -0.633316                                          0.314175  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028923749923706055, 0.00...\n",
       "221          RandomForestClassifier       No Oversampling       No Scaling             1                         RFE                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 15}                           -0.635977                                          0.253680  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0098738431930542, 0.01755...\n",
       "2620                     Perceptron       No Oversampling        LogScaler             5                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.643960                                          0.338754  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002004098892211914, 0.002...\n",
       "1937                     Perceptron       No Oversampling   StandardScaler             5                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.643960                                          0.338754  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0028924942016601562, 0.00...\n",
       "571                      Perceptron       No Oversampling       No Scaling             5                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.643960                                          0.338754  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.011967825889587402, 0.003...\n",
       "1254                     Perceptron       No Oversampling     MinMaxScaler             5                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.643960                                          0.338754  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.00208892822265625, 0.0020...\n",
       "854            ExtraTreesClassifier       No Oversampling     MinMaxScaler             1                 SelectKBest                                      [tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 15}                           -0.646621                                          0.344109  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.007480049133300781, 0.012...\n",
       "171            ExtraTreesClassifier       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 40}                           -0.646621                                          0.318148  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.00747983455657959, 0.0127...\n",
       "1537           ExtraTreesClassifier       No Oversampling   StandardScaler             1                 SelectKBest                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 70}                           -0.659925                                          0.342170  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.006582307815551758, 0.011...\n",
       "855            ExtraTreesClassifier       No Oversampling     MinMaxScaler             1                         RFE                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 35}                           -0.665247                                          0.342096  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.007380127906799316, 0.012...\n",
       "2221           ExtraTreesClassifier       No Oversampling        LogScaler             1                         RFE                                      [tsne_axis_2]         {'criterion': 'gini', 'n_estimators': 195}                           -0.665247                                          0.343630  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.005485415458679199, 0.009...\n",
       "172            ExtraTreesClassifier       No Oversampling       No Scaling             1                         RFE                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 45}                           -0.665247                                          0.338414  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.0077789068222045895, 0.01...\n",
       "646   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.699840                                          0.231817  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003091740608215332, 0.003...\n",
       "1329  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.699840                                          0.231817  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0032912492752075195, 0.00...\n",
       "2012  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.699840                                          0.231817  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028923511505126952, 0.00...\n",
       "1538           ExtraTreesClassifier       No Oversampling   StandardScaler             1                         RFE                                       [pca_axis_2]          {'criterion': 'gini', 'n_estimators': 10}                           -0.718467                                          0.363213  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.005485129356384277, 0.009...\n",
       "1933                     Perceptron       No Oversampling   StandardScaler             3                         RFE    [scannedLineItems, secondsPerEuro, tsne_axis_2]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.758382                                          0.601923  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0016941308975219726, 0.00...\n",
       "1250                     Perceptron       No Oversampling     MinMaxScaler             3                         RFE    [scannedLineItems, secondsPerEuro, tsne_axis_2]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.758382                                          0.601923  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0023941993713378906, 0.00...\n",
       "567                      Perceptron       No Oversampling       No Scaling             3                         RFE    [scannedLineItems, secondsPerEuro, tsne_axis_2]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.758382                                          0.601923  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002094602584838867, 0.002...\n",
       "2616                     Perceptron       No Oversampling        LogScaler             3                         RFE    [scannedLineItems, secondsPerEuro, tsne_axis_2]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.758382                                          0.601923  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001989459991455078, 0.001...\n",
       "648   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.766365                                          0.250215  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002992057800292969, 0.003...\n",
       "647   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.766365                                          0.250215  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003091716766357422, 0.003...\n",
       "1330  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.766365                                          0.250215  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0037897586822509765, 0.00...\n",
       "1331  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.766365                                          0.250215  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0032914400100708006, 0.00...\n",
       "2013  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.766365                                          0.250215  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002692770957946777, 0.002...\n",
       "2014  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.766365                                          0.250215  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0036898136138916017, 0.00...\n",
       "2270         RandomForestClassifier       No Oversampling        LogScaler             1                         RFE                                       [pca_axis_2]          {'criterion': 'gini', 'n_estimators': 10}                           -0.766365                                          0.409595  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010372304916381836, 0.018...\n",
       "1333  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.792975                                          0.286752  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003690195083618164, 0.003...\n",
       "1324  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.6}                           -0.792975                                          0.251640  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028921127319335937, 0.00...\n",
       "1332  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.792975                                          0.286752  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003889608383178711, 0.003...\n",
       "2007  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.6}                           -0.792975                                          0.251640  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029918909072875976, 0.00...\n",
       "2006  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.6}                           -0.792975                                          0.251640  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002992129325866699, 0.003...\n",
       "2015  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.792975                                          0.286752  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003693246841430664, 0.003...\n",
       "2016  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.792975                                          0.286752  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0033852338790893556, 0.00...\n",
       "640   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.6}                           -0.792975                                          0.251640  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002992057800292969, 0.002...\n",
       "641   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.6}                           -0.792975                                          0.251640  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029918670654296873, 0.00...\n",
       "650   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.792975                                          0.286752  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034906864166259766, 0.00...\n",
       "649   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.792975                                          0.286752  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0033908843994140624, 0.00...\n",
       "1323  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.6}                           -0.792975                                          0.251640  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0030918121337890625, 0.00...\n",
       "2017  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.806280                                          0.262763  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002895641326904297, 0.003...\n",
       "651   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.806280                                          0.262763  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003889608383178711, 0.003...\n",
       "1334  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.806280                                          0.262763  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003789973258972168, 0.003...\n",
       "1335  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.819585                                          0.257036  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0035903453826904297, 0.00...\n",
       "2018  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.819585                                          0.257036  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003091859817504883, 0.004...\n",
       "652   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.819585                                          0.257036  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034908294677734376, 0.00...\n",
       "2019  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.838212                                          0.256809  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003789854049682617, 0.004...\n",
       "1336  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.838212                                          0.256809  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003789663314819336, 0.003...\n",
       "653   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.838212                                          0.256809  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034906148910522463, 0.00...\n",
       "645   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                 {'reg_param': 0.5}                           -0.848856                                          0.266595  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00299220085144043, 0.0030...\n",
       "1328  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                 {'reg_param': 0.5}                           -0.848856                                          0.266595  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0032911300659179688, 0.00...\n",
       "2011  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                 {'reg_param': 0.5}                           -0.848856                                          0.266595  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00299220085144043, 0.0032...\n",
       "1325  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.5}                           -0.854178                                          0.233818  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003390955924987793, 0.003...\n",
       "642   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.5}                           -0.854178                                          0.233818  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0030917882919311523, 0.00...\n",
       "2008  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.5}                           -0.854178                                          0.233818  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031913995742797853, 0.00...\n",
       "2692  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.001894998550415039, 0.002...\n",
       "2693  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025927543640136717, 0.00...\n",
       "2009  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029918432235717775, 0.00...\n",
       "2010  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0026930809020996095, 0.00...\n",
       "1326  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003490638732910156, 0.002...\n",
       "1327  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034904003143310545, 0.00...\n",
       "644   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003091716766357422, 0.003...\n",
       "643   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031916141510009766, 0.00...\n",
       "1687                     GaussianNB       No Oversampling   StandardScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                                 {}                           -0.968600                                          0.301185       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0028923511505126952], 'st...\n",
       "321                      GaussianNB       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                                 {}                           -0.968600                                          0.301185       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002892279624938965], 'std...\n",
       "1004                     GaussianNB       No Oversampling     MinMaxScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                                 {}                           -0.968600                                          0.301185       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0029920339584350586], 'st...\n",
       "2370                     GaussianNB       No Oversampling        LogScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                                 {}                           -0.968600                                          0.301185       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002992129325866699], 'std...\n",
       "1684                     GaussianNB       No Oversampling   StandardScaler             1                 SelectKBest                                      [tsne_axis_2]                                                 {}                           -0.992549                                          0.314330       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002194046974182129], 'std...\n",
       "1001                     GaussianNB       No Oversampling     MinMaxScaler             1                 SelectKBest                                      [tsne_axis_2]                                                 {}                           -0.992549                                          0.314330       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002194046974182129], 'std...\n",
       "318                      GaussianNB       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]                                                 {}                           -0.992549                                          0.314330       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.00239255428314209], 'std_...\n",
       "1688                     GaussianNB       No Oversampling   StandardScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.008515                                          0.316076       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0019947052001953124], 'st...\n",
       "1005                     GaussianNB       No Oversampling     MinMaxScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.008515                                          0.316076       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002992105484008789], 'std...\n",
       "322                      GaussianNB       No Oversampling       No Scaling             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.008515                                          0.316076       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0023935317993164064], 'st...\n",
       "2688  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             5                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                 {'reg_param': 0.6}                           -1.128260                                          0.362905  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028921127319335937, 0.00...\n",
       "2689  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             6                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                 {'reg_param': 0.5}                           -1.141565                                          0.375864  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0032910823822021483, 0.00...\n",
       "2690  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             7                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                 {'reg_param': 0.6}                           -1.168175                                          0.384575  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002692413330078125, 0.003...\n",
       "2691  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             8                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                 {'reg_param': 0.6}                           -1.216072                                          0.365313  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0021939516067504884, 0.00...\n",
       "323                      GaussianNB       No Oversampling       No Scaling             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.303885                                          0.336085       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002294039726257324], 'std...\n",
       "1006                     GaussianNB       No Oversampling     MinMaxScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.303885                                          0.336085       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091740608215332], 'std...\n",
       "1689                     GaussianNB       No Oversampling   StandardScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.303885                                          0.336085       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018947839736938477], 'st...\n",
       "2371                     GaussianNB       No Oversampling        LogScaler             5                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                                 {}                           -1.335817                                          0.347962       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091764450073242], 'std...\n",
       "2020  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.575306                                          0.407667  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003391265869140625, 0.003...\n",
       "654   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.575306                                          0.407667  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003690147399902344, 0.003...\n",
       "1337  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.575306                                          0.407667  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003490710258483887, 0.003...\n",
       "2697  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.615221                                          0.437013  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0037897348403930662, 0.00...\n",
       "2021  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.641831                                          0.451592  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00458831787109375, 0.0038...\n",
       "655   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.641831                                          0.451592  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034904241561889648, 0.00...\n",
       "1338  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.641831                                          0.451592  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0039894342422485355, 0.00...\n",
       "2704  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.655136                                          0.460204  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0039947509765625, 0.00378...\n",
       "2703  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.655136                                          0.460204  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002792501449584961, 0.002...\n",
       "2698  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.655136                                          0.482530  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003191542625427246, 0.002...\n",
       "2702  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.668441                                          0.483029  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031916618347167967, 0.00...\n",
       "2700  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.668441                                          0.483029  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.004288268089294433, 0.003...\n",
       "2699  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.668441                                          0.483029  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003633713722229004, 0.002...\n",
       "2701  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.668441                                          0.483029  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0038915395736694334, 0.00...\n",
       "2705  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.695051                                          0.469873  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.004288601875305176, 0.003...\n",
       "656   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.695051                                          0.469873  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.004088997840881348, 0.004...\n",
       "324                      GaussianNB       No Oversampling       No Scaling             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.695051                                          0.354357       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994633674621582], 'std...\n",
       "2022  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.695051                                          0.469873  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003390979766845703, 0.004...\n",
       "1690                     GaussianNB       No Oversampling   StandardScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.695051                                          0.354357       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994490623474121], 'std...\n",
       "1339  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.695051                                          0.469873  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0040891170501708984, 0.00...\n",
       "1007                     GaussianNB       No Oversampling     MinMaxScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.695051                                          0.354357       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091764450073242], 'std...\n",
       "2696  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                  {'reg_param': 0.7000000000000001}                           -1.753592                                          0.336484  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002422666549682617, 0.002...\n",
       "2372                     GaussianNB       No Oversampling        LogScaler             6                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                                 {}                           -1.753592                                          0.341291       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032912254333496093], 'st...\n",
       "2694  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                 {'reg_param': 0.30000000000000004}                           -1.830761                                          0.418668  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025930404663085938, 0.00...\n",
       "2024  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.355518  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0035905361175537108, 0.00...\n",
       "2706  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.351686  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.004498291015625, 0.003986...\n",
       "2707  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.355518  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.004487967491149903, 0.004...\n",
       "2023  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.351686  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.004587340354919434, 0.004...\n",
       "657   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.351686  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.06293184757232666, 0.0369...\n",
       "658   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.355518  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.04019250869750977, 0.1039...\n",
       "1341  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.355518  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0042888402938842775, 0.00...\n",
       "1340  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.351686  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0038893938064575194, 0.00...\n",
       "2695  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...                                 {'reg_param': 0.8}                           -1.910591                                          0.392880  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028924465179443358, 0.00...\n",
       "2368                     GaussianNB       No Oversampling        LogScaler             2                 SelectKBest                          [trustLevel, tsne_axis_2]                                                 {}                           -2.038318                                          0.383335       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002692842483520508], 'std...\n",
       "2373                     GaussianNB       No Oversampling        LogScaler             7                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                                 {}                           -2.038318                                          0.365848       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091883659362793], 'std...\n",
       "1002                     GaussianNB       No Oversampling     MinMaxScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]                                                 {}                           -2.051623                                          0.377972       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0029920339584350586], 'st...\n",
       "1685                     GaussianNB       No Oversampling   StandardScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]                                                 {}                           -2.051623                                          0.377972       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0031911611557006838], 'st...\n",
       "319                      GaussianNB       No Oversampling       No Scaling             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]                                                 {}                           -2.051623                                          0.377972       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0026927947998046874], 'st...\n",
       "1008                     GaussianNB       No Oversampling     MinMaxScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -2.064928                                          0.480548       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.00299227237701416], 'std_...\n",
       "325                      GaussianNB       No Oversampling       No Scaling             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -2.064928                                          0.480548       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017951726913452148], 'st...\n",
       "1691                     GaussianNB       No Oversampling   StandardScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -2.064928                                          0.480548       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017953634262084961], 'st...\n",
       "1696                     GaussianNB       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.481187       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017951488494873046], 'st...\n",
       "330                      GaussianNB       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.481187       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016982316970825194], 'st...\n",
       "1009                     GaussianNB       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0030918121337890625], 'st...\n",
       "1694                     GaussianNB       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018950939178466798], 'st...\n",
       "1011                     GaussianNB       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032912731170654298], 'st...\n",
       "2375                     GaussianNB       No Oversampling        LogScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032912731170654298], 'st...\n",
       "328                      GaussianNB       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018949031829833985], 'st...\n",
       "326                      GaussianNB       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0022939443588256836], 'st...\n",
       "1692                     GaussianNB       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017982006072998046], 'st...\n",
       "1013                     GaussianNB       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.481187       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0030918359756469727], 'st...\n",
       "2374                     GaussianNB       No Oversampling        LogScaler             8                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                                 {}                           -2.331027                                          0.437288       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091859817504883], 'std...\n",
       "1010                     GaussianNB       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.428792       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003191637992858887], 'std...\n",
       "327                      GaussianNB       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.428792       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002293896675109863], 'std...\n",
       "1012                     GaussianNB       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.422121       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091859817504883], 'std...\n",
       "329                      GaussianNB       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.422121       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018949508666992188], 'st...\n",
       "2376                     GaussianNB       No Oversampling        LogScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.428792       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032914400100708006], 'st...\n",
       "1695                     GaussianNB       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.422121       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018948078155517577], 'st...\n",
       "1693                     GaussianNB       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.428792       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017952442169189454], 'st...\n",
       "1686                     GaussianNB       No Oversampling   StandardScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]                                                 {}                           -2.410857                                          0.491129       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032910823822021483], 'st...\n",
       "320                      GaussianNB       No Oversampling       No Scaling             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]                                                 {}                           -2.410857                                          0.491129       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002991914749145508], 'std...\n",
       "2369                     GaussianNB       No Oversampling        LogScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]                                                 {}                           -2.410857                                          0.491129       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0030918359756469727], 'st...\n",
       "1003                     GaussianNB       No Oversampling     MinMaxScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]                                                 {}                           -2.410857                                          0.491129       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0028922319412231444], 'st...\n",
       "1014                     GaussianNB       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.424162                                          0.506500       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032912492752075195], 'st...\n",
       "1697                     GaussianNB       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.424162                                          0.506500       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994800567626953], 'std...\n",
       "331                      GaussianNB       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.424162                                          0.506500       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0015930891036987304], 'st...\n",
       "1015                     GaussianNB       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.437467                                          0.521442       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0028917789459228516], 'st...\n",
       "1698                     GaussianNB       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.437467                                          0.521442       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0019947052001953124], 'st...\n",
       "332                      GaussianNB       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.437467                                          0.521442       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017978906631469726], 'st...\n",
       "1699                     GaussianNB       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.623736                                          0.579755       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018949508666992188], 'st...\n",
       "1016                     GaussianNB       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.623736                                          0.579755       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003490638732910156], 'std...\n",
       "333                      GaussianNB       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.623736                                          0.579755       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018887519836425781], 'st...\n",
       "1700                     GaussianNB       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.668973                                          0.567880       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994633674621582], 'std...\n",
       "334                      GaussianNB       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.668973                                          0.567880       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0024932384490966796], 'st...\n",
       "1017                     GaussianNB       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.668973                                          0.567880       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032913684844970703], 'st...\n",
       "2377                     GaussianNB       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.868547                                          0.374236       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032912731170654298], 'st...\n",
       "1018                     GaussianNB       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.868547                                          0.607780       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003191351890563965], 'std...\n",
       "1701                     GaussianNB       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.868547                                          0.607780       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994681358337402], 'std...\n",
       "2380                     GaussianNB       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.868547                                          0.455081       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003191494941711426], 'std...\n",
       "335                      GaussianNB       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.868547                                          0.607780       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0024959564208984373], 'st...\n",
       "2379                     GaussianNB       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.881852                                          0.449491       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0035904645919799805], 'st...\n",
       "2378                     GaussianNB       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...                                                 {}                           -2.895157                                          0.447384       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.00329129695892334], 'std_...\n",
       "1019                     GaussianNB       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.908462                                          0.618653       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003191685676574707], 'std...\n",
       "1702                     GaussianNB       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.908462                                          0.618653       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002094554901123047], 'std...\n",
       "336                      GaussianNB       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.908462                                          0.618653       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002892422676086426], 'std...\n",
       "2381                     GaussianNB       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.113358                                          0.525436       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003590512275695801], 'std...\n",
       "1703                     GaussianNB       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.153273                                          0.592028       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018949508666992188], 'st...\n",
       "337                      GaussianNB       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.153273                                          0.592028       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091716766357422], 'std...\n",
       "1020                     GaussianNB       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.153273                                          0.592028       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0030918359756469727], 'st...\n",
       "1704                     GaussianNB       No Oversampling   StandardScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.299627                                          0.557549       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994681358337402], 'std...\n",
       "338                      GaussianNB       No Oversampling       No Scaling            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.299627                                          0.557549       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002992081642150879], 'std...\n",
       "1021                     GaussianNB       No Oversampling     MinMaxScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.299627                                          0.557549       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003191709518432617], 'std...\n",
       "2382                     GaussianNB       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.326237                                          0.546788       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003590297698974609], 'std...\n",
       "2383                     GaussianNB       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.339542                                          0.564224       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003391003608703613], 'std...\n",
       "2384                     GaussianNB       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.366152                                          0.555916       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0033910512924194337], 'st...\n",
       "2385                     GaussianNB       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.432677                                          0.536953       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032912492752075195], 'st...\n",
       "1706                     GaussianNB       No Oversampling   StandardScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002293968200683594], 'std...\n",
       "1707                     GaussianNB       No Oversampling   StandardScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0020971298217773438], 'st...\n",
       "2389                     GaussianNB       No Oversampling        LogScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003490638732910156], 'std...\n",
       "1705                     GaussianNB       No Oversampling   StandardScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0021943092346191407], 'st...\n",
       "2386                     GaussianNB       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.445982                                          0.534650       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0036902666091918946], 'st...\n",
       "2388                     GaussianNB       No Oversampling        LogScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003690338134765625], 'std...\n",
       "2390                     GaussianNB       No Oversampling        LogScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0037902355194091796], 'st...\n",
       "341                      GaussianNB       No Oversampling       No Scaling            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0031914710998535156], 'st...\n",
       "340                      GaussianNB       No Oversampling       No Scaling            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003191637992858887], 'std...\n",
       "339                      GaussianNB       No Oversampling       No Scaling            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0030918359756469727], 'st...\n",
       "1024                     GaussianNB       No Oversampling     MinMaxScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0022940635681152344], 'st...\n",
       "1023                     GaussianNB       No Oversampling     MinMaxScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002592611312866211], 'std...\n",
       "1022                     GaussianNB       No Oversampling     MinMaxScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002991914749145508], 'std...\n",
       "2387                     GaussianNB       No Oversampling        LogScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.472592                                          0.553080       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0033912181854248045], 'st...\n",
       "\n",
       "[2732 rows x 11 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from math import log\n",
    "\n",
    "result_table = pd.DataFrame(columns=[\"Model\", \"Oversampling Strategy\", \"Data Preparation\", \"Feature Count\", \"Feature Selection Technique\", \"Features\", \"Optimal Parameters\", \"Monetary Value Per Instance - Mean\", \"Monetary Value Per Instance - Standard Deviation\", \"Raw Model\", 'Cross Validation Results'])\n",
    "\n",
    "for oversampling_strategy in range(1,2):  # Oversamling strategies currtently not in the loop\n",
    "    if (oversampling_strategy == 1):  \n",
    "        Y = train['fraud']\n",
    "        X = train.drop('fraud',axis=1)\n",
    "        oversampling = \"No Oversampling\"\n",
    "    elif (oversampling_strategy == 2):\n",
    "        extended_train = randomOverSampling(train)\n",
    "        Y = extended_train['fraud']\n",
    "        X = extended_train.drop('fraud',axis=1)\n",
    "        oversampling = \"Random Oversampling\"\n",
    "    elif (oversampling_strategy == 3):\n",
    "        extended_train = smoteOverSamling(train)\n",
    "        Y = extended_train['fraud']\n",
    "        X = extended_train.drop('fraud',axis=1)\n",
    "        oversampling = \"SMOTE\"\n",
    "    elif (oversampling_strategy == 3):\n",
    "        extended_train = adasynOverSamling(train)\n",
    "        Y = extended_train['fraud']\n",
    "        X = extended_train.drop('fraud',axis=1)\n",
    "        oversampling = \"Adaysin\"\n",
    "            \n",
    "    # four types of data preparation: No preparation, MaxMinScaler, StandardScaler, LogScaling\n",
    "    for data_preparation_strategy in range(1,5):\n",
    "        if (data_preparation_strategy == 1):  \n",
    "            X_scaled = X\n",
    "            data_preparation = \"No Scaling\"\n",
    "        elif (data_preparation_strategy == 2):\n",
    "            feature_scaler = MinMaxScaler()  \n",
    "            X_scaled = pd.DataFrame(feature_scaler.fit_transform(X.values), columns=X.columns, index=X.index) \n",
    "            data_preparation = \"MinMaxScaler\"\n",
    "        elif (data_preparation_strategy == 3):\n",
    "            feature_scaler = StandardScaler()  \n",
    "            X_scaled = pd.DataFrame(feature_scaler.fit_transform(X.values), columns=X.columns, index=X.index)\n",
    "            data_preparation = \"StandardScaler\"\n",
    "        elif (data_preparation_strategy == 4):\n",
    "            transformer = FunctionTransformer(np.log1p, validate=True)  \n",
    "            \n",
    "            # pca and tsne feature cause an error -> therefore no log scaling            \n",
    "            X_scaled = pd.DataFrame(transformer.transform(X.iloc[:, range(0,20)]), columns=X.iloc[:, range(0,20)].columns, index=X.iloc[:, range(0,20)].index)\n",
    "            \n",
    "            X_scaled['pca_axis_1'] = X['pca_axis_1']\n",
    "            X_scaled['pca_axis_2'] = X['pca_axis_2']\n",
    "            X_scaled['tsne_axis_1'] = X['tsne_axis_1']\n",
    "            X_scaled['tsne_axis_2'] = X['tsne_axis_2']\n",
    "\n",
    "            data_preparation = \"LogScaler\"    \n",
    "\n",
    "\n",
    "\n",
    "        for model in model_tuning_factory:   # replace with model_tuning_factory_randomized for faster results\n",
    "\n",
    "                \n",
    "            for feature_count in range(1,len(list(X))+1):\n",
    "   \n",
    "                random.seed = 42\n",
    "                model.seed = 42\n",
    "                start_time = time.time()              \n",
    "                \n",
    "                \n",
    "                # Solution with SelectKBest\n",
    "                best_features = SelectKBest(f_classif, k=feature_count).fit(X_scaled,Y)\n",
    "                best_feature_list = X.columns[best_features.get_support()]\n",
    "                X_selected_features = X[best_feature_list]\n",
    "                \n",
    "                model.fit(X_selected_features,Y)  \n",
    "                model_name = model.best_estimator_.__class__.__name__\n",
    "                score_mean = model.cv_results_['mean_test_score'][model.best_index_]\n",
    "                score_std = model.cv_results_['std_test_score'][model.best_index_]\n",
    "                cross_validation_results = model.cv_results_\n",
    "                \n",
    "                result_table = result_table.append({\n",
    "                 \"Model\": model_name,\n",
    "                 \"Oversampling Strategy\": oversampling,   \n",
    "                 \"Data Preparation\": data_preparation,   \n",
    "                 \"Feature Count\": feature_count,\n",
    "                 \"Feature Selection Technique\": \"SelectKBest\",   \n",
    "                 \"Features\": best_feature_list.values, \n",
    "                 \"Optimal Parameters\": model.best_params_,\n",
    "                 \"Monetary Value Per Instance - Mean\":  score_mean,\n",
    "                 \"Monetary Value Per Instance - Standard Deviation\": score_std,   \n",
    "                 \"Raw Model\": model.best_estimator_,\n",
    "                 \"Cross Validation Results\": cross_validation_results\n",
    "                  }, ignore_index=True)\n",
    "                \n",
    "\n",
    "                # Solution with Recursive Feature Elimination -> only works for some models\n",
    "                \n",
    "                if (model.estimator.__class__.__name__ == 'LogisticRegression'\n",
    "                 or model.estimator.__class__.__name__ == 'DecisionTreeClassifier'\n",
    "                 or model.estimator.__class__.__name__ == 'ExtraTreeClassifier'\n",
    "                 or model.estimator.__class__.__name__ == 'ExtraTreesClassifier'\n",
    "                 or model.estimator.__class__.__name__ == 'RandomForestClassifier'\n",
    "                 or model.estimator.__class__.__name__ == 'BernoulliNB'\n",
    "                 or model.estimator.__class__.__name__ == 'AdaBoostClassifier'\n",
    "                 or model.estimator.__class__.__name__ == 'GradientBoostingClassifier'\n",
    "                 or model.estimator.__class__.__name__ == 'XGBClassifier'    \n",
    "                 or model.estimator.__class__.__name__ == 'Perceptron'\n",
    "                 or model.estimator.__class__.__name__ == 'LinearDiscriminantAnalysis'):\n",
    "                \n",
    "                   # Traditional RFE\n",
    "                    rfe = RFE(model.estimator, n_features_to_select = feature_count)\n",
    "                    rfe = rfe.fit(X,Y)\n",
    "                    best_feature_list = np.array(list(X))[np.array(rfe.support_)]\n",
    "                    X_selected_features = X[best_feature_list]\n",
    "\n",
    "                    model.fit(X_selected_features,Y)  \n",
    "                    model_name = model.best_estimator_.__class__.__name__\n",
    "                    score_mean = model.cv_results_['mean_test_score'][model.best_index_]\n",
    "                    score_std = model.cv_results_['std_test_score'][model.best_index_]\n",
    "                    cross_validation_results = model.cv_results_\n",
    "\n",
    "\n",
    "                    result_table = result_table.append({\n",
    "                     \"Model\": model_name,\n",
    "                     \"Oversampling Strategy\": oversampling,   \n",
    "                     \"Data Preparation\": data_preparation,\n",
    "                     \"Feature Count\": feature_count,\n",
    "                     \"Feature Selection Technique\": \"RFE\",\n",
    "                     \"Features\": best_feature_list,\n",
    "                     \"Optimal Parameters\": model.best_params_,\n",
    "                     \"Monetary Value Per Instance - Mean\":  score_mean,\n",
    "                     \"Monetary Value Per Instance - Standard Deviation\": score_std,  \n",
    "                     \"Raw Model\": model.best_estimator_,\n",
    "                     \"Cross Validation Results\": cross_validation_results\n",
    "                      }, ignore_index=True)\n",
    "                    \n",
    "                end_time = time.time()\n",
    "\n",
    "                print(\"Finished \" + model.best_estimator_.__class__.__name__ + \" with \" + oversampling + \" and \" + data_preparation + \" and \" + str(feature_count) + \" features after \" + str(round(end_time - start_time, 2)) + \" seconds\")\n",
    "\n",
    "                 \n",
    "                \n",
    "            if (model.estimator.__class__.__name__ == 'LogisticRegression'\n",
    "             or model.estimator.__class__.__name__ == 'DecisionTreeClassifier'\n",
    "             or model.estimator.__class__.__name__ == 'ExtraTreeClassifier'\n",
    "             or model.estimator.__class__.__name__ == 'ExtraTreesClassifier'\n",
    "             or model.estimator.__class__.__name__ == 'RandomForestClassifier'\n",
    "             or model.estimator.__class__.__name__ == 'BernoulliNB'\n",
    "             or model.estimator.__class__.__name__ == 'AdaBoostClassifier'\n",
    "             or model.estimator.__class__.__name__ == 'GradientBoostingClassifier'\n",
    "             or model.estimator.__class__.__name__ == 'XGBClassifier'    \n",
    "             or model.estimator.__class__.__name__ == 'Perceptron'\n",
    "             or model.estimator.__class__.__name__ == 'LinearDiscriminantAnalysis'):\n",
    "\n",
    "                # RFE with Cross Validation -> determines the optimum feature count automatically\n",
    "                rfecv = RFECV(model.estimator, cv = skf)\n",
    "                rfecv = rfe.fit(X,Y)\n",
    "                best_feature_list = np.array(list(X))[np.array(rfecv.support_)]\n",
    "                X_selected_features = X[best_feature_list]\n",
    "\n",
    "                model.fit(X_selected_features,Y)  \n",
    "                model_name = model.best_estimator_.__class__.__name__\n",
    "                score_mean = model.cv_results_['mean_test_score'][model.best_index_]\n",
    "                score_std = model.cv_results_['std_test_score'][model.best_index_]\n",
    "                cross_validation_results = model.cv_results_\n",
    "\n",
    "                result_table = result_table.append({\n",
    "                 \"Model\": model_name,\n",
    "                 \"Oversampling Strategy\": oversampling,   \n",
    "                 \"Data Preparation\": data_preparation,\n",
    "                 \"Feature Count\": len(best_feature_list),\n",
    "                 \"Feature Selection Technique\": \"RFECV\",\n",
    "                 \"Features\": best_feature_list,\n",
    "                 \"Optimal Parameters\": model.best_params_,\n",
    "                 \"Monetary Value Per Instance - Mean\":  score_mean,\n",
    "                 \"Monetary Value Per Instance - Standard Deviation\": score_std,    \n",
    "                 \"Raw Model\": model.best_estimator_,\n",
    "                 \"Cross Validation Results\": cross_validation_results\n",
    "                  }, ignore_index=True)\n",
    "                    \n",
    "result_table.sort_values(by = \"Monetary Value Per Instance - Mean\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore the saved result table to avoid recomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_table.to_pickle(\"result_table_training_set_final.pkl\")\n",
    "result_table = pd.read_pickle(\"result_table_training_set_final.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Oversampling Strategy</th>\n",
       "      <th>Data Preparation</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Feature Selection Technique</th>\n",
       "      <th>Features</th>\n",
       "      <th>Optimal Parameters</th>\n",
       "      <th>Monetary Value Per Instance - Mean</th>\n",
       "      <th>Monetary Value Per Instance - Standard Deviation</th>\n",
       "      <th>Raw Model</th>\n",
       "      <th>Cross Validation Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05056438446044922, 0.1769...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05425493717193604, 0.2058...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0544543981552124, 0.20874...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05784509181976318, 0.1759...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05764601230621338, 0.2370...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04896423816680908, 0.1266...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05176126956939697, 0.1792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04527671337127685, 0.1219...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09464631080627442, 0.2716...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07219569683074951, 0.1997...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06940767765045167, 0.1830...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07019481658935547, 0.1837...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0765883445739746, 0.20574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08726654052734376, 0.2142...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07240362167358398, 0.1908...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.056780</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09723961353302002, 0.2663...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.067877</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04886937141418457, 0.2208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.067877</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.042185854911804196, 0.113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.067877</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06083579063415527, 0.2342...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.067877</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.054952597618103026, 0.169...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.048834</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06512563228607178, 0.2223...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.048834</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05804469585418701, 0.1643...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.048834</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06412808895111084, 0.2182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05774281024932861, 0.2250...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0528484582901001, 0.13783...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.057942438125610354, 0.167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.050159239768981935, 0.137...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06233291625976563, 0.2334...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.180947</td>\n",
       "      <td>0.065235</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06372709274291992, 0.1700...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.178286</td>\n",
       "      <td>0.037837</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05504775047302246, 0.1483...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.178286</td>\n",
       "      <td>0.037837</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0602384090423584, 0.16525...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.178286</td>\n",
       "      <td>0.037837</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05754311084747314, 0.1559...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.178286</td>\n",
       "      <td>0.037837</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08517186641693116, 0.2419...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05365612506866455, 0.1235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.055957</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10382180213928223, 0.2846...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0863689661026001, 0.22998...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06043775081634521, 0.1801...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.055957</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0884629487991333, 0.23247...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.055957</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08497223854064942, 0.2405...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.085051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.047373270988464354, 0.227...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.085051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.054852294921875, 0.197471...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07329485416412354, 0.1973...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.085051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04617421627044678, 0.1861...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.055957</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10900802612304687, 0.2985...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.055957</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08357594013214112, 0.2111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.085051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04158902168273926, 0.1112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05434770584106445, 0.1255...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0904578685760498, 0.23537...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07040562629699706, 0.1862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05115077495574951, 0.1144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.055957</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08427395820617675, 0.2062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10053102970123291, 0.2586...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.14730463027954102, 0.4388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.16774826049804686, 0.4257...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10761196613311767, 0.3731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07768809795379639, 0.2032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07998497486114502, 0.2098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10192716121673584, 0.3012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10950686931610107, 0.3880...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09693987369537353, 0.2672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.076385760307312, 0.215919...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.10561714172363282, 0.2862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08666703701019288, 0.2775...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09614214897155762, 0.2436...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07319338321685791, 0.1937...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07369229793548585, 0.1929...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.11190094947814941, 0.3706...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09414799213409424, 0.2254...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07559263706207275, 0.1951...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07579183578491211, 0.1952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.067281</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07398192882537842, 0.1952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.078339</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07119827270507813, 0.1813...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.078339</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08337123394012451, 0.2124...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.047732</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05874302387237549, 0.1603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.047732</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0631256103515625, 0.17283...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.091369</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04487769603729248, 0.2045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.091369</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0493680477142334, 0.18689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.078339</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06620879173278808, 0.1773...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.091369</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03450796604156494, 0.0948...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.091369</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03640217781066894, 0.0924...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.047732</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.054049324989318845, 0.151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.047732</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05196094512939453, 0.1405...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.170303</td>\n",
       "      <td>0.078339</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07130908966064453, 0.1967...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.075290</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07280433177947998, 0.1994...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.090441</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06432745456695557, 0.2056...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.090441</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.047373604774475095, 0.127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.053056859970092775, 0.146...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.090441</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.050658655166625974, 0.134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.075290</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06900987625122071, 0.1769...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.075290</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06621336936950684, 0.1708...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07031164169311524, 0.1879...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06103696823120117, 0.1693...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.164981</td>\n",
       "      <td>0.075290</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06394476890563965, 0.1676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.079998</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04228670597076416, 0.1146...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.099022</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06622262001037597, 0.1593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.099022</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.10741264820098877, 0.2868...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.079998</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.061435699462890625, 0.167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.083711</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04746236801147461, 0.1296...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.079998</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04567661285400391, 0.1585...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.083711</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.056648421287536624, 0.216...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.077572</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06681082248687745, 0.1788...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.084592</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.09494562149047851, 0.3041...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.083711</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.055950379371643065, 0.174...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.099022</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07051081657409668, 0.1447...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.084592</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.06971023082733155, 0.1898...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.111650</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04616866111755371, 0.1168...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.099022</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.061125540733337404, 0.167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.162320</td>\n",
       "      <td>0.084592</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07420058250427246, 0.2036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.105269</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0828782081604004, 0.24354...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.104804</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06861603260040283, 0.1392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.104804</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05385591983795166, 0.1222...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.104804</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.11539115905761718, 0.2542...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.105269</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05683636665344238, 0.1550...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.28064892292022703, 0.4285...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2594059944152832, 0.43603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.28743069171905516, 0.4544...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.33161239624023436, 0.5098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.18031723499298097, 0.2596...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.6, 'fit_intercept': True, 'solver': 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.6, class_weight=None, d...</td>\n",
       "      <td>{'mean_fit_time': [0.16236634254455568, 0.2576...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.105269</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05575084686279297, 0.1612...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.104804</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04736342430114746, 0.1138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2527</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.103828</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04557158946990967, 0.1204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.096857</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0503582239151001, 0.11379...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.129985</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.053855514526367186, 0.184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.129985</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.038297486305236814, 0.116...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.093202</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07340354919433593, 0.1800...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.132513</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.10751242637634277, 0.2949...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.129985</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.031216812133789063, 0.086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.132513</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05465312004089355, 0.1188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.129985</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.03919310569763183, 0.1301...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.132513</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06941385269165039, 0.1625...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.093202</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.07908811569213867, 0.2218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.156998</td>\n",
       "      <td>0.093202</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.08766539096832275, 0.2657...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2595245122909546, 0.45370...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.079820</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05744645595550537, 0.1536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.102953</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2513269424438477, 0.41508...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.102953</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.14659833908081055, 0.2325...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.16226651668548583, 0.2745...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.19009184837341309, 0.2466...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.165457820892334, 0.256612...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.22858889102935792, 0.3346...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.067006</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04776921272277832, 0.1276...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.25202579498291017, 0.4675...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.13912887573242189, 0.2344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.1782235860824585, 0.26489...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.102953</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2847379922866821, 0.40661...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2579099893569946, 0.44530...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.20944011211395264, 0.2536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2762607574462891, 0.41070...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.2000000000000002, 'fit_intercept': Tru...</td>\n",
       "      <td>0.154337</td>\n",
       "      <td>0.091360</td>\n",
       "      <td>LogisticRegression(C=1.2000000000000002, class...</td>\n",
       "      <td>{'mean_fit_time': [0.19019110202789308, 0.2361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.051450514793396, 0.113895...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.08128278255462647, 0.1861...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05295817852020264, 0.1214...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.110174</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.08995909690856933, 0.1844...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.128010</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0595362663269043, 0.12007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05005559921264648, 0.1152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05155096054077148, 0.1190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05224721431732178, 0.1147...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.086018</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.049364900588989256, 0.141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05914161205291748, 0.2110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.08377583026885986, 0.1908...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.048862767219543454, 0.120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.128010</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.047164511680603025, 0.122...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.083513</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0834766149520874, 0.25092...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.110174</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04756455421447754, 0.1249...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.112153</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04636678695678711, 0.1173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07030153274536133, 0.1540...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07439258098602294, 0.1579...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07638888359069824, 0.1446...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.103006</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0485623836517334, 0.11369...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.103006</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.044971132278442384, 0.109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.087476</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0974393367767334, 0.27576...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06621546745300293, 0.1588...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05305461883544922, 0.1191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05415375232696533, 0.1261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05315177440643311, 0.1204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06203348636627197, 0.1835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07469966411590576, 0.1797...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07100958824157715, 0.1970...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.054151725769042966, 0.122...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05644643306732178, 0.1194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06731901168823243, 0.1786...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06662108898162841, 0.2061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07539486885070801, 0.1741...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.128010</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07120904922485352, 0.1920...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.110174</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06283140182495117, 0.1496...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.110174</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06881554126739502, 0.1420...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05724701881408691, 0.1261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.049456214904785155, 0.150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06273086071014404, 0.2555...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05076322555541992, 0.1785...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0557497501373291, 0.14301...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.046674704551696776, 0.105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0661158800125122, 0.14271...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06821739673614502, 0.1353...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06492578983306885, 0.1444...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.107045</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06212446689605713, 0.1486...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.087476</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.062330222129821776, 0.169...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.083513</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05764613151550293, 0.1691...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.086018</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.059441137313842776, 0.184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.086018</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.04487917423248291, 0.1244...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.083513</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05772840976715088, 0.1551...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.087476</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.061433267593383786, 0.169...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05495185852050781, 0.1201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.128010</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05285842418670654, 0.1210...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0576446533203125, 0.11748...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.083513</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05744495391845703, 0.1520...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.11349625587463379, 0.2605...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>23</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.11429400444030761, 0.2532...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.10870921611785889, 0.2765...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.055741071701049805, 0.127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05374746322631836, 0.1195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFECV</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05255935192108154, 0.1206...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, grandTotal, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.087476</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.059335947036743164, 0.158...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.102131</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.049658918380737306, 0.117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.086018</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.045376038551330565, 0.123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.151676</td>\n",
       "      <td>0.072739</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.048470354080200194, 0.131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2506</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.128764</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06232442855834961, 0.1646...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.128764</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05575051307678223, 0.1297...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.102588</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2517263650894165, 0.32652...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.102588</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.3012934684753418, 0.46006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.128764</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.0739020824432373, 0.20126...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.128764</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05554981231689453, 0.1282...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2529</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.138101</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04955921173095703, 0.1181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.3000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.149015</td>\n",
       "      <td>0.102588</td>\n",
       "      <td>LogisticRegression(C=1.3000000000000003, class...</td>\n",
       "      <td>{'mean_fit_time': [0.2949108839035034, 0.43982...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.071731</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05425503253936768, 0.1473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.119093</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07219626903533935, 0.1636...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.119093</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06751933097839355, 0.1985...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.119093</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.056249046325683595, 0.141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'base_estimator': Decis...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.117310</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.05226066112518311, 0.1454...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.119093</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05714693069458008, 0.1358...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.124806</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.039194798469543456, 0.161...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.124806</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06960606575012207, 0.1556...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.114216</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.046375465393066403, 0.110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.043768787384033205, 0.113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.121461</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05384728908538818, 0.1218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.11289780139923096, 0.2836...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04537820816040039, 0.1156...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.121461</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04865508079528809, 0.1162...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05445399284362793, 0.1674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.04448013305664063, 0.1406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.114216</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.042585015296936035, 0.132...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07380204200744629, 0.1564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.3, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.121461</td>\n",
       "      <td>XGBClassifier(base_score=0.3, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.068715500831604, 0.163762...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05086121559143066, 0.1235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05166172981262207, 0.1113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.05385534763336182, 0.1165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.124806</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.06731965541839599, 0.2016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.7, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.114216</td>\n",
       "      <td>XGBClassifier(base_score=0.7, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07699387073516846, 0.2085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.143693</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>{'mean_fit_time': [0.07749254703521728, 0.1738...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.04158830642700195, 0.1239...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.030617785453796387, 0.097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.040684986114501956, 0.109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.024733757972717284, 0.099...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.08577024936676025, 0.1671...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.202372</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002000260353088379, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.06791837215423584, 0.1641...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.202372</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0021890878677368166, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.202372</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0029921531677246094, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, grandTotal, scannedLineItems, pri...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.202372</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0370006799697876, 0.02184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.05415475368499756, 0.1199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.476317</td>\n",
       "      <td>0.221986</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.06810693740844727, 0.1593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.486961</td>\n",
       "      <td>0.174131</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.005185842514038086, 0.008...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020973682403564453, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0019947290420532227, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.226355</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.0025930881500244142, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.226355</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.0024930715560913088, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002094268798828125, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.226355</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.0024931907653808595, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.176007</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.004784727096557617, 0.008...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.489622</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001698613166809082, 0.001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 20}</td>\n",
       "      <td>-0.494944</td>\n",
       "      <td>0.208084</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.00997333526611328, 0.0172...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 65}</td>\n",
       "      <td>-0.497605</td>\n",
       "      <td>0.252656</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.00827794075012207, 0.0131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.038889741897583006, 0.104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.03330719470977783, 0.1074...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.028323936462402343, 0.090...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.022838282585144042, 0.083...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.034804034233093264, 0.126...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.237846</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.006083846092224121, 0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.03789839744567871, 0.0878...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'base_estimator': DecisionTreeClassifier(clas...</td>\n",
       "      <td>-0.505588</td>\n",
       "      <td>0.243221</td>\n",
       "      <td>XGBClassifier(base_estimator=DecisionTreeClass...</td>\n",
       "      <td>{'mean_fit_time': [0.04736628532409668, 0.1230...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, tsne_axis_2]</td>\n",
       "      <td>{'n_neighbors': 15, 'p': 1, 'weights': 'distan...</td>\n",
       "      <td>-0.508249</td>\n",
       "      <td>0.278805</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "      <td>{'mean_fit_time': [0.0019944429397583006, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.508249</td>\n",
       "      <td>0.226676</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010073065757751465, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 50}</td>\n",
       "      <td>-0.510910</td>\n",
       "      <td>0.206680</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.009773635864257812, 0.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 20}</td>\n",
       "      <td>-0.513571</td>\n",
       "      <td>0.214829</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.01047201156616211, 0.0183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.513571</td>\n",
       "      <td>0.176453</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.007878780364990234, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 50}</td>\n",
       "      <td>-0.518893</td>\n",
       "      <td>0.286750</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010471916198730469, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 20}</td>\n",
       "      <td>-0.518893</td>\n",
       "      <td>0.264937</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.0059841394424438475, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 110}</td>\n",
       "      <td>-0.518893</td>\n",
       "      <td>0.226578</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010073018074035645, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 35}</td>\n",
       "      <td>-0.524215</td>\n",
       "      <td>0.268706</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.00607759952545166, 0.0106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-0.524215</td>\n",
       "      <td>0.230935</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.0074798583984375, 0.01296...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 55}</td>\n",
       "      <td>-0.526876</td>\n",
       "      <td>0.279849</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.005688071250915527, 0.009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 70}</td>\n",
       "      <td>-0.526876</td>\n",
       "      <td>0.244553</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.007480120658874512, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.529537</td>\n",
       "      <td>0.244084</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010073137283325196, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 130}</td>\n",
       "      <td>-0.534859</td>\n",
       "      <td>0.215134</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.007779240608215332, 0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 40}</td>\n",
       "      <td>-0.540181</td>\n",
       "      <td>0.215757</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.008078479766845703, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-0.558808</td>\n",
       "      <td>0.375726</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010074925422668458, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.585418</td>\n",
       "      <td>0.426356</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004488039016723633, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.585418</td>\n",
       "      <td>0.426356</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0020936250686645506, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.585418</td>\n",
       "      <td>0.426356</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.004587554931640625, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, pricePerScannedLineItem, se...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.585418</td>\n",
       "      <td>0.426356</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0018947839736938477, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 20}</td>\n",
       "      <td>-0.588079</td>\n",
       "      <td>0.330010</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.009574294090270996, 0.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.598723</td>\n",
       "      <td>0.272556</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.005787014961242676, 0.010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 80}</td>\n",
       "      <td>-0.609367</td>\n",
       "      <td>0.316002</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.0068816661834716795, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 5}</td>\n",
       "      <td>-0.612028</td>\n",
       "      <td>0.276676</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.009873700141906739, 0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 5}</td>\n",
       "      <td>-0.612028</td>\n",
       "      <td>0.341373</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.007280588150024414, 0.014...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 10}</td>\n",
       "      <td>-0.612028</td>\n",
       "      <td>0.282696</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010372328758239745, 0.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.633316</td>\n",
       "      <td>0.314175</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0027924537658691405, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.633316</td>\n",
       "      <td>0.314175</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028923749923706055, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.633316</td>\n",
       "      <td>0.314175</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0026931047439575197, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'reg_param': 0.1}</td>\n",
       "      <td>-0.633316</td>\n",
       "      <td>0.314175</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028923749923706055, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 15}</td>\n",
       "      <td>-0.635977</td>\n",
       "      <td>0.253680</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.0098738431930542, 0.01755...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2620</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.643960</td>\n",
       "      <td>0.338754</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002004098892211914, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.643960</td>\n",
       "      <td>0.338754</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0028924942016601562, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.643960</td>\n",
       "      <td>0.338754</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.011967825889587402, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[grandTotal, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.643960</td>\n",
       "      <td>0.338754</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.00208892822265625, 0.0020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 15}</td>\n",
       "      <td>-0.646621</td>\n",
       "      <td>0.344109</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.007480049133300781, 0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 40}</td>\n",
       "      <td>-0.646621</td>\n",
       "      <td>0.318148</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.00747983455657959, 0.0127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 70}</td>\n",
       "      <td>-0.659925</td>\n",
       "      <td>0.342170</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.006582307815551758, 0.011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 35}</td>\n",
       "      <td>-0.665247</td>\n",
       "      <td>0.342096</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.007380127906799316, 0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 195}</td>\n",
       "      <td>-0.665247</td>\n",
       "      <td>0.343630</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.005485415458679199, 0.009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 45}</td>\n",
       "      <td>-0.665247</td>\n",
       "      <td>0.338414</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.0077789068222045895, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.699840</td>\n",
       "      <td>0.231817</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003091740608215332, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.699840</td>\n",
       "      <td>0.231817</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0032912492752075195, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.699840</td>\n",
       "      <td>0.231817</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028923511505126952, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-0.718467</td>\n",
       "      <td>0.363213</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "      <td>{'mean_fit_time': [0.005485129356384277, 0.009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro, tsne_axis_2]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.758382</td>\n",
       "      <td>0.601923</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0016941308975219726, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro, tsne_axis_2]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.758382</td>\n",
       "      <td>0.601923</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.0023941993713378906, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro, tsne_axis_2]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.758382</td>\n",
       "      <td>0.601923</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.002094602584838867, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[scannedLineItems, secondsPerEuro, tsne_axis_2]</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': True, 'max_...</td>\n",
       "      <td>-0.758382</td>\n",
       "      <td>0.601923</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "      <td>{'mean_fit_time': [0.001989459991455078, 0.001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.250215</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002992057800292969, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.250215</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003091716766357422, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.250215</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0037897586822509765, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.250215</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0032914400100708006, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.250215</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002692770957946777, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.250215</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0036898136138916017, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[pca_axis_2]</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 10}</td>\n",
       "      <td>-0.766365</td>\n",
       "      <td>0.409595</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'mean_fit_time': [0.010372304916381836, 0.018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003690195083618164, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.251640</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028921127319335937, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003889608383178711, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.251640</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029918909072875976, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.251640</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002992129325866699, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003693246841430664, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0033852338790893556, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.251640</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002992057800292969, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.251640</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029918670654296873, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034906864166259766, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0033908843994140624, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>0.251640</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0030918121337890625, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.806280</td>\n",
       "      <td>0.262763</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002895641326904297, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.806280</td>\n",
       "      <td>0.262763</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003889608383178711, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.806280</td>\n",
       "      <td>0.262763</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003789973258972168, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.819585</td>\n",
       "      <td>0.257036</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0035903453826904297, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.819585</td>\n",
       "      <td>0.257036</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003091859817504883, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.819585</td>\n",
       "      <td>0.257036</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034908294677734376, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.838212</td>\n",
       "      <td>0.256809</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003789854049682617, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.838212</td>\n",
       "      <td>0.256809</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003789663314819336, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.838212</td>\n",
       "      <td>0.256809</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034906148910522463, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.848856</td>\n",
       "      <td>0.266595</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00299220085144043, 0.0030...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.848856</td>\n",
       "      <td>0.266595</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0032911300659179688, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.848856</td>\n",
       "      <td>0.266595</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00299220085144043, 0.0032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.854178</td>\n",
       "      <td>0.233818</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003390955924987793, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.854178</td>\n",
       "      <td>0.233818</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0030917882919311523, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.854178</td>\n",
       "      <td>0.233818</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031913995742797853, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.001894998550415039, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025927543640136717, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0029918432235717775, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0026930809020996095, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003490638732910156, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034904003143310545, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003091716766357422, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-0.947312</td>\n",
       "      <td>0.238965</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031916141510009766, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.968600</td>\n",
       "      <td>0.301185</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0028923511505126952], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.968600</td>\n",
       "      <td>0.301185</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002892279624938965], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.968600</td>\n",
       "      <td>0.301185</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0029920339584350586], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.968600</td>\n",
       "      <td>0.301185</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002992129325866699], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.992549</td>\n",
       "      <td>0.314330</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002194046974182129], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.992549</td>\n",
       "      <td>0.314330</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002194046974182129], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.992549</td>\n",
       "      <td>0.314330</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.00239255428314209], 'std_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.008515</td>\n",
       "      <td>0.316076</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0019947052001953124], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.008515</td>\n",
       "      <td>0.316076</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002992105484008789], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.008515</td>\n",
       "      <td>0.316076</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0023935317993164064], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.128260</td>\n",
       "      <td>0.362905</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028921127319335937, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'reg_param': 0.5}</td>\n",
       "      <td>-1.141565</td>\n",
       "      <td>0.375864</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0032910823822021483, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.168175</td>\n",
       "      <td>0.384575</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002692413330078125, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{'reg_param': 0.6}</td>\n",
       "      <td>-1.216072</td>\n",
       "      <td>0.365313</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0021939516067504884, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.303885</td>\n",
       "      <td>0.336085</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002294039726257324], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.303885</td>\n",
       "      <td>0.336085</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091740608215332], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.303885</td>\n",
       "      <td>0.336085</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018947839736938477], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.335817</td>\n",
       "      <td>0.347962</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091764450073242], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.575306</td>\n",
       "      <td>0.407667</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003391265869140625, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.575306</td>\n",
       "      <td>0.407667</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003690147399902344, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.575306</td>\n",
       "      <td>0.407667</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003490710258483887, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.615221</td>\n",
       "      <td>0.437013</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0037897348403930662, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.641831</td>\n",
       "      <td>0.451592</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.00458831787109375, 0.0038...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.641831</td>\n",
       "      <td>0.451592</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0034904241561889648, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.641831</td>\n",
       "      <td>0.451592</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0039894342422485355, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.655136</td>\n",
       "      <td>0.460204</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0039947509765625, 0.00378...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.655136</td>\n",
       "      <td>0.460204</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002792501449584961, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.655136</td>\n",
       "      <td>0.482530</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003191542625427246, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.668441</td>\n",
       "      <td>0.483029</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0031916618347167967, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.668441</td>\n",
       "      <td>0.483029</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.004288268089294433, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.668441</td>\n",
       "      <td>0.483029</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003633713722229004, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.668441</td>\n",
       "      <td>0.483029</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0038915395736694334, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.469873</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.004288601875305176, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.469873</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.004088997840881348, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.354357</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994633674621582], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.469873</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.003390979766845703, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.354357</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994490623474121], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.469873</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0040891170501708984, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.695051</td>\n",
       "      <td>0.354357</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091764450073242], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{'reg_param': 0.7000000000000001}</td>\n",
       "      <td>-1.753592</td>\n",
       "      <td>0.336484</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.002422666549682617, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2372</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-1.753592</td>\n",
       "      <td>0.341291</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032912254333496093], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'reg_param': 0.30000000000000004}</td>\n",
       "      <td>-1.830761</td>\n",
       "      <td>0.418668</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0025930404663085938, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.355518</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0035905361175537108, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.351686</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.004498291015625, 0.003986...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.355518</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.004487967491149903, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.351686</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.004587340354919434, 0.004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.351686</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.06293184757232666, 0.0369...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.355518</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.04019250869750977, 0.1039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.355518</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0042888402938842775, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.849388</td>\n",
       "      <td>0.351686</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0038893938064575194, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-1.910591</td>\n",
       "      <td>0.392880</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "      <td>{'mean_fit_time': [0.0028924465179443358, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.038318</td>\n",
       "      <td>0.383335</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002692842483520508], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.038318</td>\n",
       "      <td>0.365848</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091883659362793], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.051623</td>\n",
       "      <td>0.377972</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0029920339584350586], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.051623</td>\n",
       "      <td>0.377972</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0031911611557006838], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.051623</td>\n",
       "      <td>0.377972</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0026927947998046874], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.064928</td>\n",
       "      <td>0.480548</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.00299227237701416], 'std_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.064928</td>\n",
       "      <td>0.480548</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017951726913452148], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scannedLi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.064928</td>\n",
       "      <td>0.480548</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017953634262084961], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.481187</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017951488494873046], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.481187</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0016982316970825194], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0030918121337890625], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018950939178466798], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032912731170654298], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032912731170654298], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018949031829833985], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0022939443588256836], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>9</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017982006072998046], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.317722</td>\n",
       "      <td>0.481187</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0030918359756469727], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.331027</td>\n",
       "      <td>0.437288</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091859817504883], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.428792</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003191637992858887], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.428792</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002293896675109863], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.422121</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091859817504883], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.422121</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018949508666992188], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.428792</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032914400100708006], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.422121</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018948078155517577], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>10</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.344332</td>\n",
       "      <td>0.428792</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017952442169189454], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.410857</td>\n",
       "      <td>0.491129</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032910823822021483], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.410857</td>\n",
       "      <td>0.491129</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002991914749145508], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.410857</td>\n",
       "      <td>0.491129</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0030918359756469727], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>3</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.410857</td>\n",
       "      <td>0.491129</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0028922319412231444], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.424162</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032912492752075195], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.424162</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994800567626953], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.424162</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0015930891036987304], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.437467</td>\n",
       "      <td>0.521442</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0028917789459228516], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.437467</td>\n",
       "      <td>0.521442</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0019947052001953124], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.437467</td>\n",
       "      <td>0.521442</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0017978906631469726], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.623736</td>\n",
       "      <td>0.579755</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018949508666992188], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.623736</td>\n",
       "      <td>0.579755</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003490638732910156], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.623736</td>\n",
       "      <td>0.579755</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018887519836425781], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.668973</td>\n",
       "      <td>0.567880</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994633674621582], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.668973</td>\n",
       "      <td>0.567880</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0024932384490966796], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.668973</td>\n",
       "      <td>0.567880</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032913684844970703], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>11</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.868547</td>\n",
       "      <td>0.374236</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032912731170654298], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.868547</td>\n",
       "      <td>0.607780</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003191351890563965], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.868547</td>\n",
       "      <td>0.607780</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994681358337402], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.868547</td>\n",
       "      <td>0.455081</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003191494941711426], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.868547</td>\n",
       "      <td>0.607780</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0024959564208984373], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, scansWith...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.881852</td>\n",
       "      <td>0.449491</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0035904645919799805], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>12</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, valuePerS...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.895157</td>\n",
       "      <td>0.447384</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.00329129695892334], 'std_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.908462</td>\n",
       "      <td>0.618653</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003191685676574707], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.908462</td>\n",
       "      <td>0.618653</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002094554901123047], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-2.908462</td>\n",
       "      <td>0.618653</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002892422676086426], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.113358</td>\n",
       "      <td>0.525436</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003590512275695801], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.153273</td>\n",
       "      <td>0.592028</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0018949508666992188], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.153273</td>\n",
       "      <td>0.592028</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003091716766357422], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.153273</td>\n",
       "      <td>0.592028</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0030918359756469727], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.299627</td>\n",
       "      <td>0.557549</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.001994681358337402], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.299627</td>\n",
       "      <td>0.557549</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002992081642150879], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.299627</td>\n",
       "      <td>0.557549</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003191709518432617], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>16</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.326237</td>\n",
       "      <td>0.546788</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003590297698974609], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.339542</td>\n",
       "      <td>0.564224</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003391003608703613], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.366152</td>\n",
       "      <td>0.555916</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0033910512924194337], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.432677</td>\n",
       "      <td>0.536953</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0032912492752075195], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002293968200683594], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0020971298217773438], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003490638732910156], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0021943092346191407], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>20</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.534650</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0036902666091918946], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003690338134765625], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0037902355194091796], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0031914710998535156], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.003191637992858887], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0030918359756469727], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>24</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0022940635681152344], 'st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002592611312866211], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.445982</td>\n",
       "      <td>0.537931</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.002991914749145508], 'std...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-3.472592</td>\n",
       "      <td>0.553080</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "      <td>{'mean_fit_time': [0.0033912181854248045], 'st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2732 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Model Oversampling Strategy Data Preparation Feature Count Feature Selection Technique                                           Features                                 Optimal Parameters  Monetary Value Per Instance - Mean  Monetary Value Per Instance - Standard Deviation                                          Raw Model                           Cross Validation Results\n",
       "414              AdaBoostClassifier       No Oversampling       No Scaling            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05056438446044922, 0.1769...\n",
       "1095             AdaBoostClassifier       No Oversampling     MinMaxScaler            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05425493717193604, 0.2058...\n",
       "2461             AdaBoostClassifier       No Oversampling        LogScaler            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0544543981552124, 0.20874...\n",
       "1778             AdaBoostClassifier       No Oversampling   StandardScaler            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05784509181976318, 0.1759...\n",
       "1097             AdaBoostClassifier       No Oversampling     MinMaxScaler            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05764601230621338, 0.2370...\n",
       "1780             AdaBoostClassifier       No Oversampling   StandardScaler            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04896423816680908, 0.1266...\n",
       "412              AdaBoostClassifier       No Oversampling       No Scaling            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05176126956939697, 0.1792...\n",
       "2463             AdaBoostClassifier       No Oversampling        LogScaler            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04527671337127685, 0.1219...\n",
       "1115             AdaBoostClassifier       No Oversampling     MinMaxScaler            21                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09464631080627442, 0.2716...\n",
       "1117             AdaBoostClassifier       No Oversampling     MinMaxScaler            22                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07219569683074951, 0.1997...\n",
       "2481             AdaBoostClassifier       No Oversampling        LogScaler            21                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06940767765045167, 0.1830...\n",
       "2483             AdaBoostClassifier       No Oversampling        LogScaler            22                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07019481658935547, 0.1837...\n",
       "1798             AdaBoostClassifier       No Oversampling   StandardScaler            21                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0765883445739746, 0.20574...\n",
       "1800             AdaBoostClassifier       No Oversampling   StandardScaler            22                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08726654052734376, 0.2142...\n",
       "434              AdaBoostClassifier       No Oversampling       No Scaling            22                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07240362167358398, 0.1908...\n",
       "432              AdaBoostClassifier       No Oversampling       No Scaling            21                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.188930                                          0.056780  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09723961353302002, 0.2663...\n",
       "1093             AdaBoostClassifier       No Oversampling     MinMaxScaler            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.067877  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04886937141418457, 0.2208...\n",
       "1776             AdaBoostClassifier       No Oversampling   StandardScaler            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.067877  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.042185854911804196, 0.113...\n",
       "2459             AdaBoostClassifier       No Oversampling        LogScaler            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.067877  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06083579063415527, 0.2342...\n",
       "410              AdaBoostClassifier       No Oversampling       No Scaling            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.067877  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.054952597618103026, 0.169...\n",
       "419              AdaBoostClassifier       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.048834  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06512563228607178, 0.2223...\n",
       "1102             AdaBoostClassifier       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.048834  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05804469585418701, 0.1643...\n",
       "1785             AdaBoostClassifier       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.183608                                          0.048834  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06412808895111084, 0.2182...\n",
       "1099             AdaBoostClassifier       No Oversampling     MinMaxScaler            13                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.180947                                          0.063034  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05774281024932861, 0.2250...\n",
       "1101             AdaBoostClassifier       No Oversampling     MinMaxScaler            14                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.180947                                          0.063034  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0528484582901001, 0.13783...\n",
       "1784             AdaBoostClassifier       No Oversampling   StandardScaler            14                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.180947                                          0.063034  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.057942438125610354, 0.167...\n",
       "2467             AdaBoostClassifier       No Oversampling        LogScaler            14                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.180947                                          0.063034  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.050159239768981935, 0.137...\n",
       "418              AdaBoostClassifier       No Oversampling       No Scaling            14                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.180947                                          0.063034  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06233291625976563, 0.2334...\n",
       "2478             AdaBoostClassifier       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.180947                                          0.065235  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06372709274291992, 0.1700...\n",
       "2471             AdaBoostClassifier       No Oversampling        LogScaler            16                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.178286                                          0.037837  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05504775047302246, 0.1483...\n",
       "1105             AdaBoostClassifier       No Oversampling     MinMaxScaler            16                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.178286                                          0.037837  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0602384090423584, 0.16525...\n",
       "1788             AdaBoostClassifier       No Oversampling   StandardScaler            16                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.178286                                          0.037837  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05754311084747314, 0.1559...\n",
       "422              AdaBoostClassifier       No Oversampling       No Scaling            16                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.178286                                          0.037837  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08517186641693116, 0.2419...\n",
       "1165                  XGBClassifier       No Oversampling     MinMaxScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.175625                                          0.108912  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05365612506866455, 0.1235...\n",
       "1112             AdaBoostClassifier       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.055957  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10382180213928223, 0.2846...\n",
       "1116             AdaBoostClassifier       No Oversampling     MinMaxScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.089419  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0863689661026001, 0.22998...\n",
       "1848                  XGBClassifier       No Oversampling   StandardScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.175625                                          0.108912  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06043775081634521, 0.1801...\n",
       "427              AdaBoostClassifier       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.055957  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0884629487991333, 0.23247...\n",
       "1110             AdaBoostClassifier       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.055957  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08497223854064942, 0.2405...\n",
       "1091             AdaBoostClassifier       No Oversampling     MinMaxScaler             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.175625                                          0.085051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.047373270988464354, 0.227...\n",
       "2457             AdaBoostClassifier       No Oversampling        LogScaler             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.175625                                          0.085051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.054852294921875, 0.197471...\n",
       "1799             AdaBoostClassifier       No Oversampling   StandardScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.089419  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07329485416412354, 0.1973...\n",
       "408              AdaBoostClassifier       No Oversampling       No Scaling             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.175625                                          0.085051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04617421627044678, 0.1861...\n",
       "1795             AdaBoostClassifier       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.055957  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10900802612304687, 0.2985...\n",
       "1793             AdaBoostClassifier       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.055957  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08357594013214112, 0.2111...\n",
       "1774             AdaBoostClassifier       No Oversampling   StandardScaler             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.175625                                          0.085051  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04158902168273926, 0.1112...\n",
       "482                   XGBClassifier       No Oversampling       No Scaling            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.175625                                          0.108912  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05434770584106445, 0.1255...\n",
       "433              AdaBoostClassifier       No Oversampling       No Scaling            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.089419  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0904578685760498, 0.23537...\n",
       "2482             AdaBoostClassifier       No Oversampling        LogScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.089419  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07040562629699706, 0.1862...\n",
       "2531                  XGBClassifier       No Oversampling        LogScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.175625                                          0.108912  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05115077495574951, 0.1144...\n",
       "429              AdaBoostClassifier       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.175625                                          0.055957  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08427395820617675, 0.2062...\n",
       "1122             AdaBoostClassifier       No Oversampling     MinMaxScaler            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10053102970123291, 0.2586...\n",
       "1804             AdaBoostClassifier       No Oversampling   StandardScaler            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.14730463027954102, 0.4388...\n",
       "1805             AdaBoostClassifier       No Oversampling   StandardScaler            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.16774826049804686, 0.4257...\n",
       "437              AdaBoostClassifier       No Oversampling       No Scaling            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10761196613311767, 0.3731...\n",
       "1121             AdaBoostClassifier       No Oversampling     MinMaxScaler            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07768809795379639, 0.2032...\n",
       "1120             AdaBoostClassifier       No Oversampling     MinMaxScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07998497486114502, 0.2098...\n",
       "1119             AdaBoostClassifier       No Oversampling     MinMaxScaler            23                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10192716121673584, 0.3012...\n",
       "436              AdaBoostClassifier       No Oversampling       No Scaling            23                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10950686931610107, 0.3880...\n",
       "438              AdaBoostClassifier       No Oversampling       No Scaling            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09693987369537353, 0.2672...\n",
       "1118             AdaBoostClassifier       No Oversampling     MinMaxScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.076385760307312, 0.215919...\n",
       "435              AdaBoostClassifier       No Oversampling       No Scaling            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.10561714172363282, 0.2862...\n",
       "1801             AdaBoostClassifier       No Oversampling   StandardScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08666703701019288, 0.2775...\n",
       "1803             AdaBoostClassifier       No Oversampling   StandardScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09614214897155762, 0.2436...\n",
       "2484             AdaBoostClassifier       No Oversampling        LogScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07319338321685791, 0.1937...\n",
       "2485             AdaBoostClassifier       No Oversampling        LogScaler            23                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07369229793548585, 0.1929...\n",
       "439              AdaBoostClassifier       No Oversampling       No Scaling            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.11190094947814941, 0.3706...\n",
       "1802             AdaBoostClassifier       No Oversampling   StandardScaler            23                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09414799213409424, 0.2254...\n",
       "2488             AdaBoostClassifier       No Oversampling        LogScaler            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07559263706207275, 0.1951...\n",
       "2487             AdaBoostClassifier       No Oversampling        LogScaler            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07579183578491211, 0.1952...\n",
       "2486             AdaBoostClassifier       No Oversampling        LogScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.172964                                          0.067281  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07398192882537842, 0.1952...\n",
       "430              AdaBoostClassifier       No Oversampling       No Scaling            20                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.078339  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07119827270507813, 0.1813...\n",
       "1796             AdaBoostClassifier       No Oversampling   StandardScaler            20                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.078339  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08337123394012451, 0.2124...\n",
       "420              AdaBoostClassifier       No Oversampling       No Scaling            15                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.047732  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05874302387237549, 0.1603...\n",
       "1786             AdaBoostClassifier       No Oversampling   StandardScaler            15                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.047732  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0631256103515625, 0.17283...\n",
       "2455             AdaBoostClassifier       No Oversampling        LogScaler             8                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.170303                                          0.091369  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04487769603729248, 0.2045...\n",
       "1089             AdaBoostClassifier       No Oversampling     MinMaxScaler             8                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.170303                                          0.091369  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0493680477142334, 0.18689...\n",
       "2479             AdaBoostClassifier       No Oversampling        LogScaler            20                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.078339  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06620879173278808, 0.1773...\n",
       "1772             AdaBoostClassifier       No Oversampling   StandardScaler             8                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.170303                                          0.091369  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03450796604156494, 0.0948...\n",
       "406              AdaBoostClassifier       No Oversampling       No Scaling             8                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.170303                                          0.091369  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03640217781066894, 0.0924...\n",
       "1103             AdaBoostClassifier       No Oversampling     MinMaxScaler            15                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.047732  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.054049324989318845, 0.151...\n",
       "2469             AdaBoostClassifier       No Oversampling        LogScaler            15                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.047732  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05196094512939453, 0.1405...\n",
       "1113             AdaBoostClassifier       No Oversampling     MinMaxScaler            20                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.170303                                          0.078339  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07130908966064453, 0.1967...\n",
       "428              AdaBoostClassifier       No Oversampling       No Scaling            19                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.075290  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07280433177947998, 0.1994...\n",
       "416              AdaBoostClassifier       No Oversampling       No Scaling            13                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.164981                                          0.090441  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06432745456695557, 0.2056...\n",
       "2465             AdaBoostClassifier       No Oversampling        LogScaler            13                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.164981                                          0.090441  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.047373604774475095, 0.127...\n",
       "421              AdaBoostClassifier       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.164981                                          0.105500  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.053056859970092775, 0.146...\n",
       "1782             AdaBoostClassifier       No Oversampling   StandardScaler            13                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.164981                                          0.090441  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.050658655166625974, 0.134...\n",
       "1794             AdaBoostClassifier       No Oversampling   StandardScaler            19                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.075290  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06900987625122071, 0.1769...\n",
       "1111             AdaBoostClassifier       No Oversampling     MinMaxScaler            19                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.075290  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06621336936950684, 0.1708...\n",
       "1104             AdaBoostClassifier       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.164981                                          0.105500  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07031164169311524, 0.1879...\n",
       "1787             AdaBoostClassifier       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.164981                                          0.105500  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06103696823120117, 0.1693...\n",
       "2477             AdaBoostClassifier       No Oversampling        LogScaler            19                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.164981                                          0.075290  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06394476890563965, 0.1676...\n",
       "1149                  XGBClassifier       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.079998  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04228670597076416, 0.1146...\n",
       "1839                  XGBClassifier       No Oversampling   StandardScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.162320                                          0.099022  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06622262001037597, 0.1593...\n",
       "473                   XGBClassifier       No Oversampling       No Scaling            17                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.162320                                          0.099022  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.10741264820098877, 0.2868...\n",
       "466                   XGBClassifier       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.079998  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.061435699462890625, 0.167...\n",
       "417              AdaBoostClassifier       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.162320                                          0.083711  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04746236801147461, 0.1296...\n",
       "1832                  XGBClassifier       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.162320                                          0.079998  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04567661285400391, 0.1585...\n",
       "1783             AdaBoostClassifier       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.162320                                          0.083711  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.056648421287536624, 0.216...\n",
       "2480             AdaBoostClassifier       No Oversampling        LogScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.162320                                          0.077572  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06681082248687745, 0.1788...\n",
       "431              AdaBoostClassifier       No Oversampling       No Scaling            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.162320                                          0.084592  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.09494562149047851, 0.3041...\n",
       "1100             AdaBoostClassifier       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.162320                                          0.083711  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.055950379371643065, 0.174...\n",
       "1156                  XGBClassifier       No Oversampling     MinMaxScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.162320                                          0.099022  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.07051081657409668, 0.1447...\n",
       "1114             AdaBoostClassifier       No Oversampling     MinMaxScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.162320                                          0.084592  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.06971023082733155, 0.1898...\n",
       "2523                  XGBClassifier       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.162320                                          0.111650  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04616866111755371, 0.1168...\n",
       "2522                  XGBClassifier       No Oversampling        LogScaler            17                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.162320                                          0.099022  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.061125540733337404, 0.167...\n",
       "1797             AdaBoostClassifier       No Oversampling   StandardScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.162320                                          0.084592  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07420058250427246, 0.2036...\n",
       "423              AdaBoostClassifier       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.159659                                          0.105269  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0828782081604004, 0.24354...\n",
       "1847                  XGBClassifier       No Oversampling   StandardScaler            21                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.104804  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06861603260040283, 0.1392...\n",
       "1164                  XGBClassifier       No Oversampling     MinMaxScaler            21                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.104804  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05385591983795166, 0.1222...\n",
       "481                   XGBClassifier       No Oversampling       No Scaling            21                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.104804  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.11539115905761718, 0.2542...\n",
       "1789             AdaBoostClassifier       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.159659                                          0.105269  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05683636665344238, 0.1550...\n",
       "717              LogisticRegression       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.6, 'fit_intercept': True, 'solver': 'l...                            0.159659                                          0.094489  LogisticRegression(C=1.6, class_weight=None, d...  {'mean_fit_time': [0.28064892292022703, 0.4285...\n",
       "715              LogisticRegression       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.094489  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.2594059944152832, 0.43603...\n",
       "32               LogisticRegression       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.094489  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.28743069171905516, 0.4544...\n",
       "34               LogisticRegression       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.6, 'fit_intercept': True, 'solver': 'l...                            0.159659                                          0.094489  LogisticRegression(C=1.6, class_weight=None, d...  {'mean_fit_time': [0.33161239624023436, 0.5098...\n",
       "1398             LogisticRegression       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.094489  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.18031723499298097, 0.2596...\n",
       "1400             LogisticRegression       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.6, 'fit_intercept': True, 'solver': 'l...                            0.159659                                          0.094489  LogisticRegression(C=1.6, class_weight=None, d...  {'mean_fit_time': [0.16236634254455568, 0.2576...\n",
       "1106             AdaBoostClassifier       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.159659                                          0.105269  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05575084686279297, 0.1612...\n",
       "2530                  XGBClassifier       No Oversampling        LogScaler            21                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.159659                                          0.104804  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04736342430114746, 0.1138...\n",
       "2527                  XGBClassifier       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.156998                                          0.103828  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04557158946990967, 0.1204...\n",
       "2525                  XGBClassifier       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.156998                                          0.096857  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.0503582239151001, 0.11379...\n",
       "2453             AdaBoostClassifier       No Oversampling        LogScaler             7                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.156998                                          0.129985  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.053855514526367186, 0.184...\n",
       "1087             AdaBoostClassifier       No Oversampling     MinMaxScaler             7                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.156998                                          0.129985  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.038297486305236814, 0.116...\n",
       "1108             AdaBoostClassifier       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.093202  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07340354919433593, 0.1800...\n",
       "480                   XGBClassifier       No Oversampling       No Scaling            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.156998                                          0.132513  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.10751242637634277, 0.2949...\n",
       "1770             AdaBoostClassifier       No Oversampling   StandardScaler             7                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.156998                                          0.129985  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.031216812133789063, 0.086...\n",
       "1163                  XGBClassifier       No Oversampling     MinMaxScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.156998                                          0.132513  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05465312004089355, 0.1188...\n",
       "404              AdaBoostClassifier       No Oversampling       No Scaling             7                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.156998                                          0.129985  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.03919310569763183, 0.1301...\n",
       "1846                  XGBClassifier       No Oversampling   StandardScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.156998                                          0.132513  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06941385269165039, 0.1625...\n",
       "425              AdaBoostClassifier       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.093202  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.07908811569213867, 0.2218...\n",
       "1791             AdaBoostClassifier       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.156998                                          0.093202  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.08766539096832275, 0.2657...\n",
       "24               LogisticRegression       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.2595245122909546, 0.45370...\n",
       "2474             AdaBoostClassifier       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.154337                                          0.079820  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05744645595550537, 0.1536...\n",
       "22               LogisticRegression       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.102953  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.2513269424438477, 0.41508...\n",
       "1388             LogisticRegression       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.102953  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.14659833908081055, 0.2325...\n",
       "1390             LogisticRegression       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.16226651668548583, 0.2745...\n",
       "1392             LogisticRegression       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.19009184837341309, 0.2466...\n",
       "30               LogisticRegression       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.165457820892334, 0.256612...\n",
       "713              LogisticRegression       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.22858889102935792, 0.3346...\n",
       "2464             AdaBoostClassifier       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.154337                                          0.067006  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04776921272277832, 0.1276...\n",
       "707              LogisticRegression       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.25202579498291017, 0.4675...\n",
       "1394             LogisticRegression       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.13912887573242189, 0.2344...\n",
       "1396             LogisticRegression       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.1782235860824585, 0.26489...\n",
       "705              LogisticRegression       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.154337                                          0.102953  LogisticRegression(C=1.8000000000000003, class...  {'mean_fit_time': [0.2847379922866821, 0.40661...\n",
       "28               LogisticRegression       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.2579099893569946, 0.44530...\n",
       "709              LogisticRegression       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.20944011211395264, 0.2536...\n",
       "711              LogisticRegression       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.2762607574462891, 0.41070...\n",
       "26               LogisticRegression       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.2000000000000002, 'fit_intercept': Tru...                            0.154337                                          0.091360  LogisticRegression(C=1.2000000000000002, class...  {'mean_fit_time': [0.19019110202789308, 0.2361...\n",
       "2533                  XGBClassifier       No Oversampling        LogScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.051450514793396, 0.113895...\n",
       "467                   XGBClassifier       No Oversampling       No Scaling            14                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.08128278255462647, 0.1861...\n",
       "470                   XGBClassifier       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05295817852020264, 0.1214...\n",
       "477                   XGBClassifier       No Oversampling       No Scaling            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.110174  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.08995909690856933, 0.1844...\n",
       "479                   XGBClassifier       No Oversampling       No Scaling            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.128010  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.0595362663269043, 0.12007...\n",
       "2532                  XGBClassifier       No Oversampling        LogScaler            22                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05005559921264648, 0.1152...\n",
       "2535                  XGBClassifier       No Oversampling        LogScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05155096054077148, 0.1190...\n",
       "2534                  XGBClassifier       No Oversampling        LogScaler            23                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05224721431732178, 0.1147...\n",
       "415              AdaBoostClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.086018  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.049364900588989256, 0.141...\n",
       "465                   XGBClassifier       No Oversampling       No Scaling            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05914161205291748, 0.2110...\n",
       "463                   XGBClassifier       No Oversampling       No Scaling            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.08377583026885986, 0.1908...\n",
       "461                   XGBClassifier       No Oversampling       No Scaling            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.048862767219543454, 0.120...\n",
       "2528                  XGBClassifier       No Oversampling        LogScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.128010  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.047164511680603025, 0.122...\n",
       "424              AdaBoostClassifier       No Oversampling       No Scaling            17                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.083513  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0834766149520874, 0.25092...\n",
       "2526                  XGBClassifier       No Oversampling        LogScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.110174  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.04756455421447754, 0.1249...\n",
       "2521                  XGBClassifier       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.112153  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04636678695678711, 0.1173...\n",
       "2516                  XGBClassifier       No Oversampling        LogScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07030153274536133, 0.1540...\n",
       "2514                  XGBClassifier       No Oversampling        LogScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.07439258098602294, 0.1579...\n",
       "2512                  XGBClassifier       No Oversampling        LogScaler            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.07638888359069824, 0.1446...\n",
       "2511                  XGBClassifier       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.103006  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0485623836517334, 0.11369...\n",
       "2509                  XGBClassifier       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.103006  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.044971132278442384, 0.109...\n",
       "426              AdaBoostClassifier       No Oversampling       No Scaling            18                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.087476  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0974393367767334, 0.27576...\n",
       "2510                  XGBClassifier       No Oversampling        LogScaler            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06621546745300293, 0.1588...\n",
       "1170                  XGBClassifier       No Oversampling     MinMaxScaler            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05305461883544922, 0.1191...\n",
       "483                   XGBClassifier       No Oversampling       No Scaling            22                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05415375232696533, 0.1261...\n",
       "1169                  XGBClassifier       No Oversampling     MinMaxScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05315177440643311, 0.1204...\n",
       "1854                  XGBClassifier       No Oversampling   StandardScaler            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06203348636627197, 0.1835...\n",
       "1853                  XGBClassifier       No Oversampling   StandardScaler            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07469966411590576, 0.1797...\n",
       "1852                  XGBClassifier       No Oversampling   StandardScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07100958824157715, 0.1970...\n",
       "1167                  XGBClassifier       No Oversampling     MinMaxScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.054151725769042966, 0.122...\n",
       "1166                  XGBClassifier       No Oversampling     MinMaxScaler            22                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05644643306732178, 0.1194...\n",
       "1851                  XGBClassifier       No Oversampling   StandardScaler            23                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06731901168823243, 0.1786...\n",
       "1850                  XGBClassifier       No Oversampling   StandardScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06662108898162841, 0.2061...\n",
       "1849                  XGBClassifier       No Oversampling   StandardScaler            22                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07539486885070801, 0.1741...\n",
       "1845                  XGBClassifier       No Oversampling   StandardScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.128010  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.07120904922485352, 0.1920...\n",
       "1843                  XGBClassifier       No Oversampling   StandardScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.110174  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06283140182495117, 0.1496...\n",
       "1160                  XGBClassifier       No Oversampling     MinMaxScaler            19                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.110174  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06881554126739502, 0.1420...\n",
       "1836                  XGBClassifier       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05724701881408691, 0.1261...\n",
       "1833                  XGBClassifier       No Oversampling   StandardScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.049456214904785155, 0.150...\n",
       "1831                  XGBClassifier       No Oversampling   StandardScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06273086071014404, 0.2555...\n",
       "1829                  XGBClassifier       No Oversampling   StandardScaler            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05076322555541992, 0.1785...\n",
       "1827                  XGBClassifier       No Oversampling   StandardScaler            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0557497501373291, 0.14301...\n",
       "1153                  XGBClassifier       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.046674704551696776, 0.105...\n",
       "1150                  XGBClassifier       No Oversampling     MinMaxScaler            14                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.084915  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0661158800125122, 0.14271...\n",
       "1148                  XGBClassifier       No Oversampling     MinMaxScaler            13                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06821739673614502, 0.1353...\n",
       "1146                  XGBClassifier       No Oversampling     MinMaxScaler            12                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06492578983306885, 0.1444...\n",
       "1144                  XGBClassifier       No Oversampling     MinMaxScaler            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.107045  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06212446689605713, 0.1486...\n",
       "1109             AdaBoostClassifier       No Oversampling     MinMaxScaler            18                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.087476  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.062330222129821776, 0.169...\n",
       "1107             AdaBoostClassifier       No Oversampling     MinMaxScaler            17                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.083513  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05764613151550293, 0.1691...\n",
       "1098             AdaBoostClassifier       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.086018  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.059441137313842776, 0.184...\n",
       "1781             AdaBoostClassifier       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.086018  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.04487917423248291, 0.1244...\n",
       "1790             AdaBoostClassifier       No Oversampling   StandardScaler            17                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.083513  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05772840976715088, 0.1551...\n",
       "1792             AdaBoostClassifier       No Oversampling   StandardScaler            18                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.087476  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.061433267593383786, 0.169...\n",
       "1168                  XGBClassifier       No Oversampling     MinMaxScaler            23                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05495185852050781, 0.1201...\n",
       "1162                  XGBClassifier       No Oversampling     MinMaxScaler            20                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.151676                                          0.128010  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05285842418670654, 0.1210...\n",
       "2537                  XGBClassifier       No Oversampling        LogScaler            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.0576446533203125, 0.11748...\n",
       "2473             AdaBoostClassifier       No Oversampling        LogScaler            17                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.083513  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05744495391845703, 0.1520...\n",
       "484                   XGBClassifier       No Oversampling       No Scaling            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.11349625587463379, 0.2605...\n",
       "485                   XGBClassifier       No Oversampling       No Scaling            23                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.11429400444030761, 0.2532...\n",
       "486                   XGBClassifier       No Oversampling       No Scaling            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.10870921611785889, 0.2765...\n",
       "487                   XGBClassifier       No Oversampling       No Scaling            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.055741071701049805, 0.127...\n",
       "488                   XGBClassifier       No Oversampling       No Scaling            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05374746322631836, 0.1195...\n",
       "1171                  XGBClassifier       No Oversampling     MinMaxScaler            24                       RFECV  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05255935192108154, 0.1206...\n",
       "2475             AdaBoostClassifier       No Oversampling        LogScaler            18                         RFE  [totalScanTimeInSeconds, grandTotal, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.087476  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.059335947036743164, 0.158...\n",
       "2536                  XGBClassifier       No Oversampling        LogScaler            24                         RFE  [trustLevel, totalScanTimeInSeconds, grandTota...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.151676                                          0.102131  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.049658918380737306, 0.117...\n",
       "2466             AdaBoostClassifier       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.086018  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.045376038551330565, 0.123...\n",
       "2468             AdaBoostClassifier       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.151676                                          0.072739  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.048470354080200194, 0.131...\n",
       "2506                  XGBClassifier       No Oversampling        LogScaler             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.149015                                          0.128764  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.06232442855834961, 0.1646...\n",
       "1140                  XGBClassifier       No Oversampling     MinMaxScaler             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.149015                                          0.128764  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.05575051307678223, 0.1297...\n",
       "1402             LogisticRegression       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.149015                                          0.102588  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.2517263650894165, 0.32652...\n",
       "36               LogisticRegression       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.149015                                          0.102588  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.3012934684753418, 0.46006...\n",
       "457                   XGBClassifier       No Oversampling       No Scaling             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.149015                                          0.128764  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.0739020824432373, 0.20126...\n",
       "1823                  XGBClassifier       No Oversampling   StandardScaler             9                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.149015                                          0.128764  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.05554981231689453, 0.1282...\n",
       "2529                  XGBClassifier       No Oversampling        LogScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.149015                                          0.138101  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.04955921173095703, 0.1181...\n",
       "719              LogisticRegression       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.3000000000000003, 'fit_intercept': Tru...                            0.149015                                          0.102588  LogisticRegression(C=1.3000000000000003, class...  {'mean_fit_time': [0.2949108839035034, 0.43982...\n",
       "2472             AdaBoostClassifier       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.146354                                          0.071731  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05425503253936768, 0.1473...\n",
       "2518                  XGBClassifier       No Oversampling        LogScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.119093  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07219626903533935, 0.1636...\n",
       "469                   XGBClassifier       No Oversampling       No Scaling            15                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.119093  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.06751933097839355, 0.1985...\n",
       "1152                  XGBClassifier       No Oversampling     MinMaxScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.119093  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.056249046325683595, 0.141...\n",
       "2470             AdaBoostClassifier       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'algorithm': 'SAMME', 'base_estimator': Decis...                            0.146354                                          0.117310  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.05226066112518311, 0.1454...\n",
       "1835                  XGBClassifier       No Oversampling   StandardScaler            15                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.146354                                          0.119093  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.05714693069458008, 0.1358...\n",
       "1142                  XGBClassifier       No Oversampling     MinMaxScaler            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.124806  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.039194798469543456, 0.161...\n",
       "2508                  XGBClassifier       No Oversampling        LogScaler            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.124806  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06960606575012207, 0.1556...\n",
       "1145                  XGBClassifier       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.114216  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.046375465393066403, 0.110...\n",
       "2515                  XGBClassifier       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.043768787384033205, 0.113...\n",
       "475                   XGBClassifier       No Oversampling       No Scaling            18                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.143693                                          0.121461  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.05384728908538818, 0.1218...\n",
       "476                   XGBClassifier       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.11289780139923096, 0.2836...\n",
       "1147                  XGBClassifier       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04537820816040039, 0.1156...\n",
       "2524                  XGBClassifier       No Oversampling        LogScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.143693                                          0.121461  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.04865508079528809, 0.1162...\n",
       "1844                  XGBClassifier       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05445399284362793, 0.1674...\n",
       "1830                  XGBClassifier       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.04448013305664063, 0.1406...\n",
       "1828                  XGBClassifier       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.114216  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.042585015296936035, 0.132...\n",
       "1842                  XGBClassifier       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.07380204200744629, 0.1564...\n",
       "1841                  XGBClassifier       No Oversampling   StandardScaler            18                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.3, 'eval_metric': 'error', 'l...                            0.143693                                          0.121461  XGBClassifier(base_score=0.3, booster='gbtree'...  {'mean_fit_time': [0.068715500831604, 0.163762...\n",
       "478                   XGBClassifier       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05086121559143066, 0.1235...\n",
       "1161                  XGBClassifier       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05166172981262207, 0.1113...\n",
       "1159                  XGBClassifier       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.05385534763336182, 0.1165...\n",
       "459                   XGBClassifier       No Oversampling       No Scaling            10                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.124806  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.06731965541839599, 0.2016...\n",
       "462                   XGBClassifier       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.7, 'eval_metric': 'error', 'l...                            0.143693                                          0.114216  XGBClassifier(base_score=0.7, booster='gbtree'...  {'mean_fit_time': [0.07699387073516846, 0.2085...\n",
       "464                   XGBClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.143693                                          0.119511  XGBClassifier(base_score=0.5, booster='gbtree'...  {'mean_fit_time': [0.07749254703521728, 0.1738...\n",
       "...                             ...                   ...              ...           ...                         ...                                                ...                                                ...                                 ...                                               ...                                                ...                                                ...\n",
       "1856                  XGBClassifier       No Oversampling   StandardScaler             1                         RFE                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.04158830642700195, 0.1239...\n",
       "1172                  XGBClassifier       No Oversampling     MinMaxScaler             1                 SelectKBest                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.030617785453796387, 0.097...\n",
       "2540                  XGBClassifier       No Oversampling        LogScaler             2                 SelectKBest                          [trustLevel, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.040684986114501956, 0.109...\n",
       "1855                  XGBClassifier       No Oversampling   StandardScaler             1                 SelectKBest                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.024733757972717284, 0.099...\n",
       "489                   XGBClassifier       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.08577024936676025, 0.1671...\n",
       "2622                     Perceptron       No Oversampling        LogScaler             6                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.476317                                          0.202372  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002000260353088379, 0.002...\n",
       "490                   XGBClassifier       No Oversampling       No Scaling             1                         RFE                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.06791837215423584, 0.1641...\n",
       "1939                     Perceptron       No Oversampling   StandardScaler             6                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.476317                                          0.202372  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0021890878677368166, 0.00...\n",
       "1256                     Perceptron       No Oversampling     MinMaxScaler             6                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.476317                                          0.202372  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0029921531677246094, 0.00...\n",
       "573                      Perceptron       No Oversampling       No Scaling             6                         RFE  [trustLevel, grandTotal, scannedLineItems, pri...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.476317                                          0.202372  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0370006799697876, 0.02184...\n",
       "1173                  XGBClassifier       No Oversampling     MinMaxScaler             1                         RFE                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.05415475368499756, 0.1199...\n",
       "2539                  XGBClassifier       No Oversampling        LogScaler             1                         RFE                                      [tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.476317                                          0.221986  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.06810693740844727, 0.1593...\n",
       "2223           ExtraTreesClassifier       No Oversampling        LogScaler             2                         RFE                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.486961                                          0.174131  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.005185842514038086, 0.008...\n",
       "1931                     Perceptron       No Oversampling   StandardScaler             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020973682403564453, 0.00...\n",
       "1248                     Perceptron       No Oversampling     MinMaxScaler             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0019947290420532227, 0.00...\n",
       "1415           KNeighborsClassifier       No Oversampling   StandardScaler             1                 SelectKBest                                      [tsne_axis_2]   {'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}                           -0.489622                                          0.226355  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.0025930881500244142, 0.00...\n",
       "49             KNeighborsClassifier       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]   {'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}                           -0.489622                                          0.226355  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.0024930715560913088, 0.00...\n",
       "565                      Perceptron       No Oversampling       No Scaling             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002094268798828125, 0.002...\n",
       "732            KNeighborsClassifier       No Oversampling     MinMaxScaler             1                 SelectKBest                                      [tsne_axis_2]   {'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}                           -0.489622                                          0.226355  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.0024931907653808595, 0.00...\n",
       "1539           ExtraTreesClassifier       No Oversampling   StandardScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.489622                                          0.176007  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.004784727096557617, 0.008...\n",
       "2614                     Perceptron       No Oversampling        LogScaler             2                         RFE                 [scannedLineItems, secondsPerEuro]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.489622                                          0.396779  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001698613166809082, 0.001...\n",
       "2273         RandomForestClassifier       No Oversampling        LogScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 20}                           -0.494944                                          0.208084  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.00997333526611328, 0.0172...\n",
       "175            ExtraTreesClassifier       No Oversampling       No Scaling             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 65}                           -0.497605                                          0.252656  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.00827794075012207, 0.0131...\n",
       "1176                  XGBClassifier       No Oversampling     MinMaxScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.038889741897583006, 0.104...\n",
       "493                   XGBClassifier       No Oversampling       No Scaling             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.03330719470977783, 0.1074...\n",
       "1857                  XGBClassifier       No Oversampling   StandardScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.028323936462402343, 0.090...\n",
       "1859                  XGBClassifier       No Oversampling   StandardScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.022838282585144042, 0.083...\n",
       "2542                  XGBClassifier       No Oversampling        LogScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.034804034233093264, 0.126...\n",
       "173            ExtraTreesClassifier       No Oversampling       No Scaling             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.505588                                          0.237846  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.006083846092224121, 0.012...\n",
       "1174                  XGBClassifier       No Oversampling     MinMaxScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.03789839744567871, 0.0878...\n",
       "491                   XGBClassifier       No Oversampling       No Scaling             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]  {'base_estimator': DecisionTreeClassifier(clas...                           -0.505588                                          0.243221  XGBClassifier(base_estimator=DecisionTreeClass...  {'mean_fit_time': [0.04736628532409668, 0.1230...\n",
       "2099           KNeighborsClassifier       No Oversampling        LogScaler             2                 SelectKBest                          [trustLevel, tsne_axis_2]  {'n_neighbors': 15, 'p': 1, 'weights': 'distan...                           -0.508249                                          0.278805  KNeighborsClassifier(algorithm='auto', leaf_si...  {'mean_fit_time': [0.0019944429397583006, 0.00...\n",
       "905          RandomForestClassifier       No Oversampling     MinMaxScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.508249                                          0.226676  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010073065757751465, 0.017...\n",
       "224          RandomForestClassifier       No Oversampling       No Scaling             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 50}                           -0.510910                                          0.206680  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.009773635864257812, 0.016...\n",
       "1589         RandomForestClassifier       No Oversampling   StandardScaler             2                         RFE                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 20}                           -0.513571                                          0.214829  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.01047201156616211, 0.0183...\n",
       "857            ExtraTreesClassifier       No Oversampling     MinMaxScaler             2                         RFE                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.513571                                          0.176453  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.007878780364990234, 0.013...\n",
       "1588         RandomForestClassifier       No Oversampling   StandardScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 50}                           -0.518893                                          0.286750  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010471916198730469, 0.017...\n",
       "1540           ExtraTreesClassifier       No Oversampling   StandardScaler             2                         RFE                          [pca_axis_2, tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 20}                           -0.518893                                          0.264937  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.0059841394424438475, 0.01...\n",
       "1590         RandomForestClassifier       No Oversampling   StandardScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]      {'criterion': 'entropy', 'n_estimators': 110}                           -0.518893                                          0.226578  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010073018074035645, 0.017...\n",
       "907          RandomForestClassifier       No Oversampling     MinMaxScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 35}                           -0.524215                                          0.268706  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.00607759952545166, 0.0106...\n",
       "858            ExtraTreesClassifier       No Oversampling     MinMaxScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 10}                           -0.524215                                          0.230935  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.0074798583984375, 0.01296...\n",
       "2224           ExtraTreesClassifier       No Oversampling        LogScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 55}                           -0.526876                                          0.279849  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.005688071250915527, 0.009...\n",
       "2272         RandomForestClassifier       No Oversampling        LogScaler             2                         RFE                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 70}                           -0.526876                                          0.244553  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.007480120658874512, 0.013...\n",
       "223          RandomForestClassifier       No Oversampling       No Scaling             2                         RFE                          [pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.529537                                          0.244084  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010073137283325196, 0.017...\n",
       "856            ExtraTreesClassifier       No Oversampling     MinMaxScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]         {'criterion': 'gini', 'n_estimators': 130}                           -0.534859                                          0.215134  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.007779240608215332, 0.012...\n",
       "1541           ExtraTreesClassifier       No Oversampling   StandardScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 40}                           -0.540181                                          0.215757  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.008078479766845703, 0.013...\n",
       "220          RandomForestClassifier       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 10}                           -0.558808                                          0.375726  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010074925422668458, 0.017...\n",
       "1935                     Perceptron       No Oversampling   StandardScaler             4                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.585418                                          0.426356  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004488039016723633, 0.005...\n",
       "2618                     Perceptron       No Oversampling        LogScaler             4                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.585418                                          0.426356  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0020936250686645506, 0.00...\n",
       "1252                     Perceptron       No Oversampling     MinMaxScaler             4                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.585418                                          0.426356  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.004587554931640625, 0.003...\n",
       "569                      Perceptron       No Oversampling       No Scaling             4                         RFE  [scannedLineItems, pricePerScannedLineItem, se...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.585418                                          0.426356  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0018947839736938477, 0.00...\n",
       "2271         RandomForestClassifier       No Oversampling        LogScaler             2                 SelectKBest                          [trustLevel, tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 20}                           -0.588079                                          0.330010  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.009574294090270996, 0.016...\n",
       "904          RandomForestClassifier       No Oversampling     MinMaxScaler             1                         RFE                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.598723                                          0.272556  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.005787014961242676, 0.010...\n",
       "2222           ExtraTreesClassifier       No Oversampling        LogScaler             2                 SelectKBest                          [trustLevel, tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 80}                           -0.609367                                          0.316002  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.0068816661834716795, 0.01...\n",
       "1587         RandomForestClassifier       No Oversampling   StandardScaler             1                         RFE                                      [tsne_axis_2]           {'criterion': 'gini', 'n_estimators': 5}                           -0.612028                                          0.276676  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.009873700141906739, 0.017...\n",
       "903          RandomForestClassifier       No Oversampling     MinMaxScaler             1                 SelectKBest                                      [tsne_axis_2]        {'criterion': 'entropy', 'n_estimators': 5}                           -0.612028                                          0.341373  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.007280588150024414, 0.014...\n",
       "1586         RandomForestClassifier       No Oversampling   StandardScaler             1                 SelectKBest                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 10}                           -0.612028                                          0.282696  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010372328758239745, 0.016...\n",
       "2004  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                 {'reg_param': 0.1}                           -0.633316                                          0.314175  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0027924537658691405, 0.00...\n",
       "638   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                 {'reg_param': 0.1}                           -0.633316                                          0.314175  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028923749923706055, 0.00...\n",
       "2687  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                 {'reg_param': 0.1}                           -0.633316                                          0.314175  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0026931047439575197, 0.00...\n",
       "1321  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                 {'reg_param': 0.1}                           -0.633316                                          0.314175  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028923749923706055, 0.00...\n",
       "221          RandomForestClassifier       No Oversampling       No Scaling             1                         RFE                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 15}                           -0.635977                                          0.253680  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.0098738431930542, 0.01755...\n",
       "2620                     Perceptron       No Oversampling        LogScaler             5                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.643960                                          0.338754  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002004098892211914, 0.002...\n",
       "1937                     Perceptron       No Oversampling   StandardScaler             5                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.643960                                          0.338754  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0028924942016601562, 0.00...\n",
       "571                      Perceptron       No Oversampling       No Scaling             5                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.643960                                          0.338754  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.011967825889587402, 0.003...\n",
       "1254                     Perceptron       No Oversampling     MinMaxScaler             5                         RFE  [grandTotal, scannedLineItems, pricePerScanned...  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.643960                                          0.338754  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.00208892822265625, 0.0020...\n",
       "854            ExtraTreesClassifier       No Oversampling     MinMaxScaler             1                 SelectKBest                                      [tsne_axis_2]          {'criterion': 'gini', 'n_estimators': 15}                           -0.646621                                          0.344109  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.007480049133300781, 0.012...\n",
       "171            ExtraTreesClassifier       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 40}                           -0.646621                                          0.318148  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.00747983455657959, 0.0127...\n",
       "1537           ExtraTreesClassifier       No Oversampling   StandardScaler             1                 SelectKBest                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 70}                           -0.659925                                          0.342170  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.006582307815551758, 0.011...\n",
       "855            ExtraTreesClassifier       No Oversampling     MinMaxScaler             1                         RFE                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 35}                           -0.665247                                          0.342096  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.007380127906799316, 0.012...\n",
       "2221           ExtraTreesClassifier       No Oversampling        LogScaler             1                         RFE                                      [tsne_axis_2]         {'criterion': 'gini', 'n_estimators': 195}                           -0.665247                                          0.343630  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.005485415458679199, 0.009...\n",
       "172            ExtraTreesClassifier       No Oversampling       No Scaling             1                         RFE                                      [tsne_axis_2]       {'criterion': 'entropy', 'n_estimators': 45}                           -0.665247                                          0.338414  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.0077789068222045895, 0.01...\n",
       "646   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.699840                                          0.231817  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003091740608215332, 0.003...\n",
       "1329  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.699840                                          0.231817  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0032912492752075195, 0.00...\n",
       "2012  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.699840                                          0.231817  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028923511505126952, 0.00...\n",
       "1538           ExtraTreesClassifier       No Oversampling   StandardScaler             1                         RFE                                       [pca_axis_2]          {'criterion': 'gini', 'n_estimators': 10}                           -0.718467                                          0.363213  (ExtraTreeClassifier(class_weight=None, criter...  {'mean_fit_time': [0.005485129356384277, 0.009...\n",
       "1933                     Perceptron       No Oversampling   StandardScaler             3                         RFE    [scannedLineItems, secondsPerEuro, tsne_axis_2]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.758382                                          0.601923  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0016941308975219726, 0.00...\n",
       "1250                     Perceptron       No Oversampling     MinMaxScaler             3                         RFE    [scannedLineItems, secondsPerEuro, tsne_axis_2]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.758382                                          0.601923  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.0023941993713378906, 0.00...\n",
       "567                      Perceptron       No Oversampling       No Scaling             3                         RFE    [scannedLineItems, secondsPerEuro, tsne_axis_2]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.758382                                          0.601923  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.002094602584838867, 0.002...\n",
       "2616                     Perceptron       No Oversampling        LogScaler             3                         RFE    [scannedLineItems, secondsPerEuro, tsne_axis_2]  {'alpha': 0.0005, 'fit_intercept': True, 'max_...                           -0.758382                                          0.601923  Perceptron(alpha=0.0005, class_weight=None, ea...  {'mean_fit_time': [0.001989459991455078, 0.001...\n",
       "648   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.766365                                          0.250215  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002992057800292969, 0.003...\n",
       "647   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.766365                                          0.250215  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003091716766357422, 0.003...\n",
       "1330  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.766365                                          0.250215  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0037897586822509765, 0.00...\n",
       "1331  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.766365                                          0.250215  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0032914400100708006, 0.00...\n",
       "2013  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.766365                                          0.250215  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002692770957946777, 0.002...\n",
       "2014  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.766365                                          0.250215  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0036898136138916017, 0.00...\n",
       "2270         RandomForestClassifier       No Oversampling        LogScaler             1                         RFE                                       [pca_axis_2]          {'criterion': 'gini', 'n_estimators': 10}                           -0.766365                                          0.409595  (DecisionTreeClassifier(class_weight=None, cri...  {'mean_fit_time': [0.010372304916381836, 0.018...\n",
       "1333  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.792975                                          0.286752  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003690195083618164, 0.003...\n",
       "1324  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.6}                           -0.792975                                          0.251640  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028921127319335937, 0.00...\n",
       "1332  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.792975                                          0.286752  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003889608383178711, 0.003...\n",
       "2007  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.6}                           -0.792975                                          0.251640  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029918909072875976, 0.00...\n",
       "2006  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.6}                           -0.792975                                          0.251640  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002992129325866699, 0.003...\n",
       "2015  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.792975                                          0.286752  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003693246841430664, 0.003...\n",
       "2016  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.792975                                          0.286752  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0033852338790893556, 0.00...\n",
       "640   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.6}                           -0.792975                                          0.251640  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002992057800292969, 0.002...\n",
       "641   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.6}                           -0.792975                                          0.251640  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029918670654296873, 0.00...\n",
       "650   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.792975                                          0.286752  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034906864166259766, 0.00...\n",
       "649   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.792975                                          0.286752  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0033908843994140624, 0.00...\n",
       "1323  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.6}                           -0.792975                                          0.251640  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0030918121337890625, 0.00...\n",
       "2017  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.806280                                          0.262763  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002895641326904297, 0.003...\n",
       "651   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.806280                                          0.262763  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003889608383178711, 0.003...\n",
       "1334  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.806280                                          0.262763  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003789973258972168, 0.003...\n",
       "1335  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.819585                                          0.257036  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0035903453826904297, 0.00...\n",
       "2018  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.819585                                          0.257036  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003091859817504883, 0.004...\n",
       "652   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.819585                                          0.257036  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034908294677734376, 0.00...\n",
       "2019  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.838212                                          0.256809  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003789854049682617, 0.004...\n",
       "1336  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.838212                                          0.256809  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003789663314819336, 0.003...\n",
       "653   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.838212                                          0.256809  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034906148910522463, 0.00...\n",
       "645   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                 {'reg_param': 0.5}                           -0.848856                                          0.266595  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00299220085144043, 0.0030...\n",
       "1328  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                 {'reg_param': 0.5}                           -0.848856                                          0.266595  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0032911300659179688, 0.00...\n",
       "2011  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                 {'reg_param': 0.5}                           -0.848856                                          0.266595  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00299220085144043, 0.0032...\n",
       "1325  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.5}                           -0.854178                                          0.233818  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003390955924987793, 0.003...\n",
       "642   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.5}                           -0.854178                                          0.233818  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0030917882919311523, 0.00...\n",
       "2008  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                 {'reg_param': 0.5}                           -0.854178                                          0.233818  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031913995742797853, 0.00...\n",
       "2692  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.001894998550415039, 0.002...\n",
       "2693  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025927543640136717, 0.00...\n",
       "2009  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0029918432235717775, 0.00...\n",
       "2010  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0026930809020996095, 0.00...\n",
       "1326  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003490638732910156, 0.002...\n",
       "1327  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034904003143310545, 0.00...\n",
       "644   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003091716766357422, 0.003...\n",
       "643   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                 {'reg_param': 0.5}                           -0.947312                                          0.238965  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031916141510009766, 0.00...\n",
       "1687                     GaussianNB       No Oversampling   StandardScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                                 {}                           -0.968600                                          0.301185       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0028923511505126952], 'st...\n",
       "321                      GaussianNB       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                                 {}                           -0.968600                                          0.301185       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002892279624938965], 'std...\n",
       "1004                     GaussianNB       No Oversampling     MinMaxScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                                 {}                           -0.968600                                          0.301185       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0029920339584350586], 'st...\n",
       "2370                     GaussianNB       No Oversampling        LogScaler             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                                 {}                           -0.968600                                          0.301185       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002992129325866699], 'std...\n",
       "1684                     GaussianNB       No Oversampling   StandardScaler             1                 SelectKBest                                      [tsne_axis_2]                                                 {}                           -0.992549                                          0.314330       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002194046974182129], 'std...\n",
       "1001                     GaussianNB       No Oversampling     MinMaxScaler             1                 SelectKBest                                      [tsne_axis_2]                                                 {}                           -0.992549                                          0.314330       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002194046974182129], 'std...\n",
       "318                      GaussianNB       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]                                                 {}                           -0.992549                                          0.314330       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.00239255428314209], 'std_...\n",
       "1688                     GaussianNB       No Oversampling   StandardScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.008515                                          0.316076       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0019947052001953124], 'st...\n",
       "1005                     GaussianNB       No Oversampling     MinMaxScaler             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.008515                                          0.316076       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002992105484008789], 'std...\n",
       "322                      GaussianNB       No Oversampling       No Scaling             5                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.008515                                          0.316076       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0023935317993164064], 'st...\n",
       "2688  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             5                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                 {'reg_param': 0.6}                           -1.128260                                          0.362905  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028921127319335937, 0.00...\n",
       "2689  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             6                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                 {'reg_param': 0.5}                           -1.141565                                          0.375864  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0032910823822021483, 0.00...\n",
       "2690  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             7                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                 {'reg_param': 0.6}                           -1.168175                                          0.384575  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002692413330078125, 0.003...\n",
       "2691  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler             8                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                 {'reg_param': 0.6}                           -1.216072                                          0.365313  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0021939516067504884, 0.00...\n",
       "323                      GaussianNB       No Oversampling       No Scaling             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.303885                                          0.336085       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002294039726257324], 'std...\n",
       "1006                     GaussianNB       No Oversampling     MinMaxScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.303885                                          0.336085       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091740608215332], 'std...\n",
       "1689                     GaussianNB       No Oversampling   StandardScaler             6                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.303885                                          0.336085       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018947839736938477], 'st...\n",
       "2371                     GaussianNB       No Oversampling        LogScaler             5                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                                 {}                           -1.335817                                          0.347962       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091764450073242], 'std...\n",
       "2020  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.575306                                          0.407667  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003391265869140625, 0.003...\n",
       "654   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.575306                                          0.407667  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003690147399902344, 0.003...\n",
       "1337  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.575306                                          0.407667  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003490710258483887, 0.003...\n",
       "2697  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.615221                                          0.437013  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0037897348403930662, 0.00...\n",
       "2021  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.641831                                          0.451592  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.00458831787109375, 0.0038...\n",
       "655   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.641831                                          0.451592  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0034904241561889648, 0.00...\n",
       "1338  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.641831                                          0.451592  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0039894342422485355, 0.00...\n",
       "2704  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.655136                                          0.460204  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0039947509765625, 0.00378...\n",
       "2703  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.655136                                          0.460204  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002792501449584961, 0.002...\n",
       "2698  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.655136                                          0.482530  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003191542625427246, 0.002...\n",
       "2702  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.668441                                          0.483029  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0031916618347167967, 0.00...\n",
       "2700  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.668441                                          0.483029  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.004288268089294433, 0.003...\n",
       "2699  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.668441                                          0.483029  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003633713722229004, 0.002...\n",
       "2701  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.668441                                          0.483029  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0038915395736694334, 0.00...\n",
       "2705  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.695051                                          0.469873  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.004288601875305176, 0.003...\n",
       "656   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.695051                                          0.469873  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.004088997840881348, 0.004...\n",
       "324                      GaussianNB       No Oversampling       No Scaling             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.695051                                          0.354357       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994633674621582], 'std...\n",
       "2022  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.695051                                          0.469873  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.003390979766845703, 0.004...\n",
       "1690                     GaussianNB       No Oversampling   StandardScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.695051                                          0.354357       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994490623474121], 'std...\n",
       "1339  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                  {'reg_param': 0.7000000000000001}                           -1.695051                                          0.469873  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0040891170501708984, 0.00...\n",
       "1007                     GaussianNB       No Oversampling     MinMaxScaler             7                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -1.695051                                          0.354357       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091764450073242], 'std...\n",
       "2696  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                  {'reg_param': 0.7000000000000001}                           -1.753592                                          0.336484  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.002422666549682617, 0.002...\n",
       "2372                     GaussianNB       No Oversampling        LogScaler             6                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...                                                 {}                           -1.753592                                          0.341291       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032912254333496093], 'st...\n",
       "2694  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                 {'reg_param': 0.30000000000000004}                           -1.830761                                          0.418668  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0025930404663085938, 0.00...\n",
       "2024  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.355518  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0035905361175537108, 0.00...\n",
       "2706  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.351686  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.004498291015625, 0.003986...\n",
       "2707  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.355518  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.004487967491149903, 0.004...\n",
       "2023  QuadraticDiscriminantAnalysis       No Oversampling   StandardScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.351686  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.004587340354919434, 0.004...\n",
       "657   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.351686  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.06293184757232666, 0.0369...\n",
       "658   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.355518  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.04019250869750977, 0.1039...\n",
       "1341  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.355518  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0042888402938842775, 0.00...\n",
       "1340  QuadraticDiscriminantAnalysis       No Oversampling     MinMaxScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                 {'reg_param': 0.8}                           -1.849388                                          0.351686  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0038893938064575194, 0.00...\n",
       "2695  QuadraticDiscriminantAnalysis       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...                                 {'reg_param': 0.8}                           -1.910591                                          0.392880  QuadraticDiscriminantAnalysis(priors=None, reg...  {'mean_fit_time': [0.0028924465179443358, 0.00...\n",
       "2368                     GaussianNB       No Oversampling        LogScaler             2                 SelectKBest                          [trustLevel, tsne_axis_2]                                                 {}                           -2.038318                                          0.383335       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002692842483520508], 'std...\n",
       "2373                     GaussianNB       No Oversampling        LogScaler             7                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                                 {}                           -2.038318                                          0.365848       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091883659362793], 'std...\n",
       "1002                     GaussianNB       No Oversampling     MinMaxScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]                                                 {}                           -2.051623                                          0.377972       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0029920339584350586], 'st...\n",
       "1685                     GaussianNB       No Oversampling   StandardScaler             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]                                                 {}                           -2.051623                                          0.377972       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0031911611557006838], 'st...\n",
       "319                      GaussianNB       No Oversampling       No Scaling             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]                                                 {}                           -2.051623                                          0.377972       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0026927947998046874], 'st...\n",
       "1008                     GaussianNB       No Oversampling     MinMaxScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -2.064928                                          0.480548       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.00299227237701416], 'std_...\n",
       "325                      GaussianNB       No Oversampling       No Scaling             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -2.064928                                          0.480548       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017951726913452148], 'st...\n",
       "1691                     GaussianNB       No Oversampling   StandardScaler             8                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scannedLi...                                                 {}                           -2.064928                                          0.480548       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017953634262084961], 'st...\n",
       "1696                     GaussianNB       No Oversampling   StandardScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.481187       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017951488494873046], 'st...\n",
       "330                      GaussianNB       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.481187       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0016982316970825194], 'st...\n",
       "1009                     GaussianNB       No Oversampling     MinMaxScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0030918121337890625], 'st...\n",
       "1694                     GaussianNB       No Oversampling   StandardScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018950939178466798], 'st...\n",
       "1011                     GaussianNB       No Oversampling     MinMaxScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032912731170654298], 'st...\n",
       "2375                     GaussianNB       No Oversampling        LogScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032912731170654298], 'st...\n",
       "328                      GaussianNB       No Oversampling       No Scaling            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018949031829833985], 'st...\n",
       "326                      GaussianNB       No Oversampling       No Scaling             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0022939443588256836], 'st...\n",
       "1692                     GaussianNB       No Oversampling   StandardScaler             9                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.430278       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017982006072998046], 'st...\n",
       "1013                     GaussianNB       No Oversampling     MinMaxScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.317722                                          0.481187       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0030918359756469727], 'st...\n",
       "2374                     GaussianNB       No Oversampling        LogScaler             8                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                                 {}                           -2.331027                                          0.437288       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091859817504883], 'std...\n",
       "1010                     GaussianNB       No Oversampling     MinMaxScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.428792       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003191637992858887], 'std...\n",
       "327                      GaussianNB       No Oversampling       No Scaling            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.428792       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002293896675109863], 'std...\n",
       "1012                     GaussianNB       No Oversampling     MinMaxScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.422121       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091859817504883], 'std...\n",
       "329                      GaussianNB       No Oversampling       No Scaling            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.422121       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018949508666992188], 'st...\n",
       "2376                     GaussianNB       No Oversampling        LogScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.428792       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032914400100708006], 'st...\n",
       "1695                     GaussianNB       No Oversampling   StandardScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.422121       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018948078155517577], 'st...\n",
       "1693                     GaussianNB       No Oversampling   StandardScaler            10                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.344332                                          0.428792       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017952442169189454], 'st...\n",
       "1686                     GaussianNB       No Oversampling   StandardScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]                                                 {}                           -2.410857                                          0.491129       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032910823822021483], 'st...\n",
       "320                      GaussianNB       No Oversampling       No Scaling             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]                                                 {}                           -2.410857                                          0.491129       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002991914749145508], 'std...\n",
       "2369                     GaussianNB       No Oversampling        LogScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]                                                 {}                           -2.410857                                          0.491129       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0030918359756469727], 'st...\n",
       "1003                     GaussianNB       No Oversampling     MinMaxScaler             3                 SelectKBest              [trustLevel, pca_axis_2, tsne_axis_2]                                                 {}                           -2.410857                                          0.491129       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0028922319412231444], 'st...\n",
       "1014                     GaussianNB       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.424162                                          0.506500       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032912492752075195], 'st...\n",
       "1697                     GaussianNB       No Oversampling   StandardScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.424162                                          0.506500       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994800567626953], 'std...\n",
       "331                      GaussianNB       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.424162                                          0.506500       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0015930891036987304], 'st...\n",
       "1015                     GaussianNB       No Oversampling     MinMaxScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.437467                                          0.521442       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0028917789459228516], 'st...\n",
       "1698                     GaussianNB       No Oversampling   StandardScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.437467                                          0.521442       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0019947052001953124], 'st...\n",
       "332                      GaussianNB       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.437467                                          0.521442       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0017978906631469726], 'st...\n",
       "1699                     GaussianNB       No Oversampling   StandardScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.623736                                          0.579755       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018949508666992188], 'st...\n",
       "1016                     GaussianNB       No Oversampling     MinMaxScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.623736                                          0.579755       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003490638732910156], 'std...\n",
       "333                      GaussianNB       No Oversampling       No Scaling            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.623736                                          0.579755       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018887519836425781], 'st...\n",
       "1700                     GaussianNB       No Oversampling   StandardScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.668973                                          0.567880       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994633674621582], 'std...\n",
       "334                      GaussianNB       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.668973                                          0.567880       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0024932384490966796], 'st...\n",
       "1017                     GaussianNB       No Oversampling     MinMaxScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.668973                                          0.567880       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032913684844970703], 'st...\n",
       "2377                     GaussianNB       No Oversampling        LogScaler            11                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.868547                                          0.374236       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032912731170654298], 'st...\n",
       "1018                     GaussianNB       No Oversampling     MinMaxScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.868547                                          0.607780       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003191351890563965], 'std...\n",
       "1701                     GaussianNB       No Oversampling   StandardScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.868547                                          0.607780       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994681358337402], 'std...\n",
       "2380                     GaussianNB       No Oversampling        LogScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.868547                                          0.455081       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003191494941711426], 'std...\n",
       "335                      GaussianNB       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.868547                                          0.607780       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0024959564208984373], 'st...\n",
       "2379                     GaussianNB       No Oversampling        LogScaler            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, scansWith...                                                 {}                           -2.881852                                          0.449491       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0035904645919799805], 'st...\n",
       "2378                     GaussianNB       No Oversampling        LogScaler            12                 SelectKBest  [trustLevel, totalScanTimeInSeconds, valuePerS...                                                 {}                           -2.895157                                          0.447384       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.00329129695892334], 'std_...\n",
       "1019                     GaussianNB       No Oversampling     MinMaxScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.908462                                          0.618653       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003191685676574707], 'std...\n",
       "1702                     GaussianNB       No Oversampling   StandardScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.908462                                          0.618653       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002094554901123047], 'std...\n",
       "336                      GaussianNB       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -2.908462                                          0.618653       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002892422676086426], 'std...\n",
       "2381                     GaussianNB       No Oversampling        LogScaler            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.113358                                          0.525436       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003590512275695801], 'std...\n",
       "1703                     GaussianNB       No Oversampling   StandardScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.153273                                          0.592028       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0018949508666992188], 'st...\n",
       "337                      GaussianNB       No Oversampling       No Scaling            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.153273                                          0.592028       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003091716766357422], 'std...\n",
       "1020                     GaussianNB       No Oversampling     MinMaxScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.153273                                          0.592028       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0030918359756469727], 'st...\n",
       "1704                     GaussianNB       No Oversampling   StandardScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.299627                                          0.557549       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.001994681358337402], 'std...\n",
       "338                      GaussianNB       No Oversampling       No Scaling            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.299627                                          0.557549       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002992081642150879], 'std...\n",
       "1021                     GaussianNB       No Oversampling     MinMaxScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.299627                                          0.557549       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003191709518432617], 'std...\n",
       "2382                     GaussianNB       No Oversampling        LogScaler            16                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.326237                                          0.546788       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003590297698974609], 'std...\n",
       "2383                     GaussianNB       No Oversampling        LogScaler            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.339542                                          0.564224       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003391003608703613], 'std...\n",
       "2384                     GaussianNB       No Oversampling        LogScaler            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.366152                                          0.555916       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0033910512924194337], 'st...\n",
       "2385                     GaussianNB       No Oversampling        LogScaler            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.432677                                          0.536953       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0032912492752075195], 'st...\n",
       "1706                     GaussianNB       No Oversampling   StandardScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002293968200683594], 'std...\n",
       "1707                     GaussianNB       No Oversampling   StandardScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0020971298217773438], 'st...\n",
       "2389                     GaussianNB       No Oversampling        LogScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003490638732910156], 'std...\n",
       "1705                     GaussianNB       No Oversampling   StandardScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0021943092346191407], 'st...\n",
       "2386                     GaussianNB       No Oversampling        LogScaler            20                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.445982                                          0.534650       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0036902666091918946], 'st...\n",
       "2388                     GaussianNB       No Oversampling        LogScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003690338134765625], 'std...\n",
       "2390                     GaussianNB       No Oversampling        LogScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0037902355194091796], 'st...\n",
       "341                      GaussianNB       No Oversampling       No Scaling            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0031914710998535156], 'st...\n",
       "340                      GaussianNB       No Oversampling       No Scaling            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.003191637992858887], 'std...\n",
       "339                      GaussianNB       No Oversampling       No Scaling            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0030918359756469727], 'st...\n",
       "1024                     GaussianNB       No Oversampling     MinMaxScaler            24                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0022940635681152344], 'st...\n",
       "1023                     GaussianNB       No Oversampling     MinMaxScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002592611312866211], 'std...\n",
       "1022                     GaussianNB       No Oversampling     MinMaxScaler            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.445982                                          0.537931       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.002991914749145508], 'std...\n",
       "2387                     GaussianNB       No Oversampling        LogScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...                                                 {}                           -3.472592                                          0.553080       GaussianNB(priors=None, var_smoothing=1e-09)  {'mean_fit_time': [0.0033912181854248045], 'st...\n",
       "\n",
       "[2732 rows x 11 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table = result_table.sort_values(by = \"Monetary Value Per Instance - Mean\", ascending = False)\n",
    "# result_table.to_excel(\"Result-Train Set-Final.xlsx\")\n",
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to prefer no scaling in terms of the same computed value\n",
    "result_table[\"Data Preparation Technique\"] = -1\n",
    "result_table.loc[result_table[\"Data Preparation\"] == \"No Scaling\" , \"Data Preparation Technique\"] = 4\n",
    "result_table.loc[result_table[\"Data Preparation\"] == \"MinMaxScaler\", \"Data Preparation Technique\"] = 3\n",
    "result_table.loc[result_table[\"Data Preparation\"] == \"StandardScaler\", \"Data Preparation Technique\"] = 3\n",
    "result_table.loc[result_table[\"Data Preparation\"] == \"LogScaler\", \"Data Preparation Technique\"] = 1\n",
    "\n",
    "# to prefer a lower feature score\n",
    "result_table[\"Inverse Feature Count\"] = 1 / result_table[\"Feature Count\"]\n",
    "\n",
    "\n",
    "result_table = result_table.sort_values(by = [\"Monetary Value Per Instance - Mean\", \"Data Preparation Technique\", \"Inverse Feature Count\"], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result tables for all Scaling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Oversampling Strategy</th>\n",
       "      <th>Data Preparation</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Feature Selection Technique</th>\n",
       "      <th>Features</th>\n",
       "      <th>Optimal Parameters</th>\n",
       "      <th>Monetary Value Per Instance - Mean</th>\n",
       "      <th>Monetary Value Per Instance - Standard Deviation</th>\n",
       "      <th>Raw Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 200}</td>\n",
       "      <td>0.141032</td>\n",
       "      <td>0.080463</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_estimator': LogisticRegression(C=1.0, c...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.091208</td>\n",
       "      <td>(LogisticRegression(C=1.0, class_weight=None, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 45}</td>\n",
       "      <td>0.106440</td>\n",
       "      <td>0.078793</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>8</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, scansWithoutRegistrat...</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 45}</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.091681</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>5</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, scansWithoutRegistrat...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 21, 'min_sa...</td>\n",
       "      <td>0.055881</td>\n",
       "      <td>0.115128</td>\n",
       "      <td>ExtraTreeClassifier(class_weight=None, criteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>21</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 43, 'min...</td>\n",
       "      <td>0.053220</td>\n",
       "      <td>0.139132</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>23</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, grandTota...</td>\n",
       "      <td>{'activation': 'logistic', 'learning_rate': 'c...</td>\n",
       "      <td>0.047898</td>\n",
       "      <td>0.158939</td>\n",
       "      <td>MLPClassifier(activation='logistic', alpha=0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>7</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, lineItemVoidsPerPosition, scanned...</td>\n",
       "      <td>{'C': 0.5}</td>\n",
       "      <td>-0.146354</td>\n",
       "      <td>0.200883</td>\n",
       "      <td>SVC(C=0.5, cache_size=200, class_weight=None, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>6</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pricePerScanned...</td>\n",
       "      <td>{'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.170303</td>\n",
       "      <td>0.221343</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.207557</td>\n",
       "      <td>0.130813</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-0.220862</td>\n",
       "      <td>0.102408</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>LogScaler</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel]</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model Oversampling Strategy Data Preparation Feature Count Feature Selection Technique                                           Features                                 Optimal Parameters  Monetary Value Per Instance - Mean  Monetary Value Per Instance - Standard Deviation                                          Raw Model\n",
       "13             AdaBoostClassifier       No Oversampling       No Scaling            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...\n",
       "0                   XGBClassifier       No Oversampling       No Scaling            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.175625                                          0.108912  XGBClassifier(base_score=0.5, booster='gbtree'...\n",
       "10             LogisticRegression       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.094489  LogisticRegression(C=1.8000000000000003, class...\n",
       "6      GradientBoostingClassifier       No Oversampling     MinMaxScaler            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 200}                            0.141032                                          0.080463  ([DecisionTreeRegressor(criterion='friedman_ms...\n",
       "7               BaggingClassifier       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_estimator': LogisticRegression(C=1.0, c...                            0.138371                                          0.091208  (LogisticRegression(C=1.0, class_weight=None, ...\n",
       "8            ExtraTreesClassifier       No Oversampling        LogScaler             7                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...       {'criterion': 'entropy', 'n_estimators': 45}                            0.106440                                          0.078793  (ExtraTreeClassifier(class_weight=None, criter...\n",
       "5          RandomForestClassifier       No Oversampling   StandardScaler             8                         RFE  [totalScanTimeInSeconds, scansWithoutRegistrat...          {'criterion': 'gini', 'n_estimators': 45}                            0.085152                                          0.091681  (DecisionTreeClassifier(class_weight=None, cri...\n",
       "9             ExtraTreeClassifier       No Oversampling        LogScaler             5                         RFE  [totalScanTimeInSeconds, scansWithoutRegistrat...  {'criterion': 'gini', 'max_depth': 21, 'min_sa...                            0.055881                                          0.115128  ExtraTreeClassifier(class_weight=None, criteri...\n",
       "11         DecisionTreeClassifier       No Oversampling   StandardScaler            21                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'criterion': 'entropy', 'max_depth': 43, 'min...                            0.053220                                          0.139132  DecisionTreeClassifier(class_weight=None, crit...\n",
       "12                  MLPClassifier       No Oversampling   StandardScaler            23                 SelectKBest  [trustLevel, totalScanTimeInSeconds, grandTota...  {'activation': 'logistic', 'learning_rate': 'c...                            0.047898                                          0.158939  MLPClassifier(activation='logistic', alpha=0.0...\n",
       "14                            SVC       No Oversampling        LogScaler             7                 SelectKBest  [trustLevel, lineItemVoidsPerPosition, scanned...                                         {'C': 0.5}                           -0.146354                                          0.200883  SVC(C=0.5, cache_size=200, class_weight=None, ...\n",
       "15           KNeighborsClassifier       No Oversampling        LogScaler             6                 SelectKBest  [trustLevel, scannedLineItems, pricePerScanned...  {'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}                           -0.170303                                          0.221343  KNeighborsClassifier(algorithm='auto', leaf_si...\n",
       "2                      Perceptron       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.207557                                          0.130813  Perceptron(alpha=0.0005, class_weight=None, ea...\n",
       "4   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]                                 {'reg_param': 0.8}                           -0.220862                                          0.102408  QuadraticDiscriminantAnalysis(priors=None, reg...\n",
       "1                     BernoulliNB       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...\n",
       "3                      GaussianNB       No Oversampling        LogScaler             1                 SelectKBest                                       [trustLevel]                                                 {}                           -0.276743                                          0.011709       GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table_aggregated = pd.DataFrame(columns=[\"Model\", \"Oversampling Strategy\", \"Data Preparation\", \"Feature Count\", \"Feature Selection Technique\", \"Features\", \"Optimal Parameters\", \"Monetary Value Per Instance - Mean\", \"Monetary Value Per Instance - Standard Deviation\", \"Raw Model\"])\n",
    "\n",
    "\n",
    "for model in list(set(result_table[\"Model\"].values)):\n",
    "    sub_table = result_table[result_table[\"Model\"] == model]\n",
    "    result_table_aggregated = result_table_aggregated.append({\n",
    "        \"Model\": model,\n",
    "        \"Oversampling Strategy\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Oversampling Strategy\"],\n",
    "        \"Data Preparation\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Data Preparation\"],\n",
    "        \"Feature Count\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Feature Count\"],\n",
    "        \"Features\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Features\"],\n",
    "        \"Feature Selection Technique\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Feature Selection Technique\"],\n",
    "        \"Optimal Parameters\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Optimal Parameters\"],\n",
    "        \"Monetary Value Per Instance - Mean\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Monetary Value Per Instance - Mean\"],\n",
    "        \"Monetary Value Per Instance - Standard Deviation\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Monetary Value Per Instance - Standard Deviation\"],\n",
    "        \"Raw Model\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Raw Model\"]\n",
    "        }, ignore_index=True)\n",
    "\n",
    "result_table_aggregated.to_excel(\"Result-Train Set-Aggregated-Final.xlsx\")\n",
    "\n",
    "\n",
    "result_table_aggregated.sort_values(by = [\"Monetary Value Per Instance - Mean\"], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New aggregated result table with 'No scaling' only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Oversampling Strategy</th>\n",
       "      <th>Data Preparation</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Feature Selection Technique</th>\n",
       "      <th>Features</th>\n",
       "      <th>Optimal Parameters</th>\n",
       "      <th>Monetary Value Per Instance - Mean</th>\n",
       "      <th>Monetary Value Per Instance - Standard Deviation</th>\n",
       "      <th>Raw Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_estimator': LogisticRegression(C=1.0, c...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.091208</td>\n",
       "      <td>(LogisticRegression(C=1.0, class_weight=None, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 200}</td>\n",
       "      <td>0.130389</td>\n",
       "      <td>0.084181</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 190}</td>\n",
       "      <td>0.090474</td>\n",
       "      <td>0.090753</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 40}</td>\n",
       "      <td>0.082491</td>\n",
       "      <td>0.097579</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 29, 'min_sa...</td>\n",
       "      <td>0.034593</td>\n",
       "      <td>0.126854</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'activation': 'logistic', 'learning_rate': 'i...</td>\n",
       "      <td>0.023949</td>\n",
       "      <td>0.108119</td>\n",
       "      <td>MLPClassifier(activation='logistic', alpha=0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 13, 'min...</td>\n",
       "      <td>-0.026610</td>\n",
       "      <td>0.090638</td>\n",
       "      <td>ExtraTreeClassifier(class_weight=None, criteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>-0.162320</td>\n",
       "      <td>0.181452</td>\n",
       "      <td>SVC(C=0.1, cache_size=200, class_weight=None, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.180947</td>\n",
       "      <td>0.210302</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.207557</td>\n",
       "      <td>0.130813</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-0.220862</td>\n",
       "      <td>0.102408</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.968600</td>\n",
       "      <td>0.301185</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model Oversampling Strategy Data Preparation Feature Count Feature Selection Technique                                           Features                                 Optimal Parameters  Monetary Value Per Instance - Mean  Monetary Value Per Instance - Standard Deviation                                          Raw Model\n",
       "13             AdaBoostClassifier       No Oversampling       No Scaling            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...\n",
       "0                   XGBClassifier       No Oversampling       No Scaling            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.175625                                          0.108912  XGBClassifier(base_score=0.5, booster='gbtree'...\n",
       "10             LogisticRegression       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.094489  LogisticRegression(C=1.8000000000000003, class...\n",
       "7               BaggingClassifier       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_estimator': LogisticRegression(C=1.0, c...                            0.138371                                          0.091208  (LogisticRegression(C=1.0, class_weight=None, ...\n",
       "6      GradientBoostingClassifier       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 200}                            0.130389                                          0.084181  ([DecisionTreeRegressor(criterion='friedman_ms...\n",
       "8            ExtraTreesClassifier       No Oversampling       No Scaling            10                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...         {'criterion': 'gini', 'n_estimators': 190}                            0.090474                                          0.090753  (ExtraTreeClassifier(class_weight=None, criter...\n",
       "5          RandomForestClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...       {'criterion': 'entropy', 'n_estimators': 40}                            0.082491                                          0.097579  (DecisionTreeClassifier(class_weight=None, cri...\n",
       "11         DecisionTreeClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'criterion': 'gini', 'max_depth': 29, 'min_sa...                            0.034593                                          0.126854  DecisionTreeClassifier(class_weight=None, crit...\n",
       "12                  MLPClassifier       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'activation': 'logistic', 'learning_rate': 'i...                            0.023949                                          0.108119  MLPClassifier(activation='logistic', alpha=0.0...\n",
       "9             ExtraTreeClassifier       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'criterion': 'entropy', 'max_depth': 13, 'min...                           -0.026610                                          0.090638  ExtraTreeClassifier(class_weight=None, criteri...\n",
       "14                            SVC       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                         {'C': 0.1}                           -0.162320                                          0.181452  SVC(C=0.1, cache_size=200, class_weight=None, ...\n",
       "15           KNeighborsClassifier       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...  {'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}                           -0.180947                                          0.210302  KNeighborsClassifier(algorithm='auto', leaf_si...\n",
       "2                      Perceptron       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.207557                                          0.130813  Perceptron(alpha=0.0005, class_weight=None, ea...\n",
       "4   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]                                 {'reg_param': 0.8}                           -0.220862                                          0.102408  QuadraticDiscriminantAnalysis(priors=None, reg...\n",
       "1                     BernoulliNB       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...\n",
       "3                      GaussianNB       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                                 {}                           -0.968600                                          0.301185       GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table_aggregated = pd.DataFrame(columns=[\"Model\", \"Oversampling Strategy\", \"Data Preparation\", \"Feature Count\", \"Feature Selection Technique\", \"Features\", \"Optimal Parameters\", \"Monetary Value Per Instance - Mean\", \"Monetary Value Per Instance - Standard Deviation\", \"Raw Model\"])\n",
    "\n",
    "\n",
    "for model in list(set(result_table[\"Model\"].values)):\n",
    "    sub_table = result_table[(result_table[\"Model\"] == model) & (result_table[\"Data Preparation\"] == 'No Scaling')]\n",
    "    result_table_aggregated = result_table_aggregated.append({\n",
    "        \"Model\": model,\n",
    "        \"Oversampling Strategy\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Oversampling Strategy\"],\n",
    "        \"Data Preparation\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Data Preparation\"],\n",
    "        \"Feature Count\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Feature Count\"],\n",
    "        \"Features\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Features\"],\n",
    "        \"Feature Selection Technique\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Feature Selection Technique\"],\n",
    "        \"Optimal Parameters\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Optimal Parameters\"],\n",
    "        \"Monetary Value Per Instance - Mean\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Monetary Value Per Instance - Mean\"],\n",
    "        \"Monetary Value Per Instance - Standard Deviation\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Monetary Value Per Instance - Standard Deviation\"],\n",
    "        \"Raw Model\": sub_table.loc[sub_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Raw Model\"]\n",
    "        }, ignore_index=True)\n",
    "\n",
    "result_table_aggregated.sort_values(by = \"Monetary Value Per Instance - Mean\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1: Just use the best performing model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Monetary Value Per Instance - Mean')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "\n",
    "sub_table = result_table[result_table[\"Model\"] == \"AdaBoostClassifier\"]\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(sub_table[\"Feature Count\"],\n",
    "            sub_table[\"Monetary Value Per Instance - Mean\"],\n",
    "             c = 'black'),\n",
    "\n",
    "\n",
    "plt.xlabel('Number of features', fontsize=16)\n",
    "plt.ylabel('Monetary Value Per Instance - Mean', fontsize=16)\n",
    "\n",
    "#ggplot(aes(x='Feature Count', y='Monetary Value Per Instance - Mean', color='Data Preparation'), data=sub_table)  + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Monetary Value Per Instance - Mean')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(sub_table[\"Feature Count\"][sub_table['Data Preparation'] == 'No Scaling'],\n",
    "            sub_table[\"Monetary Value Per Instance - Mean\"][sub_table['Data Preparation'] == 'No Scaling'],\n",
    "             c = 'red'),\n",
    "\n",
    "\n",
    "plt.xlabel('Number of features', fontsize=16)\n",
    "plt.ylabel('Monetary Value Per Instance - Mean', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "['totalScanTimeInSeconds' 'lineItemVoids' 'scansWithoutRegistration'\n",
      " 'valuePerSecond' 'scannedLineItems'\n",
      " 'scansWithoutRegistrationPerScannedLineItem'\n",
      " 'quantityModificationsPerSecond' 'secondsPerEuro'\n",
      " 'quantityModificationsPerEuro' 'pca_axis_2' 'tsne_axis_1' 'tsne_axis_2']\n",
      "{'algorithm': 'SAMME.R', 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "best_model = result_table_aggregated.loc[result_table_aggregated[\"Monetary Value Per Instance - Mean\"].argmax()][\"Raw Model\"]\n",
    "best_model_features = result_table_aggregated.loc[result_table_aggregated[\"Monetary Value Per Instance - Mean\"].argmax()][\"Features\"]\n",
    "best_parameters = result_table.loc[result_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Optimal Parameters\"]\n",
    "\n",
    "print(best_model)\n",
    "print(best_model_features)\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the result of all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.05056438, 0.17692645, 0.37399921, 0.46425774, 0.57137134,\n",
       "        0.57177   , 0.5955121 , 0.8164155 , 0.74061856, 0.54613895,\n",
       "        0.01884937, 0.02323802, 0.02872324, 0.02732677, 0.04497972,\n",
       "        0.00907559, 0.00937464, 0.00917575, 0.00937469, 0.00937481,\n",
       "        0.0039891 , 0.00378964, 0.00408907, 0.00388956, 0.00378983,\n",
       "        0.05824387, 0.14392016, 0.27386718, 0.42496297, 0.56478891,\n",
       "        0.07998586, 0.1682497 , 0.32752368, 0.38646545, 0.50474944,\n",
       "        0.2154237 , 0.21143417, 0.48330667, 0.7181747 , 0.8779479 ,\n",
       "        0.01555858, 0.03610351, 0.02353699, 0.03859234, 0.0218416 ,\n",
       "        0.00897267, 0.00857706, 0.00897627, 0.00897574, 0.00937436,\n",
       "        0.00408933, 0.00388973, 0.00388963, 0.00408914, 0.00478721,\n",
       "        0.0622334 , 0.14900146, 0.25880735, 0.37030933, 0.50484943]),\n",
       " 'std_fit_time': array([2.67801331e-03, 2.29743703e-02, 3.03424518e-02, 6.68601587e-02,\n",
       "        3.33693535e-02, 4.05109396e-01, 3.31789183e-01, 5.03088288e-01,\n",
       "        2.09210337e-01, 4.63486984e-01, 3.07260936e-03, 1.33733740e-02,\n",
       "        2.34181883e-02, 1.43782504e-02, 5.07433823e-02, 6.97824209e-04,\n",
       "        6.61480027e-04, 7.46454924e-04, 4.88753122e-04, 6.61666789e-04,\n",
       "        3.06254210e-07, 5.98422925e-04, 2.99232068e-04, 2.99136391e-04,\n",
       "        3.98779432e-04, 2.45099631e-03, 1.12386786e-02, 1.13293331e-02,\n",
       "        2.91638246e-02, 3.38536232e-02, 7.98852581e-03, 2.56104978e-02,\n",
       "        4.19098336e-02, 5.48138467e-02, 9.48259035e-02, 2.74390890e-01,\n",
       "        2.72292848e-02, 3.57617836e-01, 4.03948915e-01, 4.01831706e-01,\n",
       "        1.11032497e-03, 2.45586369e-02, 1.22571902e-02, 4.11257887e-02,\n",
       "        9.13460069e-03, 4.46012669e-04, 6.61357614e-04, 4.46221678e-04,\n",
       "        4.45901846e-04, 6.62013503e-04, 1.04108911e-03, 5.37153874e-04,\n",
       "        5.36959018e-04, 5.36941438e-04, 2.77841997e-03, 1.49249559e-03,\n",
       "        2.72042678e-03, 3.51460239e-02, 5.95097747e-02, 9.88181338e-02]),\n",
       " 'mean_score_time': array([0.00289223, 0.00438826, 0.01236682, 0.01326642, 0.01545882,\n",
       "        0.00299218, 0.00308647, 0.00259295, 0.00309179, 0.00279229,\n",
       "        0.00299199, 0.00299215, 0.0027925 , 0.00269279, 0.00309181,\n",
       "        0.00159569, 0.00189533, 0.0016952 , 0.00189524, 0.00159574,\n",
       "        0.00169539, 0.00139642, 0.00159597, 0.00169523, 0.00169549,\n",
       "        0.00339108, 0.00518074, 0.00837767, 0.01366341, 0.01525912,\n",
       "        0.00398939, 0.00767946, 0.01466064, 0.01426201, 0.0190491 ,\n",
       "        0.00309174, 0.00658226, 0.0092766 , 0.01465793, 0.01825399,\n",
       "        0.00249326, 0.00299201, 0.00319138, 0.00289223, 0.00319142,\n",
       "        0.00199432, 0.001895  , 0.00159559, 0.00179563, 0.00189514,\n",
       "        0.00159571, 0.00159574, 0.00169554, 0.00159578, 0.00129669,\n",
       "        0.00379002, 0.00708117, 0.01007304, 0.01436408, 0.02014353]),\n",
       " 'std_score_time': array([5.37118394e-04, 4.88869806e-04, 5.73281301e-03, 2.40300880e-03,\n",
       "        1.11505282e-03, 6.30826586e-04, 6.99218327e-04, 4.88587457e-04,\n",
       "        9.40966890e-04, 3.98957885e-04, 4.46061742e-04, 5.33120150e-07,\n",
       "        3.99124839e-04, 4.57069424e-04, 6.98396244e-04, 4.88675143e-04,\n",
       "        2.99342691e-04, 4.56986212e-04, 5.37176051e-04, 4.88470714e-04,\n",
       "        4.56851084e-04, 4.88811432e-04, 4.88470772e-04, 4.57262160e-04,\n",
       "        4.57069536e-04, 4.88626367e-04, 4.01744973e-04, 4.88665296e-04,\n",
       "        4.54956724e-03, 1.09702471e-03, 7.72601474e-04, 1.34178441e-03,\n",
       "        3.48488620e-03, 3.21769924e-03, 4.51464461e-03, 8.28728001e-04,\n",
       "        6.61530129e-04, 2.36136844e-03, 3.96960101e-03, 5.62195039e-03,\n",
       "        4.98509649e-04, 4.46008549e-04, 3.98862500e-04, 5.37074050e-04,\n",
       "        3.98838608e-04, 3.50402318e-07, 2.99072513e-04, 4.88597218e-04,\n",
       "        3.98922209e-04, 2.99279233e-04, 4.88694646e-04, 4.88519541e-04,\n",
       "        4.57204875e-04, 4.88266323e-04, 4.56897928e-04, 3.98815297e-04,\n",
       "        5.37140478e-04, 2.01694680e-03, 2.89345299e-03, 4.41597204e-03]),\n",
       " 'param_algorithm': masked_array(data=['SAMME', 'SAMME', 'SAMME', 'SAMME', 'SAMME', 'SAMME',\n",
       "                    'SAMME', 'SAMME', 'SAMME', 'SAMME', 'SAMME', 'SAMME',\n",
       "                    'SAMME', 'SAMME', 'SAMME', 'SAMME', 'SAMME', 'SAMME',\n",
       "                    'SAMME', 'SAMME', 'SAMME', 'SAMME', 'SAMME', 'SAMME',\n",
       "                    'SAMME', 'SAMME', 'SAMME', 'SAMME', 'SAMME', 'SAMME',\n",
       "                    'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R',\n",
       "                    'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R',\n",
       "                    'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R',\n",
       "                    'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R',\n",
       "                    'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R',\n",
       "                    'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R', 'SAMME.R'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_base_estimator': masked_array(data=[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "                    ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "                    ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "                    ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "                    ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=None,\n",
       "           splitter='random'),\n",
       "                    ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=None,\n",
       "           splitter='random'),\n",
       "                    ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=None,\n",
       "           splitter='random'),\n",
       "                    ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=None,\n",
       "           splitter='random'),\n",
       "                    ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=None,\n",
       "           splitter='random'),\n",
       "                    None, None, None, None, None,\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "                    ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "                    ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "                    ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "                    ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "                    ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=None,\n",
       "           splitter='random'),\n",
       "                    ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=None,\n",
       "           splitter='random'),\n",
       "                    ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=None,\n",
       "           splitter='random'),\n",
       "                    ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=None,\n",
       "           splitter='random'),\n",
       "                    ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=None,\n",
       "           splitter='random'),\n",
       "                    None, None, None, None, None],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[20, 50, 100, 150, 200, 20, 50, 100, 150, 200, 20, 50,\n",
       "                    100, 150, 200, 20, 50, 100, 150, 200, 20, 50, 100, 150,\n",
       "                    200, 20, 50, 100, 150, 200, 20, 50, 100, 150, 200, 20,\n",
       "                    50, 100, 150, 200, 20, 50, 100, 150, 200, 20, 50, 100,\n",
       "                    150, 200, 20, 50, 100, 150, 200, 20, 50, 100, 150, 200],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'algorithm': 'SAMME',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 200},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'n_estimators': 200},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "   'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "   'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "   'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "   'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "   'n_estimators': 200},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 200},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, random_state=None,\n",
       "             splitter='random'),\n",
       "   'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, random_state=None,\n",
       "             splitter='random'),\n",
       "   'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, random_state=None,\n",
       "             splitter='random'),\n",
       "   'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, random_state=None,\n",
       "             splitter='random'),\n",
       "   'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME',\n",
       "   'base_estimator': ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, random_state=None,\n",
       "             splitter='random'),\n",
       "   'n_estimators': 200},\n",
       "  {'algorithm': 'SAMME', 'base_estimator': None, 'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME', 'base_estimator': None, 'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME', 'base_estimator': None, 'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME', 'base_estimator': None, 'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME', 'base_estimator': None, 'n_estimators': 200},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 200},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'n_estimators': 200},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "   'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "   'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "   'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "   'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "   'n_estimators': 200},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "               splitter='best'),\n",
       "   'n_estimators': 200},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, random_state=None,\n",
       "             splitter='random'),\n",
       "   'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, random_state=None,\n",
       "             splitter='random'),\n",
       "   'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, random_state=None,\n",
       "             splitter='random'),\n",
       "   'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, random_state=None,\n",
       "             splitter='random'),\n",
       "   'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME.R',\n",
       "   'base_estimator': ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, random_state=None,\n",
       "             splitter='random'),\n",
       "   'n_estimators': 200},\n",
       "  {'algorithm': 'SAMME.R', 'base_estimator': None, 'n_estimators': 20},\n",
       "  {'algorithm': 'SAMME.R', 'base_estimator': None, 'n_estimators': 50},\n",
       "  {'algorithm': 'SAMME.R', 'base_estimator': None, 'n_estimators': 100},\n",
       "  {'algorithm': 'SAMME.R', 'base_estimator': None, 'n_estimators': 150},\n",
       "  {'algorithm': 'SAMME.R', 'base_estimator': None, 'n_estimators': 200}],\n",
       " 'split0_test_score': array([ 0.18518519,  0.02645503,  0.15873016,  0.15873016,  0.15873016,\n",
       "        -0.29100529, -0.29100529, -0.29100529, -0.29100529, -0.29100529,\n",
       "         0.02645503, -0.13227513, -0.05291005,  0.07936508, -0.18518519,\n",
       "        -0.23809524, -0.10582011, -0.10582011, -0.10582011, -0.23809524,\n",
       "        -0.76719577, -0.26455026, -0.15873016, -0.66137566, -0.66137566,\n",
       "         0.18518519,  0.02645503,  0.15873016,  0.15873016,  0.15873016,\n",
       "         0.23809524,  0.15873016,  0.15873016,  0.10582011,  0.15873016,\n",
       "        -0.42328042, -0.23809524, -0.07936508, -0.02645503,  0.02645503,\n",
       "        -0.18518519, -0.10582011,  0.07936508,  0.07936508, -0.13227513,\n",
       "        -0.10582011, -0.10582011, -0.23809524, -0.23809524, -0.23809524,\n",
       "         0.05291005, -0.63492063, -1.00529101, -0.3968254 , -0.58201058,\n",
       "         0.23809524,  0.15873016,  0.15873016,  0.10582011,  0.15873016]),\n",
       " 'split1_test_score': array([-0.02645503,  0.10582011,  0.10582011,  0.23809524,  0.23809524,\n",
       "        -0.29100529, -0.29100529, -0.29100529, -0.29100529, -0.29100529,\n",
       "         0.05291005, -0.18518519,  0.18518519,  0.10582011,  0.        ,\n",
       "        -0.07936508, -0.07936508, -0.07936508, -0.02645503, -0.13227513,\n",
       "        -0.60846561,  0.13227513, -0.55555556, -0.68783069,  0.        ,\n",
       "        -0.02645503,  0.10582011,  0.10582011,  0.23809524,  0.23809524,\n",
       "         0.10582011,  0.23809524,  0.05291005,  0.05291005,  0.05291005,\n",
       "        -0.23809524, -0.18518519, -0.02645503,  0.13227513,  0.13227513,\n",
       "        -0.13227513,  0.23809524,  0.        ,  0.18518519,  0.18518519,\n",
       "        -0.13227513, -0.07936508, -0.13227513, -0.13227513, -0.13227513,\n",
       "        -0.42328042, -0.66137566, -0.15873016, -0.21164021, -0.3968254 ,\n",
       "         0.10582011,  0.23809524,  0.05291005,  0.05291005,  0.05291005]),\n",
       " 'split2_test_score': array([-0.21164021, -0.02645503,  0.15873016,  0.15873016,  0.15873016,\n",
       "        -0.31746032, -0.31746032, -0.31746032, -0.31746032, -0.31746032,\n",
       "        -0.07936508,  0.15873016, -0.15873016, -0.21164021,  0.18518519,\n",
       "        -0.21164021, -0.21164021, -0.26455026, -0.26455026, -0.21164021,\n",
       "        -0.82010582, -0.15873016, -0.92592593, -0.60846561, -0.31746032,\n",
       "        -0.21164021, -0.02645503,  0.15873016,  0.15873016,  0.15873016,\n",
       "         0.10582011,  0.15873016,  0.02645503,  0.02645503,  0.02645503,\n",
       "        -0.71428571, -0.21164021, -0.05291005,  0.05291005,  0.05291005,\n",
       "         0.10582011, -0.07936508,  0.05291005,  0.02645503,  0.10582011,\n",
       "        -0.21164021, -0.21164021, -0.21164021, -0.26455026, -0.34391534,\n",
       "        -0.02645503, -0.26455026, -0.58201058, -0.87301587, -0.60846561,\n",
       "         0.10582011,  0.15873016,  0.02645503,  0.02645503,  0.02645503]),\n",
       " 'split3_test_score': array([ 0.18518519,  0.18518519,  0.18518519,  0.18518519,  0.18518519,\n",
       "        -0.29100529, -0.29100529, -0.29100529, -0.29100529, -0.29100529,\n",
       "         0.18518519,  0.18518519,  0.07936508,  0.18518519,  0.18518519,\n",
       "         0.10582011, -0.02645503,  0.10582011, -0.02645503, -0.02645503,\n",
       "        -0.58201058,  0.        , -0.66137566, -0.21164021, -0.21164021,\n",
       "         0.18518519,  0.18518519,  0.18518519,  0.18518519,  0.18518519,\n",
       "         0.18518519,  0.23809524,  0.29100529,  0.23809524,  0.23809524,\n",
       "        -0.68783069, -0.31746032,  0.13227513,  0.13227513,  0.13227513,\n",
       "         0.18518519,  0.23809524,  0.18518519,  0.18518519,  0.        ,\n",
       "         0.23809524, -0.29100529, -0.15873016,  0.10582011,  0.10582011,\n",
       "        -0.52910053, -0.37037037, -0.10582011, -0.60846561, -0.42328042,\n",
       "         0.18518519,  0.23809524,  0.29100529,  0.23809524,  0.23809524]),\n",
       " 'split4_test_score': array([ 0.02659574,  0.07978723,  0.07978723,  0.21276596,  0.21276596,\n",
       "        -0.26595745, -0.26595745, -0.26595745, -0.26595745, -0.26595745,\n",
       "         0.02659574, -0.05319149, -0.05319149, -0.10638298, -0.15957447,\n",
       "        -0.10638298, -0.10638298, -0.10638298, -0.10638298, -0.10638298,\n",
       "        -0.87765957, -0.50531915, -0.26595745, -0.05319149,  0.02659574,\n",
       "         0.02659574,  0.07978723,  0.07978723,  0.21276596,  0.21276596,\n",
       "         0.21276596,  0.21276596,  0.21276596,  0.21276596,  0.21276596,\n",
       "        -0.66489362, -0.26595745,  0.15957447,  0.21276596,  0.13297872,\n",
       "         0.02659574,  0.07978723, -0.05319149, -0.10638298,  0.15957447,\n",
       "        -0.10638298, -0.10638298, -0.10638298, -0.10638298, -0.10638298,\n",
       "        -0.10638298, -0.34574468, -0.34574468, -0.21276596, -0.2393617 ,\n",
       "         0.21276596,  0.21276596,  0.21276596,  0.21276596,  0.21276596]),\n",
       " 'split5_test_score': array([-0.05347594,  0.26737968,  0.13368984,  0.13368984,  0.13368984,\n",
       "        -0.29411765, -0.29411765, -0.29411765, -0.29411765, -0.29411765,\n",
       "        -0.0802139 , -0.02673797, -0.02673797,  0.10695187,  0.05347594,\n",
       "        -0.32085561,  0.        , -0.26737968, -0.05347594, -0.13368984,\n",
       "        -0.85561497, -0.7486631 , -0.0802139 , -0.48128342, -0.80213904,\n",
       "        -0.05347594,  0.26737968,  0.13368984,  0.13368984,  0.13368984,\n",
       "         0.        ,  0.26737968,  0.13368984,  0.13368984,  0.26737968,\n",
       "        -0.80213904, -0.26737968, -0.05347594,  0.05347594,  0.16042781,\n",
       "         0.05347594,  0.05347594,  0.05347594, -0.13368984, -0.32085561,\n",
       "        -0.05347594, -0.13368984, -0.18716578, -0.24064171, -0.13368984,\n",
       "        -0.32085561, -0.29411765, -0.37433155, -0.29411765, -0.61497326,\n",
       "         0.        ,  0.26737968,  0.13368984,  0.13368984,  0.26737968]),\n",
       " 'split6_test_score': array([-0.0802139 ,  0.0802139 , -0.05347594,  0.0802139 ,  0.21390374,\n",
       "        -0.26737968, -0.26737968, -0.26737968, -0.26737968, -0.26737968,\n",
       "         0.10695187,  0.        ,  0.05347594,  0.16042781,  0.05347594,\n",
       "        -0.34759358, -0.29411765, -0.16042781, -0.02673797, -0.29411765,\n",
       "         0.10695187, -0.53475936, -0.34759358, -0.6684492 , -0.42780749,\n",
       "        -0.0802139 ,  0.0802139 , -0.05347594,  0.0802139 ,  0.21390374,\n",
       "        -0.10695187,  0.16042781,  0.02673797,  0.02673797,  0.0802139 ,\n",
       "        -0.26737968, -0.21390374,  0.        ,  0.05347594,  0.05347594,\n",
       "        -0.16042781, -0.18716578,  0.        ,  0.10695187,  0.10695187,\n",
       "        -0.16042781, -0.16042781, -0.29411765, -0.16042781, -0.16042781,\n",
       "        -0.16042781, -0.93582888, -0.29411765, -0.37433155, -0.16042781,\n",
       "        -0.10695187,  0.16042781,  0.02673797,  0.02673797,  0.0802139 ]),\n",
       " 'split7_test_score': array([ 0.21390374,  0.21390374,  0.26737968,  0.26737968,  0.26737968,\n",
       "        -0.26737968, -0.26737968, -0.26737968, -0.26737968, -0.26737968,\n",
       "         0.16042781,  0.02673797, -0.13368984,  0.16042781,  0.05347594,\n",
       "        -0.10695187, -0.10695187,  0.02673797, -0.24064171,  0.02673797,\n",
       "        -0.10695187, -0.48128342, -0.10695187, -0.16042781, -0.48128342,\n",
       "         0.21390374,  0.21390374,  0.26737968,  0.26737968,  0.26737968,\n",
       "         0.21390374,  0.21390374,  0.16042781,  0.21390374,  0.26737968,\n",
       "        -0.53475936, -0.26737968, -0.10695187,  0.05347594,  0.16042781,\n",
       "         0.10695187,  0.        ,  0.16042781,  0.16042781, -0.02673797,\n",
       "         0.02673797, -0.10695187,  0.02673797, -0.10695187, -0.10695187,\n",
       "         0.02673797, -0.18716578, -0.18716578, -0.16042781, -0.16042781,\n",
       "         0.21390374,  0.21390374,  0.16042781,  0.21390374,  0.26737968]),\n",
       " 'split8_test_score': array([-0.29411765, -0.02673797, -0.0802139 , -0.02673797, -0.0802139 ,\n",
       "        -0.26737968, -0.26737968, -0.26737968, -0.26737968, -0.26737968,\n",
       "        -0.13368984,  0.        ,  0.        , -0.26737968,  0.        ,\n",
       "        -0.16042781, -0.16042781, -0.16042781, -0.10695187, -0.10695187,\n",
       "        -0.6684492 , -0.53475936, -0.42780749,  0.05347594, -0.48128342,\n",
       "        -0.29411765, -0.02673797, -0.0802139 , -0.02673797, -0.0802139 ,\n",
       "         0.02673797,  0.16042781,  0.16042781,  0.10695187,  0.16042781,\n",
       "        -0.34759358, -0.40106952, -0.26737968, -0.10695187, -0.05347594,\n",
       "         0.10695187, -0.26737968, -0.21390374,  0.        , -0.13368984,\n",
       "        -0.24064171, -0.10695187, -0.10695187, -0.10695187, -0.10695187,\n",
       "        -0.37433155, -0.26737968, -0.37433155, -0.02673797, -0.50802139,\n",
       "         0.02673797,  0.16042781,  0.16042781,  0.10695187,  0.16042781]),\n",
       " 'split9_test_score': array([ 0.05347594,  0.05347594,  0.10695187,  0.16042781,  0.21390374,\n",
       "        -0.42780749, -0.42780749, -0.42780749, -0.42780749, -0.42780749,\n",
       "        -0.10695187, -0.13368984,  0.        , -0.05347594, -0.10695187,\n",
       "        -0.13368984, -0.13368984, -0.13368984, -0.0802139 , -0.18716578,\n",
       "        -0.50802139, -0.16042781, -0.13368984, -0.21390374, -0.29411765,\n",
       "         0.05347594,  0.05347594,  0.10695187,  0.16042781,  0.21390374,\n",
       "         0.21390374,  0.21390374,  0.21390374,  0.16042781,  0.16042781,\n",
       "        -0.53475936, -0.40106952, -0.16042781, -0.05347594, -0.05347594,\n",
       "         0.        , -0.05347594,  0.10695187,  0.05347594, -0.18716578,\n",
       "        -0.26737968, -0.26737968, -0.18716578, -0.13368984, -0.13368984,\n",
       "        -0.26737968,  0.10695187, -0.32085561, -0.02673797, -0.32085561,\n",
       "         0.21390374,  0.21390374,  0.21390374,  0.16042781,  0.16042781]),\n",
       " 'mean_test_score': array([ 0.        ,  0.09579564,  0.1064396 ,  0.1569984 ,  0.17030335,\n",
       "        -0.29803087, -0.29803087, -0.29803087, -0.29803087, -0.29803087,\n",
       "         0.01596594, -0.01596594, -0.01064396,  0.01596594,  0.00798297,\n",
       "        -0.15965939, -0.12240553, -0.11442257, -0.10377861, -0.14103246,\n",
       "        -0.56945184, -0.32464077, -0.3672166 , -0.36987759, -0.36455561,\n",
       "         0.        ,  0.09579564,  0.1064396 ,  0.1569984 ,  0.17030335,\n",
       "         0.11974454,  0.20223523,  0.14369345,  0.12772751,  0.16232038,\n",
       "        -0.52155402, -0.27674295, -0.04523683,  0.05055881,  0.07450772,\n",
       "         0.01064396, -0.00798297,  0.03725386,  0.05588079, -0.02394891,\n",
       "        -0.10111762, -0.1569984 , -0.15965939, -0.13837147, -0.13571048,\n",
       "        -0.21287919, -0.38584353, -0.37519957, -0.31931879, -0.40180947,\n",
       "         0.11974454,  0.20223523,  0.14369345,  0.12772751,  0.16232038]),\n",
       " 'std_test_score': array([0.16098944, 0.09402823, 0.09954344, 0.0791676 , 0.09158262,\n",
       "        0.04597764, 0.04597764, 0.04597764, 0.04597764, 0.04597764,\n",
       "        0.10727576, 0.1146397 , 0.09593298, 0.15573325, 0.12175593,\n",
       "        0.12392304, 0.08107099, 0.10978762, 0.08103531, 0.09145314,\n",
       "        0.31023821, 0.26420656, 0.26426852, 0.26781545, 0.25054545,\n",
       "        0.16098944, 0.09402823, 0.09954344, 0.0791676 , 0.09158262,\n",
       "        0.10913983, 0.03805113, 0.08253479, 0.07415321, 0.08223635,\n",
       "        0.18691544, 0.07131055, 0.11977549, 0.09012072, 0.07788608,\n",
       "        0.12192358, 0.15782216, 0.10861748, 0.1066565 , 0.15820879,\n",
       "        0.14070005, 0.07070333, 0.08327171, 0.09996856, 0.10768489,\n",
       "        0.19048676, 0.27563663, 0.24658119, 0.24913681, 0.16776579,\n",
       "        0.10913983, 0.03805113, 0.08253479, 0.07415321, 0.08223635]),\n",
       " 'rank_test_score': array([27, 17, 15,  7,  3, 46, 46, 46, 46, 46, 23, 31, 30, 23, 26, 42, 37,\n",
       "        36, 35, 40, 60, 52, 54, 55, 53, 27, 17, 15,  7,  3, 13,  1,  9, 11,\n",
       "         5, 59, 45, 33, 21, 19, 25, 29, 22, 20, 32, 34, 41, 42, 39, 38, 44,\n",
       "        57, 56, 51, 58, 13,  1,  9, 11,  5]),\n",
       " 'split0_train_score': array([ 0.07692308,  0.23668639,  0.27514793,  0.27514793,  0.27514793,\n",
       "        -0.27514793, -0.27514793, -0.27514793, -0.27514793, -0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.07692308,  0.23668639,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.22781065,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "        -0.47928994, -0.23372781, -0.04733728,  0.08284024,  0.10650888,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.22781065,  0.27514793,  0.27514793,  0.27514793,  0.27514793]),\n",
       " 'split1_train_score': array([ 0.0887574 ,  0.26331361,  0.27514793,  0.27514793,  0.27514793,\n",
       "        -0.27514793, -0.27514793, -0.27514793, -0.27514793, -0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.0887574 ,  0.26331361,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.23372781,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "        -0.49408284, -0.26627219, -0.0295858 ,  0.0443787 ,  0.06213018,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.23372781,  0.27514793,  0.27514793,  0.27514793,  0.27514793]),\n",
       " 'split2_train_score': array([ 0.10946746,  0.23964497,  0.27514793,  0.27514793,  0.27514793,\n",
       "        -0.19230769, -0.19230769, -0.19230769, -0.19230769, -0.19230769,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.10946746,  0.23964497,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.24556213,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "        -0.49112426, -0.28106509, -0.0591716 ,  0.06804734,  0.1035503 ,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.24556213,  0.27514793,  0.27514793,  0.27514793,  0.27514793]),\n",
       " 'split3_train_score': array([ 0.11242604,  0.23372781,  0.27514793,  0.27514793,  0.27514793,\n",
       "        -0.27514793, -0.27514793, -0.27514793, -0.27514793, -0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.11242604,  0.23372781,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.23372781,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "        -0.5       , -0.28698225, -0.05325444,  0.0591716 ,  0.10059172,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.27514793,  0.27514793,  0.27514793,  0.27514793,  0.27514793,\n",
       "         0.23372781,  0.27514793,  0.27514793,  0.27514793,  0.27514793]),\n",
       " 'split4_train_score': array([ 0.03843879,  0.20697812,  0.27794205,  0.27794205,  0.27794205,\n",
       "        -0.27794205, -0.27794205, -0.27794205, -0.27794205, -0.27794205,\n",
       "         0.27794205,  0.27794205,  0.27794205,  0.27794205,  0.27794205,\n",
       "         0.27794205,  0.27794205,  0.27794205,  0.27794205,  0.27794205,\n",
       "         0.27794205,  0.27794205,  0.27794205,  0.27794205,  0.27794205,\n",
       "         0.03843879,  0.20697812,  0.27794205,  0.27794205,  0.27794205,\n",
       "         0.25724423,  0.27794205,  0.27794205,  0.27794205,  0.27794205,\n",
       "        -0.45239503, -0.25133057, -0.05617978,  0.05617978,  0.08574808,\n",
       "         0.27794205,  0.27794205,  0.27794205,  0.27794205,  0.27794205,\n",
       "         0.27794205,  0.27794205,  0.27794205,  0.27794205,  0.27794205,\n",
       "         0.27794205,  0.27794205,  0.27794205,  0.27794205,  0.27794205,\n",
       "         0.25724423,  0.27794205,  0.27794205,  0.27794205,  0.27794205]),\n",
       " 'split5_train_score': array([ 0.10047281,  0.26300236,  0.27777778,  0.27777778,  0.27777778,\n",
       "        -0.22163121, -0.22163121, -0.22163121, -0.22163121, -0.22163121,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.10047281,  0.26300236,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.24231678,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "        -0.47576832, -0.26004728, -0.07092199,  0.06205674,  0.0856974 ,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.24231678,  0.27777778,  0.27777778,  0.27777778,  0.27777778]),\n",
       " 'split6_train_score': array([ 0.07387707,  0.26300236,  0.27777778,  0.27777778,  0.27777778,\n",
       "        -0.27777778, -0.27777778, -0.27777778, -0.27777778, -0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.07387707,  0.26300236,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.25413712,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "        -0.50236407, -0.25118203, -0.0679669 ,  0.06205674,  0.10933806,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.25413712,  0.27777778,  0.27777778,  0.27777778,  0.27777778]),\n",
       " 'split7_train_score': array([ 0.03841608,  0.21572104,  0.27777778,  0.27777778,  0.27777778,\n",
       "        -0.42257683, -0.42257683, -0.42257683, -0.42257683, -0.42257683,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.03841608,  0.21572104,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.2570922 ,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "        -0.48167849, -0.2570922 , -0.05910165,  0.06205674,  0.0856974 ,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.2570922 ,  0.27777778,  0.27777778,  0.27777778,  0.27777778]),\n",
       " 'split8_train_score': array([ 0.12115839,  0.27186761,  0.27777778,  0.27777778,  0.27777778,\n",
       "        -0.27777778, -0.27777778, -0.27777778, -0.27777778, -0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.12115839,  0.27186761,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.24231678,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "        -0.48758865, -0.21867612, -0.01182033,  0.0856974 ,  0.09456265,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.24231678,  0.27777778,  0.27777778,  0.27777778,  0.27777778]),\n",
       " 'split9_train_score': array([ 0.13297872,  0.26004728,  0.27777778,  0.27777778,  0.27777778,\n",
       "        -0.33983452, -0.32801418, -0.32801418, -0.32801418, -0.32801418,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.13297872,  0.26004728,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.25118203,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "        -0.50531915, -0.23049645, -0.04728132,  0.07092199,  0.09456265,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.27777778,  0.27777778,  0.27777778,  0.27777778,  0.27777778,\n",
       "         0.25118203,  0.27777778,  0.27777778,  0.27777778,  0.27777778]),\n",
       " 'mean_train_score': array([ 0.08929158,  0.24539916,  0.27674227,  0.27674227,  0.27674227,\n",
       "        -0.28352916, -0.28234713, -0.28234713, -0.28234713, -0.28234713,\n",
       "         0.27674227,  0.27674227,  0.27674227,  0.27674227,  0.27674227,\n",
       "         0.27674227,  0.27674227,  0.27674227,  0.27674227,  0.27674227,\n",
       "         0.27674227,  0.27674227,  0.27674227,  0.27674227,  0.27674227,\n",
       "         0.08929158,  0.24539916,  0.27674227,  0.27674227,  0.27674227,\n",
       "         0.24451176,  0.27674227,  0.27674227,  0.27674227,  0.27674227,\n",
       "        -0.48696108, -0.2536872 , -0.05026211,  0.06534072,  0.09283873,\n",
       "         0.27674227,  0.27674227,  0.27674227,  0.27674227,  0.27674227,\n",
       "         0.27674227,  0.27674227,  0.27674227,  0.27674227,  0.27674227,\n",
       "         0.27674227,  0.27674227,  0.27674227,  0.27674227,  0.27674227,\n",
       "         0.24451176,  0.27674227,  0.27674227,  0.27674227,  0.27674227]),\n",
       " 'std_train_score': array([0.03091366, 0.02111405, 0.00130263, 0.00130263, 0.00130263,\n",
       "        0.05913152, 0.05810337, 0.05810337, 0.05810337, 0.05810337,\n",
       "        0.00130263, 0.00130263, 0.00130263, 0.00130263, 0.00130263,\n",
       "        0.00130263, 0.00130263, 0.00130263, 0.00130263, 0.00130263,\n",
       "        0.00130263, 0.00130263, 0.00130263, 0.00130263, 0.00130263,\n",
       "        0.03091366, 0.02111405, 0.00130263, 0.00130263, 0.00130263,\n",
       "        0.00989837, 0.00130263, 0.00130263, 0.00130263, 0.00130263,\n",
       "        0.01489823, 0.02059691, 0.01692483, 0.01163119, 0.01317263,\n",
       "        0.00130263, 0.00130263, 0.00130263, 0.00130263, 0.00130263,\n",
       "        0.00130263, 0.00130263, 0.00130263, 0.00130263, 0.00130263,\n",
       "        0.00130263, 0.00130263, 0.00130263, 0.00130263, 0.00130263,\n",
       "        0.00989837, 0.00130263, 0.00130263, 0.00130263, 0.00130263])}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = result_table.loc[result_table[\"Monetary Value Per Instance - Mean\"].argmax()][\"Cross Validation Results\"]\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of the model on the entire train data set (when trained on the entire train data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negative:  1775\n",
      "False positive:  0\n",
      "False negative:  0\n",
      "True positive:  104\n",
      "520 for 1879 instances in the test set\n",
      "0.2767429483767962 per instance in the test set\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_monetary_value(cm):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        print(\"True negative: \", tn)\n",
    "        print(\"False positive: \", fp)\n",
    "        print(\"False negative: \", fn)\n",
    "        print(\"True positive: \", tp)\n",
    "        score = (-25)*fp + (-5)*fn + 5*tp\n",
    "        print(str(score) + \" for \" + str(sum(sum(cm))) + \" instances in the test set\")\n",
    "        print(str(score/sum(sum(cm))) + \" per instance in the test set\")\n",
    "        return score\n",
    "    \n",
    "cm = confusion_matrix(Y , best_model.predict(X[best_model_features]))\n",
    "\n",
    "monetary_value = get_monetary_value(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on the entire test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>scannedLineItems</th>\n",
       "      <th>pricePerScannedLineItem</th>\n",
       "      <th>scansWithoutRegistrationPerScannedLineItem</th>\n",
       "      <th>quantityModificationsPerScannedLineItem</th>\n",
       "      <th>lineItemVoidsPerSecond</th>\n",
       "      <th>scansWithoutRegistrationPerSecond</th>\n",
       "      <th>quantityModificationsPerSecond</th>\n",
       "      <th>secondsPerEuro</th>\n",
       "      <th>lineItemVoidsPerEuro</th>\n",
       "      <th>scansWithoutRegistrationPerEuro</th>\n",
       "      <th>quantityModificationsPerEuro</th>\n",
       "      <th>pca_axis_1</th>\n",
       "      <th>pca_axis_2</th>\n",
       "      <th>tsne_axis_1</th>\n",
       "      <th>tsne_axis_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>467</td>\n",
       "      <td>88.48</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>0.189465</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.640000</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.017131</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>5.278029</td>\n",
       "      <td>0.045208</td>\n",
       "      <td>0.090416</td>\n",
       "      <td>0.045208</td>\n",
       "      <td>0.407059</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1.288056</td>\n",
       "      <td>0.439125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1004</td>\n",
       "      <td>58.99</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026892</td>\n",
       "      <td>0.058755</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.184815</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.006972</td>\n",
       "      <td>0.005976</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>17.019834</td>\n",
       "      <td>0.118664</td>\n",
       "      <td>0.101712</td>\n",
       "      <td>0.016952</td>\n",
       "      <td>-0.404056</td>\n",
       "      <td>-0.107318</td>\n",
       "      <td>-2.048066</td>\n",
       "      <td>-0.496164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>14.00</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>11.571429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.648069</td>\n",
       "      <td>-0.481068</td>\n",
       "      <td>3.447044</td>\n",
       "      <td>0.053117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>532</td>\n",
       "      <td>84.79</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.159380</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.056429</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>6.274325</td>\n",
       "      <td>0.106145</td>\n",
       "      <td>0.035382</td>\n",
       "      <td>0.047175</td>\n",
       "      <td>0.292492</td>\n",
       "      <td>0.294786</td>\n",
       "      <td>1.344228</td>\n",
       "      <td>1.554838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>890</td>\n",
       "      <td>42.16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021348</td>\n",
       "      <td>0.047371</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.218947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.110057</td>\n",
       "      <td>0.094877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.564485</td>\n",
       "      <td>0.295128</td>\n",
       "      <td>-2.414502</td>\n",
       "      <td>1.547169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1072</td>\n",
       "      <td>12.67</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.603333</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>84.609313</td>\n",
       "      <td>0.236780</td>\n",
       "      <td>0.315706</td>\n",
       "      <td>0.078927</td>\n",
       "      <td>-0.388592</td>\n",
       "      <td>0.294494</td>\n",
       "      <td>-1.828019</td>\n",
       "      <td>1.732591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>259</td>\n",
       "      <td>93.75</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100386</td>\n",
       "      <td>0.361969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.605769</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.762667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.591763</td>\n",
       "      <td>-0.104089</td>\n",
       "      <td>-2.445146</td>\n",
       "      <td>-0.517219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1528</td>\n",
       "      <td>47.35</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009817</td>\n",
       "      <td>0.030988</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.156667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>32.270327</td>\n",
       "      <td>0.042239</td>\n",
       "      <td>0.190074</td>\n",
       "      <td>0.105597</td>\n",
       "      <td>0.458967</td>\n",
       "      <td>-0.303730</td>\n",
       "      <td>2.015723</td>\n",
       "      <td>-1.598574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>816</td>\n",
       "      <td>80.89</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017157</td>\n",
       "      <td>0.099130</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.777857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011029</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.087774</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.049450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.438512</td>\n",
       "      <td>0.506551</td>\n",
       "      <td>-2.074741</td>\n",
       "      <td>1.978520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>31.91</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1.994375</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.519524</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.501410</td>\n",
       "      <td>0.219367</td>\n",
       "      <td>0.219367</td>\n",
       "      <td>0.125353</td>\n",
       "      <td>0.211314</td>\n",
       "      <td>0.090980</td>\n",
       "      <td>1.474045</td>\n",
       "      <td>0.426568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>714</td>\n",
       "      <td>94.29</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.132059</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.857500</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.572383</td>\n",
       "      <td>0.084845</td>\n",
       "      <td>0.074239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.391550</td>\n",
       "      <td>-0.087511</td>\n",
       "      <td>-2.165339</td>\n",
       "      <td>-0.435252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>1077</td>\n",
       "      <td>66.16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015785</td>\n",
       "      <td>0.061430</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.891765</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.004643</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>16.278718</td>\n",
       "      <td>0.075574</td>\n",
       "      <td>0.120919</td>\n",
       "      <td>0.045345</td>\n",
       "      <td>0.081986</td>\n",
       "      <td>0.298952</td>\n",
       "      <td>0.382123</td>\n",
       "      <td>1.827146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>1301</td>\n",
       "      <td>84.35</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021522</td>\n",
       "      <td>0.064835</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.012500</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.007686</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>15.423829</td>\n",
       "      <td>0.035566</td>\n",
       "      <td>0.118554</td>\n",
       "      <td>0.059277</td>\n",
       "      <td>0.332642</td>\n",
       "      <td>0.084617</td>\n",
       "      <td>2.101652</td>\n",
       "      <td>0.551876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>1429</td>\n",
       "      <td>47.95</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003499</td>\n",
       "      <td>0.033555</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.590000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>29.801877</td>\n",
       "      <td>0.166840</td>\n",
       "      <td>0.020855</td>\n",
       "      <td>0.062565</td>\n",
       "      <td>0.203086</td>\n",
       "      <td>-0.094804</td>\n",
       "      <td>0.419216</td>\n",
       "      <td>-0.478411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>1196</td>\n",
       "      <td>83.77</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.070042</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.754000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009197</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.277188</td>\n",
       "      <td>0.131312</td>\n",
       "      <td>0.119374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.249300</td>\n",
       "      <td>-0.073777</td>\n",
       "      <td>-1.691172</td>\n",
       "      <td>-0.394049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>1567</td>\n",
       "      <td>75.53</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008296</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.810000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.006382</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>20.746723</td>\n",
       "      <td>0.092678</td>\n",
       "      <td>0.132398</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>-0.209044</td>\n",
       "      <td>-0.088058</td>\n",
       "      <td>-1.627112</td>\n",
       "      <td>-0.455931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>289</td>\n",
       "      <td>18.66</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086505</td>\n",
       "      <td>0.064567</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.746400</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027682</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.487674</td>\n",
       "      <td>0.428725</td>\n",
       "      <td>0.214362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.587221</td>\n",
       "      <td>0.094554</td>\n",
       "      <td>-2.528869</td>\n",
       "      <td>0.501003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>335</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032836</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.108182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>281.512605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.840336</td>\n",
       "      <td>-0.327371</td>\n",
       "      <td>-0.500976</td>\n",
       "      <td>-1.404831</td>\n",
       "      <td>-1.984003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>1304</td>\n",
       "      <td>30.51</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.023397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.906875</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>42.740085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229433</td>\n",
       "      <td>0.098328</td>\n",
       "      <td>0.047629</td>\n",
       "      <td>-0.102692</td>\n",
       "      <td>0.390086</td>\n",
       "      <td>-0.537004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>1353</td>\n",
       "      <td>82.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>0.060621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.717143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>16.495977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024384</td>\n",
       "      <td>-0.061909</td>\n",
       "      <td>0.301631</td>\n",
       "      <td>-0.469931</td>\n",
       "      <td>1.453829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>1749</td>\n",
       "      <td>46.56</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012007</td>\n",
       "      <td>0.026621</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.217143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>37.564433</td>\n",
       "      <td>0.107388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107388</td>\n",
       "      <td>0.335438</td>\n",
       "      <td>0.480083</td>\n",
       "      <td>1.940913</td>\n",
       "      <td>2.267286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>757</td>\n",
       "      <td>91.02</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019815</td>\n",
       "      <td>0.120238</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.068000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.003963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.316853</td>\n",
       "      <td>0.098879</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.454010</td>\n",
       "      <td>0.304593</td>\n",
       "      <td>-2.222024</td>\n",
       "      <td>1.415683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>1440</td>\n",
       "      <td>15.15</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.010521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.631250</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>95.049505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396040</td>\n",
       "      <td>0.330033</td>\n",
       "      <td>0.310644</td>\n",
       "      <td>-0.317246</td>\n",
       "      <td>2.062055</td>\n",
       "      <td>-1.834159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>1101</td>\n",
       "      <td>22.55</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019982</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.025000</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>0.008174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.824834</td>\n",
       "      <td>0.354767</td>\n",
       "      <td>0.399113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.516836</td>\n",
       "      <td>0.103528</td>\n",
       "      <td>-2.477338</td>\n",
       "      <td>0.480600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6</td>\n",
       "      <td>1404</td>\n",
       "      <td>59.43</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015670</td>\n",
       "      <td>0.042329</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.701364</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>0.005698</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>23.624432</td>\n",
       "      <td>0.084133</td>\n",
       "      <td>0.134612</td>\n",
       "      <td>0.084133</td>\n",
       "      <td>0.387792</td>\n",
       "      <td>0.488340</td>\n",
       "      <td>1.978658</td>\n",
       "      <td>2.308822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>1396</td>\n",
       "      <td>13.16</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010745</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.877333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007880</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106.079027</td>\n",
       "      <td>0.835866</td>\n",
       "      <td>0.151976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.479203</td>\n",
       "      <td>0.502977</td>\n",
       "      <td>-2.042129</td>\n",
       "      <td>1.932095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>248</td>\n",
       "      <td>14.00</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100806</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.036290</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>17.714286</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.217940</td>\n",
       "      <td>0.089334</td>\n",
       "      <td>-0.608626</td>\n",
       "      <td>0.511476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>359</td>\n",
       "      <td>6.40</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>0.017827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>56.093750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.136216</td>\n",
       "      <td>-0.096246</td>\n",
       "      <td>0.416303</td>\n",
       "      <td>-0.478445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6</td>\n",
       "      <td>915</td>\n",
       "      <td>36.79</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.040208</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.255714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.870889</td>\n",
       "      <td>0.244632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.401927</td>\n",
       "      <td>0.508693</td>\n",
       "      <td>-1.884966</td>\n",
       "      <td>1.719997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>617</td>\n",
       "      <td>49.57</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035656</td>\n",
       "      <td>0.080340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.253182</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>12.447045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100867</td>\n",
       "      <td>0.020173</td>\n",
       "      <td>-0.394770</td>\n",
       "      <td>0.294185</td>\n",
       "      <td>-1.889421</td>\n",
       "      <td>1.752990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>785</td>\n",
       "      <td>69.87</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.033121</td>\n",
       "      <td>0.089006</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.687308</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>11.235151</td>\n",
       "      <td>0.028625</td>\n",
       "      <td>0.014312</td>\n",
       "      <td>0.042937</td>\n",
       "      <td>-0.085711</td>\n",
       "      <td>0.281051</td>\n",
       "      <td>0.363176</td>\n",
       "      <td>2.021664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6</td>\n",
       "      <td>1576</td>\n",
       "      <td>94.01</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>0.059651</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.875625</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.003807</td>\n",
       "      <td>0.003173</td>\n",
       "      <td>0.003173</td>\n",
       "      <td>16.764174</td>\n",
       "      <td>0.063823</td>\n",
       "      <td>0.053186</td>\n",
       "      <td>0.053186</td>\n",
       "      <td>0.453287</td>\n",
       "      <td>0.491772</td>\n",
       "      <td>1.920002</td>\n",
       "      <td>2.143842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>44.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.344531</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>2.902494</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.022676</td>\n",
       "      <td>-0.305911</td>\n",
       "      <td>-0.499015</td>\n",
       "      <td>-1.549833</td>\n",
       "      <td>-2.117539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>1260</td>\n",
       "      <td>42.28</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.033556</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.456000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>29.801325</td>\n",
       "      <td>0.094607</td>\n",
       "      <td>0.165563</td>\n",
       "      <td>0.047304</td>\n",
       "      <td>0.038380</td>\n",
       "      <td>0.113650</td>\n",
       "      <td>-0.500719</td>\n",
       "      <td>0.467817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6</td>\n",
       "      <td>1206</td>\n",
       "      <td>39.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.032595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.655000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>30.679216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050878</td>\n",
       "      <td>0.017638</td>\n",
       "      <td>0.507226</td>\n",
       "      <td>-0.113821</td>\n",
       "      <td>1.689994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>883</td>\n",
       "      <td>22.42</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007928</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.202857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007928</td>\n",
       "      <td>0.005663</td>\n",
       "      <td>39.384478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312221</td>\n",
       "      <td>0.223015</td>\n",
       "      <td>0.533834</td>\n",
       "      <td>0.101324</td>\n",
       "      <td>1.874397</td>\n",
       "      <td>0.447375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>1554</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010296</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.257500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>377.184466</td>\n",
       "      <td>0.970874</td>\n",
       "      <td>2.427184</td>\n",
       "      <td>0.242718</td>\n",
       "      <td>-0.285590</td>\n",
       "      <td>-0.492750</td>\n",
       "      <td>-1.539333</td>\n",
       "      <td>-2.141390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>1734</td>\n",
       "      <td>9.38</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014994</td>\n",
       "      <td>0.005409</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.360769</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>184.861407</td>\n",
       "      <td>1.066098</td>\n",
       "      <td>0.639659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.577454</td>\n",
       "      <td>-0.303080</td>\n",
       "      <td>-2.396955</td>\n",
       "      <td>-1.499305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>1182</td>\n",
       "      <td>94.67</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.080093</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.982632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>12.485476</td>\n",
       "      <td>0.042252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042252</td>\n",
       "      <td>0.186985</td>\n",
       "      <td>-0.114655</td>\n",
       "      <td>1.540088</td>\n",
       "      <td>-0.537563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>1194</td>\n",
       "      <td>75.02</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010050</td>\n",
       "      <td>0.062831</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.251667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.004188</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>15.915756</td>\n",
       "      <td>0.066649</td>\n",
       "      <td>0.106638</td>\n",
       "      <td>0.039989</td>\n",
       "      <td>0.146536</td>\n",
       "      <td>0.104531</td>\n",
       "      <td>0.382479</td>\n",
       "      <td>0.465688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5</td>\n",
       "      <td>526</td>\n",
       "      <td>92.49</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053232</td>\n",
       "      <td>0.175837</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.303214</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.020913</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.009506</td>\n",
       "      <td>5.687101</td>\n",
       "      <td>0.118932</td>\n",
       "      <td>0.010812</td>\n",
       "      <td>0.054060</td>\n",
       "      <td>0.310278</td>\n",
       "      <td>0.276153</td>\n",
       "      <td>1.974766</td>\n",
       "      <td>1.768757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>1662</td>\n",
       "      <td>70.34</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009627</td>\n",
       "      <td>0.042323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.396250</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.628092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.467701</td>\n",
       "      <td>0.109340</td>\n",
       "      <td>-2.210949</td>\n",
       "      <td>0.471721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>1060</td>\n",
       "      <td>67.97</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.064123</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.265667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>0.005660</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>15.595115</td>\n",
       "      <td>0.102987</td>\n",
       "      <td>0.088274</td>\n",
       "      <td>0.058849</td>\n",
       "      <td>0.113458</td>\n",
       "      <td>0.081560</td>\n",
       "      <td>1.632863</td>\n",
       "      <td>0.532465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5</td>\n",
       "      <td>1276</td>\n",
       "      <td>59.30</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.046473</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.766667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.517707</td>\n",
       "      <td>0.016863</td>\n",
       "      <td>0.050590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.354376</td>\n",
       "      <td>0.315715</td>\n",
       "      <td>-1.826596</td>\n",
       "      <td>1.206667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>1075</td>\n",
       "      <td>80.75</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.013953</td>\n",
       "      <td>0.075116</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.383333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>13.312693</td>\n",
       "      <td>0.111455</td>\n",
       "      <td>0.086687</td>\n",
       "      <td>0.037152</td>\n",
       "      <td>0.124672</td>\n",
       "      <td>0.101301</td>\n",
       "      <td>0.358393</td>\n",
       "      <td>0.504236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>67.99</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.102241</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.997500</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>9.780850</td>\n",
       "      <td>0.029416</td>\n",
       "      <td>0.102956</td>\n",
       "      <td>0.044124</td>\n",
       "      <td>0.253154</td>\n",
       "      <td>-0.487076</td>\n",
       "      <td>0.424097</td>\n",
       "      <td>-1.681565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5</td>\n",
       "      <td>1768</td>\n",
       "      <td>81.84</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>0.046290</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.897143</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>21.603128</td>\n",
       "      <td>0.109971</td>\n",
       "      <td>0.012219</td>\n",
       "      <td>0.012219</td>\n",
       "      <td>-0.357523</td>\n",
       "      <td>0.293651</td>\n",
       "      <td>-1.751355</td>\n",
       "      <td>1.645385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>1772</td>\n",
       "      <td>33.45</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.018877</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.787500</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>52.974589</td>\n",
       "      <td>0.269058</td>\n",
       "      <td>0.149477</td>\n",
       "      <td>0.149477</td>\n",
       "      <td>0.498718</td>\n",
       "      <td>-0.103505</td>\n",
       "      <td>1.981186</td>\n",
       "      <td>-0.538616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2</td>\n",
       "      <td>870</td>\n",
       "      <td>90.86</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.031034</td>\n",
       "      <td>0.104437</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.365185</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>9.575171</td>\n",
       "      <td>0.088048</td>\n",
       "      <td>0.088048</td>\n",
       "      <td>0.055030</td>\n",
       "      <td>0.357115</td>\n",
       "      <td>-0.315162</td>\n",
       "      <td>2.108857</td>\n",
       "      <td>-1.824888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4</td>\n",
       "      <td>1323</td>\n",
       "      <td>40.34</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>0.030491</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.068000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>32.796232</td>\n",
       "      <td>0.173525</td>\n",
       "      <td>0.148736</td>\n",
       "      <td>0.099157</td>\n",
       "      <td>0.427446</td>\n",
       "      <td>0.108353</td>\n",
       "      <td>1.351503</td>\n",
       "      <td>0.428718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>1087</td>\n",
       "      <td>40.46</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017479</td>\n",
       "      <td>0.037222</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.129474</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.866041</td>\n",
       "      <td>0.024716</td>\n",
       "      <td>0.247158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.505859</td>\n",
       "      <td>-0.093829</td>\n",
       "      <td>-2.444576</td>\n",
       "      <td>-0.487174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3</td>\n",
       "      <td>631</td>\n",
       "      <td>34.78</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.045959</td>\n",
       "      <td>0.055119</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.199310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.012678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007924</td>\n",
       "      <td>18.142611</td>\n",
       "      <td>0.230017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143761</td>\n",
       "      <td>0.258254</td>\n",
       "      <td>-0.127098</td>\n",
       "      <td>2.137155</td>\n",
       "      <td>-0.610189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>766</td>\n",
       "      <td>23.84</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026110</td>\n",
       "      <td>0.031123</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.192000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>0.007833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.130872</td>\n",
       "      <td>0.377517</td>\n",
       "      <td>0.251678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.510724</td>\n",
       "      <td>-0.497340</td>\n",
       "      <td>-2.242349</td>\n",
       "      <td>-2.108567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4</td>\n",
       "      <td>1243</td>\n",
       "      <td>89.45</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.071963</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.065909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.004827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.896031</td>\n",
       "      <td>0.044718</td>\n",
       "      <td>0.067077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.537213</td>\n",
       "      <td>0.099864</td>\n",
       "      <td>-2.521923</td>\n",
       "      <td>0.494900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4</td>\n",
       "      <td>1328</td>\n",
       "      <td>69.66</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.052455</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.951429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.006777</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>19.064025</td>\n",
       "      <td>0.129199</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.057422</td>\n",
       "      <td>0.369560</td>\n",
       "      <td>0.100507</td>\n",
       "      <td>1.330060</td>\n",
       "      <td>0.432338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5</td>\n",
       "      <td>220</td>\n",
       "      <td>33.23</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.077273</td>\n",
       "      <td>0.151045</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.954706</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>6.620524</td>\n",
       "      <td>0.180560</td>\n",
       "      <td>0.150466</td>\n",
       "      <td>0.090280</td>\n",
       "      <td>0.053972</td>\n",
       "      <td>0.295087</td>\n",
       "      <td>0.376573</td>\n",
       "      <td>1.801181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5</td>\n",
       "      <td>416</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.060096</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.016827</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>1733.333333</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>29.166667</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>-0.423952</td>\n",
       "      <td>0.293156</td>\n",
       "      <td>-1.823436</td>\n",
       "      <td>1.677843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>330</td>\n",
       "      <td>16.71</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>0.050636</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.856667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>19.748654</td>\n",
       "      <td>0.359066</td>\n",
       "      <td>0.299222</td>\n",
       "      <td>0.179533</td>\n",
       "      <td>0.147244</td>\n",
       "      <td>-0.496160</td>\n",
       "      <td>0.358784</td>\n",
       "      <td>-2.020848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5</td>\n",
       "      <td>266</td>\n",
       "      <td>58.63</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097744</td>\n",
       "      <td>0.220414</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.255000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.536926</td>\n",
       "      <td>0.136449</td>\n",
       "      <td>0.068224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.585148</td>\n",
       "      <td>0.293887</td>\n",
       "      <td>-2.444040</td>\n",
       "      <td>1.618795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4</td>\n",
       "      <td>916</td>\n",
       "      <td>18.89</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017467</td>\n",
       "      <td>0.020622</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.180625</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>0.006550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.491265</td>\n",
       "      <td>0.211752</td>\n",
       "      <td>0.317628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.494299</td>\n",
       "      <td>0.104907</td>\n",
       "      <td>-2.445191</td>\n",
       "      <td>0.481664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>6</td>\n",
       "      <td>492</td>\n",
       "      <td>29.45</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.059858</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.227083</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>16.706282</td>\n",
       "      <td>0.033956</td>\n",
       "      <td>0.271647</td>\n",
       "      <td>0.101868</td>\n",
       "      <td>-0.030359</td>\n",
       "      <td>0.490101</td>\n",
       "      <td>0.386263</td>\n",
       "      <td>2.530044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>6</td>\n",
       "      <td>604</td>\n",
       "      <td>12.81</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.036424</td>\n",
       "      <td>0.021209</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.582273</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>47.150664</td>\n",
       "      <td>0.390320</td>\n",
       "      <td>0.234192</td>\n",
       "      <td>0.390320</td>\n",
       "      <td>0.335972</td>\n",
       "      <td>0.481823</td>\n",
       "      <td>1.968992</td>\n",
       "      <td>2.293505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4</td>\n",
       "      <td>712</td>\n",
       "      <td>83.07</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.116671</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.153500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>8.571085</td>\n",
       "      <td>0.120380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048152</td>\n",
       "      <td>0.202694</td>\n",
       "      <td>0.085536</td>\n",
       "      <td>1.476217</td>\n",
       "      <td>0.521695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4</td>\n",
       "      <td>520</td>\n",
       "      <td>39.85</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017308</td>\n",
       "      <td>0.076635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.427778</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>13.048934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100376</td>\n",
       "      <td>0.050188</td>\n",
       "      <td>-0.072150</td>\n",
       "      <td>0.103467</td>\n",
       "      <td>-0.522323</td>\n",
       "      <td>0.490866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>6</td>\n",
       "      <td>244</td>\n",
       "      <td>51.97</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.069672</td>\n",
       "      <td>0.212992</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.057059</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>4.695016</td>\n",
       "      <td>0.076967</td>\n",
       "      <td>0.115451</td>\n",
       "      <td>0.019242</td>\n",
       "      <td>-0.309943</td>\n",
       "      <td>0.501196</td>\n",
       "      <td>-1.605085</td>\n",
       "      <td>2.290144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>813</td>\n",
       "      <td>11.94</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027060</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.542727</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.090452</td>\n",
       "      <td>0.083752</td>\n",
       "      <td>0.418760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.585050</td>\n",
       "      <td>-0.502719</td>\n",
       "      <td>-2.282900</td>\n",
       "      <td>-2.140231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5</td>\n",
       "      <td>877</td>\n",
       "      <td>75.98</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023945</td>\n",
       "      <td>0.086636</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.618095</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.009122</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>11.542511</td>\n",
       "      <td>0.105291</td>\n",
       "      <td>0.039484</td>\n",
       "      <td>0.013161</td>\n",
       "      <td>-0.349019</td>\n",
       "      <td>0.295377</td>\n",
       "      <td>-1.810546</td>\n",
       "      <td>1.734635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5</td>\n",
       "      <td>280</td>\n",
       "      <td>59.81</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.213607</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.848095</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>4.681491</td>\n",
       "      <td>0.150477</td>\n",
       "      <td>0.083598</td>\n",
       "      <td>0.050159</td>\n",
       "      <td>0.032398</td>\n",
       "      <td>0.292187</td>\n",
       "      <td>0.362073</td>\n",
       "      <td>1.920195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>4</td>\n",
       "      <td>1354</td>\n",
       "      <td>21.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.015510</td>\n",
       "      <td>0.016041</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.034286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>62.338858</td>\n",
       "      <td>0.092081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092081</td>\n",
       "      <td>-0.237265</td>\n",
       "      <td>0.087244</td>\n",
       "      <td>-0.585706</td>\n",
       "      <td>0.531649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4</td>\n",
       "      <td>1564</td>\n",
       "      <td>48.20</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.030818</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.448133</td>\n",
       "      <td>0.062241</td>\n",
       "      <td>0.041494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.330304</td>\n",
       "      <td>0.117252</td>\n",
       "      <td>-1.736771</td>\n",
       "      <td>0.443097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>66.48</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022901</td>\n",
       "      <td>0.507481</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.160000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.083969</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>1.970517</td>\n",
       "      <td>0.165463</td>\n",
       "      <td>0.150421</td>\n",
       "      <td>0.015042</td>\n",
       "      <td>-0.005106</td>\n",
       "      <td>-0.272038</td>\n",
       "      <td>-0.993315</td>\n",
       "      <td>-0.934356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2</td>\n",
       "      <td>1184</td>\n",
       "      <td>35.34</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016892</td>\n",
       "      <td>0.029848</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.767000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.004223</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>33.503113</td>\n",
       "      <td>0.141483</td>\n",
       "      <td>0.226372</td>\n",
       "      <td>0.056593</td>\n",
       "      <td>-0.146389</td>\n",
       "      <td>-0.301370</td>\n",
       "      <td>-0.593195</td>\n",
       "      <td>-1.917251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5</td>\n",
       "      <td>1694</td>\n",
       "      <td>52.61</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.031057</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.305000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>32.199202</td>\n",
       "      <td>0.019008</td>\n",
       "      <td>0.190078</td>\n",
       "      <td>0.095039</td>\n",
       "      <td>0.764194</td>\n",
       "      <td>0.317292</td>\n",
       "      <td>1.221473</td>\n",
       "      <td>0.752527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1</td>\n",
       "      <td>1021</td>\n",
       "      <td>15.30</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.018609</td>\n",
       "      <td>0.014985</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.805263</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>66.732026</td>\n",
       "      <td>0.065359</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.261438</td>\n",
       "      <td>0.167253</td>\n",
       "      <td>-0.512760</td>\n",
       "      <td>1.357841</td>\n",
       "      <td>-2.423085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>1309</td>\n",
       "      <td>40.16</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.030680</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.362353</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.003820</td>\n",
       "      <td>32.594622</td>\n",
       "      <td>0.199203</td>\n",
       "      <td>0.074701</td>\n",
       "      <td>0.124502</td>\n",
       "      <td>0.418606</td>\n",
       "      <td>-0.511156</td>\n",
       "      <td>1.960953</td>\n",
       "      <td>-2.223192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>6</td>\n",
       "      <td>1409</td>\n",
       "      <td>30.54</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>0.021675</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.349231</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.136215</td>\n",
       "      <td>0.163720</td>\n",
       "      <td>0.261952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.436180</td>\n",
       "      <td>0.510618</td>\n",
       "      <td>-2.063818</td>\n",
       "      <td>1.988861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>4</td>\n",
       "      <td>952</td>\n",
       "      <td>61.40</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.064496</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.011555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>15.504886</td>\n",
       "      <td>0.179153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032573</td>\n",
       "      <td>0.265730</td>\n",
       "      <td>0.122574</td>\n",
       "      <td>3.404396</td>\n",
       "      <td>0.108386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2</td>\n",
       "      <td>1484</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.005640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.298929</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>177.299881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.836320</td>\n",
       "      <td>0.358423</td>\n",
       "      <td>-0.094212</td>\n",
       "      <td>-0.314690</td>\n",
       "      <td>0.361347</td>\n",
       "      <td>-2.027979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>81.30</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>2.463636</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.710000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.405904</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.656401</td>\n",
       "      <td>0.488213</td>\n",
       "      <td>-2.255680</td>\n",
       "      <td>2.111732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>4</td>\n",
       "      <td>555</td>\n",
       "      <td>19.91</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>0.035874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.977500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>27.875439</td>\n",
       "      <td>0.200904</td>\n",
       "      <td>0.050226</td>\n",
       "      <td>0.150678</td>\n",
       "      <td>0.184926</td>\n",
       "      <td>0.104084</td>\n",
       "      <td>0.473830</td>\n",
       "      <td>0.476910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>953</td>\n",
       "      <td>86.81</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025184</td>\n",
       "      <td>0.091091</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.617083</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.010493</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>10.977998</td>\n",
       "      <td>0.115194</td>\n",
       "      <td>0.069116</td>\n",
       "      <td>0.011519</td>\n",
       "      <td>-0.346283</td>\n",
       "      <td>-0.502986</td>\n",
       "      <td>-1.690420</td>\n",
       "      <td>-2.316562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2</td>\n",
       "      <td>437</td>\n",
       "      <td>56.71</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022883</td>\n",
       "      <td>0.129771</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.671000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>7.705872</td>\n",
       "      <td>0.035267</td>\n",
       "      <td>0.123435</td>\n",
       "      <td>0.017634</td>\n",
       "      <td>-0.228421</td>\n",
       "      <td>-0.290254</td>\n",
       "      <td>-1.455960</td>\n",
       "      <td>-1.345126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>6</td>\n",
       "      <td>675</td>\n",
       "      <td>14.47</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.021437</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.803889</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>46.648238</td>\n",
       "      <td>0.138217</td>\n",
       "      <td>0.069109</td>\n",
       "      <td>0.138217</td>\n",
       "      <td>-0.198784</td>\n",
       "      <td>0.490810</td>\n",
       "      <td>-0.567895</td>\n",
       "      <td>2.334896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>581</td>\n",
       "      <td>32.72</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.034423</td>\n",
       "      <td>0.056317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.636000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>17.756724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213936</td>\n",
       "      <td>0.152812</td>\n",
       "      <td>0.370116</td>\n",
       "      <td>-0.512009</td>\n",
       "      <td>1.933192</td>\n",
       "      <td>-2.282469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>3</td>\n",
       "      <td>1277</td>\n",
       "      <td>26.15</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.377273</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.833652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.441328</td>\n",
       "      <td>-0.088447</td>\n",
       "      <td>-2.182540</td>\n",
       "      <td>-0.449952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>4</td>\n",
       "      <td>706</td>\n",
       "      <td>16.38</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.023201</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.460000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>43.101343</td>\n",
       "      <td>0.305250</td>\n",
       "      <td>0.305250</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>0.453798</td>\n",
       "      <td>0.109272</td>\n",
       "      <td>1.300240</td>\n",
       "      <td>0.426456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>47.41</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>1.156341</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.160667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.864796</td>\n",
       "      <td>0.147648</td>\n",
       "      <td>0.042185</td>\n",
       "      <td>0.084370</td>\n",
       "      <td>0.249387</td>\n",
       "      <td>-0.108368</td>\n",
       "      <td>1.411381</td>\n",
       "      <td>-0.540051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>4</td>\n",
       "      <td>594</td>\n",
       "      <td>63.48</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.043771</td>\n",
       "      <td>0.106869</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.441538</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>9.357278</td>\n",
       "      <td>0.094518</td>\n",
       "      <td>0.094518</td>\n",
       "      <td>0.031506</td>\n",
       "      <td>-0.213568</td>\n",
       "      <td>0.090515</td>\n",
       "      <td>-0.614394</td>\n",
       "      <td>0.516220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>1044</td>\n",
       "      <td>51.44</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.049272</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.857778</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>20.295490</td>\n",
       "      <td>0.174961</td>\n",
       "      <td>0.077760</td>\n",
       "      <td>0.077760</td>\n",
       "      <td>0.239975</td>\n",
       "      <td>-0.507987</td>\n",
       "      <td>1.360582</td>\n",
       "      <td>-2.396240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5</td>\n",
       "      <td>650</td>\n",
       "      <td>59.73</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.091892</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.991000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>10.882304</td>\n",
       "      <td>0.184162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016742</td>\n",
       "      <td>-0.460335</td>\n",
       "      <td>0.283556</td>\n",
       "      <td>-1.834735</td>\n",
       "      <td>1.790446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>5</td>\n",
       "      <td>338</td>\n",
       "      <td>43.91</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.129911</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.910000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.017751</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>7.697563</td>\n",
       "      <td>0.136643</td>\n",
       "      <td>0.045548</td>\n",
       "      <td>0.091095</td>\n",
       "      <td>0.660309</td>\n",
       "      <td>0.316781</td>\n",
       "      <td>3.475505</td>\n",
       "      <td>0.123218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>3</td>\n",
       "      <td>580</td>\n",
       "      <td>88.44</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.152483</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.440000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.010345</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>6.558118</td>\n",
       "      <td>0.113071</td>\n",
       "      <td>0.067843</td>\n",
       "      <td>0.056536</td>\n",
       "      <td>1.060829</td>\n",
       "      <td>-0.069564</td>\n",
       "      <td>3.626117</td>\n",
       "      <td>0.108674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>6</td>\n",
       "      <td>314</td>\n",
       "      <td>77.71</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.070064</td>\n",
       "      <td>0.247484</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.532273</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>0.031847</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>4.040664</td>\n",
       "      <td>0.064342</td>\n",
       "      <td>0.128684</td>\n",
       "      <td>0.064342</td>\n",
       "      <td>0.409609</td>\n",
       "      <td>0.490560</td>\n",
       "      <td>1.897657</td>\n",
       "      <td>2.218347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2</td>\n",
       "      <td>1625</td>\n",
       "      <td>72.28</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004923</td>\n",
       "      <td>0.044480</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.035000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.005538</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>22.482014</td>\n",
       "      <td>0.124516</td>\n",
       "      <td>0.027670</td>\n",
       "      <td>0.027670</td>\n",
       "      <td>-0.010886</td>\n",
       "      <td>-0.293898</td>\n",
       "      <td>-0.526151</td>\n",
       "      <td>-1.437430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>4</td>\n",
       "      <td>1225</td>\n",
       "      <td>27.98</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013061</td>\n",
       "      <td>0.022841</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.748750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>43.781272</td>\n",
       "      <td>0.285919</td>\n",
       "      <td>0.357398</td>\n",
       "      <td>0.035740</td>\n",
       "      <td>-0.256314</td>\n",
       "      <td>0.108177</td>\n",
       "      <td>-1.739169</td>\n",
       "      <td>0.482357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>666</td>\n",
       "      <td>97.66</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034535</td>\n",
       "      <td>0.146637</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.246087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.006006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006006</td>\n",
       "      <td>6.819578</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.143085</td>\n",
       "      <td>-0.518497</td>\n",
       "      <td>1.288936</td>\n",
       "      <td>-2.446570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>405</td>\n",
       "      <td>62.14</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.153432</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.142667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.517541</td>\n",
       "      <td>0.032185</td>\n",
       "      <td>0.128742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.462443</td>\n",
       "      <td>-0.491470</td>\n",
       "      <td>-2.103142</td>\n",
       "      <td>-1.925902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>403</td>\n",
       "      <td>38.54</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027295</td>\n",
       "      <td>0.095633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.503636</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019851</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>10.456668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207577</td>\n",
       "      <td>0.025947</td>\n",
       "      <td>-0.250854</td>\n",
       "      <td>-0.091152</td>\n",
       "      <td>-1.569144</td>\n",
       "      <td>-0.455662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>527</td>\n",
       "      <td>79.43</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.150721</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39.715000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018975</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>6.634773</td>\n",
       "      <td>0.125897</td>\n",
       "      <td>0.075538</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>0.211066</td>\n",
       "      <td>0.323883</td>\n",
       "      <td>-0.321653</td>\n",
       "      <td>0.723815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>652</td>\n",
       "      <td>64.01</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.802000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>10.185908</td>\n",
       "      <td>0.093735</td>\n",
       "      <td>0.031245</td>\n",
       "      <td>0.062490</td>\n",
       "      <td>0.397474</td>\n",
       "      <td>-0.496382</td>\n",
       "      <td>1.219413</td>\n",
       "      <td>-1.757077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>4</td>\n",
       "      <td>1740</td>\n",
       "      <td>83.60</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.004023</td>\n",
       "      <td>0.048046</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.942857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.004023</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>20.813397</td>\n",
       "      <td>0.011962</td>\n",
       "      <td>0.083732</td>\n",
       "      <td>0.035885</td>\n",
       "      <td>0.190354</td>\n",
       "      <td>0.108247</td>\n",
       "      <td>0.396245</td>\n",
       "      <td>0.433069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2</td>\n",
       "      <td>1794</td>\n",
       "      <td>29.34</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.011148</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.467000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>61.145194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306748</td>\n",
       "      <td>0.136333</td>\n",
       "      <td>0.199407</td>\n",
       "      <td>-0.306850</td>\n",
       "      <td>1.385703</td>\n",
       "      <td>-1.713234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>3</td>\n",
       "      <td>1077</td>\n",
       "      <td>86.33</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.080158</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.776667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005571</td>\n",
       "      <td>0.009285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.475385</td>\n",
       "      <td>0.069501</td>\n",
       "      <td>0.115835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.227794</td>\n",
       "      <td>-0.071145</td>\n",
       "      <td>-1.494627</td>\n",
       "      <td>-0.318647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2</td>\n",
       "      <td>1258</td>\n",
       "      <td>36.61</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.029102</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.152500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.006359</td>\n",
       "      <td>0.003975</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>34.362196</td>\n",
       "      <td>0.218520</td>\n",
       "      <td>0.136575</td>\n",
       "      <td>0.081945</td>\n",
       "      <td>0.258578</td>\n",
       "      <td>-0.288120</td>\n",
       "      <td>0.387946</td>\n",
       "      <td>-1.289043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>6</td>\n",
       "      <td>1377</td>\n",
       "      <td>47.45</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.034459</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.450000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>29.020021</td>\n",
       "      <td>0.063224</td>\n",
       "      <td>0.126449</td>\n",
       "      <td>0.105374</td>\n",
       "      <td>0.915379</td>\n",
       "      <td>0.520606</td>\n",
       "      <td>3.539910</td>\n",
       "      <td>0.124749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>3</td>\n",
       "      <td>1788</td>\n",
       "      <td>24.98</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.012304</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.135455</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>71.577262</td>\n",
       "      <td>0.360288</td>\n",
       "      <td>0.080064</td>\n",
       "      <td>0.120096</td>\n",
       "      <td>-0.013854</td>\n",
       "      <td>-0.112010</td>\n",
       "      <td>0.345096</td>\n",
       "      <td>-0.550858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2</td>\n",
       "      <td>1162</td>\n",
       "      <td>37.26</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.032065</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.452000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>31.186259</td>\n",
       "      <td>0.268384</td>\n",
       "      <td>0.161031</td>\n",
       "      <td>0.053677</td>\n",
       "      <td>0.066070</td>\n",
       "      <td>-0.285305</td>\n",
       "      <td>-0.495923</td>\n",
       "      <td>-1.313870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>4</td>\n",
       "      <td>979</td>\n",
       "      <td>8.86</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027579</td>\n",
       "      <td>0.009050</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.328148</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>110.496614</td>\n",
       "      <td>0.902935</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.225734</td>\n",
       "      <td>-0.254634</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>-0.588204</td>\n",
       "      <td>0.536142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1</td>\n",
       "      <td>587</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022147</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.294615</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>153.263708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.349869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.463786</td>\n",
       "      <td>-0.489571</td>\n",
       "      <td>-2.020708</td>\n",
       "      <td>-1.864661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>5</td>\n",
       "      <td>1777</td>\n",
       "      <td>67.23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.037833</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.410000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>26.431653</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>-0.174948</td>\n",
       "      <td>0.310909</td>\n",
       "      <td>-1.237590</td>\n",
       "      <td>1.276763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>1685</td>\n",
       "      <td>62.39</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017804</td>\n",
       "      <td>0.037027</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.079667</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>27.007533</td>\n",
       "      <td>0.048085</td>\n",
       "      <td>0.112197</td>\n",
       "      <td>0.032056</td>\n",
       "      <td>-0.266381</td>\n",
       "      <td>-0.512390</td>\n",
       "      <td>-0.639070</td>\n",
       "      <td>-2.522591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2</td>\n",
       "      <td>1246</td>\n",
       "      <td>2.33</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.110952</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>534.763948</td>\n",
       "      <td>3.862661</td>\n",
       "      <td>0.858369</td>\n",
       "      <td>0.429185</td>\n",
       "      <td>-0.376011</td>\n",
       "      <td>-0.305903</td>\n",
       "      <td>-1.770405</td>\n",
       "      <td>-1.724881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>6</td>\n",
       "      <td>303</td>\n",
       "      <td>54.13</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.178647</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.255417</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.026403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.597635</td>\n",
       "      <td>0.036948</td>\n",
       "      <td>0.147792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.565796</td>\n",
       "      <td>0.498713</td>\n",
       "      <td>-2.246209</td>\n",
       "      <td>2.142398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2</td>\n",
       "      <td>1095</td>\n",
       "      <td>9.44</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015525</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.555294</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.010046</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>115.995763</td>\n",
       "      <td>1.165254</td>\n",
       "      <td>0.635593</td>\n",
       "      <td>0.105932</td>\n",
       "      <td>-0.288443</td>\n",
       "      <td>-0.296690</td>\n",
       "      <td>-1.678647</td>\n",
       "      <td>-1.623752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>51.57</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>6.446250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.785000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.317583</td>\n",
       "      <td>-0.479721</td>\n",
       "      <td>-1.649683</td>\n",
       "      <td>-1.441282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>3</td>\n",
       "      <td>1125</td>\n",
       "      <td>64.29</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.057147</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.430000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007111</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.498833</td>\n",
       "      <td>0.124436</td>\n",
       "      <td>0.139991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.236115</td>\n",
       "      <td>-0.072329</td>\n",
       "      <td>-1.509923</td>\n",
       "      <td>-0.325212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2</td>\n",
       "      <td>1394</td>\n",
       "      <td>86.22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.061851</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.071765</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>16.167942</td>\n",
       "      <td>0.011598</td>\n",
       "      <td>0.023196</td>\n",
       "      <td>0.011598</td>\n",
       "      <td>-0.344384</td>\n",
       "      <td>-0.302984</td>\n",
       "      <td>-1.705218</td>\n",
       "      <td>-1.615325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>3</td>\n",
       "      <td>478</td>\n",
       "      <td>4.48</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.039749</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.235789</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.023013</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>106.696429</td>\n",
       "      <td>2.455357</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>-0.114081</td>\n",
       "      <td>-0.099461</td>\n",
       "      <td>-0.564797</td>\n",
       "      <td>-0.530682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>6</td>\n",
       "      <td>1746</td>\n",
       "      <td>68.99</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.039513</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.832778</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>25.308016</td>\n",
       "      <td>0.028990</td>\n",
       "      <td>0.101464</td>\n",
       "      <td>0.028990</td>\n",
       "      <td>-0.136049</td>\n",
       "      <td>0.498734</td>\n",
       "      <td>-0.591832</td>\n",
       "      <td>2.384673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>3</td>\n",
       "      <td>1700</td>\n",
       "      <td>21.46</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.012624</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.682500</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>79.217148</td>\n",
       "      <td>0.139795</td>\n",
       "      <td>0.465983</td>\n",
       "      <td>0.232992</td>\n",
       "      <td>0.558587</td>\n",
       "      <td>-0.094934</td>\n",
       "      <td>1.831263</td>\n",
       "      <td>-0.519757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6</td>\n",
       "      <td>961</td>\n",
       "      <td>58.21</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019771</td>\n",
       "      <td>0.060572</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.063684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.509191</td>\n",
       "      <td>0.103075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.548874</td>\n",
       "      <td>0.495751</td>\n",
       "      <td>-2.238045</td>\n",
       "      <td>2.114063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>3</td>\n",
       "      <td>352</td>\n",
       "      <td>57.85</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008523</td>\n",
       "      <td>0.164347</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.283333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.084702</td>\n",
       "      <td>0.034572</td>\n",
       "      <td>0.069144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.336361</td>\n",
       "      <td>-0.082467</td>\n",
       "      <td>-1.916044</td>\n",
       "      <td>-0.345205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2</td>\n",
       "      <td>1154</td>\n",
       "      <td>26.67</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017331</td>\n",
       "      <td>0.023111</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.333500</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>0.003466</td>\n",
       "      <td>43.269591</td>\n",
       "      <td>0.262467</td>\n",
       "      <td>0.337458</td>\n",
       "      <td>0.149981</td>\n",
       "      <td>0.235012</td>\n",
       "      <td>-0.305194</td>\n",
       "      <td>1.454368</td>\n",
       "      <td>-1.824861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>3</td>\n",
       "      <td>470</td>\n",
       "      <td>8.25</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.017553</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.014894</td>\n",
       "      <td>0.014894</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>56.969697</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.042272</td>\n",
       "      <td>-0.086112</td>\n",
       "      <td>-0.470571</td>\n",
       "      <td>-0.417043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2</td>\n",
       "      <td>618</td>\n",
       "      <td>90.66</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021036</td>\n",
       "      <td>0.146699</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.973846</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.016181</td>\n",
       "      <td>0.008091</td>\n",
       "      <td>6.816678</td>\n",
       "      <td>0.022060</td>\n",
       "      <td>0.110302</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>0.508359</td>\n",
       "      <td>-0.300070</td>\n",
       "      <td>1.886692</td>\n",
       "      <td>-1.461049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>4</td>\n",
       "      <td>301</td>\n",
       "      <td>43.08</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036545</td>\n",
       "      <td>0.143123</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.916364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.013289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>6.987001</td>\n",
       "      <td>0.092851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>-0.289943</td>\n",
       "      <td>0.100431</td>\n",
       "      <td>-1.631992</td>\n",
       "      <td>0.475209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>5</td>\n",
       "      <td>1607</td>\n",
       "      <td>11.70</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.007281</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.925000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>137.350427</td>\n",
       "      <td>0.598291</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.328511</td>\n",
       "      <td>0.319326</td>\n",
       "      <td>-1.799030</td>\n",
       "      <td>1.136609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1</td>\n",
       "      <td>488</td>\n",
       "      <td>35.97</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014344</td>\n",
       "      <td>0.073709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.138571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.566861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.429844</td>\n",
       "      <td>-0.490044</td>\n",
       "      <td>-1.923349</td>\n",
       "      <td>-1.716013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>3</td>\n",
       "      <td>1466</td>\n",
       "      <td>18.47</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017735</td>\n",
       "      <td>0.012599</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.710385</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>79.371955</td>\n",
       "      <td>0.054142</td>\n",
       "      <td>0.378993</td>\n",
       "      <td>0.108284</td>\n",
       "      <td>-0.246775</td>\n",
       "      <td>-0.109779</td>\n",
       "      <td>-0.642605</td>\n",
       "      <td>-0.511483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4</td>\n",
       "      <td>1040</td>\n",
       "      <td>65.36</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020192</td>\n",
       "      <td>0.062846</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.112381</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.911873</td>\n",
       "      <td>0.152999</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.525571</td>\n",
       "      <td>0.098653</td>\n",
       "      <td>-2.509734</td>\n",
       "      <td>0.494890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>6</td>\n",
       "      <td>455</td>\n",
       "      <td>42.89</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.094264</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.299231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.017582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>10.608533</td>\n",
       "      <td>0.186524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069946</td>\n",
       "      <td>0.076987</td>\n",
       "      <td>0.493971</td>\n",
       "      <td>0.377488</td>\n",
       "      <td>2.139227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>4</td>\n",
       "      <td>1695</td>\n",
       "      <td>8.71</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.005139</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.580667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>194.603904</td>\n",
       "      <td>1.033295</td>\n",
       "      <td>0.229621</td>\n",
       "      <td>0.229621</td>\n",
       "      <td>-0.123339</td>\n",
       "      <td>0.097346</td>\n",
       "      <td>-0.585983</td>\n",
       "      <td>0.502683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>4</td>\n",
       "      <td>518</td>\n",
       "      <td>35.77</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.042471</td>\n",
       "      <td>0.069054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.625909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>14.481409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055913</td>\n",
       "      <td>0.055913</td>\n",
       "      <td>-0.238255</td>\n",
       "      <td>0.087977</td>\n",
       "      <td>-0.624167</td>\n",
       "      <td>0.530130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6</td>\n",
       "      <td>1183</td>\n",
       "      <td>55.17</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015216</td>\n",
       "      <td>0.046636</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.065000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>21.442813</td>\n",
       "      <td>0.163132</td>\n",
       "      <td>0.163132</td>\n",
       "      <td>0.054377</td>\n",
       "      <td>0.094516</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.372490</td>\n",
       "      <td>2.355158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>3</td>\n",
       "      <td>1230</td>\n",
       "      <td>80.65</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.065569</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.688333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>15.251085</td>\n",
       "      <td>0.123993</td>\n",
       "      <td>0.012399</td>\n",
       "      <td>0.037198</td>\n",
       "      <td>-0.086448</td>\n",
       "      <td>-0.120337</td>\n",
       "      <td>0.342721</td>\n",
       "      <td>-0.582871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>6</td>\n",
       "      <td>571</td>\n",
       "      <td>14.13</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.026270</td>\n",
       "      <td>0.024746</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.010508</td>\n",
       "      <td>0.014011</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>40.410474</td>\n",
       "      <td>0.424628</td>\n",
       "      <td>0.566171</td>\n",
       "      <td>0.353857</td>\n",
       "      <td>0.461146</td>\n",
       "      <td>0.494981</td>\n",
       "      <td>1.952633</td>\n",
       "      <td>2.082048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2</td>\n",
       "      <td>252</td>\n",
       "      <td>26.07</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.103452</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.738000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>9.666283</td>\n",
       "      <td>0.191791</td>\n",
       "      <td>0.345224</td>\n",
       "      <td>0.076717</td>\n",
       "      <td>-0.083666</td>\n",
       "      <td>-0.295489</td>\n",
       "      <td>-0.549774</td>\n",
       "      <td>-1.630075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1</td>\n",
       "      <td>806</td>\n",
       "      <td>37.38</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>0.046377</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.340000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>21.562333</td>\n",
       "      <td>0.133761</td>\n",
       "      <td>0.133761</td>\n",
       "      <td>0.080257</td>\n",
       "      <td>0.178951</td>\n",
       "      <td>-0.493633</td>\n",
       "      <td>0.390586</td>\n",
       "      <td>-1.946326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1</td>\n",
       "      <td>1798</td>\n",
       "      <td>4.14</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>434.299517</td>\n",
       "      <td>0.966184</td>\n",
       "      <td>1.932367</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.114780</td>\n",
       "      <td>-0.496235</td>\n",
       "      <td>0.379510</td>\n",
       "      <td>-2.147009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1</td>\n",
       "      <td>583</td>\n",
       "      <td>86.07</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.029160</td>\n",
       "      <td>0.147633</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.062941</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.015437</td>\n",
       "      <td>0.015437</td>\n",
       "      <td>0.008576</td>\n",
       "      <td>6.773556</td>\n",
       "      <td>0.104566</td>\n",
       "      <td>0.104566</td>\n",
       "      <td>0.058092</td>\n",
       "      <td>0.485475</td>\n",
       "      <td>-0.503528</td>\n",
       "      <td>1.881517</td>\n",
       "      <td>-2.084089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>5</td>\n",
       "      <td>735</td>\n",
       "      <td>48.40</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010884</td>\n",
       "      <td>0.065850</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.050000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.005442</td>\n",
       "      <td>15.185950</td>\n",
       "      <td>0.041322</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.353275</td>\n",
       "      <td>0.303670</td>\n",
       "      <td>1.336312</td>\n",
       "      <td>1.440252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6</td>\n",
       "      <td>926</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.026998</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>228.641975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.481481</td>\n",
       "      <td>1.234568</td>\n",
       "      <td>0.295811</td>\n",
       "      <td>0.480973</td>\n",
       "      <td>1.946658</td>\n",
       "      <td>2.329079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>3</td>\n",
       "      <td>1789</td>\n",
       "      <td>46.50</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.025992</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>38.473118</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.289340</td>\n",
       "      <td>-0.090152</td>\n",
       "      <td>0.546109</td>\n",
       "      <td>-0.407515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>5</td>\n",
       "      <td>1387</td>\n",
       "      <td>58.57</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.042228</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.570000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>23.681065</td>\n",
       "      <td>0.051221</td>\n",
       "      <td>0.119515</td>\n",
       "      <td>0.017074</td>\n",
       "      <td>0.047770</td>\n",
       "      <td>0.330549</td>\n",
       "      <td>3.250104</td>\n",
       "      <td>0.146613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3</td>\n",
       "      <td>252</td>\n",
       "      <td>72.77</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.128333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.043651</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>3.462966</td>\n",
       "      <td>0.151161</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>0.068710</td>\n",
       "      <td>0.591178</td>\n",
       "      <td>-0.100129</td>\n",
       "      <td>1.660981</td>\n",
       "      <td>-0.425891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6</td>\n",
       "      <td>133</td>\n",
       "      <td>13.79</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.103684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.644670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.600899</td>\n",
       "      <td>0.493726</td>\n",
       "      <td>-2.185368</td>\n",
       "      <td>2.021419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>5</td>\n",
       "      <td>1784</td>\n",
       "      <td>11.42</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.012892</td>\n",
       "      <td>0.006401</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.496522</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>156.217163</td>\n",
       "      <td>0.350263</td>\n",
       "      <td>0.437828</td>\n",
       "      <td>0.350263</td>\n",
       "      <td>0.149957</td>\n",
       "      <td>0.286002</td>\n",
       "      <td>1.517198</td>\n",
       "      <td>1.945273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2</td>\n",
       "      <td>392</td>\n",
       "      <td>20.86</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.053214</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.738333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.022959</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>18.791946</td>\n",
       "      <td>0.431448</td>\n",
       "      <td>0.239693</td>\n",
       "      <td>0.095877</td>\n",
       "      <td>-0.058865</td>\n",
       "      <td>-0.295941</td>\n",
       "      <td>-0.519896</td>\n",
       "      <td>-1.583911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6</td>\n",
       "      <td>1400</td>\n",
       "      <td>61.12</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.043657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.560000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.905759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.276335</td>\n",
       "      <td>0.525029</td>\n",
       "      <td>-1.310742</td>\n",
       "      <td>1.230293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>4</td>\n",
       "      <td>486</td>\n",
       "      <td>94.09</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.026749</td>\n",
       "      <td>0.193601</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.237692</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.022634</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>5.165267</td>\n",
       "      <td>0.116909</td>\n",
       "      <td>0.095653</td>\n",
       "      <td>0.053141</td>\n",
       "      <td>0.550154</td>\n",
       "      <td>0.101063</td>\n",
       "      <td>1.911370</td>\n",
       "      <td>0.481390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>82.36</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>0.232655</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.118000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>4.298203</td>\n",
       "      <td>0.048567</td>\n",
       "      <td>0.036425</td>\n",
       "      <td>0.048567</td>\n",
       "      <td>0.194579</td>\n",
       "      <td>-0.512313</td>\n",
       "      <td>1.333194</td>\n",
       "      <td>-2.401191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>2</td>\n",
       "      <td>1077</td>\n",
       "      <td>85.23</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022284</td>\n",
       "      <td>0.079136</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.551250</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.636396</td>\n",
       "      <td>0.011733</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.560821</td>\n",
       "      <td>-0.300450</td>\n",
       "      <td>-2.395571</td>\n",
       "      <td>-1.588696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>6</td>\n",
       "      <td>986</td>\n",
       "      <td>38.30</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011156</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.481818</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>0.004057</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>25.744125</td>\n",
       "      <td>0.052219</td>\n",
       "      <td>0.104439</td>\n",
       "      <td>0.052219</td>\n",
       "      <td>-0.087083</td>\n",
       "      <td>0.501789</td>\n",
       "      <td>-0.515210</td>\n",
       "      <td>2.188878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>4</td>\n",
       "      <td>883</td>\n",
       "      <td>71.85</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>0.081370</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.012458</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>12.289492</td>\n",
       "      <td>0.153097</td>\n",
       "      <td>0.125261</td>\n",
       "      <td>0.027836</td>\n",
       "      <td>0.042803</td>\n",
       "      <td>0.113504</td>\n",
       "      <td>-0.527827</td>\n",
       "      <td>0.491065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>3</td>\n",
       "      <td>1064</td>\n",
       "      <td>13.03</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022556</td>\n",
       "      <td>0.012246</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.542917</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.657713</td>\n",
       "      <td>0.844206</td>\n",
       "      <td>0.460476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.548807</td>\n",
       "      <td>-0.101111</td>\n",
       "      <td>-2.508782</td>\n",
       "      <td>-0.502444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>4</td>\n",
       "      <td>1387</td>\n",
       "      <td>45.70</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.019466</td>\n",
       "      <td>0.032949</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.692593</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>30.350109</td>\n",
       "      <td>0.087527</td>\n",
       "      <td>0.153173</td>\n",
       "      <td>0.087527</td>\n",
       "      <td>0.131617</td>\n",
       "      <td>0.084744</td>\n",
       "      <td>1.619102</td>\n",
       "      <td>0.531306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>6</td>\n",
       "      <td>701</td>\n",
       "      <td>97.76</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.037090</td>\n",
       "      <td>0.139458</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.760000</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.012839</td>\n",
       "      <td>0.004280</td>\n",
       "      <td>7.170622</td>\n",
       "      <td>0.040917</td>\n",
       "      <td>0.092062</td>\n",
       "      <td>0.030687</td>\n",
       "      <td>-0.007980</td>\n",
       "      <td>0.490875</td>\n",
       "      <td>0.332471</td>\n",
       "      <td>2.474133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>4</td>\n",
       "      <td>1649</td>\n",
       "      <td>88.15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.011522</td>\n",
       "      <td>0.053457</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.639474</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.004245</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>18.706750</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>0.079410</td>\n",
       "      <td>0.034033</td>\n",
       "      <td>0.037613</td>\n",
       "      <td>0.095235</td>\n",
       "      <td>0.363114</td>\n",
       "      <td>0.500594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>6</td>\n",
       "      <td>1074</td>\n",
       "      <td>24.42</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023277</td>\n",
       "      <td>0.022737</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.976800</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.980344</td>\n",
       "      <td>0.368550</td>\n",
       "      <td>0.245700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.566490</td>\n",
       "      <td>0.497118</td>\n",
       "      <td>-2.260485</td>\n",
       "      <td>2.198636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>68.76</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.892987</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>1.119837</td>\n",
       "      <td>0.159977</td>\n",
       "      <td>0.145433</td>\n",
       "      <td>0.058173</td>\n",
       "      <td>0.302100</td>\n",
       "      <td>-0.500545</td>\n",
       "      <td>1.288634</td>\n",
       "      <td>-2.168150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1</td>\n",
       "      <td>1598</td>\n",
       "      <td>16.47</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018773</td>\n",
       "      <td>0.010307</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.024894</td>\n",
       "      <td>0.364299</td>\n",
       "      <td>0.060716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.674898</td>\n",
       "      <td>-0.513560</td>\n",
       "      <td>-2.257268</td>\n",
       "      <td>-2.174318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>29.44</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.840000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.944000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.305707</td>\n",
       "      <td>0.033967</td>\n",
       "      <td>0.169837</td>\n",
       "      <td>0.495087</td>\n",
       "      <td>-0.506332</td>\n",
       "      <td>1.784030</td>\n",
       "      <td>-1.787131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1</td>\n",
       "      <td>1307</td>\n",
       "      <td>36.94</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005356</td>\n",
       "      <td>0.028263</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.277143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>35.381700</td>\n",
       "      <td>0.108284</td>\n",
       "      <td>0.027071</td>\n",
       "      <td>0.108284</td>\n",
       "      <td>0.326265</td>\n",
       "      <td>-0.501366</td>\n",
       "      <td>1.263510</td>\n",
       "      <td>-1.938844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2</td>\n",
       "      <td>1437</td>\n",
       "      <td>49.61</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>0.034523</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.611053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>28.965934</td>\n",
       "      <td>0.040314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080629</td>\n",
       "      <td>0.161229</td>\n",
       "      <td>-0.315550</td>\n",
       "      <td>1.420020</td>\n",
       "      <td>-1.908916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>5</td>\n",
       "      <td>872</td>\n",
       "      <td>18.26</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010321</td>\n",
       "      <td>0.020940</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.028889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.008028</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>47.754655</td>\n",
       "      <td>0.164294</td>\n",
       "      <td>0.383352</td>\n",
       "      <td>0.109529</td>\n",
       "      <td>-0.039924</td>\n",
       "      <td>0.307610</td>\n",
       "      <td>-0.502431</td>\n",
       "      <td>1.561197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>6</td>\n",
       "      <td>498</td>\n",
       "      <td>56.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014056</td>\n",
       "      <td>0.114116</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.118571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.762977</td>\n",
       "      <td>0.017596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.440682</td>\n",
       "      <td>0.506328</td>\n",
       "      <td>-1.885966</td>\n",
       "      <td>1.746130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>5</td>\n",
       "      <td>174</td>\n",
       "      <td>21.04</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.120920</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.337778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.028736</td>\n",
       "      <td>8.269962</td>\n",
       "      <td>0.285171</td>\n",
       "      <td>0.285171</td>\n",
       "      <td>0.237643</td>\n",
       "      <td>0.528609</td>\n",
       "      <td>0.299278</td>\n",
       "      <td>1.921933</td>\n",
       "      <td>1.361451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>3</td>\n",
       "      <td>1273</td>\n",
       "      <td>4.72</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013354</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.277647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>269.703390</td>\n",
       "      <td>1.483051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.059322</td>\n",
       "      <td>0.378371</td>\n",
       "      <td>-0.115637</td>\n",
       "      <td>2.115145</td>\n",
       "      <td>-0.579228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>6</td>\n",
       "      <td>1019</td>\n",
       "      <td>63.42</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012758</td>\n",
       "      <td>0.062237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.878462</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007851</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>16.067487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126143</td>\n",
       "      <td>0.015768</td>\n",
       "      <td>-0.266321</td>\n",
       "      <td>0.506886</td>\n",
       "      <td>-1.476022</td>\n",
       "      <td>2.055631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>5</td>\n",
       "      <td>1706</td>\n",
       "      <td>49.16</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016999</td>\n",
       "      <td>0.028816</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.695172</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>34.703011</td>\n",
       "      <td>0.223759</td>\n",
       "      <td>0.101709</td>\n",
       "      <td>0.020342</td>\n",
       "      <td>-0.417305</td>\n",
       "      <td>0.290406</td>\n",
       "      <td>-1.838035</td>\n",
       "      <td>1.807709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2</td>\n",
       "      <td>475</td>\n",
       "      <td>34.30</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056842</td>\n",
       "      <td>0.072211</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.270370</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016842</td>\n",
       "      <td>0.012632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.848397</td>\n",
       "      <td>0.233236</td>\n",
       "      <td>0.174927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.589245</td>\n",
       "      <td>-0.304641</td>\n",
       "      <td>-2.434409</td>\n",
       "      <td>-1.585167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2</td>\n",
       "      <td>934</td>\n",
       "      <td>56.45</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016060</td>\n",
       "      <td>0.060439</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.763333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>16.545616</td>\n",
       "      <td>0.159433</td>\n",
       "      <td>0.177148</td>\n",
       "      <td>0.035430</td>\n",
       "      <td>-0.044783</td>\n",
       "      <td>-0.292619</td>\n",
       "      <td>-0.550653</td>\n",
       "      <td>-1.667654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>6</td>\n",
       "      <td>605</td>\n",
       "      <td>91.49</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039669</td>\n",
       "      <td>0.151223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.812083</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.612745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.593271</td>\n",
       "      <td>0.494126</td>\n",
       "      <td>-2.242729</td>\n",
       "      <td>2.116446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>5</td>\n",
       "      <td>1001</td>\n",
       "      <td>17.92</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018981</td>\n",
       "      <td>0.017902</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.943158</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>55.859375</td>\n",
       "      <td>0.279018</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.167411</td>\n",
       "      <td>0.012420</td>\n",
       "      <td>0.291662</td>\n",
       "      <td>0.363158</td>\n",
       "      <td>1.954131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2</td>\n",
       "      <td>1621</td>\n",
       "      <td>33.91</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017273</td>\n",
       "      <td>0.020919</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.211071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.803008</td>\n",
       "      <td>0.235919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.645039</td>\n",
       "      <td>-0.312165</td>\n",
       "      <td>-2.441574</td>\n",
       "      <td>-1.590580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1</td>\n",
       "      <td>1197</td>\n",
       "      <td>11.15</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010860</td>\n",
       "      <td>0.009315</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.857692</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>107.354260</td>\n",
       "      <td>0.089686</td>\n",
       "      <td>0.807175</td>\n",
       "      <td>0.269058</td>\n",
       "      <td>0.097118</td>\n",
       "      <td>-0.497028</td>\n",
       "      <td>0.380270</td>\n",
       "      <td>-2.195638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>33.51</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.193333</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.155517</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>4.476276</td>\n",
       "      <td>0.238735</td>\n",
       "      <td>0.089526</td>\n",
       "      <td>0.029842</td>\n",
       "      <td>-0.450280</td>\n",
       "      <td>-0.312841</td>\n",
       "      <td>-1.811161</td>\n",
       "      <td>-1.732991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>36.88</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.024587</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.048889</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>40.672451</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.162690</td>\n",
       "      <td>0.108460</td>\n",
       "      <td>0.207546</td>\n",
       "      <td>0.491452</td>\n",
       "      <td>1.395871</td>\n",
       "      <td>2.374583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2</td>\n",
       "      <td>817</td>\n",
       "      <td>24.61</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012240</td>\n",
       "      <td>0.030122</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.461000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.013464</td>\n",
       "      <td>0.012240</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>33.197887</td>\n",
       "      <td>0.446973</td>\n",
       "      <td>0.406339</td>\n",
       "      <td>0.081268</td>\n",
       "      <td>0.018372</td>\n",
       "      <td>-0.287020</td>\n",
       "      <td>-0.559148</td>\n",
       "      <td>-1.401861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2</td>\n",
       "      <td>1525</td>\n",
       "      <td>1.98</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004590</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>770.202020</td>\n",
       "      <td>3.535354</td>\n",
       "      <td>1.515152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.316800</td>\n",
       "      <td>-0.280733</td>\n",
       "      <td>-1.658054</td>\n",
       "      <td>-1.095683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>5</td>\n",
       "      <td>478</td>\n",
       "      <td>84.37</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.039749</td>\n",
       "      <td>0.176506</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.440526</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.010460</td>\n",
       "      <td>0.010460</td>\n",
       "      <td>5.665521</td>\n",
       "      <td>0.071115</td>\n",
       "      <td>0.059263</td>\n",
       "      <td>0.059263</td>\n",
       "      <td>0.414915</td>\n",
       "      <td>0.288380</td>\n",
       "      <td>2.070851</td>\n",
       "      <td>1.650042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>5</td>\n",
       "      <td>972</td>\n",
       "      <td>71.76</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.073827</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.352000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.008230</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>13.545151</td>\n",
       "      <td>0.083612</td>\n",
       "      <td>0.111483</td>\n",
       "      <td>0.041806</td>\n",
       "      <td>0.264242</td>\n",
       "      <td>0.313588</td>\n",
       "      <td>0.379195</td>\n",
       "      <td>1.312437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>3</td>\n",
       "      <td>509</td>\n",
       "      <td>21.03</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035363</td>\n",
       "      <td>0.041316</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.168333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.203519</td>\n",
       "      <td>0.047551</td>\n",
       "      <td>0.142653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.553342</td>\n",
       "      <td>-0.101330</td>\n",
       "      <td>-2.473010</td>\n",
       "      <td>-0.486504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>5</td>\n",
       "      <td>84</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.060833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.425833</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>16.438356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.782779</td>\n",
       "      <td>0.195695</td>\n",
       "      <td>-0.306115</td>\n",
       "      <td>0.302235</td>\n",
       "      <td>-1.567058</td>\n",
       "      <td>1.470711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>4</td>\n",
       "      <td>373</td>\n",
       "      <td>36.81</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072386</td>\n",
       "      <td>0.098686</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.363333</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010724</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.133116</td>\n",
       "      <td>0.108666</td>\n",
       "      <td>0.135833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.615962</td>\n",
       "      <td>0.092926</td>\n",
       "      <td>-2.577707</td>\n",
       "      <td>0.498381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>3</td>\n",
       "      <td>1315</td>\n",
       "      <td>64.80</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.049278</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.005323</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>20.293210</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.108025</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.437612</td>\n",
       "      <td>-0.090601</td>\n",
       "      <td>1.287018</td>\n",
       "      <td>-0.483106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>3</td>\n",
       "      <td>760</td>\n",
       "      <td>32.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.011842</td>\n",
       "      <td>0.043289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.655556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>23.100304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121581</td>\n",
       "      <td>0.264727</td>\n",
       "      <td>-0.106542</td>\n",
       "      <td>1.449901</td>\n",
       "      <td>-0.521575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>5</td>\n",
       "      <td>1808</td>\n",
       "      <td>44.36</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.013827</td>\n",
       "      <td>0.024535</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.774400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>40.757439</td>\n",
       "      <td>0.112714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067628</td>\n",
       "      <td>-0.076099</td>\n",
       "      <td>0.281653</td>\n",
       "      <td>0.394807</td>\n",
       "      <td>1.997327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2</td>\n",
       "      <td>1689</td>\n",
       "      <td>23.54</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.362857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>71.750212</td>\n",
       "      <td>0.424809</td>\n",
       "      <td>0.297366</td>\n",
       "      <td>0.212404</td>\n",
       "      <td>0.593354</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>1.767289</td>\n",
       "      <td>-1.242308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>73.73</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>1.152031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.266429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.868032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054252</td>\n",
       "      <td>0.040689</td>\n",
       "      <td>0.064350</td>\n",
       "      <td>0.495696</td>\n",
       "      <td>0.348556</td>\n",
       "      <td>2.204473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>5</td>\n",
       "      <td>1005</td>\n",
       "      <td>45.66</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.026866</td>\n",
       "      <td>0.045433</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.691111</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>22.010512</td>\n",
       "      <td>0.087604</td>\n",
       "      <td>0.043802</td>\n",
       "      <td>0.087604</td>\n",
       "      <td>0.095280</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>1.511743</td>\n",
       "      <td>1.986378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>3</td>\n",
       "      <td>993</td>\n",
       "      <td>57.11</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020141</td>\n",
       "      <td>0.057513</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.855500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010070</td>\n",
       "      <td>0.010070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.387498</td>\n",
       "      <td>0.175101</td>\n",
       "      <td>0.175101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.465014</td>\n",
       "      <td>-0.092257</td>\n",
       "      <td>-2.405918</td>\n",
       "      <td>-0.470611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>35.59</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.593167</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.368846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.685867</td>\n",
       "      <td>0.280978</td>\n",
       "      <td>0.056196</td>\n",
       "      <td>0.112391</td>\n",
       "      <td>0.134467</td>\n",
       "      <td>-0.318643</td>\n",
       "      <td>1.377448</td>\n",
       "      <td>-1.868006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>6</td>\n",
       "      <td>679</td>\n",
       "      <td>86.06</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.019146</td>\n",
       "      <td>0.126745</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.620000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.011782</td>\n",
       "      <td>0.011782</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>7.889844</td>\n",
       "      <td>0.092958</td>\n",
       "      <td>0.092958</td>\n",
       "      <td>0.034859</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.504049</td>\n",
       "      <td>0.374560</td>\n",
       "      <td>2.175920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2</td>\n",
       "      <td>1717</td>\n",
       "      <td>45.44</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.026465</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.672941</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>37.786092</td>\n",
       "      <td>0.088028</td>\n",
       "      <td>0.198063</td>\n",
       "      <td>0.044014</td>\n",
       "      <td>-0.106768</td>\n",
       "      <td>-0.297170</td>\n",
       "      <td>-0.578729</td>\n",
       "      <td>-1.813681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>97.26</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.097143</td>\n",
       "      <td>0.555771</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.721176</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>1.799301</td>\n",
       "      <td>0.102817</td>\n",
       "      <td>0.071972</td>\n",
       "      <td>0.010282</td>\n",
       "      <td>-0.254858</td>\n",
       "      <td>-0.494790</td>\n",
       "      <td>-1.508955</td>\n",
       "      <td>-2.094675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2</td>\n",
       "      <td>888</td>\n",
       "      <td>96.19</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>0.108322</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.687778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.231729</td>\n",
       "      <td>0.062377</td>\n",
       "      <td>0.051980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.379825</td>\n",
       "      <td>-0.286914</td>\n",
       "      <td>-2.101203</td>\n",
       "      <td>-1.326087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>4</td>\n",
       "      <td>167</td>\n",
       "      <td>30.81</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113772</td>\n",
       "      <td>0.184491</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.621579</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.059880</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>5.420318</td>\n",
       "      <td>0.324570</td>\n",
       "      <td>0.064914</td>\n",
       "      <td>0.097371</td>\n",
       "      <td>0.028507</td>\n",
       "      <td>0.090757</td>\n",
       "      <td>0.384752</td>\n",
       "      <td>0.510863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2</td>\n",
       "      <td>1314</td>\n",
       "      <td>17.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.780000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.903262</td>\n",
       "      <td>0.112486</td>\n",
       "      <td>0.056243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.341165</td>\n",
       "      <td>-0.283304</td>\n",
       "      <td>-1.684899</td>\n",
       "      <td>-1.202937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>4</td>\n",
       "      <td>849</td>\n",
       "      <td>92.81</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.109317</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.405000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011779</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>9.147721</td>\n",
       "      <td>0.107747</td>\n",
       "      <td>0.032324</td>\n",
       "      <td>0.021549</td>\n",
       "      <td>0.185274</td>\n",
       "      <td>0.120086</td>\n",
       "      <td>-0.295869</td>\n",
       "      <td>0.278374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>5</td>\n",
       "      <td>303</td>\n",
       "      <td>2.72</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.226667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.033003</td>\n",
       "      <td>0.026403</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>111.397059</td>\n",
       "      <td>3.676471</td>\n",
       "      <td>2.941176</td>\n",
       "      <td>0.367647</td>\n",
       "      <td>-0.221813</td>\n",
       "      <td>0.309872</td>\n",
       "      <td>-1.446125</td>\n",
       "      <td>1.335933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>6</td>\n",
       "      <td>186</td>\n",
       "      <td>42.15</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.226613</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.683333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.037634</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>4.412811</td>\n",
       "      <td>0.166074</td>\n",
       "      <td>0.071174</td>\n",
       "      <td>0.094899</td>\n",
       "      <td>0.332402</td>\n",
       "      <td>0.498828</td>\n",
       "      <td>1.264839</td>\n",
       "      <td>1.934676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2</td>\n",
       "      <td>1816</td>\n",
       "      <td>72.32</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014317</td>\n",
       "      <td>0.039824</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.781538</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>25.110619</td>\n",
       "      <td>0.027655</td>\n",
       "      <td>0.082965</td>\n",
       "      <td>0.069137</td>\n",
       "      <td>0.316737</td>\n",
       "      <td>-0.317950</td>\n",
       "      <td>2.086976</td>\n",
       "      <td>-1.886836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>5</td>\n",
       "      <td>471</td>\n",
       "      <td>60.59</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.048832</td>\n",
       "      <td>0.128641</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.634348</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.004246</td>\n",
       "      <td>0.008493</td>\n",
       "      <td>7.773560</td>\n",
       "      <td>0.132035</td>\n",
       "      <td>0.033009</td>\n",
       "      <td>0.066017</td>\n",
       "      <td>0.165588</td>\n",
       "      <td>0.283829</td>\n",
       "      <td>1.430841</td>\n",
       "      <td>1.856731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>5</td>\n",
       "      <td>591</td>\n",
       "      <td>11.09</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.018765</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.090000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.011844</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>53.291253</td>\n",
       "      <td>0.631199</td>\n",
       "      <td>0.090171</td>\n",
       "      <td>0.360685</td>\n",
       "      <td>0.616504</td>\n",
       "      <td>0.313159</td>\n",
       "      <td>3.442124</td>\n",
       "      <td>0.126928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>4</td>\n",
       "      <td>401</td>\n",
       "      <td>84.09</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.049875</td>\n",
       "      <td>0.209701</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.204500</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>4.768700</td>\n",
       "      <td>0.035676</td>\n",
       "      <td>0.023784</td>\n",
       "      <td>0.059460</td>\n",
       "      <td>0.365517</td>\n",
       "      <td>0.083210</td>\n",
       "      <td>2.087890</td>\n",
       "      <td>0.531684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>2</td>\n",
       "      <td>574</td>\n",
       "      <td>0.07</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015679</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8200.000000</td>\n",
       "      <td>142.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.435042</td>\n",
       "      <td>-0.293384</td>\n",
       "      <td>-1.985836</td>\n",
       "      <td>-1.258672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>6</td>\n",
       "      <td>1227</td>\n",
       "      <td>7.76</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.517333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006520</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>158.118557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.030928</td>\n",
       "      <td>0.386598</td>\n",
       "      <td>0.058595</td>\n",
       "      <td>0.498770</td>\n",
       "      <td>0.351063</td>\n",
       "      <td>2.302593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>4</td>\n",
       "      <td>605</td>\n",
       "      <td>73.28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.121124</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.328000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>8.256004</td>\n",
       "      <td>0.027293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>-0.277961</td>\n",
       "      <td>0.101422</td>\n",
       "      <td>-1.562473</td>\n",
       "      <td>0.449867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>6</td>\n",
       "      <td>877</td>\n",
       "      <td>14.63</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.030787</td>\n",
       "      <td>0.016682</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.541852</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.006842</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>59.945318</td>\n",
       "      <td>0.136705</td>\n",
       "      <td>0.410116</td>\n",
       "      <td>0.205058</td>\n",
       "      <td>-0.078370</td>\n",
       "      <td>0.485045</td>\n",
       "      <td>0.389452</td>\n",
       "      <td>2.560760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>5</td>\n",
       "      <td>1149</td>\n",
       "      <td>19.94</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013925</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.246250</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.006092</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>57.622869</td>\n",
       "      <td>0.351053</td>\n",
       "      <td>0.050150</td>\n",
       "      <td>0.200602</td>\n",
       "      <td>0.219046</td>\n",
       "      <td>0.289120</td>\n",
       "      <td>1.397904</td>\n",
       "      <td>1.686173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>76.96</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>1.509020</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.748571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.662682</td>\n",
       "      <td>0.142931</td>\n",
       "      <td>0.103950</td>\n",
       "      <td>0.038981</td>\n",
       "      <td>-0.008287</td>\n",
       "      <td>-0.310349</td>\n",
       "      <td>0.346922</td>\n",
       "      <td>-1.870742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>6</td>\n",
       "      <td>1570</td>\n",
       "      <td>81.60</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.264000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>19.240196</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.036765</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>0.156245</td>\n",
       "      <td>0.483390</td>\n",
       "      <td>1.401493</td>\n",
       "      <td>2.479143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "      <td>55.66</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.376081</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.319167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>2.659001</td>\n",
       "      <td>0.035932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071865</td>\n",
       "      <td>0.108193</td>\n",
       "      <td>-0.320773</td>\n",
       "      <td>1.394666</td>\n",
       "      <td>-1.952027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1</td>\n",
       "      <td>1037</td>\n",
       "      <td>33.26</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005786</td>\n",
       "      <td>0.032073</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.543333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>0.005786</td>\n",
       "      <td>0.003857</td>\n",
       "      <td>31.178593</td>\n",
       "      <td>0.300661</td>\n",
       "      <td>0.180397</td>\n",
       "      <td>0.120265</td>\n",
       "      <td>0.421547</td>\n",
       "      <td>-0.491970</td>\n",
       "      <td>1.209323</td>\n",
       "      <td>-1.778674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>6</td>\n",
       "      <td>698</td>\n",
       "      <td>15.93</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.034384</td>\n",
       "      <td>0.022822</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.663750</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.005731</td>\n",
       "      <td>0.014327</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>43.816698</td>\n",
       "      <td>0.251099</td>\n",
       "      <td>0.627746</td>\n",
       "      <td>0.313873</td>\n",
       "      <td>0.360773</td>\n",
       "      <td>0.487661</td>\n",
       "      <td>1.959383</td>\n",
       "      <td>2.308124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1</td>\n",
       "      <td>1332</td>\n",
       "      <td>9.19</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010511</td>\n",
       "      <td>0.006899</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.656429</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>144.940152</td>\n",
       "      <td>0.544070</td>\n",
       "      <td>1.088139</td>\n",
       "      <td>0.435256</td>\n",
       "      <td>0.297457</td>\n",
       "      <td>-0.498436</td>\n",
       "      <td>1.259410</td>\n",
       "      <td>-2.106750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2</td>\n",
       "      <td>658</td>\n",
       "      <td>58.00</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.022796</td>\n",
       "      <td>0.088146</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.866667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.012158</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>11.344828</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>-0.096190</td>\n",
       "      <td>-0.296949</td>\n",
       "      <td>-0.555048</td>\n",
       "      <td>-1.738005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>3</td>\n",
       "      <td>723</td>\n",
       "      <td>31.00</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>0.042877</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.583333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.015214</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>23.322581</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>-0.013583</td>\n",
       "      <td>-0.090532</td>\n",
       "      <td>-0.528755</td>\n",
       "      <td>-0.525878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>6</td>\n",
       "      <td>1391</td>\n",
       "      <td>69.14</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.049705</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.828000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.007908</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>20.118600</td>\n",
       "      <td>0.159097</td>\n",
       "      <td>0.028927</td>\n",
       "      <td>0.028927</td>\n",
       "      <td>0.051391</td>\n",
       "      <td>0.510208</td>\n",
       "      <td>-0.505678</td>\n",
       "      <td>1.634006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>4</td>\n",
       "      <td>1643</td>\n",
       "      <td>73.39</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017651</td>\n",
       "      <td>0.044668</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.530690</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>22.387246</td>\n",
       "      <td>0.081755</td>\n",
       "      <td>0.013626</td>\n",
       "      <td>0.013626</td>\n",
       "      <td>-0.463446</td>\n",
       "      <td>0.084954</td>\n",
       "      <td>-2.024617</td>\n",
       "      <td>0.533434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>6</td>\n",
       "      <td>852</td>\n",
       "      <td>42.93</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034038</td>\n",
       "      <td>0.050387</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.480345</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.010563</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>19.846261</td>\n",
       "      <td>0.069881</td>\n",
       "      <td>0.209644</td>\n",
       "      <td>0.046587</td>\n",
       "      <td>-0.246956</td>\n",
       "      <td>0.489775</td>\n",
       "      <td>-0.644835</td>\n",
       "      <td>2.516904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>1</td>\n",
       "      <td>868</td>\n",
       "      <td>62.30</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010369</td>\n",
       "      <td>0.071774</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.922222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005760</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.932584</td>\n",
       "      <td>0.080257</td>\n",
       "      <td>0.096308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.390702</td>\n",
       "      <td>-0.486453</td>\n",
       "      <td>-2.012085</td>\n",
       "      <td>-1.829113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>1</td>\n",
       "      <td>1186</td>\n",
       "      <td>91.64</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007589</td>\n",
       "      <td>0.077268</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.182222</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.941947</td>\n",
       "      <td>0.043649</td>\n",
       "      <td>0.109123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.352883</td>\n",
       "      <td>-0.481367</td>\n",
       "      <td>-1.802917</td>\n",
       "      <td>-1.667604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>3</td>\n",
       "      <td>867</td>\n",
       "      <td>61.82</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.012687</td>\n",
       "      <td>0.071303</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.620000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.011534</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>14.024588</td>\n",
       "      <td>0.080880</td>\n",
       "      <td>0.161760</td>\n",
       "      <td>0.048528</td>\n",
       "      <td>0.170813</td>\n",
       "      <td>-0.092140</td>\n",
       "      <td>0.380211</td>\n",
       "      <td>-0.502559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>5</td>\n",
       "      <td>1489</td>\n",
       "      <td>93.17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003358</td>\n",
       "      <td>0.062572</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.634000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>15.981539</td>\n",
       "      <td>0.032199</td>\n",
       "      <td>0.075131</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>-0.133884</td>\n",
       "      <td>0.316928</td>\n",
       "      <td>-1.280134</td>\n",
       "      <td>1.250465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>1</td>\n",
       "      <td>309</td>\n",
       "      <td>54.72</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067961</td>\n",
       "      <td>0.177087</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.605714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.029126</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>5.646930</td>\n",
       "      <td>0.018275</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.036550</td>\n",
       "      <td>-0.163369</td>\n",
       "      <td>-0.502154</td>\n",
       "      <td>-0.640747</td>\n",
       "      <td>-2.371208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>3</td>\n",
       "      <td>1709</td>\n",
       "      <td>12.78</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006437</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.161818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>133.724570</td>\n",
       "      <td>0.312989</td>\n",
       "      <td>0.078247</td>\n",
       "      <td>0.234742</td>\n",
       "      <td>0.075566</td>\n",
       "      <td>-0.103639</td>\n",
       "      <td>0.428342</td>\n",
       "      <td>-0.515022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>3</td>\n",
       "      <td>161</td>\n",
       "      <td>39.01</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.093168</td>\n",
       "      <td>0.242298</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.600667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049689</td>\n",
       "      <td>0.012422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.127147</td>\n",
       "      <td>0.205076</td>\n",
       "      <td>0.051269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.484544</td>\n",
       "      <td>-0.097448</td>\n",
       "      <td>-2.346547</td>\n",
       "      <td>-0.439370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>2</td>\n",
       "      <td>208</td>\n",
       "      <td>2.23</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.091346</td>\n",
       "      <td>0.010721</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.117368</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>93.273543</td>\n",
       "      <td>0.448430</td>\n",
       "      <td>4.484305</td>\n",
       "      <td>0.896861</td>\n",
       "      <td>-0.151180</td>\n",
       "      <td>-0.299799</td>\n",
       "      <td>-0.623690</td>\n",
       "      <td>-1.723073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2</td>\n",
       "      <td>1572</td>\n",
       "      <td>83.24</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013995</td>\n",
       "      <td>0.052952</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.783636</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>18.885151</td>\n",
       "      <td>0.072081</td>\n",
       "      <td>0.036040</td>\n",
       "      <td>0.060067</td>\n",
       "      <td>0.364162</td>\n",
       "      <td>-0.316287</td>\n",
       "      <td>2.119008</td>\n",
       "      <td>-1.840066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>6</td>\n",
       "      <td>1299</td>\n",
       "      <td>54.84</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.042217</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.280000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>23.687090</td>\n",
       "      <td>0.091174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018235</td>\n",
       "      <td>-0.161877</td>\n",
       "      <td>0.510681</td>\n",
       "      <td>-1.261282</td>\n",
       "      <td>1.653437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1</td>\n",
       "      <td>1701</td>\n",
       "      <td>94.49</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008818</td>\n",
       "      <td>0.055550</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.299333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>18.001905</td>\n",
       "      <td>0.031749</td>\n",
       "      <td>0.074082</td>\n",
       "      <td>0.010583</td>\n",
       "      <td>-0.270420</td>\n",
       "      <td>-0.494255</td>\n",
       "      <td>-1.523670</td>\n",
       "      <td>-2.134762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>4</td>\n",
       "      <td>410</td>\n",
       "      <td>11.13</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.027146</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.017073</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>36.837376</td>\n",
       "      <td>0.628931</td>\n",
       "      <td>0.179695</td>\n",
       "      <td>0.179695</td>\n",
       "      <td>-0.298526</td>\n",
       "      <td>0.081630</td>\n",
       "      <td>-0.602282</td>\n",
       "      <td>0.557966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2</td>\n",
       "      <td>859</td>\n",
       "      <td>55.80</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.019790</td>\n",
       "      <td>0.064959</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.282353</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.012806</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>15.394265</td>\n",
       "      <td>0.197133</td>\n",
       "      <td>0.179211</td>\n",
       "      <td>0.071685</td>\n",
       "      <td>0.308933</td>\n",
       "      <td>-0.299581</td>\n",
       "      <td>1.388300</td>\n",
       "      <td>-1.639994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>4</td>\n",
       "      <td>1106</td>\n",
       "      <td>35.40</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011754</td>\n",
       "      <td>0.032007</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.723077</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008137</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.242938</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.450975</td>\n",
       "      <td>0.106126</td>\n",
       "      <td>-2.329400</td>\n",
       "      <td>0.458421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>6</td>\n",
       "      <td>1073</td>\n",
       "      <td>64.26</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022367</td>\n",
       "      <td>0.059888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.677500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>16.697790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046685</td>\n",
       "      <td>0.015562</td>\n",
       "      <td>-0.427074</td>\n",
       "      <td>0.490085</td>\n",
       "      <td>-1.721572</td>\n",
       "      <td>2.358052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1</td>\n",
       "      <td>489</td>\n",
       "      <td>67.27</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014315</td>\n",
       "      <td>0.137566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.610000</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.269214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.366393</td>\n",
       "      <td>-0.482060</td>\n",
       "      <td>-1.730990</td>\n",
       "      <td>-1.520159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>60.15</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1.136364</td>\n",
       "      <td>2.734091</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.406000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.365752</td>\n",
       "      <td>0.133001</td>\n",
       "      <td>0.149626</td>\n",
       "      <td>0.049875</td>\n",
       "      <td>0.011980</td>\n",
       "      <td>0.092557</td>\n",
       "      <td>0.359428</td>\n",
       "      <td>0.520287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>58.11</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.527500</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.905500</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068835</td>\n",
       "      <td>0.120461</td>\n",
       "      <td>0.120461</td>\n",
       "      <td>0.068835</td>\n",
       "      <td>0.232398</td>\n",
       "      <td>-0.307440</td>\n",
       "      <td>1.266045</td>\n",
       "      <td>-1.291368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>28.58</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>4.082857</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.198462</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244927</td>\n",
       "      <td>0.104969</td>\n",
       "      <td>0.034990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500124</td>\n",
       "      <td>-0.098199</td>\n",
       "      <td>-2.258512</td>\n",
       "      <td>-0.437479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>4</td>\n",
       "      <td>208</td>\n",
       "      <td>77.53</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.372740</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.753000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>2.682832</td>\n",
       "      <td>0.025796</td>\n",
       "      <td>0.064491</td>\n",
       "      <td>0.025796</td>\n",
       "      <td>-0.050841</td>\n",
       "      <td>0.104713</td>\n",
       "      <td>-0.561977</td>\n",
       "      <td>0.473652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>4</td>\n",
       "      <td>1331</td>\n",
       "      <td>62.88</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.047243</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.576000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>0.006762</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>21.167303</td>\n",
       "      <td>0.111323</td>\n",
       "      <td>0.143130</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>-0.104432</td>\n",
       "      <td>0.120352</td>\n",
       "      <td>-1.396567</td>\n",
       "      <td>0.415872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>5</td>\n",
       "      <td>619</td>\n",
       "      <td>32.10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033926</td>\n",
       "      <td>0.051858</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.528571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.004847</td>\n",
       "      <td>0.009693</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>19.283489</td>\n",
       "      <td>0.093458</td>\n",
       "      <td>0.186916</td>\n",
       "      <td>0.062305</td>\n",
       "      <td>-0.183667</td>\n",
       "      <td>0.294208</td>\n",
       "      <td>-0.576585</td>\n",
       "      <td>1.917542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "      <td>51.88</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>1.152889</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.995385</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.867386</td>\n",
       "      <td>0.154202</td>\n",
       "      <td>0.134927</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>-0.199700</td>\n",
       "      <td>0.491629</td>\n",
       "      <td>-0.620596</td>\n",
       "      <td>2.438824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>6</td>\n",
       "      <td>291</td>\n",
       "      <td>87.69</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.079038</td>\n",
       "      <td>0.301340</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.812609</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.024055</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.017182</td>\n",
       "      <td>3.318508</td>\n",
       "      <td>0.079827</td>\n",
       "      <td>0.011404</td>\n",
       "      <td>0.057019</td>\n",
       "      <td>0.345184</td>\n",
       "      <td>0.479855</td>\n",
       "      <td>1.864071</td>\n",
       "      <td>2.233412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>5</td>\n",
       "      <td>1077</td>\n",
       "      <td>98.03</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>0.091021</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.535333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>10.986433</td>\n",
       "      <td>0.030603</td>\n",
       "      <td>0.020402</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>-0.307090</td>\n",
       "      <td>0.299263</td>\n",
       "      <td>-1.685744</td>\n",
       "      <td>1.584835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>6</td>\n",
       "      <td>1216</td>\n",
       "      <td>1.76</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.022204</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.065185</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.004934</td>\n",
       "      <td>0.005757</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>690.909091</td>\n",
       "      <td>3.409091</td>\n",
       "      <td>3.977273</td>\n",
       "      <td>1.136364</td>\n",
       "      <td>-0.238180</td>\n",
       "      <td>0.489873</td>\n",
       "      <td>-0.638376</td>\n",
       "      <td>2.504639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>5</td>\n",
       "      <td>877</td>\n",
       "      <td>12.59</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.023945</td>\n",
       "      <td>0.014356</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.599524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>69.658459</td>\n",
       "      <td>0.397141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397141</td>\n",
       "      <td>0.324896</td>\n",
       "      <td>0.279568</td>\n",
       "      <td>2.101542</td>\n",
       "      <td>1.746174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>6</td>\n",
       "      <td>1698</td>\n",
       "      <td>86.47</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>0.050925</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.860909</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>19.636868</td>\n",
       "      <td>0.069388</td>\n",
       "      <td>0.057824</td>\n",
       "      <td>0.046259</td>\n",
       "      <td>0.330386</td>\n",
       "      <td>0.499763</td>\n",
       "      <td>1.305311</td>\n",
       "      <td>2.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497871</th>\n",
       "      <td>3</td>\n",
       "      <td>214</td>\n",
       "      <td>42.32</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.102804</td>\n",
       "      <td>0.197757</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.923636</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.037383</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>5.056711</td>\n",
       "      <td>0.189036</td>\n",
       "      <td>0.236295</td>\n",
       "      <td>0.023629</td>\n",
       "      <td>-0.318887</td>\n",
       "      <td>-0.097898</td>\n",
       "      <td>-1.847975</td>\n",
       "      <td>-0.463108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497872</th>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>63.03</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.575750</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.634618</td>\n",
       "      <td>0.063462</td>\n",
       "      <td>0.031731</td>\n",
       "      <td>0.031731</td>\n",
       "      <td>-0.093333</td>\n",
       "      <td>-0.500276</td>\n",
       "      <td>-0.563991</td>\n",
       "      <td>-2.141233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497873</th>\n",
       "      <td>4</td>\n",
       "      <td>822</td>\n",
       "      <td>8.40</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.035280</td>\n",
       "      <td>0.010219</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.289655</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>97.857143</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>-0.050245</td>\n",
       "      <td>0.088211</td>\n",
       "      <td>0.382395</td>\n",
       "      <td>0.540968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497874</th>\n",
       "      <td>3</td>\n",
       "      <td>277</td>\n",
       "      <td>80.70</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.064982</td>\n",
       "      <td>0.291336</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.483333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.036101</td>\n",
       "      <td>0.014440</td>\n",
       "      <td>0.018051</td>\n",
       "      <td>3.432466</td>\n",
       "      <td>0.123916</td>\n",
       "      <td>0.049566</td>\n",
       "      <td>0.061958</td>\n",
       "      <td>0.439175</td>\n",
       "      <td>-0.110510</td>\n",
       "      <td>2.066726</td>\n",
       "      <td>-0.575874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497875</th>\n",
       "      <td>3</td>\n",
       "      <td>1291</td>\n",
       "      <td>10.97</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019365</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.002324</td>\n",
       "      <td>0.007746</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>117.684594</td>\n",
       "      <td>0.273473</td>\n",
       "      <td>0.911577</td>\n",
       "      <td>0.091158</td>\n",
       "      <td>-0.388849</td>\n",
       "      <td>-0.102270</td>\n",
       "      <td>-1.958143</td>\n",
       "      <td>-0.480060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497876</th>\n",
       "      <td>6</td>\n",
       "      <td>1723</td>\n",
       "      <td>99.46</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012768</td>\n",
       "      <td>0.057725</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.520909</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.004643</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>17.323547</td>\n",
       "      <td>0.080434</td>\n",
       "      <td>0.050271</td>\n",
       "      <td>0.050271</td>\n",
       "      <td>0.393984</td>\n",
       "      <td>0.486245</td>\n",
       "      <td>1.960557</td>\n",
       "      <td>2.257914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497877</th>\n",
       "      <td>4</td>\n",
       "      <td>889</td>\n",
       "      <td>43.05</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033746</td>\n",
       "      <td>0.048425</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.435000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.011249</td>\n",
       "      <td>0.006749</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>20.650407</td>\n",
       "      <td>0.232288</td>\n",
       "      <td>0.139373</td>\n",
       "      <td>0.092915</td>\n",
       "      <td>0.120549</td>\n",
       "      <td>0.082003</td>\n",
       "      <td>1.558144</td>\n",
       "      <td>0.521085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497878</th>\n",
       "      <td>3</td>\n",
       "      <td>1657</td>\n",
       "      <td>26.99</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015691</td>\n",
       "      <td>0.016288</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.038077</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>61.393109</td>\n",
       "      <td>0.370508</td>\n",
       "      <td>0.370508</td>\n",
       "      <td>0.185254</td>\n",
       "      <td>0.371521</td>\n",
       "      <td>-0.112003</td>\n",
       "      <td>2.102036</td>\n",
       "      <td>-0.596868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497879</th>\n",
       "      <td>6</td>\n",
       "      <td>239</td>\n",
       "      <td>39.45</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.071130</td>\n",
       "      <td>0.165063</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.320588</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>6.058302</td>\n",
       "      <td>0.101394</td>\n",
       "      <td>0.050697</td>\n",
       "      <td>0.076046</td>\n",
       "      <td>0.022993</td>\n",
       "      <td>0.491016</td>\n",
       "      <td>0.382550</td>\n",
       "      <td>2.412438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497880</th>\n",
       "      <td>5</td>\n",
       "      <td>988</td>\n",
       "      <td>22.33</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>0.022601</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.582500</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>44.245410</td>\n",
       "      <td>0.089566</td>\n",
       "      <td>0.358262</td>\n",
       "      <td>0.179131</td>\n",
       "      <td>0.429917</td>\n",
       "      <td>0.310101</td>\n",
       "      <td>1.256478</td>\n",
       "      <td>1.248799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497881</th>\n",
       "      <td>5</td>\n",
       "      <td>683</td>\n",
       "      <td>56.40</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.038067</td>\n",
       "      <td>0.082577</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.169231</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>12.109929</td>\n",
       "      <td>0.195035</td>\n",
       "      <td>0.053191</td>\n",
       "      <td>0.035461</td>\n",
       "      <td>-0.212855</td>\n",
       "      <td>0.288189</td>\n",
       "      <td>-0.590204</td>\n",
       "      <td>1.968852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497882</th>\n",
       "      <td>5</td>\n",
       "      <td>1736</td>\n",
       "      <td>23.54</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.013560</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.846667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003456</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.746814</td>\n",
       "      <td>0.254885</td>\n",
       "      <td>0.127443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.338180</td>\n",
       "      <td>0.316965</td>\n",
       "      <td>-1.869084</td>\n",
       "      <td>1.200675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497883</th>\n",
       "      <td>1</td>\n",
       "      <td>1637</td>\n",
       "      <td>55.99</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>0.034203</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.153462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>29.237364</td>\n",
       "      <td>0.017860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053581</td>\n",
       "      <td>-0.102898</td>\n",
       "      <td>-0.519752</td>\n",
       "      <td>0.340993</td>\n",
       "      <td>-2.515509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497884</th>\n",
       "      <td>6</td>\n",
       "      <td>1456</td>\n",
       "      <td>4.47</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>325.727069</td>\n",
       "      <td>1.565996</td>\n",
       "      <td>1.118568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.446601</td>\n",
       "      <td>0.508332</td>\n",
       "      <td>-2.032781</td>\n",
       "      <td>1.927589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497885</th>\n",
       "      <td>1</td>\n",
       "      <td>711</td>\n",
       "      <td>60.04</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.084444</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.003333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>11.842105</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>0.033311</td>\n",
       "      <td>0.083278</td>\n",
       "      <td>0.442906</td>\n",
       "      <td>-0.508993</td>\n",
       "      <td>1.862295</td>\n",
       "      <td>-1.996423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497886</th>\n",
       "      <td>6</td>\n",
       "      <td>65</td>\n",
       "      <td>89.17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>1.371846</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.585000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.728945</td>\n",
       "      <td>0.044858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.312971</td>\n",
       "      <td>0.515902</td>\n",
       "      <td>-1.350984</td>\n",
       "      <td>1.242417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497887</th>\n",
       "      <td>5</td>\n",
       "      <td>1277</td>\n",
       "      <td>3.81</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010180</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.293077</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.007048</td>\n",
       "      <td>0.004699</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>335.170604</td>\n",
       "      <td>2.362205</td>\n",
       "      <td>1.574803</td>\n",
       "      <td>1.049869</td>\n",
       "      <td>0.298286</td>\n",
       "      <td>0.298441</td>\n",
       "      <td>1.390683</td>\n",
       "      <td>1.576313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497888</th>\n",
       "      <td>5</td>\n",
       "      <td>690</td>\n",
       "      <td>26.73</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010145</td>\n",
       "      <td>0.038739</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.818571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.005797</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>25.813692</td>\n",
       "      <td>0.224467</td>\n",
       "      <td>0.149645</td>\n",
       "      <td>0.037411</td>\n",
       "      <td>-0.202973</td>\n",
       "      <td>0.309912</td>\n",
       "      <td>-1.421479</td>\n",
       "      <td>1.358110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497889</th>\n",
       "      <td>3</td>\n",
       "      <td>1221</td>\n",
       "      <td>71.01</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019656</td>\n",
       "      <td>0.058157</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.958750</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.194761</td>\n",
       "      <td>0.042248</td>\n",
       "      <td>0.070413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.577679</td>\n",
       "      <td>-0.103624</td>\n",
       "      <td>-2.590283</td>\n",
       "      <td>-0.505800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497890</th>\n",
       "      <td>1</td>\n",
       "      <td>1810</td>\n",
       "      <td>53.38</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009392</td>\n",
       "      <td>0.029492</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>33.907831</td>\n",
       "      <td>0.093668</td>\n",
       "      <td>0.018734</td>\n",
       "      <td>0.037467</td>\n",
       "      <td>-0.158978</td>\n",
       "      <td>-0.505922</td>\n",
       "      <td>-0.612788</td>\n",
       "      <td>-2.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497891</th>\n",
       "      <td>3</td>\n",
       "      <td>1710</td>\n",
       "      <td>39.55</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.023129</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.955000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.236410</td>\n",
       "      <td>0.126422</td>\n",
       "      <td>0.075853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.436691</td>\n",
       "      <td>-0.091548</td>\n",
       "      <td>-2.252739</td>\n",
       "      <td>-0.443313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497892</th>\n",
       "      <td>3</td>\n",
       "      <td>1181</td>\n",
       "      <td>50.24</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.025402</td>\n",
       "      <td>0.042540</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.674667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>23.507166</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.159236</td>\n",
       "      <td>0.059713</td>\n",
       "      <td>-0.075250</td>\n",
       "      <td>-0.114245</td>\n",
       "      <td>0.359917</td>\n",
       "      <td>-0.577615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497893</th>\n",
       "      <td>6</td>\n",
       "      <td>768</td>\n",
       "      <td>51.22</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029948</td>\n",
       "      <td>0.066693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.226957</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>14.994143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156189</td>\n",
       "      <td>0.019524</td>\n",
       "      <td>-0.383327</td>\n",
       "      <td>0.496569</td>\n",
       "      <td>-1.712215</td>\n",
       "      <td>2.338566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497894</th>\n",
       "      <td>4</td>\n",
       "      <td>1695</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>543.269231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.320513</td>\n",
       "      <td>-0.301135</td>\n",
       "      <td>0.102399</td>\n",
       "      <td>-1.564669</td>\n",
       "      <td>0.485726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497895</th>\n",
       "      <td>6</td>\n",
       "      <td>902</td>\n",
       "      <td>96.04</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011086</td>\n",
       "      <td>0.106475</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.604000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.009978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.391920</td>\n",
       "      <td>0.010412</td>\n",
       "      <td>0.093711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.389311</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>-1.863031</td>\n",
       "      <td>1.772185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497896</th>\n",
       "      <td>1</td>\n",
       "      <td>902</td>\n",
       "      <td>16.64</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>0.018448</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.080000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.005543</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>54.206731</td>\n",
       "      <td>0.180288</td>\n",
       "      <td>0.300481</td>\n",
       "      <td>0.120192</td>\n",
       "      <td>-0.043057</td>\n",
       "      <td>-0.493192</td>\n",
       "      <td>-0.497411</td>\n",
       "      <td>-2.050189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497897</th>\n",
       "      <td>3</td>\n",
       "      <td>630</td>\n",
       "      <td>7.77</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.295000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>81.081081</td>\n",
       "      <td>1.287001</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>-0.189188</td>\n",
       "      <td>-0.090140</td>\n",
       "      <td>-1.430743</td>\n",
       "      <td>-0.415446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497898</th>\n",
       "      <td>5</td>\n",
       "      <td>1140</td>\n",
       "      <td>38.15</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>0.033465</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.238889</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>29.882045</td>\n",
       "      <td>0.209699</td>\n",
       "      <td>0.262123</td>\n",
       "      <td>0.026212</td>\n",
       "      <td>-0.165749</td>\n",
       "      <td>0.315800</td>\n",
       "      <td>-1.402647</td>\n",
       "      <td>1.371706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497899</th>\n",
       "      <td>3</td>\n",
       "      <td>492</td>\n",
       "      <td>29.87</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030488</td>\n",
       "      <td>0.060711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.991333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>16.471376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200870</td>\n",
       "      <td>0.033478</td>\n",
       "      <td>-0.315887</td>\n",
       "      <td>-0.097712</td>\n",
       "      <td>-1.750163</td>\n",
       "      <td>-0.497995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497900</th>\n",
       "      <td>1</td>\n",
       "      <td>1078</td>\n",
       "      <td>15.34</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.014230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.670000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>70.273794</td>\n",
       "      <td>0.130378</td>\n",
       "      <td>0.130378</td>\n",
       "      <td>0.260756</td>\n",
       "      <td>0.447825</td>\n",
       "      <td>-0.493488</td>\n",
       "      <td>1.373389</td>\n",
       "      <td>-1.405849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497901</th>\n",
       "      <td>6</td>\n",
       "      <td>965</td>\n",
       "      <td>28.15</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.029171</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.815000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.008290</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>34.280639</td>\n",
       "      <td>0.284192</td>\n",
       "      <td>0.213144</td>\n",
       "      <td>0.106572</td>\n",
       "      <td>0.156169</td>\n",
       "      <td>0.504222</td>\n",
       "      <td>0.378608</td>\n",
       "      <td>2.115143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497902</th>\n",
       "      <td>1</td>\n",
       "      <td>1151</td>\n",
       "      <td>49.09</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025195</td>\n",
       "      <td>0.042650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.692759</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006950</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>23.446730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162966</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>-0.449582</td>\n",
       "      <td>-0.508612</td>\n",
       "      <td>-1.650975</td>\n",
       "      <td>-2.349825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497903</th>\n",
       "      <td>3</td>\n",
       "      <td>542</td>\n",
       "      <td>69.85</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.128875</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.492500</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>7.759485</td>\n",
       "      <td>0.057266</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>0.057266</td>\n",
       "      <td>0.182536</td>\n",
       "      <td>-0.113831</td>\n",
       "      <td>1.563627</td>\n",
       "      <td>-0.576309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497904</th>\n",
       "      <td>2</td>\n",
       "      <td>314</td>\n",
       "      <td>31.63</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.022293</td>\n",
       "      <td>0.100732</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.518571</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.028662</td>\n",
       "      <td>0.028662</td>\n",
       "      <td>0.009554</td>\n",
       "      <td>9.927284</td>\n",
       "      <td>0.284540</td>\n",
       "      <td>0.284540</td>\n",
       "      <td>0.094847</td>\n",
       "      <td>0.234081</td>\n",
       "      <td>-0.287762</td>\n",
       "      <td>0.338303</td>\n",
       "      <td>-1.328888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497905</th>\n",
       "      <td>5</td>\n",
       "      <td>149</td>\n",
       "      <td>26.28</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.093960</td>\n",
       "      <td>0.176376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.877143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>5.669711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152207</td>\n",
       "      <td>0.038052</td>\n",
       "      <td>-0.321065</td>\n",
       "      <td>0.300549</td>\n",
       "      <td>-1.616944</td>\n",
       "      <td>1.539526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497906</th>\n",
       "      <td>5</td>\n",
       "      <td>374</td>\n",
       "      <td>40.41</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.069519</td>\n",
       "      <td>0.108048</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.554231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.021390</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>9.255135</td>\n",
       "      <td>0.197971</td>\n",
       "      <td>0.049493</td>\n",
       "      <td>0.049493</td>\n",
       "      <td>-0.240248</td>\n",
       "      <td>0.286005</td>\n",
       "      <td>-0.575605</td>\n",
       "      <td>1.961350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497907</th>\n",
       "      <td>3</td>\n",
       "      <td>765</td>\n",
       "      <td>53.54</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.069987</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.149412</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>14.288383</td>\n",
       "      <td>0.205454</td>\n",
       "      <td>0.074710</td>\n",
       "      <td>0.093388</td>\n",
       "      <td>0.446651</td>\n",
       "      <td>-0.109417</td>\n",
       "      <td>2.143117</td>\n",
       "      <td>-0.575129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497908</th>\n",
       "      <td>2</td>\n",
       "      <td>1338</td>\n",
       "      <td>38.12</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011211</td>\n",
       "      <td>0.028490</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.541333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>35.099685</td>\n",
       "      <td>0.262329</td>\n",
       "      <td>0.183631</td>\n",
       "      <td>0.026233</td>\n",
       "      <td>-0.253148</td>\n",
       "      <td>-0.293300</td>\n",
       "      <td>-1.606304</td>\n",
       "      <td>-1.544726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497909</th>\n",
       "      <td>6</td>\n",
       "      <td>1050</td>\n",
       "      <td>76.05</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017143</td>\n",
       "      <td>0.072429</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.225000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>13.806706</td>\n",
       "      <td>0.131492</td>\n",
       "      <td>0.013149</td>\n",
       "      <td>0.052597</td>\n",
       "      <td>0.230571</td>\n",
       "      <td>0.488465</td>\n",
       "      <td>1.348774</td>\n",
       "      <td>2.320577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497910</th>\n",
       "      <td>1</td>\n",
       "      <td>656</td>\n",
       "      <td>8.07</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033537</td>\n",
       "      <td>0.012302</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.366818</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>81.288724</td>\n",
       "      <td>0.247831</td>\n",
       "      <td>0.867410</td>\n",
       "      <td>0.495663</td>\n",
       "      <td>0.166070</td>\n",
       "      <td>-0.511053</td>\n",
       "      <td>1.370506</td>\n",
       "      <td>-2.410851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497911</th>\n",
       "      <td>4</td>\n",
       "      <td>693</td>\n",
       "      <td>45.34</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>0.065426</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.113333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.002886</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>15.284517</td>\n",
       "      <td>0.198500</td>\n",
       "      <td>0.044111</td>\n",
       "      <td>0.110278</td>\n",
       "      <td>0.666552</td>\n",
       "      <td>0.105446</td>\n",
       "      <td>1.575251</td>\n",
       "      <td>0.342943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497912</th>\n",
       "      <td>2</td>\n",
       "      <td>1225</td>\n",
       "      <td>29.08</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017143</td>\n",
       "      <td>0.023739</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.384762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.007347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>42.125172</td>\n",
       "      <td>0.309491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137552</td>\n",
       "      <td>0.167730</td>\n",
       "      <td>-0.315987</td>\n",
       "      <td>1.417449</td>\n",
       "      <td>-1.883173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497913</th>\n",
       "      <td>6</td>\n",
       "      <td>976</td>\n",
       "      <td>89.50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.091701</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.187500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>10.905028</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.055866</td>\n",
       "      <td>0.517734</td>\n",
       "      <td>0.496522</td>\n",
       "      <td>1.771479</td>\n",
       "      <td>1.781639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497914</th>\n",
       "      <td>5</td>\n",
       "      <td>451</td>\n",
       "      <td>36.47</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.050998</td>\n",
       "      <td>0.080865</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.585652</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.013304</td>\n",
       "      <td>0.019956</td>\n",
       "      <td>0.011086</td>\n",
       "      <td>12.366328</td>\n",
       "      <td>0.164519</td>\n",
       "      <td>0.246778</td>\n",
       "      <td>0.137099</td>\n",
       "      <td>0.382066</td>\n",
       "      <td>0.288344</td>\n",
       "      <td>2.117811</td>\n",
       "      <td>1.728118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497915</th>\n",
       "      <td>1</td>\n",
       "      <td>925</td>\n",
       "      <td>97.29</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>0.105178</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.120526</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.008649</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>0.003243</td>\n",
       "      <td>9.507658</td>\n",
       "      <td>0.082228</td>\n",
       "      <td>0.092507</td>\n",
       "      <td>0.030836</td>\n",
       "      <td>0.092716</td>\n",
       "      <td>-0.500268</td>\n",
       "      <td>0.336133</td>\n",
       "      <td>-2.353075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497916</th>\n",
       "      <td>1</td>\n",
       "      <td>1646</td>\n",
       "      <td>57.60</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015188</td>\n",
       "      <td>0.034994</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.304000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.576389</td>\n",
       "      <td>0.121528</td>\n",
       "      <td>0.017361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.602324</td>\n",
       "      <td>-0.507928</td>\n",
       "      <td>-2.269138</td>\n",
       "      <td>-2.158515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497917</th>\n",
       "      <td>3</td>\n",
       "      <td>566</td>\n",
       "      <td>46.97</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017668</td>\n",
       "      <td>0.082986</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.697000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.050245</td>\n",
       "      <td>0.042580</td>\n",
       "      <td>0.127741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.425786</td>\n",
       "      <td>-0.089053</td>\n",
       "      <td>-2.216361</td>\n",
       "      <td>-0.442334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497918</th>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>11.63</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.011607</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.969167</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>86.156492</td>\n",
       "      <td>0.515907</td>\n",
       "      <td>0.773861</td>\n",
       "      <td>0.171969</td>\n",
       "      <td>-0.047894</td>\n",
       "      <td>-0.491920</td>\n",
       "      <td>-0.520650</td>\n",
       "      <td>-2.125407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497919</th>\n",
       "      <td>2</td>\n",
       "      <td>629</td>\n",
       "      <td>53.94</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>0.085755</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.568571</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.006359</td>\n",
       "      <td>11.661105</td>\n",
       "      <td>0.018539</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>0.074156</td>\n",
       "      <td>0.150477</td>\n",
       "      <td>-0.315663</td>\n",
       "      <td>1.427899</td>\n",
       "      <td>-1.941239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497920</th>\n",
       "      <td>2</td>\n",
       "      <td>611</td>\n",
       "      <td>44.62</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.073028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.487333</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011457</td>\n",
       "      <td>0.008183</td>\n",
       "      <td>13.693411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156880</td>\n",
       "      <td>0.112057</td>\n",
       "      <td>0.261445</td>\n",
       "      <td>-0.321915</td>\n",
       "      <td>2.089285</td>\n",
       "      <td>-1.877363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497921</th>\n",
       "      <td>4</td>\n",
       "      <td>950</td>\n",
       "      <td>17.65</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.018579</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.825000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>53.824363</td>\n",
       "      <td>0.226629</td>\n",
       "      <td>0.113314</td>\n",
       "      <td>0.283286</td>\n",
       "      <td>0.670363</td>\n",
       "      <td>0.105083</td>\n",
       "      <td>1.627837</td>\n",
       "      <td>0.256227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497922</th>\n",
       "      <td>5</td>\n",
       "      <td>101</td>\n",
       "      <td>57.19</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.566238</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.382917</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>1.766043</td>\n",
       "      <td>0.069942</td>\n",
       "      <td>0.122399</td>\n",
       "      <td>0.017486</td>\n",
       "      <td>-0.378796</td>\n",
       "      <td>0.295495</td>\n",
       "      <td>-1.843209</td>\n",
       "      <td>1.743744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497923</th>\n",
       "      <td>2</td>\n",
       "      <td>1699</td>\n",
       "      <td>21.95</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013537</td>\n",
       "      <td>0.012919</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.954348</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.006474</td>\n",
       "      <td>0.005886</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>77.403189</td>\n",
       "      <td>0.501139</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>0.182232</td>\n",
       "      <td>0.226269</td>\n",
       "      <td>-0.305935</td>\n",
       "      <td>1.415527</td>\n",
       "      <td>-1.808570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497924</th>\n",
       "      <td>1</td>\n",
       "      <td>1076</td>\n",
       "      <td>55.47</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.051552</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.470000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.004647</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>0.004647</td>\n",
       "      <td>19.397873</td>\n",
       "      <td>0.090139</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.090139</td>\n",
       "      <td>0.919136</td>\n",
       "      <td>-0.480284</td>\n",
       "      <td>3.563014</td>\n",
       "      <td>0.086820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497925</th>\n",
       "      <td>6</td>\n",
       "      <td>232</td>\n",
       "      <td>7.20</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012931</td>\n",
       "      <td>0.031034</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.021552</td>\n",
       "      <td>32.222222</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.644360</td>\n",
       "      <td>0.508524</td>\n",
       "      <td>1.554195</td>\n",
       "      <td>1.392249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497926</th>\n",
       "      <td>5</td>\n",
       "      <td>453</td>\n",
       "      <td>96.32</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017660</td>\n",
       "      <td>0.212627</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.040000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.703073</td>\n",
       "      <td>0.093439</td>\n",
       "      <td>0.020764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.373520</td>\n",
       "      <td>0.311056</td>\n",
       "      <td>-1.889049</td>\n",
       "      <td>1.237284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497927</th>\n",
       "      <td>5</td>\n",
       "      <td>972</td>\n",
       "      <td>45.90</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.047222</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.639286</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>21.176471</td>\n",
       "      <td>0.087146</td>\n",
       "      <td>0.217865</td>\n",
       "      <td>0.087146</td>\n",
       "      <td>0.142679</td>\n",
       "      <td>0.286934</td>\n",
       "      <td>1.460048</td>\n",
       "      <td>2.007905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497928</th>\n",
       "      <td>4</td>\n",
       "      <td>1750</td>\n",
       "      <td>76.78</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.043874</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.780000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>22.792394</td>\n",
       "      <td>0.078145</td>\n",
       "      <td>0.013024</td>\n",
       "      <td>0.013024</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>0.123184</td>\n",
       "      <td>3.183962</td>\n",
       "      <td>0.101032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497929</th>\n",
       "      <td>1</td>\n",
       "      <td>1111</td>\n",
       "      <td>5.52</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027003</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>201.268116</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>1.449275</td>\n",
       "      <td>0.181159</td>\n",
       "      <td>-0.464504</td>\n",
       "      <td>-0.509576</td>\n",
       "      <td>-1.759948</td>\n",
       "      <td>-2.285219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497930</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>41.17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>3.166923</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.940714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315764</td>\n",
       "      <td>0.097158</td>\n",
       "      <td>0.024290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.501541</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>-2.266945</td>\n",
       "      <td>1.432820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497931</th>\n",
       "      <td>4</td>\n",
       "      <td>955</td>\n",
       "      <td>94.26</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.098702</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.473333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011518</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.131551</td>\n",
       "      <td>0.116698</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.376634</td>\n",
       "      <td>0.110731</td>\n",
       "      <td>-2.025526</td>\n",
       "      <td>0.445616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497932</th>\n",
       "      <td>1</td>\n",
       "      <td>990</td>\n",
       "      <td>66.95</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.021212</td>\n",
       "      <td>0.067626</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.188095</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>14.787155</td>\n",
       "      <td>0.104556</td>\n",
       "      <td>0.149365</td>\n",
       "      <td>0.059746</td>\n",
       "      <td>0.244647</td>\n",
       "      <td>-0.504483</td>\n",
       "      <td>1.356587</td>\n",
       "      <td>-2.414823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497933</th>\n",
       "      <td>1</td>\n",
       "      <td>1549</td>\n",
       "      <td>24.13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.015578</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.043333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>64.193949</td>\n",
       "      <td>0.041442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124327</td>\n",
       "      <td>0.180683</td>\n",
       "      <td>-0.496246</td>\n",
       "      <td>0.476644</td>\n",
       "      <td>-1.811791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497934</th>\n",
       "      <td>2</td>\n",
       "      <td>1660</td>\n",
       "      <td>67.63</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.661429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.006024</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>24.545320</td>\n",
       "      <td>0.147863</td>\n",
       "      <td>0.014786</td>\n",
       "      <td>0.073932</td>\n",
       "      <td>0.561913</td>\n",
       "      <td>-0.301452</td>\n",
       "      <td>1.735964</td>\n",
       "      <td>-1.260534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497935</th>\n",
       "      <td>1</td>\n",
       "      <td>1337</td>\n",
       "      <td>87.25</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>0.065258</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.232143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>15.323782</td>\n",
       "      <td>0.114613</td>\n",
       "      <td>0.114613</td>\n",
       "      <td>0.034384</td>\n",
       "      <td>0.167772</td>\n",
       "      <td>-0.493282</td>\n",
       "      <td>0.349151</td>\n",
       "      <td>-2.056843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497936</th>\n",
       "      <td>2</td>\n",
       "      <td>1310</td>\n",
       "      <td>46.16</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.035237</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.540000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.005344</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>28.379549</td>\n",
       "      <td>0.151646</td>\n",
       "      <td>0.064991</td>\n",
       "      <td>0.021664</td>\n",
       "      <td>-0.146173</td>\n",
       "      <td>-0.285910</td>\n",
       "      <td>-1.329197</td>\n",
       "      <td>-1.262713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497937</th>\n",
       "      <td>6</td>\n",
       "      <td>473</td>\n",
       "      <td>42.82</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.019027</td>\n",
       "      <td>0.090529</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.757778</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.012685</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.008457</td>\n",
       "      <td>11.046240</td>\n",
       "      <td>0.140121</td>\n",
       "      <td>0.023354</td>\n",
       "      <td>0.093414</td>\n",
       "      <td>0.310647</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>1.274024</td>\n",
       "      <td>2.005695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497938</th>\n",
       "      <td>5</td>\n",
       "      <td>944</td>\n",
       "      <td>21.03</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.022278</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.751071</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>44.888255</td>\n",
       "      <td>0.190204</td>\n",
       "      <td>0.285307</td>\n",
       "      <td>0.095102</td>\n",
       "      <td>-0.259895</td>\n",
       "      <td>0.287508</td>\n",
       "      <td>-0.609184</td>\n",
       "      <td>2.023527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497939</th>\n",
       "      <td>6</td>\n",
       "      <td>595</td>\n",
       "      <td>83.51</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.140353</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.918333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>7.124895</td>\n",
       "      <td>0.023949</td>\n",
       "      <td>0.035924</td>\n",
       "      <td>0.023949</td>\n",
       "      <td>-0.007990</td>\n",
       "      <td>0.506929</td>\n",
       "      <td>-0.482164</td>\n",
       "      <td>1.881562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497940</th>\n",
       "      <td>2</td>\n",
       "      <td>1620</td>\n",
       "      <td>98.43</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.060759</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.921500</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>16.458397</td>\n",
       "      <td>0.020319</td>\n",
       "      <td>0.030479</td>\n",
       "      <td>0.050798</td>\n",
       "      <td>0.371833</td>\n",
       "      <td>-0.315134</td>\n",
       "      <td>2.034848</td>\n",
       "      <td>-1.807346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497941</th>\n",
       "      <td>3</td>\n",
       "      <td>310</td>\n",
       "      <td>49.27</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.158935</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.211667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>6.291861</td>\n",
       "      <td>0.081185</td>\n",
       "      <td>0.202963</td>\n",
       "      <td>0.040593</td>\n",
       "      <td>0.049810</td>\n",
       "      <td>-0.084002</td>\n",
       "      <td>-0.522279</td>\n",
       "      <td>-0.458336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497942</th>\n",
       "      <td>1</td>\n",
       "      <td>1390</td>\n",
       "      <td>60.46</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>0.043496</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.030667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.004317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.990407</td>\n",
       "      <td>0.066159</td>\n",
       "      <td>0.099239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.468515</td>\n",
       "      <td>-0.493005</td>\n",
       "      <td>-2.171754</td>\n",
       "      <td>-2.018435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497943</th>\n",
       "      <td>3</td>\n",
       "      <td>109</td>\n",
       "      <td>37.82</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.346972</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.454615</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.100917</td>\n",
       "      <td>0.082569</td>\n",
       "      <td>0.036697</td>\n",
       "      <td>2.882073</td>\n",
       "      <td>0.290851</td>\n",
       "      <td>0.237969</td>\n",
       "      <td>0.105764</td>\n",
       "      <td>0.191321</td>\n",
       "      <td>-0.110475</td>\n",
       "      <td>1.533154</td>\n",
       "      <td>-0.592224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497944</th>\n",
       "      <td>3</td>\n",
       "      <td>1726</td>\n",
       "      <td>93.51</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.054177</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.585000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.457919</td>\n",
       "      <td>0.042776</td>\n",
       "      <td>0.106940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.308373</td>\n",
       "      <td>-0.077549</td>\n",
       "      <td>-1.825610</td>\n",
       "      <td>-0.478387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497945</th>\n",
       "      <td>4</td>\n",
       "      <td>717</td>\n",
       "      <td>51.97</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009763</td>\n",
       "      <td>0.072483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.424286</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011158</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>13.796421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153935</td>\n",
       "      <td>0.076967</td>\n",
       "      <td>0.367547</td>\n",
       "      <td>0.105673</td>\n",
       "      <td>1.383200</td>\n",
       "      <td>0.455537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497946</th>\n",
       "      <td>1</td>\n",
       "      <td>1385</td>\n",
       "      <td>90.09</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009386</td>\n",
       "      <td>0.065047</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.930000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.373515</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.451828</td>\n",
       "      <td>-0.495266</td>\n",
       "      <td>-2.047025</td>\n",
       "      <td>-1.886567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497947</th>\n",
       "      <td>2</td>\n",
       "      <td>614</td>\n",
       "      <td>37.52</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.048860</td>\n",
       "      <td>0.061107</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.250667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>16.364606</td>\n",
       "      <td>0.239872</td>\n",
       "      <td>0.239872</td>\n",
       "      <td>0.053305</td>\n",
       "      <td>-0.229100</td>\n",
       "      <td>-0.309281</td>\n",
       "      <td>-0.634213</td>\n",
       "      <td>-1.971938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497948</th>\n",
       "      <td>1</td>\n",
       "      <td>379</td>\n",
       "      <td>6.76</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034301</td>\n",
       "      <td>0.017836</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.021108</td>\n",
       "      <td>0.013193</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>56.065089</td>\n",
       "      <td>1.183432</td>\n",
       "      <td>0.739645</td>\n",
       "      <td>0.295858</td>\n",
       "      <td>-0.081448</td>\n",
       "      <td>-0.497355</td>\n",
       "      <td>-0.558030</td>\n",
       "      <td>-2.194152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497949</th>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>72.47</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>2.196061</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.494000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.455361</td>\n",
       "      <td>0.110391</td>\n",
       "      <td>0.082793</td>\n",
       "      <td>0.041396</td>\n",
       "      <td>0.260391</td>\n",
       "      <td>0.511392</td>\n",
       "      <td>0.384186</td>\n",
       "      <td>1.738853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497950</th>\n",
       "      <td>3</td>\n",
       "      <td>1709</td>\n",
       "      <td>64.66</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009362</td>\n",
       "      <td>0.037835</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.041250</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.004681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.430560</td>\n",
       "      <td>0.108259</td>\n",
       "      <td>0.123724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.448012</td>\n",
       "      <td>-0.090996</td>\n",
       "      <td>-2.335179</td>\n",
       "      <td>-0.467485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497951</th>\n",
       "      <td>1</td>\n",
       "      <td>1411</td>\n",
       "      <td>36.17</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010631</td>\n",
       "      <td>0.025634</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.411333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.007087</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>39.010229</td>\n",
       "      <td>0.221178</td>\n",
       "      <td>0.276472</td>\n",
       "      <td>0.110589</td>\n",
       "      <td>0.310510</td>\n",
       "      <td>-0.498269</td>\n",
       "      <td>1.306725</td>\n",
       "      <td>-2.195733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497952</th>\n",
       "      <td>1</td>\n",
       "      <td>1323</td>\n",
       "      <td>38.93</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.009826</td>\n",
       "      <td>0.029426</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.994615</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.007559</td>\n",
       "      <td>0.006047</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>33.984074</td>\n",
       "      <td>0.256871</td>\n",
       "      <td>0.205497</td>\n",
       "      <td>0.077061</td>\n",
       "      <td>0.147743</td>\n",
       "      <td>-0.495217</td>\n",
       "      <td>0.356603</td>\n",
       "      <td>-2.157650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497953</th>\n",
       "      <td>4</td>\n",
       "      <td>505</td>\n",
       "      <td>61.61</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.043564</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.800455</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.015842</td>\n",
       "      <td>0.007921</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>8.196721</td>\n",
       "      <td>0.129849</td>\n",
       "      <td>0.064925</td>\n",
       "      <td>0.081156</td>\n",
       "      <td>0.375232</td>\n",
       "      <td>0.084578</td>\n",
       "      <td>2.143034</td>\n",
       "      <td>0.525180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497954</th>\n",
       "      <td>6</td>\n",
       "      <td>1792</td>\n",
       "      <td>98.21</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.054805</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.777059</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>18.246614</td>\n",
       "      <td>0.081458</td>\n",
       "      <td>0.020365</td>\n",
       "      <td>0.030547</td>\n",
       "      <td>0.062847</td>\n",
       "      <td>0.493280</td>\n",
       "      <td>0.401105</td>\n",
       "      <td>2.330642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497955</th>\n",
       "      <td>5</td>\n",
       "      <td>1603</td>\n",
       "      <td>33.78</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.021073</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.260000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.006238</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>47.454115</td>\n",
       "      <td>0.059207</td>\n",
       "      <td>0.296033</td>\n",
       "      <td>0.088810</td>\n",
       "      <td>0.296223</td>\n",
       "      <td>0.317961</td>\n",
       "      <td>0.387246</td>\n",
       "      <td>1.120883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497956</th>\n",
       "      <td>2</td>\n",
       "      <td>927</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.861667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179.303675</td>\n",
       "      <td>0.386847</td>\n",
       "      <td>0.580271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.419540</td>\n",
       "      <td>-0.288909</td>\n",
       "      <td>-2.062783</td>\n",
       "      <td>-1.302834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497957</th>\n",
       "      <td>5</td>\n",
       "      <td>1009</td>\n",
       "      <td>63.24</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010902</td>\n",
       "      <td>0.062676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.749091</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.010902</td>\n",
       "      <td>0.006938</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>15.955092</td>\n",
       "      <td>0.173941</td>\n",
       "      <td>0.110689</td>\n",
       "      <td>0.063251</td>\n",
       "      <td>0.365522</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>1.334533</td>\n",
       "      <td>1.481709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497958</th>\n",
       "      <td>2</td>\n",
       "      <td>257</td>\n",
       "      <td>90.49</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058366</td>\n",
       "      <td>0.352101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.032667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.840093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.462979</td>\n",
       "      <td>-0.291816</td>\n",
       "      <td>-2.142370</td>\n",
       "      <td>-1.380672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497959</th>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>53.68</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.511238</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.917143</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>1.956036</td>\n",
       "      <td>0.093145</td>\n",
       "      <td>0.018629</td>\n",
       "      <td>0.074516</td>\n",
       "      <td>0.085535</td>\n",
       "      <td>-0.122962</td>\n",
       "      <td>1.508674</td>\n",
       "      <td>-0.579206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497960</th>\n",
       "      <td>2</td>\n",
       "      <td>1553</td>\n",
       "      <td>6.14</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017386</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.227407</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>252.931596</td>\n",
       "      <td>1.628664</td>\n",
       "      <td>0.325733</td>\n",
       "      <td>0.651466</td>\n",
       "      <td>0.112413</td>\n",
       "      <td>-0.319560</td>\n",
       "      <td>1.427250</td>\n",
       "      <td>-1.957337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497961</th>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>71.89</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.133758</td>\n",
       "      <td>0.457898</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.423333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031847</td>\n",
       "      <td>0.019108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.183892</td>\n",
       "      <td>0.069551</td>\n",
       "      <td>0.041730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.547886</td>\n",
       "      <td>-0.502436</td>\n",
       "      <td>-2.263123</td>\n",
       "      <td>-2.100250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497962</th>\n",
       "      <td>4</td>\n",
       "      <td>380</td>\n",
       "      <td>17.38</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.044737</td>\n",
       "      <td>0.045737</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.022353</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>21.864212</td>\n",
       "      <td>0.345224</td>\n",
       "      <td>0.345224</td>\n",
       "      <td>0.115075</td>\n",
       "      <td>-0.127697</td>\n",
       "      <td>0.098907</td>\n",
       "      <td>-0.579296</td>\n",
       "      <td>0.504211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497963</th>\n",
       "      <td>2</td>\n",
       "      <td>1203</td>\n",
       "      <td>82.83</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.068853</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.805000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.004156</td>\n",
       "      <td>14.523723</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>0.060365</td>\n",
       "      <td>0.533351</td>\n",
       "      <td>-0.302982</td>\n",
       "      <td>1.737878</td>\n",
       "      <td>-1.260504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497964</th>\n",
       "      <td>3</td>\n",
       "      <td>1598</td>\n",
       "      <td>31.89</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017522</td>\n",
       "      <td>0.019956</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.138929</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.004380</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>50.109752</td>\n",
       "      <td>0.156789</td>\n",
       "      <td>0.219505</td>\n",
       "      <td>0.031358</td>\n",
       "      <td>-0.427080</td>\n",
       "      <td>-0.107822</td>\n",
       "      <td>-2.040401</td>\n",
       "      <td>-0.491006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497965</th>\n",
       "      <td>3</td>\n",
       "      <td>858</td>\n",
       "      <td>26.06</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>0.030373</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.212000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>32.924021</td>\n",
       "      <td>0.191865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191865</td>\n",
       "      <td>0.544451</td>\n",
       "      <td>-0.102936</td>\n",
       "      <td>1.894366</td>\n",
       "      <td>-0.498546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497966</th>\n",
       "      <td>6</td>\n",
       "      <td>449</td>\n",
       "      <td>47.71</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004454</td>\n",
       "      <td>0.106258</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.855000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.017817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.411025</td>\n",
       "      <td>0.083840</td>\n",
       "      <td>0.167680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.249344</td>\n",
       "      <td>0.526457</td>\n",
       "      <td>-1.385854</td>\n",
       "      <td>1.224655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497967</th>\n",
       "      <td>4</td>\n",
       "      <td>848</td>\n",
       "      <td>61.01</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.071946</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.336667</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>0.008255</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>13.899361</td>\n",
       "      <td>0.098345</td>\n",
       "      <td>0.114735</td>\n",
       "      <td>0.065563</td>\n",
       "      <td>0.508678</td>\n",
       "      <td>0.114009</td>\n",
       "      <td>1.209460</td>\n",
       "      <td>0.461805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497968</th>\n",
       "      <td>3</td>\n",
       "      <td>1191</td>\n",
       "      <td>69.48</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.058338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.740000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.141623</td>\n",
       "      <td>0.057571</td>\n",
       "      <td>0.086356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.257001</td>\n",
       "      <td>-0.075054</td>\n",
       "      <td>-1.558675</td>\n",
       "      <td>-0.304257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497969</th>\n",
       "      <td>6</td>\n",
       "      <td>583</td>\n",
       "      <td>70.73</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>0.121321</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.841250</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.242613</td>\n",
       "      <td>0.113106</td>\n",
       "      <td>0.014138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.398151</td>\n",
       "      <td>0.508976</td>\n",
       "      <td>-1.887037</td>\n",
       "      <td>1.748286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497970</th>\n",
       "      <td>6</td>\n",
       "      <td>229</td>\n",
       "      <td>64.02</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.074236</td>\n",
       "      <td>0.279563</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.765882</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030568</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.577007</td>\n",
       "      <td>0.109341</td>\n",
       "      <td>0.046860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.496375</td>\n",
       "      <td>0.501370</td>\n",
       "      <td>-2.147514</td>\n",
       "      <td>2.030763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497971</th>\n",
       "      <td>6</td>\n",
       "      <td>411</td>\n",
       "      <td>62.33</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>0.151655</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.308519</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.026764</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>6.593936</td>\n",
       "      <td>0.176480</td>\n",
       "      <td>0.032087</td>\n",
       "      <td>0.016044</td>\n",
       "      <td>-0.411929</td>\n",
       "      <td>0.488627</td>\n",
       "      <td>-1.692066</td>\n",
       "      <td>2.316603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497972</th>\n",
       "      <td>1</td>\n",
       "      <td>685</td>\n",
       "      <td>29.13</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.030657</td>\n",
       "      <td>0.042526</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.387143</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.013139</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>23.515276</td>\n",
       "      <td>0.034329</td>\n",
       "      <td>0.308960</td>\n",
       "      <td>0.171644</td>\n",
       "      <td>0.377302</td>\n",
       "      <td>-0.510459</td>\n",
       "      <td>1.913296</td>\n",
       "      <td>-2.275492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497973</th>\n",
       "      <td>1</td>\n",
       "      <td>1646</td>\n",
       "      <td>13.33</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004860</td>\n",
       "      <td>0.008098</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.666250</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.006683</td>\n",
       "      <td>0.006075</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>123.480870</td>\n",
       "      <td>0.825206</td>\n",
       "      <td>0.750188</td>\n",
       "      <td>0.075019</td>\n",
       "      <td>-0.144314</td>\n",
       "      <td>-0.481809</td>\n",
       "      <td>-1.318567</td>\n",
       "      <td>-1.700728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497974</th>\n",
       "      <td>3</td>\n",
       "      <td>1549</td>\n",
       "      <td>23.01</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012266</td>\n",
       "      <td>0.014855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.211053</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>67.318557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217297</td>\n",
       "      <td>0.043459</td>\n",
       "      <td>-0.371256</td>\n",
       "      <td>-0.102658</td>\n",
       "      <td>-1.896519</td>\n",
       "      <td>-0.482931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497975</th>\n",
       "      <td>6</td>\n",
       "      <td>1331</td>\n",
       "      <td>35.55</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.015778</td>\n",
       "      <td>0.026709</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.692857</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.006011</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>37.440225</td>\n",
       "      <td>0.225035</td>\n",
       "      <td>0.112518</td>\n",
       "      <td>0.056259</td>\n",
       "      <td>-0.172472</td>\n",
       "      <td>0.493332</td>\n",
       "      <td>-0.591919</td>\n",
       "      <td>2.473187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497976</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>60.86</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>3.043000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.532727</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.328623</td>\n",
       "      <td>0.049293</td>\n",
       "      <td>0.147880</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>-0.217885</td>\n",
       "      <td>0.111022</td>\n",
       "      <td>-1.527506</td>\n",
       "      <td>0.472024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497977</th>\n",
       "      <td>3</td>\n",
       "      <td>1584</td>\n",
       "      <td>76.14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>0.048068</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.007368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>20.803783</td>\n",
       "      <td>0.078802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013134</td>\n",
       "      <td>-0.359569</td>\n",
       "      <td>-0.106157</td>\n",
       "      <td>-1.883287</td>\n",
       "      <td>-0.455039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497978</th>\n",
       "      <td>6</td>\n",
       "      <td>1508</td>\n",
       "      <td>46.81</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.031041</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.405000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.215339</td>\n",
       "      <td>0.064089</td>\n",
       "      <td>0.149541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.271931</td>\n",
       "      <td>0.524596</td>\n",
       "      <td>-1.409193</td>\n",
       "      <td>1.295561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497979</th>\n",
       "      <td>6</td>\n",
       "      <td>906</td>\n",
       "      <td>55.48</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.025386</td>\n",
       "      <td>0.061236</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.412174</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>16.330209</td>\n",
       "      <td>0.162221</td>\n",
       "      <td>0.036049</td>\n",
       "      <td>0.090123</td>\n",
       "      <td>0.351504</td>\n",
       "      <td>0.481314</td>\n",
       "      <td>1.957192</td>\n",
       "      <td>2.290627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497980</th>\n",
       "      <td>6</td>\n",
       "      <td>1125</td>\n",
       "      <td>66.48</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.059093</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.006222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>16.922383</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060168</td>\n",
       "      <td>0.507549</td>\n",
       "      <td>0.507919</td>\n",
       "      <td>1.079857</td>\n",
       "      <td>1.089849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497981</th>\n",
       "      <td>2</td>\n",
       "      <td>1561</td>\n",
       "      <td>86.14</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017937</td>\n",
       "      <td>0.055183</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.076429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.006406</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>18.121662</td>\n",
       "      <td>0.116090</td>\n",
       "      <td>0.046436</td>\n",
       "      <td>0.023218</td>\n",
       "      <td>-0.223493</td>\n",
       "      <td>-0.311944</td>\n",
       "      <td>-0.612456</td>\n",
       "      <td>-1.974560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497982</th>\n",
       "      <td>6</td>\n",
       "      <td>1562</td>\n",
       "      <td>48.35</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011524</td>\n",
       "      <td>0.030954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.686111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.306101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.497936</td>\n",
       "      <td>0.506739</td>\n",
       "      <td>-2.083967</td>\n",
       "      <td>2.013481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497983</th>\n",
       "      <td>5</td>\n",
       "      <td>1070</td>\n",
       "      <td>92.80</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.021495</td>\n",
       "      <td>0.086729</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.034783</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>11.530172</td>\n",
       "      <td>0.118534</td>\n",
       "      <td>0.010776</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.183623</td>\n",
       "      <td>0.284008</td>\n",
       "      <td>1.379803</td>\n",
       "      <td>1.823357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497984</th>\n",
       "      <td>4</td>\n",
       "      <td>1128</td>\n",
       "      <td>39.32</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.034858</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.638333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>28.687691</td>\n",
       "      <td>0.101729</td>\n",
       "      <td>0.025432</td>\n",
       "      <td>0.076297</td>\n",
       "      <td>-0.063716</td>\n",
       "      <td>0.083412</td>\n",
       "      <td>0.352569</td>\n",
       "      <td>0.543517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497985</th>\n",
       "      <td>6</td>\n",
       "      <td>172</td>\n",
       "      <td>61.32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098837</td>\n",
       "      <td>0.356512</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.607059</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.029070</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>2.804958</td>\n",
       "      <td>0.081539</td>\n",
       "      <td>0.032616</td>\n",
       "      <td>0.016308</td>\n",
       "      <td>-0.331533</td>\n",
       "      <td>0.496927</td>\n",
       "      <td>-1.555691</td>\n",
       "      <td>2.213828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497986</th>\n",
       "      <td>6</td>\n",
       "      <td>369</td>\n",
       "      <td>82.69</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.224092</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.890833</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.018970</td>\n",
       "      <td>0.005420</td>\n",
       "      <td>0.005420</td>\n",
       "      <td>4.462450</td>\n",
       "      <td>0.084654</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>-0.070648</td>\n",
       "      <td>0.500310</td>\n",
       "      <td>-0.527367</td>\n",
       "      <td>2.119238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497987</th>\n",
       "      <td>5</td>\n",
       "      <td>161</td>\n",
       "      <td>88.57</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111801</td>\n",
       "      <td>0.550124</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.920556</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.031056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.817771</td>\n",
       "      <td>0.079034</td>\n",
       "      <td>0.056453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.484007</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>-2.262990</td>\n",
       "      <td>1.453666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497988</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>50.17</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1.416667</td>\n",
       "      <td>4.180833</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.951176</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.239187</td>\n",
       "      <td>0.039864</td>\n",
       "      <td>0.199322</td>\n",
       "      <td>0.099661</td>\n",
       "      <td>0.444640</td>\n",
       "      <td>-0.105156</td>\n",
       "      <td>1.934557</td>\n",
       "      <td>-0.590354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497989</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>87.02</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.230769</td>\n",
       "      <td>6.693846</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.438750</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.149391</td>\n",
       "      <td>0.068950</td>\n",
       "      <td>0.091933</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>-0.259910</td>\n",
       "      <td>-0.493856</td>\n",
       "      <td>-1.526429</td>\n",
       "      <td>-2.059335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497990</th>\n",
       "      <td>5</td>\n",
       "      <td>1312</td>\n",
       "      <td>29.28</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013720</td>\n",
       "      <td>0.022317</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.626667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.004573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>44.808743</td>\n",
       "      <td>0.204918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170765</td>\n",
       "      <td>0.369721</td>\n",
       "      <td>0.283148</td>\n",
       "      <td>2.120246</td>\n",
       "      <td>1.692070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497991</th>\n",
       "      <td>6</td>\n",
       "      <td>1768</td>\n",
       "      <td>45.98</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>0.026007</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.196000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>38.451501</td>\n",
       "      <td>0.130492</td>\n",
       "      <td>0.086994</td>\n",
       "      <td>0.065246</td>\n",
       "      <td>0.215417</td>\n",
       "      <td>0.507956</td>\n",
       "      <td>0.365629</td>\n",
       "      <td>1.862777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497992</th>\n",
       "      <td>6</td>\n",
       "      <td>1331</td>\n",
       "      <td>86.17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015026</td>\n",
       "      <td>0.064741</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.308500</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.446211</td>\n",
       "      <td>0.034815</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.558711</td>\n",
       "      <td>0.495557</td>\n",
       "      <td>-2.205069</td>\n",
       "      <td>2.095035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497993</th>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>75.16</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.174375</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.684286</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.851517</td>\n",
       "      <td>0.039915</td>\n",
       "      <td>0.079830</td>\n",
       "      <td>0.039915</td>\n",
       "      <td>-0.064012</td>\n",
       "      <td>0.484775</td>\n",
       "      <td>0.356794</td>\n",
       "      <td>2.512816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497994</th>\n",
       "      <td>2</td>\n",
       "      <td>217</td>\n",
       "      <td>30.89</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027650</td>\n",
       "      <td>0.142350</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.148333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>0.004608</td>\n",
       "      <td>7.024927</td>\n",
       "      <td>0.097119</td>\n",
       "      <td>0.097119</td>\n",
       "      <td>0.032373</td>\n",
       "      <td>-0.212926</td>\n",
       "      <td>-0.290807</td>\n",
       "      <td>-1.399179</td>\n",
       "      <td>-1.320059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497995</th>\n",
       "      <td>3</td>\n",
       "      <td>1123</td>\n",
       "      <td>98.64</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013357</td>\n",
       "      <td>0.087836</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.576000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.008014</td>\n",
       "      <td>0.008014</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>11.384834</td>\n",
       "      <td>0.091241</td>\n",
       "      <td>0.091241</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>-0.221773</td>\n",
       "      <td>-0.090620</td>\n",
       "      <td>-1.666022</td>\n",
       "      <td>-0.446184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497996</th>\n",
       "      <td>2</td>\n",
       "      <td>404</td>\n",
       "      <td>39.61</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.074257</td>\n",
       "      <td>0.098045</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.320333</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.017327</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>10.199445</td>\n",
       "      <td>0.252461</td>\n",
       "      <td>0.176723</td>\n",
       "      <td>0.100985</td>\n",
       "      <td>0.127347</td>\n",
       "      <td>-0.316765</td>\n",
       "      <td>1.455288</td>\n",
       "      <td>-1.925460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497997</th>\n",
       "      <td>6</td>\n",
       "      <td>1724</td>\n",
       "      <td>61.73</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.035806</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.115333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>27.928074</td>\n",
       "      <td>0.097197</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.048599</td>\n",
       "      <td>0.055712</td>\n",
       "      <td>0.493124</td>\n",
       "      <td>0.396777</td>\n",
       "      <td>2.330573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497998</th>\n",
       "      <td>4</td>\n",
       "      <td>1072</td>\n",
       "      <td>48.65</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.018657</td>\n",
       "      <td>0.045382</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.432500</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.004664</td>\n",
       "      <td>22.034943</td>\n",
       "      <td>0.041110</td>\n",
       "      <td>0.164440</td>\n",
       "      <td>0.102775</td>\n",
       "      <td>0.392459</td>\n",
       "      <td>0.089610</td>\n",
       "      <td>2.167140</td>\n",
       "      <td>0.520323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497999</th>\n",
       "      <td>4</td>\n",
       "      <td>1146</td>\n",
       "      <td>83.19</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020942</td>\n",
       "      <td>0.072592</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.466250</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>13.775694</td>\n",
       "      <td>0.120207</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.036062</td>\n",
       "      <td>-0.018873</td>\n",
       "      <td>0.085490</td>\n",
       "      <td>0.346169</td>\n",
       "      <td>0.530277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498000</th>\n",
       "      <td>4</td>\n",
       "      <td>618</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012945</td>\n",
       "      <td>0.006440</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.008091</td>\n",
       "      <td>155.276382</td>\n",
       "      <td>0.502513</td>\n",
       "      <td>0.251256</td>\n",
       "      <td>1.256281</td>\n",
       "      <td>0.472828</td>\n",
       "      <td>0.093066</td>\n",
       "      <td>1.900436</td>\n",
       "      <td>0.471335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498001</th>\n",
       "      <td>4</td>\n",
       "      <td>1120</td>\n",
       "      <td>58.73</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.018750</td>\n",
       "      <td>0.052437</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.796667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>19.070322</td>\n",
       "      <td>0.170271</td>\n",
       "      <td>0.119190</td>\n",
       "      <td>0.085135</td>\n",
       "      <td>0.417751</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>2.159581</td>\n",
       "      <td>0.548878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498002</th>\n",
       "      <td>2</td>\n",
       "      <td>878</td>\n",
       "      <td>82.84</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012528</td>\n",
       "      <td>0.094351</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.530909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>10.598745</td>\n",
       "      <td>0.048286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>-0.275274</td>\n",
       "      <td>-0.298547</td>\n",
       "      <td>-1.514400</td>\n",
       "      <td>-1.433863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498003</th>\n",
       "      <td>2</td>\n",
       "      <td>881</td>\n",
       "      <td>35.51</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030647</td>\n",
       "      <td>0.040306</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.315185</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.005675</td>\n",
       "      <td>0.005675</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>24.809913</td>\n",
       "      <td>0.140805</td>\n",
       "      <td>0.140805</td>\n",
       "      <td>0.028161</td>\n",
       "      <td>-0.428842</td>\n",
       "      <td>-0.309141</td>\n",
       "      <td>-1.906032</td>\n",
       "      <td>-1.846047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498004</th>\n",
       "      <td>1</td>\n",
       "      <td>892</td>\n",
       "      <td>95.82</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.023543</td>\n",
       "      <td>0.107422</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.562857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.011211</td>\n",
       "      <td>0.006726</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>9.309121</td>\n",
       "      <td>0.104362</td>\n",
       "      <td>0.062617</td>\n",
       "      <td>0.031309</td>\n",
       "      <td>0.056918</td>\n",
       "      <td>-0.505272</td>\n",
       "      <td>0.323255</td>\n",
       "      <td>-2.369564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498005</th>\n",
       "      <td>4</td>\n",
       "      <td>1154</td>\n",
       "      <td>9.15</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018198</td>\n",
       "      <td>0.007929</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.435714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>126.120219</td>\n",
       "      <td>1.202186</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.109290</td>\n",
       "      <td>-0.311736</td>\n",
       "      <td>0.102476</td>\n",
       "      <td>-1.858715</td>\n",
       "      <td>0.462043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498006</th>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>50.93</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.101860</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.183125</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>9.817396</td>\n",
       "      <td>0.137444</td>\n",
       "      <td>0.098174</td>\n",
       "      <td>0.039270</td>\n",
       "      <td>-0.106676</td>\n",
       "      <td>-0.300231</td>\n",
       "      <td>-0.552954</td>\n",
       "      <td>-1.736231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498007</th>\n",
       "      <td>6</td>\n",
       "      <td>438</td>\n",
       "      <td>23.23</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.025114</td>\n",
       "      <td>0.053037</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.111818</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.004566</td>\n",
       "      <td>18.854929</td>\n",
       "      <td>0.172191</td>\n",
       "      <td>0.387430</td>\n",
       "      <td>0.086096</td>\n",
       "      <td>-0.042063</td>\n",
       "      <td>0.507938</td>\n",
       "      <td>-0.523065</td>\n",
       "      <td>2.124035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498008</th>\n",
       "      <td>5</td>\n",
       "      <td>580</td>\n",
       "      <td>10.63</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.018328</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.379643</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>54.562559</td>\n",
       "      <td>0.752587</td>\n",
       "      <td>0.282220</td>\n",
       "      <td>0.282220</td>\n",
       "      <td>-0.082119</td>\n",
       "      <td>0.282185</td>\n",
       "      <td>0.356861</td>\n",
       "      <td>1.975282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498009</th>\n",
       "      <td>5</td>\n",
       "      <td>1607</td>\n",
       "      <td>71.42</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.044443</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.806667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>22.500700</td>\n",
       "      <td>0.112013</td>\n",
       "      <td>0.084010</td>\n",
       "      <td>0.042005</td>\n",
       "      <td>0.323571</td>\n",
       "      <td>0.316354</td>\n",
       "      <td>0.399155</td>\n",
       "      <td>1.137945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498010</th>\n",
       "      <td>5</td>\n",
       "      <td>182</td>\n",
       "      <td>56.29</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.309286</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.036250</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>3.233256</td>\n",
       "      <td>0.124356</td>\n",
       "      <td>0.177651</td>\n",
       "      <td>0.071061</td>\n",
       "      <td>0.410503</td>\n",
       "      <td>0.308894</td>\n",
       "      <td>1.298575</td>\n",
       "      <td>1.347872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498011</th>\n",
       "      <td>2</td>\n",
       "      <td>873</td>\n",
       "      <td>56.69</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028637</td>\n",
       "      <td>0.064937</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.267600</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.399541</td>\n",
       "      <td>0.088199</td>\n",
       "      <td>0.088199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.582822</td>\n",
       "      <td>-0.304221</td>\n",
       "      <td>-2.523409</td>\n",
       "      <td>-1.617010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498012</th>\n",
       "      <td>5</td>\n",
       "      <td>1398</td>\n",
       "      <td>27.71</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.019821</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.065769</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.007153</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>50.451101</td>\n",
       "      <td>0.360881</td>\n",
       "      <td>0.180440</td>\n",
       "      <td>0.072176</td>\n",
       "      <td>-0.213236</td>\n",
       "      <td>0.290083</td>\n",
       "      <td>-0.611810</td>\n",
       "      <td>1.999489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498013</th>\n",
       "      <td>5</td>\n",
       "      <td>1014</td>\n",
       "      <td>63.96</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.063077</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.995000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>15.853659</td>\n",
       "      <td>0.031270</td>\n",
       "      <td>0.093809</td>\n",
       "      <td>0.031270</td>\n",
       "      <td>-0.021911</td>\n",
       "      <td>0.308046</td>\n",
       "      <td>-0.489122</td>\n",
       "      <td>1.506339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498014</th>\n",
       "      <td>5</td>\n",
       "      <td>775</td>\n",
       "      <td>40.67</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016774</td>\n",
       "      <td>0.052477</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.128462</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>19.055815</td>\n",
       "      <td>0.024588</td>\n",
       "      <td>0.245881</td>\n",
       "      <td>0.098353</td>\n",
       "      <td>0.299922</td>\n",
       "      <td>0.301256</td>\n",
       "      <td>1.405477</td>\n",
       "      <td>1.577039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498015</th>\n",
       "      <td>4</td>\n",
       "      <td>1286</td>\n",
       "      <td>58.18</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006998</td>\n",
       "      <td>0.045241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.464444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>22.103816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051564</td>\n",
       "      <td>0.017188</td>\n",
       "      <td>-0.259565</td>\n",
       "      <td>0.105410</td>\n",
       "      <td>-1.556376</td>\n",
       "      <td>0.435602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498016</th>\n",
       "      <td>1</td>\n",
       "      <td>603</td>\n",
       "      <td>71.47</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.118524</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.735000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.018242</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>8.437106</td>\n",
       "      <td>0.153911</td>\n",
       "      <td>0.027984</td>\n",
       "      <td>0.041976</td>\n",
       "      <td>0.369656</td>\n",
       "      <td>-0.483745</td>\n",
       "      <td>0.332684</td>\n",
       "      <td>-0.963786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498017</th>\n",
       "      <td>2</td>\n",
       "      <td>209</td>\n",
       "      <td>91.97</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086124</td>\n",
       "      <td>0.440048</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.109444</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043062</td>\n",
       "      <td>0.014354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.272480</td>\n",
       "      <td>0.097858</td>\n",
       "      <td>0.032619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.487168</td>\n",
       "      <td>-0.298225</td>\n",
       "      <td>-2.261619</td>\n",
       "      <td>-1.476540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498018</th>\n",
       "      <td>5</td>\n",
       "      <td>1001</td>\n",
       "      <td>74.24</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008991</td>\n",
       "      <td>0.074166</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.248889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.005994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.483297</td>\n",
       "      <td>0.094289</td>\n",
       "      <td>0.080819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375193</td>\n",
       "      <td>0.313870</td>\n",
       "      <td>-2.112742</td>\n",
       "      <td>1.316592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498019</th>\n",
       "      <td>5</td>\n",
       "      <td>1324</td>\n",
       "      <td>28.98</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.021888</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.980000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>45.686680</td>\n",
       "      <td>0.069013</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.138026</td>\n",
       "      <td>0.604215</td>\n",
       "      <td>0.314179</td>\n",
       "      <td>3.388991</td>\n",
       "      <td>0.134462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498020</th>\n",
       "      <td>5</td>\n",
       "      <td>951</td>\n",
       "      <td>10.74</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005258</td>\n",
       "      <td>0.011293</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.148000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009464</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.005258</td>\n",
       "      <td>88.547486</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.186220</td>\n",
       "      <td>0.465549</td>\n",
       "      <td>0.580511</td>\n",
       "      <td>0.300506</td>\n",
       "      <td>1.736223</td>\n",
       "      <td>1.153623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498021</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>41.16</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>10.290000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.646400</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.097182</td>\n",
       "      <td>0.145773</td>\n",
       "      <td>0.170068</td>\n",
       "      <td>0.097182</td>\n",
       "      <td>0.164367</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>1.208001</td>\n",
       "      <td>0.006072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498022</th>\n",
       "      <td>6</td>\n",
       "      <td>1341</td>\n",
       "      <td>86.28</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.064340</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.392500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.002983</td>\n",
       "      <td>15.542420</td>\n",
       "      <td>0.069541</td>\n",
       "      <td>0.115902</td>\n",
       "      <td>0.046361</td>\n",
       "      <td>0.304757</td>\n",
       "      <td>0.500109</td>\n",
       "      <td>1.370524</td>\n",
       "      <td>2.253173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498023</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>56.76</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>4.054286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.352000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246653</td>\n",
       "      <td>0.088090</td>\n",
       "      <td>0.017618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.380183</td>\n",
       "      <td>-0.288317</td>\n",
       "      <td>-1.854480</td>\n",
       "      <td>-1.200103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498024</th>\n",
       "      <td>4</td>\n",
       "      <td>588</td>\n",
       "      <td>40.94</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.069626</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.274444</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>14.362482</td>\n",
       "      <td>0.024426</td>\n",
       "      <td>0.097704</td>\n",
       "      <td>0.122130</td>\n",
       "      <td>0.378489</td>\n",
       "      <td>0.086512</td>\n",
       "      <td>2.127613</td>\n",
       "      <td>0.537959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498025</th>\n",
       "      <td>3</td>\n",
       "      <td>1580</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010759</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.350588</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.003165</td>\n",
       "      <td>265.100671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.671141</td>\n",
       "      <td>0.838926</td>\n",
       "      <td>0.372136</td>\n",
       "      <td>-0.112801</td>\n",
       "      <td>2.034317</td>\n",
       "      <td>-0.552250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498026</th>\n",
       "      <td>1</td>\n",
       "      <td>1814</td>\n",
       "      <td>12.55</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>0.006918</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.140909</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.004961</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>144.541833</td>\n",
       "      <td>0.318725</td>\n",
       "      <td>0.717131</td>\n",
       "      <td>0.318725</td>\n",
       "      <td>0.323437</td>\n",
       "      <td>-0.496505</td>\n",
       "      <td>1.259555</td>\n",
       "      <td>-2.058402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498027</th>\n",
       "      <td>1</td>\n",
       "      <td>847</td>\n",
       "      <td>54.17</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028335</td>\n",
       "      <td>0.063955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.257083</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011806</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>15.635961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184604</td>\n",
       "      <td>0.036921</td>\n",
       "      <td>-0.195251</td>\n",
       "      <td>-0.504122</td>\n",
       "      <td>-0.658638</td>\n",
       "      <td>-2.439608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498028</th>\n",
       "      <td>2</td>\n",
       "      <td>856</td>\n",
       "      <td>42.39</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>0.049521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.027857</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>20.193442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117952</td>\n",
       "      <td>0.117952</td>\n",
       "      <td>0.428977</td>\n",
       "      <td>-0.308174</td>\n",
       "      <td>2.082868</td>\n",
       "      <td>-1.612717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498029</th>\n",
       "      <td>3</td>\n",
       "      <td>1380</td>\n",
       "      <td>92.66</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013768</td>\n",
       "      <td>0.067145</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.876842</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>14.893158</td>\n",
       "      <td>0.043169</td>\n",
       "      <td>0.032376</td>\n",
       "      <td>0.053961</td>\n",
       "      <td>0.391932</td>\n",
       "      <td>-0.113856</td>\n",
       "      <td>2.113660</td>\n",
       "      <td>-0.578362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498030</th>\n",
       "      <td>2</td>\n",
       "      <td>560</td>\n",
       "      <td>51.69</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030357</td>\n",
       "      <td>0.092304</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.040588</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>10.833817</td>\n",
       "      <td>0.193461</td>\n",
       "      <td>0.096731</td>\n",
       "      <td>0.019346</td>\n",
       "      <td>-0.286111</td>\n",
       "      <td>-0.297724</td>\n",
       "      <td>-1.692565</td>\n",
       "      <td>-1.584580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498031</th>\n",
       "      <td>3</td>\n",
       "      <td>865</td>\n",
       "      <td>1.37</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017341</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.091333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.012717</td>\n",
       "      <td>0.008092</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>631.386861</td>\n",
       "      <td>8.029197</td>\n",
       "      <td>5.109489</td>\n",
       "      <td>1.459854</td>\n",
       "      <td>-0.076531</td>\n",
       "      <td>-0.096455</td>\n",
       "      <td>-0.581668</td>\n",
       "      <td>-0.510481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498032</th>\n",
       "      <td>1</td>\n",
       "      <td>1377</td>\n",
       "      <td>44.34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.021060</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.528966</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>31.055480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045106</td>\n",
       "      <td>0.045106</td>\n",
       "      <td>-0.312384</td>\n",
       "      <td>-0.518153</td>\n",
       "      <td>-0.633360</td>\n",
       "      <td>-2.528799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498033</th>\n",
       "      <td>4</td>\n",
       "      <td>1162</td>\n",
       "      <td>50.89</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.010327</td>\n",
       "      <td>0.043795</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.240833</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.006885</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>22.833563</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>0.058951</td>\n",
       "      <td>0.107338</td>\n",
       "      <td>0.097948</td>\n",
       "      <td>0.396946</td>\n",
       "      <td>0.495607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498034</th>\n",
       "      <td>4</td>\n",
       "      <td>442</td>\n",
       "      <td>93.62</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042986</td>\n",
       "      <td>0.211810</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.927368</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.721213</td>\n",
       "      <td>0.106815</td>\n",
       "      <td>0.106815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.440895</td>\n",
       "      <td>0.108936</td>\n",
       "      <td>-2.268001</td>\n",
       "      <td>0.498674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498035</th>\n",
       "      <td>6</td>\n",
       "      <td>426</td>\n",
       "      <td>86.54</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.203146</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.817500</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>4.922579</td>\n",
       "      <td>0.069332</td>\n",
       "      <td>0.034666</td>\n",
       "      <td>0.023111</td>\n",
       "      <td>-0.013686</td>\n",
       "      <td>0.505769</td>\n",
       "      <td>-0.504177</td>\n",
       "      <td>1.956118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498036</th>\n",
       "      <td>3</td>\n",
       "      <td>1115</td>\n",
       "      <td>12.44</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008072</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.382222</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.006278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.630225</td>\n",
       "      <td>0.160772</td>\n",
       "      <td>0.562701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.419783</td>\n",
       "      <td>-0.087195</td>\n",
       "      <td>-2.167825</td>\n",
       "      <td>-0.411364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498037</th>\n",
       "      <td>2</td>\n",
       "      <td>994</td>\n",
       "      <td>51.18</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>0.051489</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.795000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>19.421649</td>\n",
       "      <td>0.175850</td>\n",
       "      <td>0.195389</td>\n",
       "      <td>0.078156</td>\n",
       "      <td>0.512460</td>\n",
       "      <td>-0.283281</td>\n",
       "      <td>1.147365</td>\n",
       "      <td>-1.182094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498038</th>\n",
       "      <td>6</td>\n",
       "      <td>1006</td>\n",
       "      <td>76.66</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.020875</td>\n",
       "      <td>0.076203</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.650476</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>13.122880</td>\n",
       "      <td>0.026089</td>\n",
       "      <td>0.130446</td>\n",
       "      <td>0.052178</td>\n",
       "      <td>0.221764</td>\n",
       "      <td>0.493659</td>\n",
       "      <td>1.374972</td>\n",
       "      <td>2.395316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498039</th>\n",
       "      <td>3</td>\n",
       "      <td>1377</td>\n",
       "      <td>87.61</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>0.063624</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.964545</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.717384</td>\n",
       "      <td>0.068485</td>\n",
       "      <td>0.022828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.432407</td>\n",
       "      <td>-0.092852</td>\n",
       "      <td>-2.211316</td>\n",
       "      <td>-0.455448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498040</th>\n",
       "      <td>5</td>\n",
       "      <td>695</td>\n",
       "      <td>3.77</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011511</td>\n",
       "      <td>0.005424</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.471250</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>184.350133</td>\n",
       "      <td>0.530504</td>\n",
       "      <td>0.530504</td>\n",
       "      <td>1.326260</td>\n",
       "      <td>0.480751</td>\n",
       "      <td>0.294168</td>\n",
       "      <td>1.935878</td>\n",
       "      <td>1.365138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498041</th>\n",
       "      <td>4</td>\n",
       "      <td>235</td>\n",
       "      <td>58.68</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.046809</td>\n",
       "      <td>0.249702</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.334545</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.038298</td>\n",
       "      <td>0.012766</td>\n",
       "      <td>4.004772</td>\n",
       "      <td>0.170416</td>\n",
       "      <td>0.153374</td>\n",
       "      <td>0.051125</td>\n",
       "      <td>0.189705</td>\n",
       "      <td>0.107790</td>\n",
       "      <td>0.357380</td>\n",
       "      <td>0.473245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498042</th>\n",
       "      <td>5</td>\n",
       "      <td>1580</td>\n",
       "      <td>59.92</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>0.037924</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.960000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.003165</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>26.368491</td>\n",
       "      <td>0.083445</td>\n",
       "      <td>0.150200</td>\n",
       "      <td>0.066756</td>\n",
       "      <td>0.591405</td>\n",
       "      <td>0.320574</td>\n",
       "      <td>0.931662</td>\n",
       "      <td>0.800498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498043</th>\n",
       "      <td>3</td>\n",
       "      <td>1467</td>\n",
       "      <td>86.97</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.059284</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.663333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>16.867885</td>\n",
       "      <td>0.080488</td>\n",
       "      <td>0.068989</td>\n",
       "      <td>0.011498</td>\n",
       "      <td>-0.184398</td>\n",
       "      <td>-0.088165</td>\n",
       "      <td>-1.532502</td>\n",
       "      <td>-0.446634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498044</th>\n",
       "      <td>2</td>\n",
       "      <td>436</td>\n",
       "      <td>56.54</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.043578</td>\n",
       "      <td>0.129679</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.975789</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.011468</td>\n",
       "      <td>7.711355</td>\n",
       "      <td>0.159179</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>0.088433</td>\n",
       "      <td>0.398903</td>\n",
       "      <td>-0.314244</td>\n",
       "      <td>2.098449</td>\n",
       "      <td>-1.709724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498045</th>\n",
       "      <td>1</td>\n",
       "      <td>276</td>\n",
       "      <td>55.27</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.200254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.047037</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.018116</td>\n",
       "      <td>4.993667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162837</td>\n",
       "      <td>0.090465</td>\n",
       "      <td>0.313270</td>\n",
       "      <td>-0.516511</td>\n",
       "      <td>1.872921</td>\n",
       "      <td>-2.332967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498046</th>\n",
       "      <td>3</td>\n",
       "      <td>1150</td>\n",
       "      <td>31.81</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>0.027661</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.446923</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>36.152153</td>\n",
       "      <td>0.062873</td>\n",
       "      <td>0.251493</td>\n",
       "      <td>0.062873</td>\n",
       "      <td>-0.082362</td>\n",
       "      <td>-0.095233</td>\n",
       "      <td>-0.550952</td>\n",
       "      <td>-0.499150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498047</th>\n",
       "      <td>3</td>\n",
       "      <td>1257</td>\n",
       "      <td>68.08</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012729</td>\n",
       "      <td>0.054161</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.255000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>18.463572</td>\n",
       "      <td>0.029377</td>\n",
       "      <td>0.058754</td>\n",
       "      <td>0.014689</td>\n",
       "      <td>-0.319074</td>\n",
       "      <td>-0.099784</td>\n",
       "      <td>-1.836191</td>\n",
       "      <td>-0.450929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498048</th>\n",
       "      <td>6</td>\n",
       "      <td>1329</td>\n",
       "      <td>90.91</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020316</td>\n",
       "      <td>0.068405</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.367037</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.618854</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.109999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.577908</td>\n",
       "      <td>0.498470</td>\n",
       "      <td>-2.215459</td>\n",
       "      <td>2.116667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498049</th>\n",
       "      <td>6</td>\n",
       "      <td>74</td>\n",
       "      <td>39.14</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>0.528919</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.348889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>1.890649</td>\n",
       "      <td>0.229944</td>\n",
       "      <td>0.025549</td>\n",
       "      <td>0.076648</td>\n",
       "      <td>0.140629</td>\n",
       "      <td>0.499588</td>\n",
       "      <td>0.382790</td>\n",
       "      <td>1.905114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498050</th>\n",
       "      <td>5</td>\n",
       "      <td>296</td>\n",
       "      <td>90.15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101351</td>\n",
       "      <td>0.304561</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.005000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.283417</td>\n",
       "      <td>0.033278</td>\n",
       "      <td>0.044370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.643651</td>\n",
       "      <td>0.289025</td>\n",
       "      <td>-2.446935</td>\n",
       "      <td>1.607977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498051</th>\n",
       "      <td>5</td>\n",
       "      <td>550</td>\n",
       "      <td>39.44</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.071709</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.440000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>13.945233</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.050710</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>-0.086355</td>\n",
       "      <td>0.317151</td>\n",
       "      <td>-0.224669</td>\n",
       "      <td>0.960902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498052</th>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>97.15</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.630844</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48.575000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>1.585178</td>\n",
       "      <td>0.072054</td>\n",
       "      <td>0.020587</td>\n",
       "      <td>0.051467</td>\n",
       "      <td>0.758849</td>\n",
       "      <td>-0.489491</td>\n",
       "      <td>1.198422</td>\n",
       "      <td>-0.940836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498053</th>\n",
       "      <td>4</td>\n",
       "      <td>1674</td>\n",
       "      <td>22.70</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.013560</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.675000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>73.744493</td>\n",
       "      <td>0.308370</td>\n",
       "      <td>0.176211</td>\n",
       "      <td>0.220264</td>\n",
       "      <td>0.618356</td>\n",
       "      <td>0.104645</td>\n",
       "      <td>1.677158</td>\n",
       "      <td>0.414093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498054</th>\n",
       "      <td>5</td>\n",
       "      <td>1542</td>\n",
       "      <td>79.01</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.019455</td>\n",
       "      <td>0.051239</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.633667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.003243</td>\n",
       "      <td>19.516517</td>\n",
       "      <td>0.075940</td>\n",
       "      <td>0.050627</td>\n",
       "      <td>0.063283</td>\n",
       "      <td>0.279533</td>\n",
       "      <td>0.276435</td>\n",
       "      <td>2.125745</td>\n",
       "      <td>1.806038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498055</th>\n",
       "      <td>2</td>\n",
       "      <td>1210</td>\n",
       "      <td>83.79</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021488</td>\n",
       "      <td>0.069248</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.222692</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>0.004959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.440864</td>\n",
       "      <td>0.107411</td>\n",
       "      <td>0.071608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.557716</td>\n",
       "      <td>-0.302632</td>\n",
       "      <td>-2.440468</td>\n",
       "      <td>-1.617686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498056</th>\n",
       "      <td>5</td>\n",
       "      <td>289</td>\n",
       "      <td>80.54</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103806</td>\n",
       "      <td>0.278685</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.684667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.588279</td>\n",
       "      <td>0.037249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.675826</td>\n",
       "      <td>0.284363</td>\n",
       "      <td>-2.426243</td>\n",
       "      <td>1.566113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498057</th>\n",
       "      <td>6</td>\n",
       "      <td>1208</td>\n",
       "      <td>44.88</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.020695</td>\n",
       "      <td>0.037152</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.795200</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>26.916221</td>\n",
       "      <td>0.222816</td>\n",
       "      <td>0.200535</td>\n",
       "      <td>0.044563</td>\n",
       "      <td>-0.167101</td>\n",
       "      <td>0.495667</td>\n",
       "      <td>-0.624764</td>\n",
       "      <td>2.493826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498058</th>\n",
       "      <td>2</td>\n",
       "      <td>1401</td>\n",
       "      <td>17.78</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.012691</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.780000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>78.796400</td>\n",
       "      <td>0.562430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056243</td>\n",
       "      <td>-0.025829</td>\n",
       "      <td>-0.279715</td>\n",
       "      <td>3.210981</td>\n",
       "      <td>0.048958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498059</th>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>26.42</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.287174</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.210000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>3.482210</td>\n",
       "      <td>0.302801</td>\n",
       "      <td>0.227101</td>\n",
       "      <td>0.113550</td>\n",
       "      <td>0.356206</td>\n",
       "      <td>-0.081713</td>\n",
       "      <td>0.458718</td>\n",
       "      <td>-0.467975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498060</th>\n",
       "      <td>1</td>\n",
       "      <td>513</td>\n",
       "      <td>88.74</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015595</td>\n",
       "      <td>0.172982</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.092500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.013645</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.007797</td>\n",
       "      <td>5.780933</td>\n",
       "      <td>0.078882</td>\n",
       "      <td>0.022538</td>\n",
       "      <td>0.045076</td>\n",
       "      <td>0.358400</td>\n",
       "      <td>-0.499654</td>\n",
       "      <td>1.207683</td>\n",
       "      <td>-1.849035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498061</th>\n",
       "      <td>5</td>\n",
       "      <td>1161</td>\n",
       "      <td>80.91</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.018949</td>\n",
       "      <td>0.069690</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.677727</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.008613</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>14.349277</td>\n",
       "      <td>0.123594</td>\n",
       "      <td>0.061797</td>\n",
       "      <td>0.049438</td>\n",
       "      <td>0.215484</td>\n",
       "      <td>0.289231</td>\n",
       "      <td>1.471976</td>\n",
       "      <td>1.844743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498062</th>\n",
       "      <td>6</td>\n",
       "      <td>1219</td>\n",
       "      <td>90.48</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009024</td>\n",
       "      <td>0.074225</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.225455</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>13.472591</td>\n",
       "      <td>0.088417</td>\n",
       "      <td>0.088417</td>\n",
       "      <td>0.044209</td>\n",
       "      <td>0.366828</td>\n",
       "      <td>0.503845</td>\n",
       "      <td>1.267551</td>\n",
       "      <td>1.988822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498063</th>\n",
       "      <td>5</td>\n",
       "      <td>1006</td>\n",
       "      <td>19.05</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015905</td>\n",
       "      <td>0.018936</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.190625</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.007952</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>52.808399</td>\n",
       "      <td>0.419948</td>\n",
       "      <td>0.524934</td>\n",
       "      <td>0.157480</td>\n",
       "      <td>0.108321</td>\n",
       "      <td>0.302500</td>\n",
       "      <td>0.356638</td>\n",
       "      <td>1.712943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498064</th>\n",
       "      <td>3</td>\n",
       "      <td>1311</td>\n",
       "      <td>9.85</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.012967</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.579412</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>133.096447</td>\n",
       "      <td>0.101523</td>\n",
       "      <td>1.015228</td>\n",
       "      <td>0.304569</td>\n",
       "      <td>0.056719</td>\n",
       "      <td>-0.100213</td>\n",
       "      <td>0.369381</td>\n",
       "      <td>-0.576603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498065</th>\n",
       "      <td>5</td>\n",
       "      <td>1345</td>\n",
       "      <td>61.57</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.045777</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.676957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>21.845054</td>\n",
       "      <td>0.064967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064967</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.280736</td>\n",
       "      <td>1.504133</td>\n",
       "      <td>1.936499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498066</th>\n",
       "      <td>6</td>\n",
       "      <td>80</td>\n",
       "      <td>64.53</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.806625</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.585000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.239733</td>\n",
       "      <td>0.061987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077483</td>\n",
       "      <td>0.372415</td>\n",
       "      <td>0.482587</td>\n",
       "      <td>1.873563</td>\n",
       "      <td>2.192983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498067</th>\n",
       "      <td>2</td>\n",
       "      <td>1623</td>\n",
       "      <td>12.69</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.007819</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.345000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>127.895981</td>\n",
       "      <td>0.630418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236407</td>\n",
       "      <td>0.273070</td>\n",
       "      <td>-0.290999</td>\n",
       "      <td>0.547632</td>\n",
       "      <td>-1.137739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498068</th>\n",
       "      <td>3</td>\n",
       "      <td>361</td>\n",
       "      <td>79.04</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022161</td>\n",
       "      <td>0.218947</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.880000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027701</td>\n",
       "      <td>0.008310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.567308</td>\n",
       "      <td>0.126518</td>\n",
       "      <td>0.037955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.366288</td>\n",
       "      <td>-0.087476</td>\n",
       "      <td>-2.002003</td>\n",
       "      <td>-0.397231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498069</th>\n",
       "      <td>2</td>\n",
       "      <td>1501</td>\n",
       "      <td>59.95</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.039940</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.987500</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>25.037531</td>\n",
       "      <td>0.133445</td>\n",
       "      <td>0.133445</td>\n",
       "      <td>0.083403</td>\n",
       "      <td>0.682855</td>\n",
       "      <td>-0.288454</td>\n",
       "      <td>1.605850</td>\n",
       "      <td>-1.109372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498070</th>\n",
       "      <td>6</td>\n",
       "      <td>1137</td>\n",
       "      <td>6.33</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023747</td>\n",
       "      <td>0.005567</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.234444</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179.620853</td>\n",
       "      <td>0.789889</td>\n",
       "      <td>0.631912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.629039</td>\n",
       "      <td>0.491641</td>\n",
       "      <td>-2.294293</td>\n",
       "      <td>2.207567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498071</th>\n",
       "      <td>2</td>\n",
       "      <td>948</td>\n",
       "      <td>38.69</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.040812</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.448333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.502455</td>\n",
       "      <td>0.103386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.418069</td>\n",
       "      <td>-0.291281</td>\n",
       "      <td>-2.071975</td>\n",
       "      <td>-1.265852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498072</th>\n",
       "      <td>5</td>\n",
       "      <td>1473</td>\n",
       "      <td>59.90</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0.040665</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.990000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>24.590985</td>\n",
       "      <td>0.033389</td>\n",
       "      <td>0.166945</td>\n",
       "      <td>0.083472</td>\n",
       "      <td>0.537267</td>\n",
       "      <td>0.302683</td>\n",
       "      <td>1.916720</td>\n",
       "      <td>1.386184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498073</th>\n",
       "      <td>2</td>\n",
       "      <td>165</td>\n",
       "      <td>95.42</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.157576</td>\n",
       "      <td>0.578303</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>1.729197</td>\n",
       "      <td>0.094320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031440</td>\n",
       "      <td>-0.048566</td>\n",
       "      <td>-0.317767</td>\n",
       "      <td>0.336128</td>\n",
       "      <td>-1.880551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498074</th>\n",
       "      <td>6</td>\n",
       "      <td>1539</td>\n",
       "      <td>42.31</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>0.027492</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.839565</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.374380</td>\n",
       "      <td>0.023635</td>\n",
       "      <td>0.070905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.601733</td>\n",
       "      <td>0.493902</td>\n",
       "      <td>-2.266427</td>\n",
       "      <td>2.160896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498075</th>\n",
       "      <td>6</td>\n",
       "      <td>1543</td>\n",
       "      <td>53.43</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016850</td>\n",
       "      <td>0.034627</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.055000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>28.878907</td>\n",
       "      <td>0.187161</td>\n",
       "      <td>0.149729</td>\n",
       "      <td>0.018716</td>\n",
       "      <td>-0.365887</td>\n",
       "      <td>0.496422</td>\n",
       "      <td>-1.670767</td>\n",
       "      <td>2.344311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498076</th>\n",
       "      <td>6</td>\n",
       "      <td>1196</td>\n",
       "      <td>78.10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.065301</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.620000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.003344</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>15.313700</td>\n",
       "      <td>0.051216</td>\n",
       "      <td>0.076825</td>\n",
       "      <td>0.025608</td>\n",
       "      <td>0.046391</td>\n",
       "      <td>0.512911</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>1.882704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498077</th>\n",
       "      <td>4</td>\n",
       "      <td>676</td>\n",
       "      <td>65.13</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013314</td>\n",
       "      <td>0.096346</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.236667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.008876</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>10.379242</td>\n",
       "      <td>0.122831</td>\n",
       "      <td>0.092123</td>\n",
       "      <td>0.015354</td>\n",
       "      <td>-0.186779</td>\n",
       "      <td>0.111434</td>\n",
       "      <td>-1.541044</td>\n",
       "      <td>0.441140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498078</th>\n",
       "      <td>5</td>\n",
       "      <td>1100</td>\n",
       "      <td>46.66</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.042418</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.332000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.006364</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>23.574796</td>\n",
       "      <td>0.150021</td>\n",
       "      <td>0.171453</td>\n",
       "      <td>0.042863</td>\n",
       "      <td>0.068788</td>\n",
       "      <td>0.315959</td>\n",
       "      <td>-0.455508</td>\n",
       "      <td>1.325383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498079</th>\n",
       "      <td>6</td>\n",
       "      <td>311</td>\n",
       "      <td>29.78</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.086817</td>\n",
       "      <td>0.095756</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.102963</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.025723</td>\n",
       "      <td>0.012862</td>\n",
       "      <td>10.443251</td>\n",
       "      <td>0.033580</td>\n",
       "      <td>0.268637</td>\n",
       "      <td>0.134318</td>\n",
       "      <td>0.119375</td>\n",
       "      <td>0.484379</td>\n",
       "      <td>1.382392</td>\n",
       "      <td>2.459363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498080</th>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "      <td>69.82</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.030357</td>\n",
       "      <td>0.124679</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.107059</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>8.020624</td>\n",
       "      <td>0.143225</td>\n",
       "      <td>0.042968</td>\n",
       "      <td>0.071613</td>\n",
       "      <td>0.439937</td>\n",
       "      <td>-0.510433</td>\n",
       "      <td>1.927774</td>\n",
       "      <td>-2.153347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498081</th>\n",
       "      <td>6</td>\n",
       "      <td>1733</td>\n",
       "      <td>43.08</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.024859</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.958182</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.004616</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>40.227484</td>\n",
       "      <td>0.046425</td>\n",
       "      <td>0.185701</td>\n",
       "      <td>0.092851</td>\n",
       "      <td>0.183670</td>\n",
       "      <td>0.490147</td>\n",
       "      <td>1.409201</td>\n",
       "      <td>2.442498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498082</th>\n",
       "      <td>3</td>\n",
       "      <td>1437</td>\n",
       "      <td>36.29</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.016006</td>\n",
       "      <td>0.025254</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.577826</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>39.597685</td>\n",
       "      <td>0.248002</td>\n",
       "      <td>0.192891</td>\n",
       "      <td>0.055112</td>\n",
       "      <td>-0.167162</td>\n",
       "      <td>-0.104527</td>\n",
       "      <td>-0.600645</td>\n",
       "      <td>-0.548197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498083</th>\n",
       "      <td>5</td>\n",
       "      <td>101</td>\n",
       "      <td>46.25</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.457921</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>2.183784</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.043243</td>\n",
       "      <td>0.064865</td>\n",
       "      <td>0.341486</td>\n",
       "      <td>0.313902</td>\n",
       "      <td>0.400110</td>\n",
       "      <td>0.867679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498084</th>\n",
       "      <td>4</td>\n",
       "      <td>587</td>\n",
       "      <td>60.88</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.045997</td>\n",
       "      <td>0.103714</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.254815</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>9.641919</td>\n",
       "      <td>0.016426</td>\n",
       "      <td>0.131406</td>\n",
       "      <td>0.049277</td>\n",
       "      <td>-0.053252</td>\n",
       "      <td>0.087779</td>\n",
       "      <td>0.351923</td>\n",
       "      <td>0.526065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498085</th>\n",
       "      <td>1</td>\n",
       "      <td>1270</td>\n",
       "      <td>25.98</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.998462</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>48.883757</td>\n",
       "      <td>0.192456</td>\n",
       "      <td>0.038491</td>\n",
       "      <td>0.153965</td>\n",
       "      <td>0.246751</td>\n",
       "      <td>-0.507737</td>\n",
       "      <td>1.323659</td>\n",
       "      <td>-2.250099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498086</th>\n",
       "      <td>4</td>\n",
       "      <td>185</td>\n",
       "      <td>63.43</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.342865</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.686000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.048649</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>2.916601</td>\n",
       "      <td>0.157654</td>\n",
       "      <td>0.141889</td>\n",
       "      <td>0.031531</td>\n",
       "      <td>0.105984</td>\n",
       "      <td>0.118611</td>\n",
       "      <td>-0.462521</td>\n",
       "      <td>0.424089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498087</th>\n",
       "      <td>6</td>\n",
       "      <td>1480</td>\n",
       "      <td>19.87</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.013426</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.870000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>74.484147</td>\n",
       "      <td>0.352290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100654</td>\n",
       "      <td>0.160370</td>\n",
       "      <td>0.515485</td>\n",
       "      <td>3.165331</td>\n",
       "      <td>0.183863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498088</th>\n",
       "      <td>4</td>\n",
       "      <td>403</td>\n",
       "      <td>50.53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.125385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007444</td>\n",
       "      <td>7.975460</td>\n",
       "      <td>0.019790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059371</td>\n",
       "      <td>0.342761</td>\n",
       "      <td>0.110730</td>\n",
       "      <td>3.198606</td>\n",
       "      <td>0.121134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498089</th>\n",
       "      <td>6</td>\n",
       "      <td>154</td>\n",
       "      <td>84.63</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.549545</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.630000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>1.819686</td>\n",
       "      <td>0.070897</td>\n",
       "      <td>0.023632</td>\n",
       "      <td>0.047265</td>\n",
       "      <td>0.710191</td>\n",
       "      <td>0.519869</td>\n",
       "      <td>3.510299</td>\n",
       "      <td>0.123292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498090</th>\n",
       "      <td>3</td>\n",
       "      <td>285</td>\n",
       "      <td>74.97</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.263053</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74.970000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.014035</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>3.801521</td>\n",
       "      <td>0.053355</td>\n",
       "      <td>0.093371</td>\n",
       "      <td>0.026677</td>\n",
       "      <td>0.307022</td>\n",
       "      <td>-0.068999</td>\n",
       "      <td>3.409179</td>\n",
       "      <td>0.105333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498091</th>\n",
       "      <td>4</td>\n",
       "      <td>1691</td>\n",
       "      <td>22.89</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007688</td>\n",
       "      <td>0.013536</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.760769</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>73.875055</td>\n",
       "      <td>0.436872</td>\n",
       "      <td>0.262123</td>\n",
       "      <td>0.218436</td>\n",
       "      <td>0.494801</td>\n",
       "      <td>0.096569</td>\n",
       "      <td>1.999168</td>\n",
       "      <td>0.488506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498092</th>\n",
       "      <td>1</td>\n",
       "      <td>1685</td>\n",
       "      <td>61.99</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.036789</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.995000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>27.181804</td>\n",
       "      <td>0.096790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064527</td>\n",
       "      <td>0.495132</td>\n",
       "      <td>-0.492042</td>\n",
       "      <td>1.173011</td>\n",
       "      <td>-1.210670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498093</th>\n",
       "      <td>4</td>\n",
       "      <td>275</td>\n",
       "      <td>48.23</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.175382</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.384545</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.021818</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.010909</td>\n",
       "      <td>5.701845</td>\n",
       "      <td>0.124404</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>0.062202</td>\n",
       "      <td>0.101001</td>\n",
       "      <td>0.096966</td>\n",
       "      <td>0.374923</td>\n",
       "      <td>0.507155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498094</th>\n",
       "      <td>5</td>\n",
       "      <td>1085</td>\n",
       "      <td>50.15</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.046221</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.582143</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.005530</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>21.635095</td>\n",
       "      <td>0.059821</td>\n",
       "      <td>0.119641</td>\n",
       "      <td>0.039880</td>\n",
       "      <td>-0.097823</td>\n",
       "      <td>0.301640</td>\n",
       "      <td>-0.543317</td>\n",
       "      <td>1.711947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498095</th>\n",
       "      <td>5</td>\n",
       "      <td>687</td>\n",
       "      <td>33.47</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036390</td>\n",
       "      <td>0.048719</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.338800</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.010189</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>20.525844</td>\n",
       "      <td>0.209143</td>\n",
       "      <td>0.029878</td>\n",
       "      <td>0.029878</td>\n",
       "      <td>-0.427005</td>\n",
       "      <td>0.288301</td>\n",
       "      <td>-1.870431</td>\n",
       "      <td>1.752422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498096</th>\n",
       "      <td>6</td>\n",
       "      <td>425</td>\n",
       "      <td>18.91</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009412</td>\n",
       "      <td>0.044494</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.727500</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016471</td>\n",
       "      <td>0.021176</td>\n",
       "      <td>0.009412</td>\n",
       "      <td>22.474881</td>\n",
       "      <td>0.370175</td>\n",
       "      <td>0.475939</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>0.472386</td>\n",
       "      <td>0.513056</td>\n",
       "      <td>1.143937</td>\n",
       "      <td>1.454934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498097</th>\n",
       "      <td>2</td>\n",
       "      <td>671</td>\n",
       "      <td>86.03</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.038748</td>\n",
       "      <td>0.128212</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.308846</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>7.799605</td>\n",
       "      <td>0.127862</td>\n",
       "      <td>0.011624</td>\n",
       "      <td>0.058119</td>\n",
       "      <td>0.330910</td>\n",
       "      <td>-0.321494</td>\n",
       "      <td>1.989479</td>\n",
       "      <td>-1.814723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498098</th>\n",
       "      <td>4</td>\n",
       "      <td>1705</td>\n",
       "      <td>98.66</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.057865</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.665000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>17.281573</td>\n",
       "      <td>0.050679</td>\n",
       "      <td>0.030407</td>\n",
       "      <td>0.020272</td>\n",
       "      <td>0.058991</td>\n",
       "      <td>0.112030</td>\n",
       "      <td>-0.503091</td>\n",
       "      <td>0.457804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498099</th>\n",
       "      <td>1</td>\n",
       "      <td>1822</td>\n",
       "      <td>75.30</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016465</td>\n",
       "      <td>0.041328</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>24.196547</td>\n",
       "      <td>0.119522</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.053121</td>\n",
       "      <td>0.096394</td>\n",
       "      <td>-0.521803</td>\n",
       "      <td>1.458583</td>\n",
       "      <td>-2.411263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498100</th>\n",
       "      <td>3</td>\n",
       "      <td>1151</td>\n",
       "      <td>11.37</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021720</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.231310</td>\n",
       "      <td>0.263852</td>\n",
       "      <td>0.087951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.637499</td>\n",
       "      <td>-0.109895</td>\n",
       "      <td>-2.545646</td>\n",
       "      <td>-0.485906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498101</th>\n",
       "      <td>1</td>\n",
       "      <td>796</td>\n",
       "      <td>85.72</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.107688</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.720000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>9.286048</td>\n",
       "      <td>0.104993</td>\n",
       "      <td>0.034998</td>\n",
       "      <td>0.011666</td>\n",
       "      <td>0.095151</td>\n",
       "      <td>-0.469309</td>\n",
       "      <td>3.355964</td>\n",
       "      <td>0.082610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498102</th>\n",
       "      <td>2</td>\n",
       "      <td>1379</td>\n",
       "      <td>48.64</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014503</td>\n",
       "      <td>0.035272</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.432000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>28.351151</td>\n",
       "      <td>0.226151</td>\n",
       "      <td>0.143914</td>\n",
       "      <td>0.102796</td>\n",
       "      <td>0.431208</td>\n",
       "      <td>-0.308803</td>\n",
       "      <td>2.077825</td>\n",
       "      <td>-1.749740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498103</th>\n",
       "      <td>6</td>\n",
       "      <td>885</td>\n",
       "      <td>92.41</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.104418</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.160667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.576886</td>\n",
       "      <td>0.064928</td>\n",
       "      <td>0.054107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.454381</td>\n",
       "      <td>0.506017</td>\n",
       "      <td>-2.100359</td>\n",
       "      <td>2.010543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498104</th>\n",
       "      <td>3</td>\n",
       "      <td>441</td>\n",
       "      <td>36.91</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.083696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.455000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018141</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>11.947982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216743</td>\n",
       "      <td>0.108372</td>\n",
       "      <td>0.517835</td>\n",
       "      <td>-0.084884</td>\n",
       "      <td>1.092574</td>\n",
       "      <td>-0.370675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498105</th>\n",
       "      <td>2</td>\n",
       "      <td>319</td>\n",
       "      <td>57.74</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.094044</td>\n",
       "      <td>0.181003</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.924667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.524766</td>\n",
       "      <td>0.034638</td>\n",
       "      <td>0.017319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.680588</td>\n",
       "      <td>-0.314602</td>\n",
       "      <td>-2.465426</td>\n",
       "      <td>-1.618736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498106</th>\n",
       "      <td>4</td>\n",
       "      <td>1228</td>\n",
       "      <td>74.44</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.022801</td>\n",
       "      <td>0.060619</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.658571</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>16.496507</td>\n",
       "      <td>0.080602</td>\n",
       "      <td>0.067168</td>\n",
       "      <td>0.040301</td>\n",
       "      <td>-0.057299</td>\n",
       "      <td>0.084976</td>\n",
       "      <td>0.348254</td>\n",
       "      <td>0.540835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498107</th>\n",
       "      <td>4</td>\n",
       "      <td>708</td>\n",
       "      <td>19.56</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.027627</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.780000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>36.196319</td>\n",
       "      <td>0.408998</td>\n",
       "      <td>0.051125</td>\n",
       "      <td>0.153374</td>\n",
       "      <td>0.290756</td>\n",
       "      <td>0.110333</td>\n",
       "      <td>0.517168</td>\n",
       "      <td>0.377922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498108</th>\n",
       "      <td>2</td>\n",
       "      <td>1726</td>\n",
       "      <td>99.80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007532</td>\n",
       "      <td>0.057822</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.676923</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>17.294589</td>\n",
       "      <td>0.010020</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.010020</td>\n",
       "      <td>-0.232505</td>\n",
       "      <td>-0.289245</td>\n",
       "      <td>-1.525648</td>\n",
       "      <td>-1.405017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498109</th>\n",
       "      <td>4</td>\n",
       "      <td>961</td>\n",
       "      <td>18.62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.019376</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.206667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>51.611171</td>\n",
       "      <td>0.053706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053706</td>\n",
       "      <td>-0.213660</td>\n",
       "      <td>0.107843</td>\n",
       "      <td>-1.371567</td>\n",
       "      <td>0.448997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498110</th>\n",
       "      <td>3</td>\n",
       "      <td>585</td>\n",
       "      <td>26.17</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.013077</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>22.353840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343905</td>\n",
       "      <td>0.076423</td>\n",
       "      <td>-0.086967</td>\n",
       "      <td>-0.094858</td>\n",
       "      <td>-0.582623</td>\n",
       "      <td>-0.487519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498111</th>\n",
       "      <td>4</td>\n",
       "      <td>983</td>\n",
       "      <td>57.72</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030519</td>\n",
       "      <td>0.058718</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.924000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.004069</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.004069</td>\n",
       "      <td>17.030492</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.155925</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.117063</td>\n",
       "      <td>0.084078</td>\n",
       "      <td>1.641820</td>\n",
       "      <td>0.505170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498112</th>\n",
       "      <td>1</td>\n",
       "      <td>1342</td>\n",
       "      <td>69.89</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>0.052079</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.472500</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>19.201603</td>\n",
       "      <td>0.114466</td>\n",
       "      <td>0.143082</td>\n",
       "      <td>0.071541</td>\n",
       "      <td>0.707169</td>\n",
       "      <td>-0.485454</td>\n",
       "      <td>1.496246</td>\n",
       "      <td>-1.416488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498113</th>\n",
       "      <td>5</td>\n",
       "      <td>553</td>\n",
       "      <td>95.44</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.045208</td>\n",
       "      <td>0.172586</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.817600</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.016275</td>\n",
       "      <td>0.009042</td>\n",
       "      <td>0.009042</td>\n",
       "      <td>5.794216</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>0.052389</td>\n",
       "      <td>0.052389</td>\n",
       "      <td>0.364355</td>\n",
       "      <td>0.283246</td>\n",
       "      <td>2.064971</td>\n",
       "      <td>1.756742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498114</th>\n",
       "      <td>6</td>\n",
       "      <td>1767</td>\n",
       "      <td>65.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013016</td>\n",
       "      <td>0.037340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.868696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>26.780843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015156</td>\n",
       "      <td>-0.438262</td>\n",
       "      <td>0.487835</td>\n",
       "      <td>-1.685405</td>\n",
       "      <td>2.273375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498115</th>\n",
       "      <td>6</td>\n",
       "      <td>879</td>\n",
       "      <td>96.34</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>0.109602</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.352222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.010239</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>9.123936</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.093419</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>-0.294484</td>\n",
       "      <td>0.503851</td>\n",
       "      <td>-1.571458</td>\n",
       "      <td>2.208047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498116</th>\n",
       "      <td>4</td>\n",
       "      <td>783</td>\n",
       "      <td>59.10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012771</td>\n",
       "      <td>0.075479</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.910000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.248731</td>\n",
       "      <td>0.033841</td>\n",
       "      <td>0.033841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.453159</td>\n",
       "      <td>0.106284</td>\n",
       "      <td>-2.261474</td>\n",
       "      <td>0.454349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498117</th>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>98.90</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.050360</td>\n",
       "      <td>0.355755</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.064286</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.032374</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>2.810920</td>\n",
       "      <td>0.091001</td>\n",
       "      <td>0.050556</td>\n",
       "      <td>0.040445</td>\n",
       "      <td>0.313497</td>\n",
       "      <td>-0.502220</td>\n",
       "      <td>1.283820</td>\n",
       "      <td>-2.074560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498118</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>5.41</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.018033</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.601111</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>55.452865</td>\n",
       "      <td>1.109057</td>\n",
       "      <td>1.109057</td>\n",
       "      <td>0.739372</td>\n",
       "      <td>0.336591</td>\n",
       "      <td>-0.098049</td>\n",
       "      <td>1.371059</td>\n",
       "      <td>-0.498357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498119</th>\n",
       "      <td>2</td>\n",
       "      <td>1524</td>\n",
       "      <td>33.97</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005906</td>\n",
       "      <td>0.022290</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.774444</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>44.863115</td>\n",
       "      <td>0.058875</td>\n",
       "      <td>0.147189</td>\n",
       "      <td>0.088313</td>\n",
       "      <td>0.130232</td>\n",
       "      <td>-0.296911</td>\n",
       "      <td>0.386430</td>\n",
       "      <td>-1.575245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498120</th>\n",
       "      <td>3</td>\n",
       "      <td>1456</td>\n",
       "      <td>56.97</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.039128</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.034643</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.007555</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>25.557311</td>\n",
       "      <td>0.193084</td>\n",
       "      <td>0.122872</td>\n",
       "      <td>0.035106</td>\n",
       "      <td>-0.206077</td>\n",
       "      <td>-0.108752</td>\n",
       "      <td>-0.618502</td>\n",
       "      <td>-0.542440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498121 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  valuePerSecond  lineItemVoidsPerPosition  scannedLineItems  pricePerScannedLineItem  scansWithoutRegistrationPerScannedLineItem  quantityModificationsPerScannedLineItem  lineItemVoidsPerSecond  scansWithoutRegistrationPerSecond  quantityModificationsPerSecond  secondsPerEuro  lineItemVoidsPerEuro  scansWithoutRegistrationPerEuro  quantityModificationsPerEuro  pca_axis_1  pca_axis_2  tsne_axis_1  tsne_axis_2\n",
       "0                4                     467       88.48              4                         8                      4                   0.014989        0.189465                  0.571429               7.0                12.640000                                    1.142857                                 0.571429                0.008565                           0.017131                        0.008565        5.278029              0.045208                         0.090416                      0.045208    0.407059    0.107610     1.288056     0.439125\n",
       "1                3                    1004       58.99              7                         6                      1                   0.026892        0.058755                  0.259259              27.0                 2.184815                                    0.222222                                 0.037037                0.006972                           0.005976                        0.000996       17.019834              0.118664                         0.101712                      0.016952   -0.404056   -0.107318    -2.048066    -0.496164\n",
       "2                1                     162       14.00              4                         5                      4                   0.006173        0.086420                  4.000000               1.0                14.000000                                    5.000000                                 4.000000                0.024691                           0.030864                        0.024691       11.571429              0.285714                         0.357143                      0.285714    0.648069   -0.481068     3.447044     0.053117\n",
       "3                5                     532       84.79              9                         3                      4                   0.026316        0.159380                  0.642857              14.0                 6.056429                                    0.214286                                 0.285714                0.016917                           0.005639                        0.007519        6.274325              0.106145                         0.035382                      0.047175    0.292492    0.294786     1.344228     1.554838\n",
       "4                5                     890       42.16              4                         0                      0                   0.021348        0.047371                  0.210526              19.0                 2.218947                                    0.000000                                 0.000000                0.004494                           0.000000                        0.000000       21.110057              0.094877                         0.000000                      0.000000   -0.564485    0.295128    -2.414502     1.547169\n",
       "5                5                    1072       12.67              3                         4                      1                   0.019590        0.011819                  0.142857              21.0                 0.603333                                    0.190476                                 0.047619                0.002799                           0.003731                        0.000933       84.609313              0.236780                         0.315706                      0.078927   -0.388592    0.294494    -1.828019     1.732591\n",
       "6                3                     259       93.75              0                         7                      0                   0.100386        0.361969                  0.000000              26.0                 3.605769                                    0.269231                                 0.000000                0.000000                           0.027027                        0.000000        2.762667              0.000000                         0.074667                      0.000000   -0.591763   -0.104089    -2.445146    -0.517219\n",
       "7                2                    1528       47.35              2                         9                      5                   0.009817        0.030988                  0.133333              15.0                 3.156667                                    0.600000                                 0.333333                0.001309                           0.005890                        0.003272       32.270327              0.042239                         0.190074                      0.105597    0.458967   -0.303730     2.015723    -1.598574\n",
       "8                6                     816       80.89              9                         4                      0                   0.017157        0.099130                  0.642857              14.0                 5.777857                                    0.285714                                 0.000000                0.011029                           0.004902                        0.000000       10.087774              0.111262                         0.049450                      0.000000   -0.438512    0.506551    -2.074741     1.978520\n",
       "9                4                      16       31.91              7                         7                      4                   1.312500        1.994375                  0.333333              21.0                 1.519524                                    0.333333                                 0.190476                0.437500                           0.437500                        0.250000        0.501410              0.219367                         0.219367                      0.125353    0.211314    0.090980     1.474045     0.426568\n",
       "10               3                     714       94.29              8                         7                      0                   0.016807        0.132059                  0.666667              12.0                 7.857500                                    0.583333                                 0.000000                0.011204                           0.009804                        0.000000        7.572383              0.084845                         0.074239                      0.000000   -0.391550   -0.087511    -2.165339    -0.435252\n",
       "11               5                    1077       66.16              5                         8                      3                   0.015785        0.061430                  0.294118              17.0                 3.891765                                    0.470588                                 0.176471                0.004643                           0.007428                        0.002786       16.278718              0.075574                         0.120919                      0.045345    0.081986    0.298952     0.382123     1.827146\n",
       "12               4                    1301       84.35              3                        10                      5                   0.021522        0.064835                  0.107143              28.0                 3.012500                                    0.357143                                 0.178571                0.002306                           0.007686                        0.003843       15.423829              0.035566                         0.118554                      0.059277    0.332642    0.084617     2.101652     0.551876\n",
       "13               3                    1429       47.95              8                         1                      3                   0.003499        0.033555                  1.600000               5.0                 9.590000                                    0.200000                                 0.600000                0.005598                           0.000700                        0.002099       29.801877              0.166840                         0.020855                      0.062565    0.203086   -0.094804     0.419216    -0.478411\n",
       "14               3                    1196       83.77             11                        10                      0                   0.004181        0.070042                  2.200000               5.0                16.754000                                    2.000000                                 0.000000                0.009197                           0.008361                        0.000000       14.277188              0.131312                         0.119374                      0.000000   -0.249300   -0.073777    -1.691172    -0.394049\n",
       "15               3                    1567       75.53              7                        10                      1                   0.008296        0.048200                  0.538462              13.0                 5.810000                                    0.769231                                 0.076923                0.004467                           0.006382                        0.000638       20.746723              0.092678                         0.132398                      0.013240   -0.209044   -0.088058    -1.627112    -0.455931\n",
       "16               4                     289       18.66              8                         4                      0                   0.086505        0.064567                  0.320000              25.0                 0.746400                                    0.160000                                 0.000000                0.027682                           0.013841                        0.000000       15.487674              0.428725                         0.214362                      0.000000   -0.587221    0.094554    -2.528869     0.501003\n",
       "17               1                     335        1.19              0                         0                      1                   0.032836        0.003552                  0.000000              11.0                 0.108182                                    0.000000                                 0.090909                0.000000                           0.000000                        0.002985      281.512605              0.000000                         0.000000                      0.840336   -0.327371   -0.500976    -1.404831    -1.984003\n",
       "18               3                    1304       30.51              0                         7                      3                   0.012270        0.023397                  0.000000              16.0                 1.906875                                    0.437500                                 0.187500                0.000000                           0.005368                        0.002301       42.740085              0.000000                         0.229433                      0.098328    0.047629   -0.102692     0.390086    -0.537004\n",
       "19               5                    1353       82.02              0                         0                      2                   0.005174        0.060621                  0.000000               7.0                11.717143                                    0.000000                                 0.285714                0.000000                           0.000000                        0.001478       16.495977              0.000000                         0.000000                      0.024384   -0.061909    0.301631    -0.469931     1.453829\n",
       "20               6                    1749       46.56              5                         0                      5                   0.012007        0.026621                  0.238095              21.0                 2.217143                                    0.000000                                 0.238095                0.002859                           0.000000                        0.002859       37.564433              0.107388                         0.000000                      0.107388    0.335438    0.480083     1.940913     2.267286\n",
       "21               5                     757       91.02              9                         3                      0                   0.019815        0.120238                  0.600000              15.0                 6.068000                                    0.200000                                 0.000000                0.011889                           0.003963                        0.000000        8.316853              0.098879                         0.032960                      0.000000   -0.454010    0.304593    -2.222024     1.415683\n",
       "22               2                    1440       15.15              0                         6                      5                   0.016667        0.010521                  0.000000              24.0                 0.631250                                    0.250000                                 0.208333                0.000000                           0.004167                        0.003472       95.049505              0.000000                         0.396040                      0.330033    0.310644   -0.317246     2.062055    -1.834159\n",
       "23               4                    1101       22.55              8                         9                      0                   0.019982        0.020481                  0.363636              22.0                 1.025000                                    0.409091                                 0.000000                0.007266                           0.008174                        0.000000       48.824834              0.354767                         0.399113                      0.000000   -0.516836    0.103528    -2.477338     0.480600\n",
       "24               6                    1404       59.43              5                         8                      5                   0.015670        0.042329                  0.227273              22.0                 2.701364                                    0.363636                                 0.227273                0.003561                           0.005698                        0.003561       23.624432              0.084133                         0.134612                      0.084133    0.387792    0.488340     1.978658     2.308822\n",
       "25               6                    1396       13.16             11                         2                      0                   0.010745        0.009427                  0.733333              15.0                 0.877333                                    0.133333                                 0.000000                0.007880                           0.001433                        0.000000      106.079027              0.835866                         0.151976                      0.000000   -0.479203    0.502977    -2.042129     1.932095\n",
       "26               4                     248       14.00              9                         4                      2                   0.100806        0.056452                  0.360000              25.0                 0.560000                                    0.160000                                 0.080000                0.036290                           0.016129                        0.008065       17.714286              0.642857                         0.285714                      0.142857   -0.217940    0.089334    -0.608626     0.511476\n",
       "27               3                     359        6.40              0                         5                      3                   0.019499        0.017827                  0.000000               7.0                 0.914286                                    0.714286                                 0.428571                0.000000                           0.013928                        0.008357       56.093750              0.000000                         0.781250                      0.468750    0.136216   -0.096246     0.416303    -0.478445\n",
       "28               6                     915       36.79              9                         0                      0                   0.007650        0.040208                  1.285714               7.0                 5.255714                                    0.000000                                 0.000000                0.009836                           0.000000                        0.000000       24.870889              0.244632                         0.000000                      0.000000   -0.401927    0.508693    -1.884966     1.719997\n",
       "29               5                     617       49.57              0                         5                      1                   0.035656        0.080340                  0.000000              22.0                 2.253182                                    0.227273                                 0.045455                0.000000                           0.008104                        0.001621       12.447045              0.000000                         0.100867                      0.020173   -0.394770    0.294185    -1.889421     1.752990\n",
       "30               5                     785       69.87              2                         1                      3                   0.033121        0.089006                  0.076923              26.0                 2.687308                                    0.038462                                 0.115385                0.002548                           0.001274                        0.003822       11.235151              0.028625                         0.014312                      0.042937   -0.085711    0.281051     0.363176     2.021664\n",
       "31               6                    1576       94.01              6                         5                      5                   0.010152        0.059651                  0.375000              16.0                 5.875625                                    0.312500                                 0.312500                0.003807                           0.003173                        0.003173       16.764174              0.063823                         0.053186                      0.053186    0.453287    0.491772     1.920002     2.143842\n",
       "32               1                     128       44.10              3                         3                      1                   0.109375        0.344531                  0.214286              14.0                 3.150000                                    0.214286                                 0.071429                0.023438                           0.023438                        0.007812        2.902494              0.068027                         0.068027                      0.022676   -0.305911   -0.499015    -1.549833    -2.117539\n",
       "33               4                    1260       42.28              4                         7                      2                   0.003968        0.033556                  0.800000               5.0                 8.456000                                    1.400000                                 0.400000                0.003175                           0.005556                        0.001587       29.801325              0.094607                         0.165563                      0.047304    0.038380    0.113650    -0.500719     0.467817\n",
       "34               6                    1206       39.31              0                         0                      2                   0.001658        0.032595                  0.000000               2.0                19.655000                                    0.000000                                 1.000000                0.000000                           0.000000                        0.001658       30.679216              0.000000                         0.000000                      0.050878    0.017638    0.507226    -0.113821     1.689994\n",
       "35               4                     883       22.42              0                         7                      5                   0.007928        0.025391                  0.000000               7.0                 3.202857                                    1.000000                                 0.714286                0.000000                           0.007928                        0.005663       39.384478              0.000000                         0.312221                      0.223015    0.533834    0.101324     1.874397     0.447375\n",
       "36               1                    1554        4.12              4                        10                      1                   0.010296        0.002651                  0.250000              16.0                 0.257500                                    0.625000                                 0.062500                0.002574                           0.006435                        0.000644      377.184466              0.970874                         2.427184                      0.242718   -0.285590   -0.492750    -1.539333    -2.141390\n",
       "37               2                    1734        9.38             10                         6                      0                   0.014994        0.005409                  0.384615              26.0                 0.360769                                    0.230769                                 0.000000                0.005767                           0.003460                        0.000000      184.861407              1.066098                         0.639659                      0.000000   -0.577454   -0.303080    -2.396955    -1.499305\n",
       "38               3                    1182       94.67              4                         0                      4                   0.016074        0.080093                  0.210526              19.0                 4.982632                                    0.000000                                 0.210526                0.003384                           0.000000                        0.003384       12.485476              0.042252                         0.000000                      0.042252    0.186985   -0.114655     1.540088    -0.537563\n",
       "39               4                    1194       75.02              5                         8                      3                   0.010050        0.062831                  0.416667              12.0                 6.251667                                    0.666667                                 0.250000                0.004188                           0.006700                        0.002513       15.915756              0.066649                         0.106638                      0.039989    0.146536    0.104531     0.382479     0.465688\n",
       "40               5                     526       92.49             11                         1                      5                   0.053232        0.175837                  0.392857              28.0                 3.303214                                    0.035714                                 0.178571                0.020913                           0.001901                        0.009506        5.687101              0.118932                         0.010812                      0.054060    0.310278    0.276153     1.974766     1.768757\n",
       "41               4                    1662       70.34              0                        10                      0                   0.009627        0.042323                  0.000000              16.0                 4.396250                                    0.625000                                 0.000000                0.000000                           0.006017                        0.000000       23.628092              0.000000                         0.142167                      0.000000   -0.467701    0.109340    -2.210949     0.471721\n",
       "42               4                    1060       67.97              7                         6                      4                   0.028302        0.064123                  0.233333              30.0                 2.265667                                    0.200000                                 0.133333                0.006604                           0.005660                        0.003774       15.595115              0.102987                         0.088274                      0.058849    0.113458    0.081560     1.632863     0.532465\n",
       "43               5                    1276       59.30              1                         3                      0                   0.002351        0.046473                  0.333333               3.0                19.766667                                    1.000000                                 0.000000                0.000784                           0.002351                        0.000000       21.517707              0.016863                         0.050590                      0.000000   -0.354376    0.315715    -1.826596     1.206667\n",
       "44               4                    1075       80.75              9                         7                      3                   0.013953        0.075116                  0.600000              15.0                 5.383333                                    0.466667                                 0.200000                0.008372                           0.006512                        0.002791       13.312693              0.111455                         0.086687                      0.037152    0.124672    0.101301     0.358393     0.504236\n",
       "45               1                     665       67.99              2                         7                      3                   0.006015        0.102241                  0.500000               4.0                16.997500                                    1.750000                                 0.750000                0.003008                           0.010526                        0.004511        9.780850              0.029416                         0.102956                      0.044124    0.253154   -0.487076     0.424097    -1.681565\n",
       "46               5                    1768       81.84              9                         1                      1                   0.011878        0.046290                  0.428571              21.0                 3.897143                                    0.047619                                 0.047619                0.005090                           0.000566                        0.000566       21.603128              0.109971                         0.012219                      0.012219   -0.357523    0.293651    -1.751355     1.645385\n",
       "47               3                    1772       33.45              9                         5                      5                   0.006772        0.018877                  0.750000              12.0                 2.787500                                    0.416667                                 0.416667                0.005079                           0.002822                        0.002822       52.974589              0.269058                         0.149477                      0.149477    0.498718   -0.103505     1.981186    -0.538616\n",
       "48               2                     870       90.86              8                         8                      5                   0.031034        0.104437                  0.296296              27.0                 3.365185                                    0.296296                                 0.185185                0.009195                           0.009195                        0.005747        9.575171              0.088048                         0.088048                      0.055030    0.357115   -0.315162     2.108857    -1.824888\n",
       "49               4                    1323       40.34              7                         6                      4                   0.003779        0.030491                  1.400000               5.0                 8.068000                                    1.200000                                 0.800000                0.005291                           0.004535                        0.003023       32.796232              0.173525                         0.148736                      0.099157    0.427446    0.108353     1.351503     0.428718\n",
       "50               3                    1087       40.46              1                        10                      0                   0.017479        0.037222                  0.052632              19.0                 2.129474                                    0.526316                                 0.000000                0.000920                           0.009200                        0.000000       26.866041              0.024716                         0.247158                      0.000000   -0.505859   -0.093829    -2.444576    -0.487174\n",
       "51               3                     631       34.78              8                         0                      5                   0.045959        0.055119                  0.275862              29.0                 1.199310                                    0.000000                                 0.172414                0.012678                           0.000000                        0.007924       18.142611              0.230017                         0.000000                      0.143761    0.258254   -0.127098     2.137155    -0.610189\n",
       "52               1                     766       23.84              9                         6                      0                   0.026110        0.031123                  0.450000              20.0                 1.192000                                    0.300000                                 0.000000                0.011749                           0.007833                        0.000000       32.130872              0.377517                         0.251678                      0.000000   -0.510724   -0.497340    -2.242349    -2.108567\n",
       "53               4                    1243       89.45              4                         6                      0                   0.017699        0.071963                  0.181818              22.0                 4.065909                                    0.272727                                 0.000000                0.003218                           0.004827                        0.000000       13.896031              0.044718                         0.067077                      0.000000   -0.537213    0.099864    -2.521923     0.494900\n",
       "54               4                    1328       69.66              9                         1                      4                   0.005271        0.052455                  1.285714               7.0                 9.951429                                    0.142857                                 0.571429                0.006777                           0.000753                        0.003012       19.064025              0.129199                         0.014355                      0.057422    0.369560    0.100507     1.330060     0.432338\n",
       "55               5                     220       33.23              6                         5                      3                   0.077273        0.151045                  0.352941              17.0                 1.954706                                    0.294118                                 0.176471                0.027273                           0.022727                        0.013636        6.620524              0.180560                         0.150466                      0.090280    0.053972    0.295087     0.376573     1.801181\n",
       "56               5                     416        0.24              1                         7                      1                   0.060096        0.000577                  0.040000              25.0                 0.009600                                    0.280000                                 0.040000                0.002404                           0.016827                        0.002404     1733.333333              4.166667                        29.166667                      4.166667   -0.423952    0.293156    -1.823436     1.677843\n",
       "57               1                     330       16.71              6                         5                      3                   0.027273        0.050636                  0.666667               9.0                 1.856667                                    0.555556                                 0.333333                0.018182                           0.015152                        0.009091       19.748654              0.359066                         0.299222                      0.179533    0.147244   -0.496160     0.358784    -2.020848\n",
       "58               5                     266       58.63              8                         4                      0                   0.097744        0.220414                  0.307692              26.0                 2.255000                                    0.153846                                 0.000000                0.030075                           0.015038                        0.000000        4.536926              0.136449                         0.068224                      0.000000   -0.585148    0.293887    -2.444040     1.618795\n",
       "59               4                     916       18.89              4                         6                      0                   0.017467        0.020622                  0.250000              16.0                 1.180625                                    0.375000                                 0.000000                0.004367                           0.006550                        0.000000       48.491265              0.211752                         0.317628                      0.000000   -0.494299    0.104907    -2.445191     0.481664\n",
       "60               6                     492       29.45              1                         8                      3                   0.048780        0.059858                  0.041667              24.0                 1.227083                                    0.333333                                 0.125000                0.002033                           0.016260                        0.006098       16.706282              0.033956                         0.271647                      0.101868   -0.030359    0.490101     0.386263     2.530044\n",
       "61               6                     604       12.81              5                         3                      5                   0.036424        0.021209                  0.227273              22.0                 0.582273                                    0.136364                                 0.227273                0.008278                           0.004967                        0.008278       47.150664              0.390320                         0.234192                      0.390320    0.335972    0.481823     1.968992     2.293505\n",
       "62               4                     712       83.07             10                         0                      4                   0.028090        0.116671                  0.500000              20.0                 4.153500                                    0.000000                                 0.200000                0.014045                           0.000000                        0.005618        8.571085              0.120380                         0.000000                      0.048152    0.202694    0.085536     1.476217     0.521695\n",
       "63               4                     520       39.85              0                         4                      2                   0.017308        0.076635                  0.000000               9.0                 4.427778                                    0.444444                                 0.222222                0.000000                           0.007692                        0.003846       13.048934              0.000000                         0.100376                      0.050188   -0.072150    0.103467    -0.522323     0.490866\n",
       "64               6                     244       51.97              4                         6                      1                   0.069672        0.212992                  0.235294              17.0                 3.057059                                    0.352941                                 0.058824                0.016393                           0.024590                        0.004098        4.695016              0.076967                         0.115451                      0.019242   -0.309943    0.501196    -1.605085     2.290144\n",
       "65               1                     813       11.94              1                         5                      0                   0.027060        0.014686                  0.045455              22.0                 0.542727                                    0.227273                                 0.000000                0.001230                           0.006150                        0.000000       68.090452              0.083752                         0.418760                      0.000000   -0.585050   -0.502719    -2.282900    -2.140231\n",
       "66               5                     877       75.98              8                         3                      1                   0.023945        0.086636                  0.380952              21.0                 3.618095                                    0.142857                                 0.047619                0.009122                           0.003421                        0.001140       11.542511              0.105291                         0.039484                      0.013161   -0.349019    0.295377    -1.810546     1.734635\n",
       "67               5                     280       59.81              9                         5                      3                   0.075000        0.213607                  0.428571              21.0                 2.848095                                    0.238095                                 0.142857                0.032143                           0.017857                        0.010714        4.681491              0.150477                         0.083598                      0.050159    0.032398    0.292187     0.362073     1.920195\n",
       "68               4                    1354       21.72              2                         0                      2                   0.015510        0.016041                  0.095238              21.0                 1.034286                                    0.000000                                 0.095238                0.001477                           0.000000                        0.001477       62.338858              0.092081                         0.000000                      0.092081   -0.237265    0.087244    -0.585706     0.531649\n",
       "69               4                    1564       48.20              3                         2                      0                   0.001279        0.030818                  1.500000               2.0                24.100000                                    1.000000                                 0.000000                0.001918                           0.001279                        0.000000       32.448133              0.062241                         0.041494                      0.000000   -0.330304    0.117252    -1.736771     0.443097\n",
       "70               2                     131       66.48             11                        10                      1                   0.022901        0.507481                  3.666667               3.0                22.160000                                    3.333333                                 0.333333                0.083969                           0.076336                        0.007634        1.970517              0.165463                         0.150421                      0.015042   -0.005106   -0.272038    -0.993315    -0.934356\n",
       "71               2                    1184       35.34              5                         8                      2                   0.016892        0.029848                  0.250000              20.0                 1.767000                                    0.400000                                 0.100000                0.004223                           0.006757                        0.001689       33.503113              0.141483                         0.226372                      0.056593   -0.146389   -0.301370    -0.593195    -1.917251\n",
       "72               5                    1694       52.61              1                        10                      5                   0.001181        0.031057                  0.500000               2.0                26.305000                                    5.000000                                 2.500000                0.000590                           0.005903                        0.002952       32.199202              0.019008                         0.190078                      0.095039    0.764194    0.317292     1.221473     0.752527\n",
       "73               1                    1021       15.30              1                         3                      4                   0.018609        0.014985                  0.052632              19.0                 0.805263                                    0.157895                                 0.210526                0.000979                           0.002938                        0.003918       66.732026              0.065359                         0.196078                      0.261438    0.167253   -0.512760     1.357841    -2.423085\n",
       "74               1                    1309       40.16              8                         3                      5                   0.012987        0.030680                  0.470588              17.0                 2.362353                                    0.176471                                 0.294118                0.006112                           0.002292                        0.003820       32.594622              0.199203                         0.074701                      0.124502    0.418606   -0.511156     1.960953    -2.223192\n",
       "75               6                    1409       30.54              5                         8                      0                   0.009226        0.021675                  0.384615              13.0                 2.349231                                    0.615385                                 0.000000                0.003549                           0.005678                        0.000000       46.136215              0.163720                         0.261952                      0.000000   -0.436180    0.510618    -2.063818     1.988861\n",
       "76               4                     952       61.40             11                         0                      2                   0.001050        0.064496                 11.000000               1.0                61.400000                                    0.000000                                 2.000000                0.011555                           0.000000                        0.002101       15.504886              0.179153                         0.000000                      0.032573    0.265730    0.122574     3.404396     0.108386\n",
       "77               2                    1484        8.37              0                         7                      3                   0.018868        0.005640                  0.000000              28.0                 0.298929                                    0.250000                                 0.107143                0.000000                           0.004717                        0.002022      177.299881              0.000000                         0.836320                      0.358423   -0.094212   -0.314690     0.361347    -2.027979\n",
       "78               6                      33       81.30              1                         4                      0                   0.909091        2.463636                  0.033333              30.0                 2.710000                                    0.133333                                 0.000000                0.030303                           0.121212                        0.000000        0.405904              0.012300                         0.049200                      0.000000   -0.656401    0.488213    -2.255680     2.111732\n",
       "79               4                     555       19.91              4                         1                      3                   0.007207        0.035874                  1.000000               4.0                 4.977500                                    0.250000                                 0.750000                0.007207                           0.001802                        0.005405       27.875439              0.200904                         0.050226                      0.150678    0.184926    0.104084     0.473830     0.476910\n",
       "80               1                     953       86.81             10                         6                      1                   0.025184        0.091091                  0.416667              24.0                 3.617083                                    0.250000                                 0.041667                0.010493                           0.006296                        0.001049       10.977998              0.115194                         0.069116                      0.011519   -0.346283   -0.502986    -1.690420    -2.316562\n",
       "81               2                     437       56.71              2                         7                      1                   0.022883        0.129771                  0.200000              10.0                 5.671000                                    0.700000                                 0.100000                0.004577                           0.016018                        0.002288        7.705872              0.035267                         0.123435                      0.017634   -0.228421   -0.290254    -1.455960    -1.345126\n",
       "82               6                     675       14.47              2                         1                      2                   0.026667        0.021437                  0.111111              18.0                 0.803889                                    0.055556                                 0.111111                0.002963                           0.001481                        0.002963       46.648238              0.138217                         0.069109                      0.138217   -0.198784    0.490810    -0.567895     2.334896\n",
       "83               1                     581       32.72              0                         7                      5                   0.034423        0.056317                  0.000000              20.0                 1.636000                                    0.350000                                 0.250000                0.000000                           0.012048                        0.008606       17.756724              0.000000                         0.213936                      0.152812    0.370116   -0.512009     1.933192    -2.282469\n",
       "84               3                    1277       26.15              0                         8                      0                   0.008614        0.020478                  0.000000              11.0                 2.377273                                    0.727273                                 0.000000                0.000000                           0.006265                        0.000000       48.833652              0.000000                         0.305927                      0.000000   -0.441328   -0.088447    -2.182540    -0.449952\n",
       "85               4                     706       16.38              5                         5                      4                   0.004249        0.023201                  1.666667               3.0                 5.460000                                    1.666667                                 1.333333                0.007082                           0.007082                        0.005666       43.101343              0.305250                         0.305250                      0.244200    0.453798    0.109272     1.300240     0.426456\n",
       "86               3                      41       47.41              7                         2                      4                   0.365854        1.156341                  0.466667              15.0                 3.160667                                    0.133333                                 0.266667                0.170732                           0.048780                        0.097561        0.864796              0.147648                         0.042185                      0.084370    0.249387   -0.108368     1.411381    -0.540051\n",
       "87               4                     594       63.48              6                         6                      2                   0.043771        0.106869                  0.230769              26.0                 2.441538                                    0.230769                                 0.076923                0.010101                           0.010101                        0.003367        9.357278              0.094518                         0.094518                      0.031506   -0.213568    0.090515    -0.614394     0.516220\n",
       "88               1                    1044       51.44              9                         4                      4                   0.017241        0.049272                  0.500000              18.0                 2.857778                                    0.222222                                 0.222222                0.008621                           0.003831                        0.003831       20.295490              0.174961                         0.077760                      0.077760    0.239975   -0.507987     1.360582    -2.396240\n",
       "89               5                     650       59.73             11                         0                      1                   0.046154        0.091892                  0.366667              30.0                 1.991000                                    0.000000                                 0.033333                0.016923                           0.000000                        0.001538       10.882304              0.184162                         0.000000                      0.016742   -0.460335    0.283556    -1.834735     1.790446\n",
       "90               5                     338       43.91              6                         2                      4                   0.002959        0.129911                  6.000000               1.0                43.910000                                    2.000000                                 4.000000                0.017751                           0.005917                        0.011834        7.697563              0.136643                         0.045548                      0.091095    0.660309    0.316781     3.475505     0.123218\n",
       "91               3                     580       88.44             10                         6                      5                   0.001724        0.152483                 10.000000               1.0                88.440000                                    6.000000                                 5.000000                0.017241                           0.010345                        0.008621        6.558118              0.113071                         0.067843                      0.056536    1.060829   -0.069564     3.626117     0.108674\n",
       "92               6                     314       77.71              5                        10                      5                   0.070064        0.247484                  0.227273              22.0                 3.532273                                    0.454545                                 0.227273                0.015924                           0.031847                        0.015924        4.040664              0.064342                         0.128684                      0.064342    0.409609    0.490560     1.897657     2.218347\n",
       "93               2                    1625       72.28              9                         2                      2                   0.004923        0.044480                  1.125000               8.0                 9.035000                                    0.250000                                 0.250000                0.005538                           0.001231                        0.001231       22.482014              0.124516                         0.027670                      0.027670   -0.010886   -0.293898    -0.526151    -1.437430\n",
       "94               4                    1225       27.98              8                        10                      1                   0.013061        0.022841                  0.500000              16.0                 1.748750                                    0.625000                                 0.062500                0.006531                           0.008163                        0.000816       43.781272              0.285919                         0.357398                      0.035740   -0.256314    0.108177    -1.739169     0.482357\n",
       "95               1                     666       97.66              4                         0                      4                   0.034535        0.146637                  0.173913              23.0                 4.246087                                    0.000000                                 0.173913                0.006006                           0.000000                        0.006006        6.819578              0.040958                         0.000000                      0.040958    0.143085   -0.518497     1.288936    -2.446570\n",
       "96               1                     405       62.14              2                         8                      0                   0.037037        0.153432                  0.133333              15.0                 4.142667                                    0.533333                                 0.000000                0.004938                           0.019753                        0.000000        6.517541              0.032185                         0.128742                      0.000000   -0.462443   -0.491470    -2.103142    -1.925902\n",
       "97               3                     403       38.54              0                         8                      1                   0.027295        0.095633                  0.000000              11.0                 3.503636                                    0.727273                                 0.090909                0.000000                           0.019851                        0.002481       10.456668              0.000000                         0.207577                      0.025947   -0.250854   -0.091152    -1.569144    -0.455662\n",
       "98               5                     527       79.43             10                         6                      2                   0.003795        0.150721                  5.000000               2.0                39.715000                                    3.000000                                 1.000000                0.018975                           0.011385                        0.003795        6.634773              0.125897                         0.075538                      0.025179    0.211066    0.323883    -0.321653     0.723815\n",
       "99               1                     652       64.01              6                         2                      4                   0.007669        0.098175                  1.200000               5.0                12.802000                                    0.400000                                 0.800000                0.009202                           0.003067                        0.006135       10.185908              0.093735                         0.031245                      0.062490    0.397474   -0.496382     1.219413    -1.757077\n",
       "100              4                    1740       83.60              1                         7                      3                   0.004023        0.048046                  0.142857               7.0                11.942857                                    1.000000                                 0.428571                0.000575                           0.004023                        0.001724       20.813397              0.011962                         0.083732                      0.035885    0.190354    0.108247     0.396245     0.433069\n",
       "101              2                    1794       29.34              0                         9                      4                   0.011148        0.016355                  0.000000              20.0                 1.467000                                    0.450000                                 0.200000                0.000000                           0.005017                        0.002230       61.145194              0.000000                         0.306748                      0.136333    0.199407   -0.306850     1.385703    -1.713234\n",
       "102              3                    1077       86.33              6                        10                      0                   0.002786        0.080158                  2.000000               3.0                28.776667                                    3.333333                                 0.000000                0.005571                           0.009285                        0.000000       12.475385              0.069501                         0.115835                      0.000000   -0.227794   -0.071145    -1.494627    -0.318647\n",
       "103              2                    1258       36.61              8                         5                      3                   0.003180        0.029102                  2.000000               4.0                 9.152500                                    1.250000                                 0.750000                0.006359                           0.003975                        0.002385       34.362196              0.218520                         0.136575                      0.081945    0.258578   -0.288120     0.387946    -1.289043\n",
       "104              6                    1377       47.45              3                         6                      5                   0.000726        0.034459                  3.000000               1.0                47.450000                                    6.000000                                 5.000000                0.002179                           0.004357                        0.003631       29.020021              0.063224                         0.126449                      0.105374    0.915379    0.520606     3.539910     0.124749\n",
       "105              3                    1788       24.98              9                         2                      3                   0.012304        0.013971                  0.409091              22.0                 1.135455                                    0.090909                                 0.136364                0.005034                           0.001119                        0.001678       71.577262              0.360288                         0.080064                      0.120096   -0.013854   -0.112010     0.345096    -0.550858\n",
       "106              2                    1162       37.26             10                         6                      2                   0.004303        0.032065                  2.000000               5.0                 7.452000                                    1.200000                                 0.400000                0.008606                           0.005164                        0.001721       31.186259              0.268384                         0.161031                      0.053677    0.066070   -0.285305    -0.495923    -1.313870\n",
       "107              4                     979        8.86              8                         3                      2                   0.027579        0.009050                  0.296296              27.0                 0.328148                                    0.111111                                 0.074074                0.008172                           0.003064                        0.002043      110.496614              0.902935                         0.338600                      0.225734   -0.254634    0.086100    -0.588204     0.536142\n",
       "108              1                     587        3.83              0                         9                      0                   0.022147        0.006525                  0.000000              13.0                 0.294615                                    0.692308                                 0.000000                0.000000                           0.015332                        0.000000      153.263708              0.000000                         2.349869                      0.000000   -0.463786   -0.489571    -2.020708    -1.864661\n",
       "109              5                    1777       67.23              1                         1                      1                   0.001688        0.037833                  0.333333               3.0                22.410000                                    0.333333                                 0.333333                0.000563                           0.000563                        0.000563       26.431653              0.014874                         0.014874                      0.014874   -0.174948    0.310909    -1.237590     1.276763\n",
       "110              1                    1685       62.39              3                         7                      2                   0.017804        0.037027                  0.100000              30.0                 2.079667                                    0.233333                                 0.066667                0.001780                           0.004154                        0.001187       27.007533              0.048085                         0.112197                      0.032056   -0.266381   -0.512390    -0.639070    -2.522591\n",
       "111              2                    1246        2.33              9                         2                      1                   0.016854        0.001870                  0.428571              21.0                 0.110952                                    0.095238                                 0.047619                0.007223                           0.001605                        0.000803      534.763948              3.862661                         0.858369                      0.429185   -0.376011   -0.305903    -1.770405    -1.724881\n",
       "112              6                     303       54.13              2                         8                      0                   0.079208        0.178647                  0.083333              24.0                 2.255417                                    0.333333                                 0.000000                0.006601                           0.026403                        0.000000        5.597635              0.036948                         0.147792                      0.000000   -0.565796    0.498713    -2.246209     2.142398\n",
       "113              2                    1095        9.44             11                         6                      1                   0.015525        0.008621                  0.647059              17.0                 0.555294                                    0.352941                                 0.058824                0.010046                           0.005479                        0.000913      115.995763              1.165254                         0.635593                      0.105932   -0.288443   -0.296690    -1.678647    -1.623752\n",
       "114              1                       8       51.57              0                         5                      0                   0.250000        6.446250                  0.000000               2.0                25.785000                                    2.500000                                 0.000000                0.000000                           0.625000                        0.000000        0.155129              0.000000                         0.096956                      0.000000   -0.317583   -0.479721    -1.649683    -1.441282\n",
       "115              3                    1125       64.29              8                         9                      0                   0.002667        0.057147                  2.666667               3.0                21.430000                                    3.000000                                 0.000000                0.007111                           0.008000                        0.000000       17.498833              0.124436                         0.139991                      0.000000   -0.236115   -0.072329    -1.509923    -0.325212\n",
       "116              2                    1394       86.22              1                         2                      1                   0.012195        0.061851                  0.058824              17.0                 5.071765                                    0.117647                                 0.058824                0.000717                           0.001435                        0.000717       16.167942              0.011598                         0.023196                      0.011598   -0.344384   -0.302984    -1.705218    -1.615325\n",
       "117              3                     478        4.48             11                         8                      2                   0.039749        0.009372                  0.578947              19.0                 0.235789                                    0.421053                                 0.105263                0.023013                           0.016736                        0.004184      106.696429              2.455357                         1.785714                      0.446429   -0.114081   -0.099461    -0.564797    -0.530682\n",
       "118              6                    1746       68.99              2                         7                      2                   0.010309        0.039513                  0.111111              18.0                 3.832778                                    0.388889                                 0.111111                0.001145                           0.004009                        0.001145       25.308016              0.028990                         0.101464                      0.028990   -0.136049    0.498734    -0.591832     2.384673\n",
       "119              3                    1700       21.46              3                        10                      5                   0.004706        0.012624                  0.375000               8.0                 2.682500                                    1.250000                                 0.625000                0.001765                           0.005882                        0.002941       79.217148              0.139795                         0.465983                      0.232992    0.558587   -0.094934     1.831263    -0.519757\n",
       "120              6                     961       58.21              6                         0                      0                   0.019771        0.060572                  0.315789              19.0                 3.063684                                    0.000000                                 0.000000                0.006243                           0.000000                        0.000000       16.509191              0.103075                         0.000000                      0.000000   -0.548874    0.495751    -2.238045     2.114063\n",
       "121              3                     352       57.85              2                         4                      0                   0.008523        0.164347                  0.666667               3.0                19.283333                                    1.333333                                 0.000000                0.005682                           0.011364                        0.000000        6.084702              0.034572                         0.069144                      0.000000   -0.336361   -0.082467    -1.916044    -0.345205\n",
       "122              2                    1154       26.67              7                         9                      4                   0.017331        0.023111                  0.350000              20.0                 1.333500                                    0.450000                                 0.200000                0.006066                           0.007799                        0.003466       43.269591              0.262467                         0.337458                      0.149981    0.235012   -0.305194     1.454368    -1.824861\n",
       "123              3                     470        8.25              7                         7                      2                   0.010638        0.017553                  1.400000               5.0                 1.650000                                    1.400000                                 0.400000                0.014894                           0.014894                        0.004255       56.969697              0.848485                         0.848485                      0.242424    0.042272   -0.086112    -0.470571    -0.417043\n",
       "124              2                     618       90.66              2                        10                      5                   0.021036        0.146699                  0.153846              13.0                 6.973846                                    0.769231                                 0.384615                0.003236                           0.016181                        0.008091        6.816678              0.022060                         0.110302                      0.055151    0.508359   -0.300070     1.886692    -1.461049\n",
       "125              4                     301       43.08              4                         0                      1                   0.036545        0.143123                  0.363636              11.0                 3.916364                                    0.000000                                 0.090909                0.013289                           0.000000                        0.003322        6.987001              0.092851                         0.000000                      0.023213   -0.289943    0.100431    -1.631992     0.475209\n",
       "126              5                    1607       11.70              7                         6                      0                   0.002489        0.007281                  1.750000               4.0                 2.925000                                    1.500000                                 0.000000                0.004356                           0.003734                        0.000000      137.350427              0.598291                         0.512821                      0.000000   -0.328511    0.319326    -1.799030     1.136609\n",
       "127              1                     488       35.97              0                         3                      0                   0.014344        0.073709                  0.000000               7.0                 5.138571                                    0.428571                                 0.000000                0.000000                           0.006148                        0.000000       13.566861              0.000000                         0.083403                      0.000000   -0.429844   -0.490044    -1.923349    -1.716013\n",
       "128              3                    1466       18.47              1                         7                      2                   0.017735        0.012599                  0.038462              26.0                 0.710385                                    0.269231                                 0.076923                0.000682                           0.004775                        0.001364       79.371955              0.054142                         0.378993                      0.108284   -0.246775   -0.109779    -0.642605    -0.511483\n",
       "129              4                    1040       65.36             10                         3                      0                   0.020192        0.062846                  0.476190              21.0                 3.112381                                    0.142857                                 0.000000                0.009615                           0.002885                        0.000000       15.911873              0.152999                         0.045900                      0.000000   -0.525571    0.098653    -2.509734     0.494890\n",
       "130              6                     455       42.89              8                         0                      3                   0.028571        0.094264                  0.615385              13.0                 3.299231                                    0.000000                                 0.230769                0.017582                           0.000000                        0.006593       10.608533              0.186524                         0.000000                      0.069946    0.076987    0.493971     0.377488     2.139227\n",
       "131              4                    1695        8.71              9                         2                      2                   0.008850        0.005139                  0.600000              15.0                 0.580667                                    0.133333                                 0.133333                0.005310                           0.001180                        0.001180      194.603904              1.033295                         0.229621                      0.229621   -0.123339    0.097346    -0.585983     0.502683\n",
       "132              4                     518       35.77              0                         2                      2                   0.042471        0.069054                  0.000000              22.0                 1.625909                                    0.090909                                 0.090909                0.000000                           0.003861                        0.003861       14.481409              0.000000                         0.055913                      0.055913   -0.238255    0.087977    -0.624167     0.530130\n",
       "133              6                    1183       55.17              9                         9                      3                   0.015216        0.046636                  0.500000              18.0                 3.065000                                    0.500000                                 0.166667                0.007608                           0.007608                        0.002536       21.442813              0.163132                         0.163132                      0.054377    0.094516    0.499920     0.372490     2.355158\n",
       "134              3                    1230       80.65             10                         1                      3                   0.024390        0.065569                  0.333333              30.0                 2.688333                                    0.033333                                 0.100000                0.008130                           0.000813                        0.002439       15.251085              0.123993                         0.012399                      0.037198   -0.086448   -0.120337     0.342721    -0.582871\n",
       "135              6                     571       14.13              6                         8                      5                   0.026270        0.024746                  0.400000              15.0                 0.942000                                    0.533333                                 0.333333                0.010508                           0.014011                        0.008757       40.410474              0.424628                         0.566171                      0.353857    0.461146    0.494981     1.952633     2.082048\n",
       "136              2                     252       26.07              5                         9                      2                   0.059524        0.103452                  0.333333              15.0                 1.738000                                    0.600000                                 0.133333                0.019841                           0.035714                        0.007937        9.666283              0.191791                         0.345224                      0.076717   -0.083666   -0.295489    -0.549774    -1.630075\n",
       "137              1                     806       37.38              5                         5                      3                   0.008685        0.046377                  0.714286               7.0                 5.340000                                    0.714286                                 0.428571                0.006203                           0.006203                        0.003722       21.562333              0.133761                         0.133761                      0.080257    0.178951   -0.493633     0.390586    -1.946326\n",
       "138              1                    1798        4.14              4                         8                      3                   0.006674        0.002303                  0.333333              12.0                 0.345000                                    0.666667                                 0.250000                0.002225                           0.004449                        0.001669      434.299517              0.966184                         1.932367                      0.724638    0.114780   -0.496235     0.379510    -2.147009\n",
       "139              1                     583       86.07              9                         9                      5                   0.029160        0.147633                  0.529412              17.0                 5.062941                                    0.529412                                 0.294118                0.015437                           0.015437                        0.008576        6.773556              0.104566                         0.104566                      0.058092    0.485475   -0.503528     1.881517    -2.084089\n",
       "140              5                     735       48.40              2                         7                      4                   0.010884        0.065850                  0.250000               8.0                 6.050000                                    0.875000                                 0.500000                0.002721                           0.009524                        0.005442       15.185950              0.041322                         0.144628                      0.082645    0.353275    0.303670     1.336312     1.440252\n",
       "141              6                     926        4.05              0                         6                      5                   0.026998        0.004374                  0.000000              25.0                 0.162000                                    0.240000                                 0.200000                0.000000                           0.006479                        0.005400      228.641975              0.000000                         1.481481                      1.234568    0.295811    0.480973     1.946658     2.329079\n",
       "142              3                    1789       46.50              7                         0                      3                   0.001118        0.025992                  3.500000               2.0                23.250000                                    0.000000                                 1.500000                0.003913                           0.000000                        0.001677       38.473118              0.150538                         0.000000                      0.064516    0.289340   -0.090152     0.546109    -0.407515\n",
       "143              5                    1387       58.57              3                         7                      1                   0.000721        0.042228                  3.000000               1.0                58.570000                                    7.000000                                 1.000000                0.002163                           0.005047                        0.000721       23.681065              0.051221                         0.119515                      0.017074    0.047770    0.330549     3.250104     0.146613\n",
       "144              3                     252       72.77             11                         1                      5                   0.023810        0.288770                  1.833333               6.0                12.128333                                    0.166667                                 0.833333                0.043651                           0.003968                        0.019841        3.462966              0.151161                         0.013742                      0.068710    0.591178   -0.100129     1.660981    -0.425891\n",
       "145              6                     133       13.79              0                         2                      0                   0.157895        0.103684                  0.000000              21.0                 0.656667                                    0.095238                                 0.000000                0.000000                           0.015038                        0.000000        9.644670              0.000000                         0.145033                      0.000000   -0.600899    0.493726    -2.185368     2.021419\n",
       "146              5                    1784       11.42              4                         5                      4                   0.012892        0.006401                  0.173913              23.0                 0.496522                                    0.217391                                 0.173913                0.002242                           0.002803                        0.002242      156.217163              0.350263                         0.437828                      0.350263    0.149957    0.286002     1.517198     1.945273\n",
       "147              2                     392       20.86              9                         5                      2                   0.030612        0.053214                  0.750000              12.0                 1.738333                                    0.416667                                 0.166667                0.022959                           0.012755                        0.005102       18.791946              0.431448                         0.239693                      0.095877   -0.058865   -0.295941    -0.519896    -1.583911\n",
       "148              6                    1400       61.12              0                         8                      0                   0.001429        0.043657                  0.000000               2.0                30.560000                                    4.000000                                 0.000000                0.000000                           0.005714                        0.000000       22.905759              0.000000                         0.130890                      0.000000   -0.276335    0.525029    -1.310742     1.230293\n",
       "149              4                     486       94.09             11                         9                      5                   0.026749        0.193601                  0.846154              13.0                 7.237692                                    0.692308                                 0.384615                0.022634                           0.018519                        0.010288        5.165267              0.116909                         0.095653                      0.053141    0.550154    0.101063     1.911370     0.481390\n",
       "150              1                     354       82.36              4                         3                      4                   0.056497        0.232655                  0.200000              20.0                 4.118000                                    0.150000                                 0.200000                0.011299                           0.008475                        0.011299        4.298203              0.048567                         0.036425                      0.048567    0.194579   -0.512313     1.333194    -2.401191\n",
       "151              2                    1077       85.23              1                         8                      0                   0.022284        0.079136                  0.041667              24.0                 3.551250                                    0.333333                                 0.000000                0.000929                           0.007428                        0.000000       12.636396              0.011733                         0.093864                      0.000000   -0.560821   -0.300450    -2.395571    -1.588696\n",
       "152              6                     986       38.30              2                         4                      2                   0.011156        0.038844                  0.181818              11.0                 3.481818                                    0.363636                                 0.181818                0.002028                           0.004057                        0.002028       25.744125              0.052219                         0.104439                      0.052219   -0.087083    0.501789    -0.515210     2.188878\n",
       "153              4                     883       71.85             11                         9                      2                   0.010193        0.081370                  1.222222               9.0                 7.983333                                    1.000000                                 0.222222                0.012458                           0.010193                        0.002265       12.289492              0.153097                         0.125261                      0.027836    0.042803    0.113504    -0.527827     0.491065\n",
       "154              3                    1064       13.03             11                         6                      0                   0.022556        0.012246                  0.458333              24.0                 0.542917                                    0.250000                                 0.000000                0.010338                           0.005639                        0.000000       81.657713              0.844206                         0.460476                      0.000000   -0.548807   -0.101111    -2.508782    -0.502444\n",
       "155              4                    1387       45.70              4                         7                      4                   0.019466        0.032949                  0.148148              27.0                 1.692593                                    0.259259                                 0.148148                0.002884                           0.005047                        0.002884       30.350109              0.087527                         0.153173                      0.087527    0.131617    0.084744     1.619102     0.531306\n",
       "156              6                     701       97.76              4                         9                      3                   0.037090        0.139458                  0.153846              26.0                 3.760000                                    0.346154                                 0.115385                0.005706                           0.012839                        0.004280        7.170622              0.040917                         0.092062                      0.030687   -0.007980    0.490875     0.332471     2.474133\n",
       "157              4                    1649       88.15              1                         7                      3                   0.011522        0.053457                  0.052632              19.0                 4.639474                                    0.368421                                 0.157895                0.000606                           0.004245                        0.001819       18.706750              0.011344                         0.079410                      0.034033    0.037613    0.095235     0.363114     0.500594\n",
       "158              6                    1074       24.42              9                         6                      0                   0.023277        0.022737                  0.360000              25.0                 0.976800                                    0.240000                                 0.000000                0.008380                           0.005587                        0.000000       43.980344              0.368550                         0.245700                      0.000000   -0.566490    0.497118    -2.260485     2.198636\n",
       "159              1                      77       68.76             11                        10                      4                   0.233766        0.892987                  0.611111              18.0                 3.820000                                    0.555556                                 0.222222                0.142857                           0.129870                        0.051948        1.119837              0.159977                         0.145433                      0.058173    0.302100   -0.500545     1.288634    -2.168150\n",
       "160              1                    1598       16.47              6                         1                      0                   0.018773        0.010307                  0.200000              30.0                 0.549000                                    0.033333                                 0.000000                0.003755                           0.000626                        0.000000       97.024894              0.364299                         0.060716                      0.000000   -0.674898   -0.513560    -2.257268    -2.174318\n",
       "161              1                      16       29.44              9                         1                      5                   0.625000        1.840000                  0.900000              10.0                 2.944000                                    0.100000                                 0.500000                0.562500                           0.062500                        0.312500        0.543478              0.305707                         0.033967                      0.169837    0.495087   -0.506332     1.784030    -1.787131\n",
       "162              1                    1307       36.94              4                         1                      4                   0.005356        0.028263                  0.571429               7.0                 5.277143                                    0.142857                                 0.571429                0.003060                           0.000765                        0.003060       35.381700              0.108284                         0.027071                      0.108284    0.326265   -0.501366     1.263510    -1.938844\n",
       "163              2                    1437       49.61              2                         0                      4                   0.013222        0.034523                  0.105263              19.0                 2.611053                                    0.000000                                 0.210526                0.001392                           0.000000                        0.002784       28.965934              0.040314                         0.000000                      0.080629    0.161229   -0.315550     1.420020    -1.908916\n",
       "164              5                     872       18.26              3                         7                      2                   0.010321        0.020940                  0.333333               9.0                 2.028889                                    0.777778                                 0.222222                0.003440                           0.008028                        0.002294       47.754655              0.164294                         0.383352                      0.109529   -0.039924    0.307610    -0.502431     1.561197\n",
       "165              6                     498       56.83              1                         0                      0                   0.014056        0.114116                  0.142857               7.0                 8.118571                                    0.000000                                 0.000000                0.002008                           0.000000                        0.000000        8.762977              0.017596                         0.000000                      0.000000   -0.440682    0.506328    -1.885966     1.746130\n",
       "166              5                     174       21.04              6                         6                      5                   0.051724        0.120920                  0.666667               9.0                 2.337778                                    0.666667                                 0.555556                0.034483                           0.034483                        0.028736        8.269962              0.285171                         0.285171                      0.237643    0.528609    0.299278     1.921933     1.361451\n",
       "167              3                    1273        4.72              7                         0                      5                   0.013354        0.003708                  0.411765              17.0                 0.277647                                    0.000000                                 0.294118                0.005499                           0.000000                        0.003928      269.703390              1.483051                         0.000000                      1.059322    0.378371   -0.115637     2.115145    -0.579228\n",
       "168              6                    1019       63.42              0                         8                      1                   0.012758        0.062237                  0.000000              13.0                 4.878462                                    0.615385                                 0.076923                0.000000                           0.007851                        0.000981       16.067487              0.000000                         0.126143                      0.015768   -0.266321    0.506886    -1.476022     2.055631\n",
       "169              5                    1706       49.16             11                         5                      1                   0.016999        0.028816                  0.379310              29.0                 1.695172                                    0.172414                                 0.034483                0.006448                           0.002931                        0.000586       34.703011              0.223759                         0.101709                      0.020342   -0.417305    0.290406    -1.838035     1.807709\n",
       "170              2                     475       34.30              8                         6                      0                   0.056842        0.072211                  0.296296              27.0                 1.270370                                    0.222222                                 0.000000                0.016842                           0.012632                        0.000000       13.848397              0.233236                         0.174927                      0.000000   -0.589245   -0.304641    -2.434409    -1.585167\n",
       "171              2                     934       56.45              9                        10                      2                   0.016060        0.060439                  0.600000              15.0                 3.763333                                    0.666667                                 0.133333                0.009636                           0.010707                        0.002141       16.545616              0.159433                         0.177148                      0.035430   -0.044783   -0.292619    -0.550653    -1.667654\n",
       "172              6                     605       91.49              0                         4                      0                   0.039669        0.151223                  0.000000              24.0                 3.812083                                    0.166667                                 0.000000                0.000000                           0.006612                        0.000000        6.612745              0.000000                         0.043721                      0.000000   -0.593271    0.494126    -2.242729     2.116446\n",
       "173              5                    1001       17.92              5                         4                      3                   0.018981        0.017902                  0.263158              19.0                 0.943158                                    0.210526                                 0.157895                0.004995                           0.003996                        0.002997       55.859375              0.279018                         0.223214                      0.167411    0.012420    0.291662     0.363158     1.954131\n",
       "174              2                    1621       33.91              8                         0                      0                   0.017273        0.020919                  0.285714              28.0                 1.211071                                    0.000000                                 0.000000                0.004935                           0.000000                        0.000000       47.803008              0.235919                         0.000000                      0.000000   -0.645039   -0.312165    -2.441574    -1.590580\n",
       "175              1                    1197       11.15              1                         9                      3                   0.010860        0.009315                  0.076923              13.0                 0.857692                                    0.692308                                 0.230769                0.000835                           0.007519                        0.002506      107.354260              0.089686                         0.807175                      0.269058    0.097118   -0.497028     0.380270    -2.195638\n",
       "176              2                     150       33.51              8                         3                      1                   0.193333        0.223400                  0.275862              29.0                 1.155517                                    0.103448                                 0.034483                0.053333                           0.020000                        0.006667        4.476276              0.238735                         0.089526                      0.029842   -0.450280   -0.312841    -1.811161    -1.732991\n",
       "177              6                    1500       36.88              1                         6                      4                   0.012000        0.024587                  0.055556              18.0                 2.048889                                    0.333333                                 0.222222                0.000667                           0.004000                        0.002667       40.672451              0.027115                         0.162690                      0.108460    0.207546    0.491452     1.395871     2.374583\n",
       "178              2                     817       24.61             11                        10                      2                   0.012240        0.030122                  1.100000              10.0                 2.461000                                    1.000000                                 0.200000                0.013464                           0.012240                        0.002448       33.197887              0.446973                         0.406339                      0.081268    0.018372   -0.287020    -0.559148    -1.401861\n",
       "179              2                    1525        1.98              7                         3                      0                   0.001311        0.001298                  3.500000               2.0                 0.990000                                    1.500000                                 0.000000                0.004590                           0.001967                        0.000000      770.202020              3.535354                         1.515152                      0.000000   -0.316800   -0.280733    -1.658054    -1.095683\n",
       "180              5                     478       84.37              6                         5                      5                   0.039749        0.176506                  0.315789              19.0                 4.440526                                    0.263158                                 0.263158                0.012552                           0.010460                        0.010460        5.665521              0.071115                         0.059263                      0.059263    0.414915    0.288380     2.070851     1.650042\n",
       "181              5                     972       71.76              6                         8                      3                   0.005144        0.073827                  1.200000               5.0                14.352000                                    1.600000                                 0.600000                0.006173                           0.008230                        0.003086       13.545151              0.083612                         0.111483                      0.041806    0.264242    0.313588     0.379195     1.312437\n",
       "182              3                     509       21.03              1                         3                      0                   0.035363        0.041316                  0.055556              18.0                 1.168333                                    0.166667                                 0.000000                0.001965                           0.005894                        0.000000       24.203519              0.047551                         0.142653                      0.000000   -0.553342   -0.101330    -2.473010    -0.486504\n",
       "183              5                      84        5.11              0                         4                      1                   0.142857        0.060833                  0.000000              12.0                 0.425833                                    0.333333                                 0.083333                0.000000                           0.047619                        0.011905       16.438356              0.000000                         0.782779                      0.195695   -0.306115    0.302235    -1.567058     1.470711\n",
       "184              4                     373       36.81              4                         5                      0                   0.072386        0.098686                  0.148148              27.0                 1.363333                                    0.185185                                 0.000000                0.010724                           0.013405                        0.000000       10.133116              0.108666                         0.135833                      0.000000   -0.615962    0.092926    -2.577707     0.498381\n",
       "185              3                    1315       64.80              9                         7                      4                   0.004563        0.049278                  1.500000               6.0                10.800000                                    1.166667                                 0.666667                0.006844                           0.005323                        0.003042       20.293210              0.138889                         0.108025                      0.061728    0.437612   -0.090601     1.287018    -0.483106\n",
       "186              3                     760       32.90              0                         0                      4                   0.011842        0.043289                  0.000000               9.0                 3.655556                                    0.000000                                 0.444444                0.000000                           0.000000                        0.005263       23.100304              0.000000                         0.000000                      0.121581    0.264727   -0.106542     1.449901    -0.521575\n",
       "187              5                    1808       44.36              5                         0                      3                   0.013827        0.024535                  0.200000              25.0                 1.774400                                    0.000000                                 0.120000                0.002765                           0.000000                        0.001659       40.757439              0.112714                         0.000000                      0.067628   -0.076099    0.281653     0.394807     1.997327\n",
       "188              2                    1689       23.54             10                         7                      5                   0.004144        0.013937                  1.428571               7.0                 3.362857                                    1.000000                                 0.714286                0.005921                           0.004144                        0.002960       71.750212              0.424809                         0.297366                      0.212404    0.593354   -0.294885     1.767289    -1.242308\n",
       "189              6                      64       73.73              0                         4                      3                   0.218750        1.152031                  0.000000              14.0                 5.266429                                    0.285714                                 0.214286                0.000000                           0.062500                        0.046875        0.868032              0.000000                         0.054252                      0.040689    0.064350    0.495696     0.348556     2.204473\n",
       "190              5                    1005       45.66              4                         2                      4                   0.026866        0.045433                  0.148148              27.0                 1.691111                                    0.074074                                 0.148148                0.003980                           0.001990                        0.003980       22.010512              0.087604                         0.043802                      0.087604    0.095280    0.278800     1.511743     1.986378\n",
       "191              3                     993       57.11             10                        10                      0                   0.020141        0.057513                  0.500000              20.0                 2.855500                                    0.500000                                 0.000000                0.010070                           0.010070                        0.000000       17.387498              0.175101                         0.175101                      0.000000   -0.465014   -0.092257    -2.405918    -0.470611\n",
       "192              2                      60       35.59             10                         2                      4                   0.433333        0.593167                  0.384615              26.0                 1.368846                                    0.076923                                 0.153846                0.166667                           0.033333                        0.066667        1.685867              0.280978                         0.056196                      0.112391    0.134467   -0.318643     1.377448    -1.868006\n",
       "193              6                     679       86.06              8                         8                      3                   0.019146        0.126745                  0.615385              13.0                 6.620000                                    0.615385                                 0.230769                0.011782                           0.011782                        0.004418        7.889844              0.092958                         0.092958                      0.034859    0.154100    0.504049     0.374560     2.175920\n",
       "194              2                    1717       45.44              4                         9                      2                   0.009901        0.026465                  0.235294              17.0                 2.672941                                    0.529412                                 0.117647                0.002330                           0.005242                        0.001165       37.786092              0.088028                         0.198063                      0.044014   -0.106768   -0.297170    -0.578729    -1.813681\n",
       "195              1                     175       97.26             10                         7                      1                   0.097143        0.555771                  0.588235              17.0                 5.721176                                    0.411765                                 0.058824                0.057143                           0.040000                        0.005714        1.799301              0.102817                         0.071972                      0.010282   -0.254858   -0.494790    -1.508955    -2.094675\n",
       "196              2                     888       96.19              6                         5                      0                   0.010135        0.108322                  0.666667               9.0                10.687778                                    0.555556                                 0.000000                0.006757                           0.005631                        0.000000        9.231729              0.062377                         0.051980                      0.000000   -0.379825   -0.286914    -2.101203    -1.326087\n",
       "197              4                     167       30.81             10                         2                      3                   0.113772        0.184491                  0.526316              19.0                 1.621579                                    0.105263                                 0.157895                0.059880                           0.011976                        0.017964        5.420318              0.324570                         0.064914                      0.097371    0.028507    0.090757     0.384752     0.510863\n",
       "198              2                    1314       17.78              2                         1                      0                   0.000761        0.013531                  2.000000               1.0                17.780000                                    1.000000                                 0.000000                0.001522                           0.000761                        0.000000       73.903262              0.112486                         0.056243                      0.000000   -0.341165   -0.283304    -1.684899    -1.202937\n",
       "199              4                     849       92.81             10                         3                      2                   0.002356        0.109317                  5.000000               2.0                46.405000                                    1.500000                                 1.000000                0.011779                           0.003534                        0.002356        9.147721              0.107747                         0.032324                      0.021549    0.185274    0.120086    -0.295869     0.278374\n",
       "200              5                     303        2.72             10                         8                      1                   0.039604        0.008977                  0.833333              12.0                 0.226667                                    0.666667                                 0.083333                0.033003                           0.026403                        0.003300      111.397059              3.676471                         2.941176                      0.367647   -0.221813    0.309872    -1.446125     1.335933\n",
       "201              6                     186       42.15              7                         3                      4                   0.048387        0.226613                  0.777778               9.0                 4.683333                                    0.333333                                 0.444444                0.037634                           0.016129                        0.021505        4.412811              0.166074                         0.071174                      0.094899    0.332402    0.498828     1.264839     1.934676\n",
       "202              2                    1816       72.32              2                         6                      5                   0.014317        0.039824                  0.076923              26.0                 2.781538                                    0.230769                                 0.192308                0.001101                           0.003304                        0.002753       25.110619              0.027655                         0.082965                      0.069137    0.316737   -0.317950     2.086976    -1.886836\n",
       "203              5                     471       60.59              8                         2                      4                   0.048832        0.128641                  0.347826              23.0                 2.634348                                    0.086957                                 0.173913                0.016985                           0.004246                        0.008493        7.773560              0.132035                         0.033009                      0.066017    0.165588    0.283829     1.430841     1.856731\n",
       "204              5                     591       11.09              7                         1                      4                   0.001692        0.018765                  7.000000               1.0                11.090000                                    1.000000                                 4.000000                0.011844                           0.001692                        0.006768       53.291253              0.631199                         0.090171                      0.360685    0.616504    0.313159     3.442124     0.126928\n",
       "205              4                     401       84.09              3                         2                      5                   0.049875        0.209701                  0.150000              20.0                 4.204500                                    0.100000                                 0.250000                0.007481                           0.004988                        0.012469        4.768700              0.035676                         0.023784                      0.059460    0.365517    0.083210     2.087890     0.531684\n",
       "206              2                     574        0.07             10                         0                      0                   0.015679        0.000122                  1.111111               9.0                 0.007778                                    0.000000                                 0.000000                0.017422                           0.000000                        0.000000     8200.000000            142.857143                         0.000000                      0.000000   -0.435042   -0.293384    -1.985836    -1.258672\n",
       "207              6                    1227        7.76              0                         8                      3                   0.012225        0.006324                  0.000000              15.0                 0.517333                                    0.533333                                 0.200000                0.000000                           0.006520                        0.002445      158.118557              0.000000                         1.030928                      0.386598    0.058595    0.498770     0.351063     2.302593\n",
       "208              4                     605       73.28              2                         0                      1                   0.016529        0.121124                  0.200000              10.0                 7.328000                                    0.000000                                 0.100000                0.003306                           0.000000                        0.001653        8.256004              0.027293                         0.000000                      0.013646   -0.277961    0.101422    -1.562473     0.449867\n",
       "209              6                     877       14.63              2                         6                      3                   0.030787        0.016682                  0.074074              27.0                 0.541852                                    0.222222                                 0.111111                0.002281                           0.006842                        0.003421       59.945318              0.136705                         0.410116                      0.205058   -0.078370    0.485045     0.389452     2.560760\n",
       "210              5                    1149       19.94              7                         1                      4                   0.013925        0.017354                  0.437500              16.0                 1.246250                                    0.062500                                 0.250000                0.006092                           0.000870                        0.003481       57.622869              0.351053                         0.050150                      0.200602    0.219046    0.289120     1.397904     1.686173\n",
       "211              2                      51       76.96             11                         8                      3                   0.549020        1.509020                  0.392857              28.0                 2.748571                                    0.285714                                 0.107143                0.215686                           0.156863                        0.058824        0.662682              0.142931                         0.103950                      0.038981   -0.008287   -0.310349     0.346922    -1.870742\n",
       "212              6                    1570       81.60              8                         3                      4                   0.015924        0.051975                  0.320000              25.0                 3.264000                                    0.120000                                 0.160000                0.005096                           0.001911                        0.002548       19.240196              0.098039                         0.036765                      0.049020    0.156245    0.483390     1.401493     2.479143\n",
       "213              2                     148       55.66              2                         0                      4                   0.162162        0.376081                  0.083333              24.0                 2.319167                                    0.000000                                 0.166667                0.013514                           0.000000                        0.027027        2.659001              0.035932                         0.000000                      0.071865    0.108193   -0.320773     1.394666    -1.952027\n",
       "214              1                    1037       33.26             10                         6                      4                   0.005786        0.032073                  1.666667               6.0                 5.543333                                    1.000000                                 0.666667                0.009643                           0.005786                        0.003857       31.178593              0.300661                         0.180397                      0.120265    0.421547   -0.491970     1.209323    -1.778674\n",
       "215              6                     698       15.93              4                        10                      5                   0.034384        0.022822                  0.166667              24.0                 0.663750                                    0.416667                                 0.208333                0.005731                           0.014327                        0.007163       43.816698              0.251099                         0.627746                      0.313873    0.360773    0.487661     1.959383     2.308124\n",
       "216              1                    1332        9.19              5                        10                      4                   0.010511        0.006899                  0.357143              14.0                 0.656429                                    0.714286                                 0.285714                0.003754                           0.007508                        0.003003      144.940152              0.544070                         1.088139                      0.435256    0.297457   -0.498436     1.259410    -2.106750\n",
       "217              2                     658       58.00              2                         8                      2                   0.022796        0.088146                  0.133333              15.0                 3.866667                                    0.533333                                 0.133333                0.003040                           0.012158                        0.003040       11.344828              0.034483                         0.137931                      0.034483   -0.096190   -0.296949    -0.555048    -1.738005\n",
       "218              3                     723       31.00             11                         9                      2                   0.016598        0.042877                  0.916667              12.0                 2.583333                                    0.750000                                 0.166667                0.015214                           0.012448                        0.002766       23.322581              0.354839                         0.290323                      0.064516   -0.013583   -0.090532    -0.528755    -0.525878\n",
       "219              6                    1391       69.14             11                         2                      2                   0.003595        0.049705                  2.200000               5.0                13.828000                                    0.400000                                 0.400000                0.007908                           0.001438                        0.001438       20.118600              0.159097                         0.028927                      0.028927    0.051391    0.510208    -0.505678     1.634006\n",
       "220              4                    1643       73.39              6                         1                      1                   0.017651        0.044668                  0.206897              29.0                 2.530690                                    0.034483                                 0.034483                0.003652                           0.000609                        0.000609       22.387246              0.081755                         0.013626                      0.013626   -0.463446    0.084954    -2.024617     0.533434\n",
       "221              6                     852       42.93              3                         9                      2                   0.034038        0.050387                  0.103448              29.0                 1.480345                                    0.310345                                 0.068966                0.003521                           0.010563                        0.002347       19.846261              0.069881                         0.209644                      0.046587   -0.246956    0.489775    -0.644835     2.516904\n",
       "222              1                     868       62.30              5                         6                      0                   0.010369        0.071774                  0.555556               9.0                 6.922222                                    0.666667                                 0.000000                0.005760                           0.006912                        0.000000       13.932584              0.080257                         0.096308                      0.000000   -0.390702   -0.486453    -2.012085    -1.829113\n",
       "223              1                    1186       91.64              4                        10                      0                   0.007589        0.077268                  0.444444               9.0                10.182222                                    1.111111                                 0.000000                0.003373                           0.008432                        0.000000       12.941947              0.043649                         0.109123                      0.000000   -0.352883   -0.481367    -1.802917    -1.667604\n",
       "224              3                     867       61.82              5                        10                      3                   0.012687        0.071303                  0.454545              11.0                 5.620000                                    0.909091                                 0.272727                0.005767                           0.011534                        0.003460       14.024588              0.080880                         0.161760                      0.048528    0.170813   -0.092140     0.380211    -0.502559\n",
       "225              5                    1489       93.17              3                         7                      1                   0.003358        0.062572                  0.600000               5.0                18.634000                                    1.400000                                 0.200000                0.002015                           0.004701                        0.000672       15.981539              0.032199                         0.075131                      0.010733   -0.133884    0.316928    -1.280134     1.250465\n",
       "226              1                     309       54.72              1                         9                      2                   0.067961        0.177087                  0.047619              21.0                 2.605714                                    0.428571                                 0.095238                0.003236                           0.029126                        0.006472        5.646930              0.018275                         0.164474                      0.036550   -0.163369   -0.502154    -0.640747    -2.371208\n",
       "227              3                    1709       12.78              4                         1                      3                   0.006437        0.007478                  0.363636              11.0                 1.161818                                    0.090909                                 0.272727                0.002341                           0.000585                        0.001755      133.724570              0.312989                         0.078247                      0.234742    0.075566   -0.103639     0.428342    -0.515022\n",
       "228              3                     161       39.01              8                         2                      0                   0.093168        0.242298                  0.533333              15.0                 2.600667                                    0.133333                                 0.000000                0.049689                           0.012422                        0.000000        4.127147              0.205076                         0.051269                      0.000000   -0.484544   -0.097448    -2.346547    -0.439370\n",
       "229              2                     208        2.23              1                        10                      2                   0.091346        0.010721                  0.052632              19.0                 0.117368                                    0.526316                                 0.105263                0.004808                           0.048077                        0.009615       93.273543              0.448430                         4.484305                      0.896861   -0.151180   -0.299799    -0.623690    -1.723073\n",
       "230              2                    1572       83.24              6                         3                      5                   0.013995        0.052952                  0.272727              22.0                 3.783636                                    0.136364                                 0.227273                0.003817                           0.001908                        0.003181       18.885151              0.072081                         0.036040                      0.060067    0.364162   -0.316287     2.119008    -1.840066\n",
       "231              6                    1299       54.84              5                         0                      1                   0.002309        0.042217                  1.666667               3.0                18.280000                                    0.000000                                 0.333333                0.003849                           0.000000                        0.000770       23.687090              0.091174                         0.000000                      0.018235   -0.161877    0.510681    -1.261282     1.653437\n",
       "232              1                    1701       94.49              3                         7                      1                   0.008818        0.055550                  0.200000              15.0                 6.299333                                    0.466667                                 0.066667                0.001764                           0.004115                        0.000588       18.001905              0.031749                         0.074082                      0.010583   -0.270420   -0.494255    -1.523670    -2.134762\n",
       "233              4                     410       11.13              7                         2                      2                   0.073171        0.027146                  0.233333              30.0                 0.371000                                    0.066667                                 0.066667                0.017073                           0.004878                        0.004878       36.837376              0.628931                         0.179695                      0.179695   -0.298526    0.081630    -0.602282     0.557966\n",
       "234              2                     859       55.80             11                        10                      4                   0.019790        0.064959                  0.647059              17.0                 3.282353                                    0.588235                                 0.235294                0.012806                           0.011641                        0.004657       15.394265              0.197133                         0.179211                      0.071685    0.308933   -0.299581     1.388300    -1.639994\n",
       "235              4                    1106       35.40              9                         3                      0                   0.011754        0.032007                  0.692308              13.0                 2.723077                                    0.230769                                 0.000000                0.008137                           0.002712                        0.000000       31.242938              0.254237                         0.084746                      0.000000   -0.450975    0.106126    -2.329400     0.458421\n",
       "236              6                    1073       64.26              0                         3                      1                   0.022367        0.059888                  0.000000              24.0                 2.677500                                    0.125000                                 0.041667                0.000000                           0.002796                        0.000932       16.697790              0.000000                         0.046685                      0.015562   -0.427074    0.490085    -1.721572     2.358052\n",
       "237              1                     489       67.27              0                         9                      0                   0.014315        0.137566                  0.000000               7.0                 9.610000                                    1.285714                                 0.000000                0.000000                           0.018405                        0.000000        7.269214              0.000000                         0.133789                      0.000000   -0.366393   -0.482060    -1.730990    -1.520159\n",
       "238              4                      22       60.15              8                         9                      3                   1.136364        2.734091                  0.320000              25.0                 2.406000                                    0.360000                                 0.120000                0.363636                           0.409091                        0.136364        0.365752              0.133001                         0.149626                      0.049875    0.011980    0.092557     0.359428     0.520287\n",
       "239              2                       4       58.11              7                         7                      4                   5.000000       14.527500                  0.350000              20.0                 2.905500                                    0.350000                                 0.200000                1.750000                           1.750000                        1.000000        0.068835              0.120461                         0.120461                      0.068835    0.232398   -0.307440     1.266045    -1.291368\n",
       "240              3                       7       28.58              3                         1                      0                   1.857143        4.082857                  0.230769              13.0                 2.198462                                    0.076923                                 0.000000                0.428571                           0.142857                        0.000000        0.244927              0.104969                         0.034990                      0.000000   -0.500124   -0.098199    -2.258512    -0.437479\n",
       "241              4                     208       77.53              2                         5                      2                   0.048077        0.372740                  0.200000              10.0                 7.753000                                    0.500000                                 0.200000                0.009615                           0.024038                        0.009615        2.682832              0.025796                         0.064491                      0.025796   -0.050841    0.104713    -0.561977     0.473652\n",
       "242              4                    1331       62.88              7                         9                      1                   0.003757        0.047243                  1.400000               5.0                12.576000                                    1.800000                                 0.200000                0.005259                           0.006762                        0.000751       21.167303              0.111323                         0.143130                      0.015903   -0.104432    0.120352    -1.396567     0.415872\n",
       "243              5                     619       32.10              3                         6                      2                   0.033926        0.051858                  0.142857              21.0                 1.528571                                    0.285714                                 0.095238                0.004847                           0.009693                        0.003231       19.283489              0.093458                         0.186916                      0.062305   -0.183667    0.294208    -0.576585     1.917542\n",
       "244              6                      45       51.88              8                         7                      2                   0.577778        1.152889                  0.307692              26.0                 1.995385                                    0.269231                                 0.076923                0.177778                           0.155556                        0.044444        0.867386              0.154202                         0.134927                      0.038551   -0.199700    0.491629    -0.620596     2.438824\n",
       "245              6                     291       87.69              7                         1                      5                   0.079038        0.301340                  0.304348              23.0                 3.812609                                    0.043478                                 0.217391                0.024055                           0.003436                        0.017182        3.318508              0.079827                         0.011404                      0.057019    0.345184    0.479855     1.864071     2.233412\n",
       "246              5                    1077       98.03              3                         2                      1                   0.013928        0.091021                  0.200000              15.0                 6.535333                                    0.133333                                 0.066667                0.002786                           0.001857                        0.000929       10.986433              0.030603                         0.020402                      0.010201   -0.307090    0.299263    -1.685744     1.584835\n",
       "247              6                    1216        1.76              6                         7                      2                   0.022204        0.001447                  0.222222              27.0                 0.065185                                    0.259259                                 0.074074                0.004934                           0.005757                        0.001645      690.909091              3.409091                         3.977273                      1.136364   -0.238180    0.489873    -0.638376     2.504639\n",
       "248              5                     877       12.59              5                         0                      5                   0.023945        0.014356                  0.238095              21.0                 0.599524                                    0.000000                                 0.238095                0.005701                           0.000000                        0.005701       69.658459              0.397141                         0.000000                      0.397141    0.324896    0.279568     2.101542     1.746174\n",
       "249              6                    1698       86.47              6                         5                      4                   0.006478        0.050925                  0.545455              11.0                 7.860909                                    0.454545                                 0.363636                0.003534                           0.002945                        0.002356       19.636868              0.069388                         0.057824                      0.046259    0.330386    0.499763     1.305311     2.059500\n",
       "...            ...                     ...         ...            ...                       ...                    ...                        ...             ...                       ...               ...                      ...                                         ...                                      ...                     ...                                ...                             ...             ...                   ...                              ...                           ...         ...         ...          ...          ...\n",
       "497871           3                     214       42.32              8                        10                      1                   0.102804        0.197757                  0.363636              22.0                 1.923636                                    0.454545                                 0.045455                0.037383                           0.046729                        0.004673        5.056711              0.189036                         0.236295                      0.023629   -0.318887   -0.097898    -1.847975    -0.463108\n",
       "497872           1                      40       63.03              4                         2                      2                   0.300000        1.575750                  0.333333              12.0                 5.252500                                    0.166667                                 0.166667                0.100000                           0.050000                        0.050000        0.634618              0.063462                         0.031731                      0.031731   -0.093333   -0.500276    -0.563991    -2.141233\n",
       "497873           4                     822        8.40              8                         9                      3                   0.035280        0.010219                  0.275862              29.0                 0.289655                                    0.310345                                 0.103448                0.009732                           0.010949                        0.003650       97.857143              0.952381                         1.071429                      0.357143   -0.050245    0.088211     0.382395     0.540968\n",
       "497874           3                     277       80.70             10                         4                      5                   0.064982        0.291336                  0.555556              18.0                 4.483333                                    0.222222                                 0.277778                0.036101                           0.014440                        0.018051        3.432466              0.123916                         0.049566                      0.061958    0.439175   -0.110510     2.066726    -0.575874\n",
       "497875           3                    1291       10.97              3                        10                      1                   0.019365        0.008497                  0.120000              25.0                 0.438800                                    0.400000                                 0.040000                0.002324                           0.007746                        0.000775      117.684594              0.273473                         0.911577                      0.091158   -0.388849   -0.102270    -1.958143    -0.480060\n",
       "497876           6                    1723       99.46              8                         5                      5                   0.012768        0.057725                  0.363636              22.0                 4.520909                                    0.227273                                 0.227273                0.004643                           0.002902                        0.002902       17.323547              0.080434                         0.050271                      0.050271    0.393984    0.486245     1.960557     2.257914\n",
       "497877           4                     889       43.05             10                         6                      4                   0.033746        0.048425                  0.333333              30.0                 1.435000                                    0.200000                                 0.133333                0.011249                           0.006749                        0.004499       20.650407              0.232288                         0.139373                      0.092915    0.120549    0.082003     1.558144     0.521085\n",
       "497878           3                    1657       26.99             10                        10                      5                   0.015691        0.016288                  0.384615              26.0                 1.038077                                    0.384615                                 0.192308                0.006035                           0.006035                        0.003018       61.393109              0.370508                         0.370508                      0.185254    0.371521   -0.112003     2.102036    -0.596868\n",
       "497879           6                     239       39.45              4                         2                      3                   0.071130        0.165063                  0.235294              17.0                 2.320588                                    0.117647                                 0.176471                0.016736                           0.008368                        0.012552        6.058302              0.101394                         0.050697                      0.076046    0.022993    0.491016     0.382550     2.412438\n",
       "497880           5                     988       22.33              2                         8                      4                   0.004049        0.022601                  0.500000               4.0                 5.582500                                    2.000000                                 1.000000                0.002024                           0.008097                        0.004049       44.245410              0.089566                         0.358262                      0.179131    0.429917    0.310101     1.256478     1.248799\n",
       "497881           5                     683       56.40             11                         3                      2                   0.038067        0.082577                  0.423077              26.0                 2.169231                                    0.115385                                 0.076923                0.016105                           0.004392                        0.002928       12.109929              0.195035                         0.053191                      0.035461   -0.212855    0.288189    -0.590204     1.968852\n",
       "497882           5                    1736       23.54              6                         3                      0                   0.001728        0.013560                  2.000000               3.0                 7.846667                                    1.000000                                 0.000000                0.003456                           0.001728                        0.000000       73.746814              0.254885                         0.127443                      0.000000   -0.338180    0.316965    -1.869084     1.200675\n",
       "497883           1                    1637       55.99              1                         0                      3                   0.015883        0.034203                  0.038462              26.0                 2.153462                                    0.000000                                 0.115385                0.000611                           0.000000                        0.001833       29.237364              0.017860                         0.000000                      0.053581   -0.102898   -0.519752     0.340993    -2.515509\n",
       "497884           6                    1456        4.47              7                         5                      0                   0.008242        0.003070                  0.583333              12.0                 0.372500                                    0.416667                                 0.000000                0.004808                           0.003434                        0.000000      325.727069              1.565996                         1.118568                      0.000000   -0.446601    0.508332    -2.032781     1.927589\n",
       "497885           1                     711       60.04              1                         2                      5                   0.016878        0.084444                  0.083333              12.0                 5.003333                                    0.166667                                 0.416667                0.001406                           0.002813                        0.007032       11.842105              0.016656                         0.033311                      0.083278    0.442906   -0.508993     1.862295    -1.996423\n",
       "497886           6                      65       89.17              4                         0                      0                   0.030769        1.371846                  2.000000               2.0                44.585000                                    0.000000                                 0.000000                0.061538                           0.000000                        0.000000        0.728945              0.044858                         0.000000                      0.000000   -0.312971    0.515902    -1.350984     1.242417\n",
       "497887           5                    1277        3.81              9                         6                      4                   0.010180        0.002984                  0.692308              13.0                 0.293077                                    0.461538                                 0.307692                0.007048                           0.004699                        0.003132      335.170604              2.362205                         1.574803                      1.049869    0.298286    0.298441     1.390683     1.576313\n",
       "497888           5                     690       26.73              6                         4                      1                   0.010145        0.038739                  0.857143               7.0                 3.818571                                    0.571429                                 0.142857                0.008696                           0.005797                        0.001449       25.813692              0.224467                         0.149645                      0.037411   -0.202973    0.309912    -1.421479     1.358110\n",
       "497889           3                    1221       71.01              3                         5                      0                   0.019656        0.058157                  0.125000              24.0                 2.958750                                    0.208333                                 0.000000                0.002457                           0.004095                        0.000000       17.194761              0.042248                         0.070413                      0.000000   -0.577679   -0.103624    -2.590283    -0.505800\n",
       "497890           1                    1810       53.38              5                         1                      2                   0.009392        0.029492                  0.294118              17.0                 3.140000                                    0.058824                                 0.117647                0.002762                           0.000552                        0.001105       33.907831              0.093668                         0.018734                      0.037467   -0.158978   -0.505922    -0.612788    -2.348500\n",
       "497891           3                    1710       39.55              5                         3                      0                   0.005848        0.023129                  0.500000              10.0                 3.955000                                    0.300000                                 0.000000                0.002924                           0.001754                        0.000000       43.236410              0.126422                         0.075853                      0.000000   -0.436691   -0.091548    -2.252739    -0.443313\n",
       "497892           3                    1181       50.24              4                         8                      3                   0.025402        0.042540                  0.133333              30.0                 1.674667                                    0.266667                                 0.100000                0.003387                           0.006774                        0.002540       23.507166              0.079618                         0.159236                      0.059713   -0.075250   -0.114245     0.359917    -0.577615\n",
       "497893           6                     768       51.22              0                         8                      1                   0.029948        0.066693                  0.000000              23.0                 2.226957                                    0.347826                                 0.043478                0.000000                           0.010417                        0.001302       14.994143              0.000000                         0.156189                      0.019524   -0.383327    0.496569    -1.712215     2.338566\n",
       "497894           4                    1695        3.12              0                         2                      1                   0.005900        0.001841                  0.000000              10.0                 0.312000                                    0.200000                                 0.100000                0.000000                           0.001180                        0.000590      543.269231              0.000000                         0.641026                      0.320513   -0.301135    0.102399    -1.564669     0.485726\n",
       "497895           6                     902       96.04              1                         9                      0                   0.011086        0.106475                  0.100000              10.0                 9.604000                                    0.900000                                 0.000000                0.001109                           0.009978                        0.000000        9.391920              0.010412                         0.093711                      0.000000   -0.389311    0.514706    -1.863031     1.772185\n",
       "497896           1                     902       16.64              3                         5                      2                   0.008869        0.018448                  0.375000               8.0                 2.080000                                    0.625000                                 0.250000                0.003326                           0.005543                        0.002217       54.206731              0.180288                         0.300481                      0.120192   -0.043057   -0.493192    -0.497411    -2.050189\n",
       "497897           3                     630        7.77             10                         2                      1                   0.009524        0.012333                  1.666667               6.0                 1.295000                                    0.333333                                 0.166667                0.015873                           0.003175                        0.001587       81.081081              1.287001                         0.257400                      0.128700   -0.189188   -0.090140    -1.430743    -0.415446\n",
       "497898           5                    1140       38.15              8                        10                      1                   0.007895        0.033465                  0.888889               9.0                 4.238889                                    1.111111                                 0.111111                0.007018                           0.008772                        0.000877       29.882045              0.209699                         0.262123                      0.026212   -0.165749    0.315800    -1.402647     1.371706\n",
       "497899           3                     492       29.87              0                         6                      1                   0.030488        0.060711                  0.000000              15.0                 1.991333                                    0.400000                                 0.066667                0.000000                           0.012195                        0.002033       16.471376              0.000000                         0.200870                      0.033478   -0.315887   -0.097712    -1.750163    -0.497995\n",
       "497900           1                    1078       15.34              2                         2                      4                   0.001855        0.014230                  1.000000               2.0                 7.670000                                    1.000000                                 2.000000                0.001855                           0.001855                        0.003711       70.273794              0.130378                         0.130378                      0.260756    0.447825   -0.493488     1.373389    -1.405849\n",
       "497901           6                     965       28.15              8                         6                      3                   0.010363        0.029171                  0.800000              10.0                 2.815000                                    0.600000                                 0.300000                0.008290                           0.006218                        0.003109       34.280639              0.284192                         0.213144                      0.106572    0.156169    0.504222     0.378608     2.115143\n",
       "497902           1                    1151       49.09              0                         8                      1                   0.025195        0.042650                  0.000000              29.0                 1.692759                                    0.275862                                 0.034483                0.000000                           0.006950                        0.000869       23.446730              0.000000                         0.162966                      0.020371   -0.449582   -0.508612    -1.650975    -2.349825\n",
       "497903           3                     542       69.85              4                         2                      4                   0.036900        0.128875                  0.200000              20.0                 3.492500                                    0.100000                                 0.200000                0.007380                           0.003690                        0.007380        7.759485              0.057266                         0.028633                      0.057266    0.182536   -0.113831     1.563627    -0.576309\n",
       "497904           2                     314       31.63              9                         9                      3                   0.022293        0.100732                  1.285714               7.0                 4.518571                                    1.285714                                 0.428571                0.028662                           0.028662                        0.009554        9.927284              0.284540                         0.284540                      0.094847    0.234081   -0.287762     0.338303    -1.328888\n",
       "497905           5                     149       26.28              0                         4                      1                   0.093960        0.176376                  0.000000              14.0                 1.877143                                    0.285714                                 0.071429                0.000000                           0.026846                        0.006711        5.669711              0.000000                         0.152207                      0.038052   -0.321065    0.300549    -1.616944     1.539526\n",
       "497906           5                     374       40.41              8                         2                      2                   0.069519        0.108048                  0.307692              26.0                 1.554231                                    0.076923                                 0.076923                0.021390                           0.005348                        0.005348        9.255135              0.197971                         0.049493                      0.049493   -0.240248    0.286005    -0.575605     1.961350\n",
       "497907           3                     765       53.54             11                         4                      5                   0.022222        0.069987                  0.647059              17.0                 3.149412                                    0.235294                                 0.294118                0.014379                           0.005229                        0.006536       14.288383              0.205454                         0.074710                      0.093388    0.446651   -0.109417     2.143117    -0.575129\n",
       "497908           2                    1338       38.12             10                         7                      1                   0.011211        0.028490                  0.666667              15.0                 2.541333                                    0.466667                                 0.066667                0.007474                           0.005232                        0.000747       35.099685              0.262329                         0.183631                      0.026233   -0.253148   -0.293300    -1.606304    -1.544726\n",
       "497909           6                    1050       76.05             10                         1                      4                   0.017143        0.072429                  0.555556              18.0                 4.225000                                    0.055556                                 0.222222                0.009524                           0.000952                        0.003810       13.806706              0.131492                         0.013149                      0.052597    0.230571    0.488465     1.348774     2.320577\n",
       "497910           1                     656        8.07              2                         7                      4                   0.033537        0.012302                  0.090909              22.0                 0.366818                                    0.318182                                 0.181818                0.003049                           0.010671                        0.006098       81.288724              0.247831                         0.867410                      0.495663    0.166070   -0.511053     1.370506    -2.410851\n",
       "497911           4                     693       45.34              9                         2                      5                   0.004329        0.065426                  3.000000               3.0                15.113333                                    0.666667                                 1.666667                0.012987                           0.002886                        0.007215       15.284517              0.198500                         0.044111                      0.110278    0.666552    0.105446     1.575251     0.342943\n",
       "497912           2                    1225       29.08              9                         0                      4                   0.017143        0.023739                  0.428571              21.0                 1.384762                                    0.000000                                 0.190476                0.007347                           0.000000                        0.003265       42.125172              0.309491                         0.000000                      0.137552    0.167730   -0.315987     1.417449    -1.883173\n",
       "497913           6                     976       89.50              1                         3                      5                   0.008197        0.091701                  0.125000               8.0                11.187500                                    0.375000                                 0.625000                0.001025                           0.003074                        0.005123       10.905028              0.011173                         0.033520                      0.055866    0.517734    0.496522     1.771479     1.781639\n",
       "497914           5                     451       36.47              6                         9                      5                   0.050998        0.080865                  0.260870              23.0                 1.585652                                    0.391304                                 0.217391                0.013304                           0.019956                        0.011086       12.366328              0.164519                         0.246778                      0.137099    0.382066    0.288344     2.117811     1.728118\n",
       "497915           1                     925       97.29              8                         9                      3                   0.020541        0.105178                  0.421053              19.0                 5.120526                                    0.473684                                 0.157895                0.008649                           0.009730                        0.003243        9.507658              0.082228                         0.092507                      0.030836    0.092716   -0.500268     0.336133    -2.353075\n",
       "497916           1                    1646       57.60              7                         1                      0                   0.015188        0.034994                  0.280000              25.0                 2.304000                                    0.040000                                 0.000000                0.004253                           0.000608                        0.000000       28.576389              0.121528                         0.017361                      0.000000   -0.602324   -0.507928    -2.269138    -2.158515\n",
       "497917           3                     566       46.97              2                         6                      0                   0.017668        0.082986                  0.200000              10.0                 4.697000                                    0.600000                                 0.000000                0.003534                           0.010601                        0.000000       12.050245              0.042580                         0.127741                      0.000000   -0.425786   -0.089053    -2.216361    -0.442334\n",
       "497918           1                    1002       11.63              6                         9                      2                   0.011976        0.011607                  0.500000              12.0                 0.969167                                    0.750000                                 0.166667                0.005988                           0.008982                        0.001996       86.156492              0.515907                         0.773861                      0.171969   -0.047894   -0.491920    -0.520650    -2.125407\n",
       "497919           2                     629       53.94              1                         2                      4                   0.033386        0.085755                  0.047619              21.0                 2.568571                                    0.095238                                 0.190476                0.001590                           0.003180                        0.006359       11.661105              0.018539                         0.037078                      0.074156    0.150477   -0.315663     1.427899    -1.941239\n",
       "497920           2                     611       44.62              0                         7                      5                   0.049100        0.073028                  0.000000              30.0                 1.487333                                    0.233333                                 0.166667                0.000000                           0.011457                        0.008183       13.693411              0.000000                         0.156880                      0.112057    0.261445   -0.321915     2.089285    -1.877363\n",
       "497921           4                     950       17.65              4                         2                      5                   0.002105        0.018579                  2.000000               2.0                 8.825000                                    1.000000                                 2.500000                0.004211                           0.002105                        0.005263       53.824363              0.226629                         0.113314                      0.283286    0.670363    0.105083     1.627837     0.256227\n",
       "497922           5                     101       57.19              4                         7                      1                   0.237624        0.566238                  0.166667              24.0                 2.382917                                    0.291667                                 0.041667                0.039604                           0.069307                        0.009901        1.766043              0.069942                         0.122399                      0.017486   -0.378796    0.295495    -1.843209     1.743744\n",
       "497923           2                    1699       21.95             11                        10                      4                   0.013537        0.012919                  0.478261              23.0                 0.954348                                    0.434783                                 0.173913                0.006474                           0.005886                        0.002354       77.403189              0.501139                         0.455581                      0.182232    0.226269   -0.305935     1.415527    -1.808570\n",
       "497924           1                    1076       55.47              5                         4                      5                   0.000929        0.051552                  5.000000               1.0                55.470000                                    4.000000                                 5.000000                0.004647                           0.003717                        0.004647       19.397873              0.090139                         0.072111                      0.090139    0.919136   -0.480284     3.563014     0.086820\n",
       "497925           6                     232        7.20              1                         8                      5                   0.012931        0.031034                  0.333333               3.0                 2.400000                                    2.666667                                 1.666667                0.004310                           0.034483                        0.021552       32.222222              0.138889                         1.111111                      0.694444    0.644360    0.508524     1.554195     1.392249\n",
       "497926           5                     453       96.32              9                         2                      0                   0.017660        0.212627                  1.125000               8.0                12.040000                                    0.250000                                 0.000000                0.019868                           0.004415                        0.000000        4.703073              0.093439                         0.020764                      0.000000   -0.373520    0.311056    -1.889049     1.237284\n",
       "497927           5                     972       45.90              4                        10                      4                   0.028807        0.047222                  0.142857              28.0                 1.639286                                    0.357143                                 0.142857                0.004115                           0.010288                        0.004115       21.176471              0.087146                         0.217865                      0.087146    0.142679    0.286934     1.460048     2.007905\n",
       "497928           4                    1750       76.78              6                         1                      1                   0.000571        0.043874                  6.000000               1.0                76.780000                                    1.000000                                 1.000000                0.003429                           0.000571                        0.000571       22.792394              0.078145                         0.013024                      0.013024    0.008865    0.123184     3.183962     0.101032\n",
       "497929           1                    1111        5.52              2                         8                      1                   0.027003        0.004968                  0.066667              30.0                 0.184000                                    0.266667                                 0.033333                0.001800                           0.007201                        0.000900      201.268116              0.362319                         1.449275                      0.181159   -0.464504   -0.509576    -1.759948    -2.285219\n",
       "497930           5                      13       41.17              4                         1                      0                   1.076923        3.166923                  0.285714              14.0                 2.940714                                    0.071429                                 0.000000                0.307692                           0.076923                        0.000000        0.315764              0.097158                         0.024290                      0.000000   -0.501541    0.301000    -2.266945     1.432820\n",
       "497931           4                     955       94.26             11                         2                      0                   0.009424        0.098702                  1.222222               9.0                10.473333                                    0.222222                                 0.000000                0.011518                           0.002094                        0.000000       10.131551              0.116698                         0.021218                      0.000000   -0.376634    0.110731    -2.025526     0.445616\n",
       "497932           1                     990       66.95              7                        10                      4                   0.021212        0.067626                  0.333333              21.0                 3.188095                                    0.476190                                 0.190476                0.007071                           0.010101                        0.004040       14.787155              0.104556                         0.149365                      0.059746    0.244647   -0.504483     1.356587    -2.414823\n",
       "497933           1                    1549       24.13              1                         0                      3                   0.001937        0.015578                  0.333333               3.0                 8.043333                                    0.000000                                 1.000000                0.000646                           0.000000                        0.001937       64.193949              0.041442                         0.000000                      0.124327    0.180683   -0.496246     0.476644    -1.811791\n",
       "497934           2                    1660       67.63             10                         1                      5                   0.004217        0.040741                  1.428571               7.0                 9.661429                                    0.142857                                 0.714286                0.006024                           0.000602                        0.003012       24.545320              0.147863                         0.014786                      0.073932    0.561913   -0.301452     1.735964    -1.260534\n",
       "497935           1                    1337       87.25             10                        10                      3                   0.010471        0.065258                  0.714286              14.0                 6.232143                                    0.714286                                 0.214286                0.007479                           0.007479                        0.002244       15.323782              0.114613                         0.114613                      0.034384    0.167772   -0.493282     0.349151    -2.056843\n",
       "497936           2                    1310       46.16              7                         3                      1                   0.003053        0.035237                  1.750000               4.0                11.540000                                    0.750000                                 0.250000                0.005344                           0.002290                        0.000763       28.379549              0.151646                         0.064991                      0.021664   -0.146173   -0.285910    -1.329197    -1.262713\n",
       "497937           6                     473       42.82              6                         1                      4                   0.019027        0.090529                  0.666667               9.0                 4.757778                                    0.111111                                 0.444444                0.012685                           0.002114                        0.008457       11.046240              0.140121                         0.023354                      0.093414    0.310647    0.496200     1.274024     2.005695\n",
       "497938           5                     944       21.03              4                         6                      2                   0.029661        0.022278                  0.142857              28.0                 0.751071                                    0.214286                                 0.071429                0.004237                           0.006356                        0.002119       44.888255              0.190204                         0.285307                      0.095102   -0.259895    0.287508    -0.609184     2.023527\n",
       "497939           6                     595       83.51              2                         3                      2                   0.010084        0.140353                  0.333333               6.0                13.918333                                    0.500000                                 0.333333                0.003361                           0.005042                        0.003361        7.124895              0.023949                         0.035924                      0.023949   -0.007990    0.506929    -0.482164     1.881562\n",
       "497940           2                    1620       98.43              2                         3                      5                   0.012346        0.060759                  0.100000              20.0                 4.921500                                    0.150000                                 0.250000                0.001235                           0.001852                        0.003086       16.458397              0.020319                         0.030479                      0.050798    0.371833   -0.315134     2.034848    -1.807346\n",
       "497941           3                     310       49.27              4                        10                      2                   0.019355        0.158935                  0.666667               6.0                 8.211667                                    1.666667                                 0.333333                0.012903                           0.032258                        0.006452        6.291861              0.081185                         0.202963                      0.040593    0.049810   -0.084002    -0.522279    -0.458336\n",
       "497942           1                    1390       60.46              4                         6                      0                   0.010791        0.043496                  0.266667              15.0                 4.030667                                    0.400000                                 0.000000                0.002878                           0.004317                        0.000000       22.990407              0.066159                         0.099239                      0.000000   -0.468515   -0.493005    -2.171754    -2.018435\n",
       "497943           3                     109       37.82             11                         9                      4                   0.238532        0.346972                  0.423077              26.0                 1.454615                                    0.346154                                 0.153846                0.100917                           0.082569                        0.036697        2.882073              0.290851                         0.237969                      0.105764    0.191321   -0.110475     1.533154    -0.592224\n",
       "497944           3                    1726       93.51              4                        10                      0                   0.003476        0.054177                  0.666667               6.0                15.585000                                    1.666667                                 0.000000                0.002317                           0.005794                        0.000000       18.457919              0.042776                         0.106940                      0.000000   -0.308373   -0.077549    -1.825610    -0.478387\n",
       "497945           4                     717       51.97              0                         8                      4                   0.009763        0.072483                  0.000000               7.0                 7.424286                                    1.142857                                 0.571429                0.000000                           0.011158                        0.005579       13.796421              0.000000                         0.153935                      0.076967    0.367547    0.105673     1.383200     0.455537\n",
       "497946           1                    1385       90.09              8                         1                      0                   0.009386        0.065047                  0.615385              13.0                 6.930000                                    0.076923                                 0.000000                0.005776                           0.000722                        0.000000       15.373515              0.088800                         0.011100                      0.000000   -0.451828   -0.495266    -2.047025    -1.886567\n",
       "497947           2                     614       37.52              9                         9                      2                   0.048860        0.061107                  0.300000              30.0                 1.250667                                    0.300000                                 0.066667                0.014658                           0.014658                        0.003257       16.364606              0.239872                         0.239872                      0.053305   -0.229100   -0.309281    -0.634213    -1.971938\n",
       "497948           1                     379        6.76              8                         5                      2                   0.034301        0.017836                  0.615385              13.0                 0.520000                                    0.384615                                 0.153846                0.021108                           0.013193                        0.005277       56.065089              1.183432                         0.739645                      0.295858   -0.081448   -0.497355    -0.558030    -2.194152\n",
       "497949           6                      33       72.47              8                         6                      3                   0.151515        2.196061                  1.600000               5.0                14.494000                                    1.200000                                 0.600000                0.242424                           0.181818                        0.090909        0.455361              0.110391                         0.082793                      0.041396    0.260391    0.511392     0.384186     1.738853\n",
       "497950           3                    1709       64.66              7                         8                      0                   0.009362        0.037835                  0.437500              16.0                 4.041250                                    0.500000                                 0.000000                0.004096                           0.004681                        0.000000       26.430560              0.108259                         0.123724                      0.000000   -0.448012   -0.090996    -2.335179    -0.467485\n",
       "497951           1                    1411       36.17              8                        10                      4                   0.010631        0.025634                  0.533333              15.0                 2.411333                                    0.666667                                 0.266667                0.005670                           0.007087                        0.002835       39.010229              0.221178                         0.276472                      0.110589    0.310510   -0.498269     1.306725    -2.195733\n",
       "497952           1                    1323       38.93             10                         8                      3                   0.009826        0.029426                  0.769231              13.0                 2.994615                                    0.615385                                 0.230769                0.007559                           0.006047                        0.002268       33.984074              0.256871                         0.205497                      0.077061    0.147743   -0.495217     0.356603    -2.157650\n",
       "497953           4                     505       61.61              8                         4                      5                   0.043564        0.122000                  0.363636              22.0                 2.800455                                    0.181818                                 0.227273                0.015842                           0.007921                        0.009901        8.196721              0.129849                         0.064925                      0.081156    0.375232    0.084578     2.143034     0.525180\n",
       "497954           6                    1792       98.21              8                         2                      3                   0.009487        0.054805                  0.470588              17.0                 5.777059                                    0.117647                                 0.176471                0.004464                           0.001116                        0.001674       18.246614              0.081458                         0.020365                      0.030547    0.062847    0.493280     0.401105     2.330642\n",
       "497955           5                    1603       33.78              2                        10                      3                   0.001871        0.021073                  0.666667               3.0                11.260000                                    3.333333                                 1.000000                0.001248                           0.006238                        0.001871       47.454115              0.059207                         0.296033                      0.088810    0.296223    0.317961     0.387246     1.120883\n",
       "497956           2                     927        5.17              2                         3                      0                   0.006472        0.005577                  0.333333               6.0                 0.861667                                    0.500000                                 0.000000                0.002157                           0.003236                        0.000000      179.303675              0.386847                         0.580271                      0.000000   -0.419540   -0.288909    -2.062783    -1.302834\n",
       "497957           5                    1009       63.24             11                         7                      4                   0.010902        0.062676                  1.000000              11.0                 5.749091                                    0.636364                                 0.363636                0.010902                           0.006938                        0.003964       15.955092              0.173941                         0.110689                      0.063251    0.365522    0.303200     1.334533     1.481709\n",
       "497958           2                     257       90.49              0                         8                      0                   0.058366        0.352101                  0.000000              15.0                 6.032667                                    0.533333                                 0.000000                0.000000                           0.031128                        0.000000        2.840093              0.000000                         0.088408                      0.000000   -0.462979   -0.291816    -2.142370    -1.380672\n",
       "497959           3                     105       53.68              5                         1                      4                   0.266667        0.511238                  0.178571              28.0                 1.917143                                    0.035714                                 0.142857                0.047619                           0.009524                        0.038095        1.956036              0.093145                         0.018629                      0.074516    0.085535   -0.122962     1.508674    -0.579206\n",
       "497960           2                    1553        6.14             10                         2                      4                   0.017386        0.003954                  0.370370              27.0                 0.227407                                    0.074074                                 0.148148                0.006439                           0.001288                        0.002576      252.931596              1.628664                         0.325733                      0.651466    0.112413   -0.319560     1.427250    -1.957337\n",
       "497961           1                     157       71.89              5                         3                      0                   0.133758        0.457898                  0.238095              21.0                 3.423333                                    0.142857                                 0.000000                0.031847                           0.019108                        0.000000        2.183892              0.069551                         0.041730                      0.000000   -0.547886   -0.502436    -2.263123    -2.100250\n",
       "497962           4                     380       17.38              6                         6                      2                   0.044737        0.045737                  0.352941              17.0                 1.022353                                    0.352941                                 0.117647                0.015789                           0.015789                        0.005263       21.864212              0.345224                         0.345224                      0.115075   -0.127697    0.098907    -0.579296     0.504211\n",
       "497963           2                    1203       82.83              1                         1                      5                   0.004988        0.068853                  0.166667               6.0                13.805000                                    0.166667                                 0.833333                0.000831                           0.000831                        0.004156       14.523723              0.012073                         0.012073                      0.060365    0.533351   -0.302982     1.737878    -1.260504\n",
       "497964           3                    1598       31.89              5                         7                      1                   0.017522        0.019956                  0.178571              28.0                 1.138929                                    0.250000                                 0.035714                0.003129                           0.004380                        0.000626       50.109752              0.156789                         0.219505                      0.031358   -0.427080   -0.107822    -2.040401    -0.491006\n",
       "497965           3                     858       26.06              5                         0                      5                   0.005828        0.030373                  1.000000               5.0                 5.212000                                    0.000000                                 1.000000                0.005828                           0.000000                        0.005828       32.924021              0.191865                         0.000000                      0.191865    0.544451   -0.102936     1.894366    -0.498546\n",
       "497966           6                     449       47.71              4                         8                      0                   0.004454        0.106258                  2.000000               2.0                23.855000                                    4.000000                                 0.000000                0.008909                           0.017817                        0.000000        9.411025              0.083840                         0.167680                      0.000000   -0.249344    0.526457    -1.385854     1.224655\n",
       "497967           4                     848       61.01              6                         7                      4                   0.003538        0.071946                  2.000000               3.0                20.336667                                    2.333333                                 1.333333                0.007075                           0.008255                        0.004717       13.899361              0.098345                         0.114735                      0.065563    0.508678    0.114009     1.209460     0.461805\n",
       "497968           3                    1191       69.48              4                         6                      0                   0.001679        0.058338                  2.000000               2.0                34.740000                                    3.000000                                 0.000000                0.003359                           0.005038                        0.000000       17.141623              0.057571                         0.086356                      0.000000   -0.257001   -0.075054    -1.558675    -0.304257\n",
       "497969           6                     583       70.73              8                         1                      0                   0.013722        0.121321                  1.000000               8.0                 8.841250                                    0.125000                                 0.000000                0.013722                           0.001715                        0.000000        8.242613              0.113106                         0.014138                      0.000000   -0.398151    0.508976    -1.887037     1.748286\n",
       "497970           6                     229       64.02              7                         3                      0                   0.074236        0.279563                  0.411765              17.0                 3.765882                                    0.176471                                 0.000000                0.030568                           0.013100                        0.000000        3.577007              0.109341                         0.046860                      0.000000   -0.496375    0.501370    -2.147514     2.030763\n",
       "497971           6                     411       62.33             11                         2                      1                   0.065693        0.151655                  0.407407              27.0                 2.308519                                    0.074074                                 0.037037                0.026764                           0.004866                        0.002433        6.593936              0.176480                         0.032087                      0.016044   -0.411929    0.488627    -1.692066     2.316603\n",
       "497972           1                     685       29.13              1                         9                      5                   0.030657        0.042526                  0.047619              21.0                 1.387143                                    0.428571                                 0.238095                0.001460                           0.013139                        0.007299       23.515276              0.034329                         0.308960                      0.171644    0.377302   -0.510459     1.913296    -2.275492\n",
       "497973           1                    1646       13.33             11                        10                      1                   0.004860        0.008098                  1.375000               8.0                 1.666250                                    1.250000                                 0.125000                0.006683                           0.006075                        0.000608      123.480870              0.825206                         0.750188                      0.075019   -0.144314   -0.481809    -1.318567    -1.700728\n",
       "497974           3                    1549       23.01              0                         5                      1                   0.012266        0.014855                  0.000000              19.0                 1.211053                                    0.263158                                 0.052632                0.000000                           0.003228                        0.000646       67.318557              0.000000                         0.217297                      0.043459   -0.371256   -0.102658    -1.896519    -0.482931\n",
       "497975           6                    1331       35.55              8                         4                      2                   0.015778        0.026709                  0.380952              21.0                 1.692857                                    0.190476                                 0.095238                0.006011                           0.003005                        0.001503       37.440225              0.225035                         0.112518                      0.056259   -0.172472    0.493332    -0.591919     2.473187\n",
       "497976           4                      20       60.86              3                         9                      1                   0.550000        3.043000                  0.272727              11.0                 5.532727                                    0.818182                                 0.090909                0.150000                           0.450000                        0.050000        0.328623              0.049293                         0.147880                      0.016431   -0.217885    0.111022    -1.527506     0.472024\n",
       "497977           3                    1584       76.14              6                         0                      1                   0.011995        0.048068                  0.315789              19.0                 4.007368                                    0.000000                                 0.052632                0.003788                           0.000000                        0.000631       20.803783              0.078802                         0.000000                      0.013134   -0.359569   -0.106157    -1.883287    -0.455039\n",
       "497978           6                    1508       46.81              3                         7                      0                   0.001326        0.031041                  1.500000               2.0                23.405000                                    3.500000                                 0.000000                0.001989                           0.004642                        0.000000       32.215339              0.064089                         0.149541                      0.000000   -0.271931    0.524596    -1.409193     1.295561\n",
       "497979           6                     906       55.48              9                         2                      5                   0.025386        0.061236                  0.391304              23.0                 2.412174                                    0.086957                                 0.217391                0.009934                           0.002208                        0.005519       16.330209              0.162221                         0.036049                      0.090123    0.351504    0.481314     1.957192     2.290627\n",
       "497980           6                    1125       66.48              7                         0                      4                   0.001778        0.059093                  3.500000               2.0                33.240000                                    0.000000                                 2.000000                0.006222                           0.000000                        0.003556       16.922383              0.105295                         0.000000                      0.060168    0.507549    0.507919     1.079857     1.089849\n",
       "497981           2                    1561       86.14             10                         4                      2                   0.017937        0.055183                  0.357143              28.0                 3.076429                                    0.142857                                 0.071429                0.006406                           0.002562                        0.001281       18.121662              0.116090                         0.046436                      0.023218   -0.223493   -0.311944    -0.612456    -1.974560\n",
       "497982           6                    1562       48.35              0                        10                      0                   0.011524        0.030954                  0.000000              18.0                 2.686111                                    0.555556                                 0.000000                0.000000                           0.006402                        0.000000       32.306101              0.000000                         0.206825                      0.000000   -0.497936    0.506739    -2.083967     2.013481\n",
       "497983           5                    1070       92.80             11                         1                      4                   0.021495        0.086729                  0.478261              23.0                 4.034783                                    0.043478                                 0.173913                0.010280                           0.000935                        0.003738       11.530172              0.118534                         0.010776                      0.043103    0.183623    0.284008     1.379803     1.823357\n",
       "497984           4                    1128       39.32              4                         1                      3                   0.021277        0.034858                  0.166667              24.0                 1.638333                                    0.041667                                 0.125000                0.003546                           0.000887                        0.002660       28.687691              0.101729                         0.025432                      0.076297   -0.063716    0.083412     0.352569     0.543517\n",
       "497985           6                     172       61.32              5                         2                      1                   0.098837        0.356512                  0.294118              17.0                 3.607059                                    0.117647                                 0.058824                0.029070                           0.011628                        0.005814        2.804958              0.081539                         0.032616                      0.016308   -0.331533    0.496927    -1.555691     2.213828\n",
       "497986           6                     369       82.69              7                         2                      2                   0.032520        0.224092                  0.583333              12.0                 6.890833                                    0.166667                                 0.166667                0.018970                           0.005420                        0.005420        4.462450              0.084654                         0.024187                      0.024187   -0.070648    0.500310    -0.527367     2.119238\n",
       "497987           5                     161       88.57              7                         5                      0                   0.111801        0.550124                  0.388889              18.0                 4.920556                                    0.277778                                 0.000000                0.043478                           0.031056                        0.000000        1.817771              0.079034                         0.056453                      0.000000   -0.484007    0.303100    -2.262990     1.453666\n",
       "497988           3                      12       50.17              2                        10                      5                   1.416667        4.180833                  0.117647              17.0                 2.951176                                    0.588235                                 0.294118                0.166667                           0.833333                        0.416667        0.239187              0.039864                         0.199322                      0.099661    0.444640   -0.105156     1.934557    -0.590354\n",
       "497989           1                      13       87.02              6                         8                      1                   1.230769        6.693846                  0.375000              16.0                 5.438750                                    0.500000                                 0.062500                0.461538                           0.615385                        0.076923        0.149391              0.068950                         0.091933                      0.011492   -0.259910   -0.493856    -1.526429    -2.059335\n",
       "497990           5                    1312       29.28              6                         0                      5                   0.013720        0.022317                  0.333333              18.0                 1.626667                                    0.000000                                 0.277778                0.004573                           0.000000                        0.003811       44.808743              0.204918                         0.000000                      0.170765    0.369721    0.283148     2.120246     1.692070\n",
       "497991           6                    1768       45.98              6                         4                      3                   0.002828        0.026007                  1.200000               5.0                 9.196000                                    0.800000                                 0.600000                0.003394                           0.002262                        0.001697       38.451501              0.130492                         0.086994                      0.065246    0.215417    0.507956     0.365629     1.862777\n",
       "497992           6                    1331       86.17              3                         1                      0                   0.015026        0.064741                  0.150000              20.0                 4.308500                                    0.050000                                 0.000000                0.002254                           0.000751                        0.000000       15.446211              0.034815                         0.011605                      0.000000   -0.558711    0.495557    -2.205069     2.095035\n",
       "497993           6                      64       75.16              3                         6                      3                   0.437500        1.174375                  0.107143              28.0                 2.684286                                    0.214286                                 0.107143                0.046875                           0.093750                        0.046875        0.851517              0.039915                         0.079830                      0.039915   -0.064012    0.484775     0.356794     2.512816\n",
       "497994           2                     217       30.89              3                         3                      1                   0.027650        0.142350                  0.500000               6.0                 5.148333                                    0.500000                                 0.166667                0.013825                           0.013825                        0.004608        7.024927              0.097119                         0.097119                      0.032373   -0.212926   -0.290807    -1.399179    -1.320059\n",
       "497995           3                    1123       98.64              9                         9                      1                   0.013357        0.087836                  0.600000              15.0                 6.576000                                    0.600000                                 0.066667                0.008014                           0.008014                        0.000890       11.384834              0.091241                         0.091241                      0.010138   -0.221773   -0.090620    -1.666022    -0.446184\n",
       "497996           2                     404       39.61             10                         7                      4                   0.074257        0.098045                  0.333333              30.0                 1.320333                                    0.233333                                 0.133333                0.024752                           0.017327                        0.009901       10.199445              0.252461                         0.176723                      0.100985    0.127347   -0.316765     1.455288    -1.925460\n",
       "497997           6                    1724       61.73              6                         1                      3                   0.008701        0.035806                  0.400000              15.0                 4.115333                                    0.066667                                 0.200000                0.003480                           0.000580                        0.001740       27.928074              0.097197                         0.016200                      0.048599    0.055712    0.493124     0.396777     2.330573\n",
       "497998           4                    1072       48.65              2                         8                      5                   0.018657        0.045382                  0.100000              20.0                 2.432500                                    0.400000                                 0.250000                0.001866                           0.007463                        0.004664       22.034943              0.041110                         0.164440                      0.102775    0.392459    0.089610     2.167140     0.520323\n",
       "497999           4                    1146       83.19             10                         1                      3                   0.020942        0.072592                  0.416667              24.0                 3.466250                                    0.041667                                 0.125000                0.008726                           0.000873                        0.002618       13.775694              0.120207                         0.012021                      0.036062   -0.018873    0.085490     0.346169     0.530277\n",
       "498000           4                     618        3.98              2                         1                      5                   0.012945        0.006440                  0.250000               8.0                 0.497500                                    0.125000                                 0.625000                0.003236                           0.001618                        0.008091      155.276382              0.502513                         0.251256                      1.256281    0.472828    0.093066     1.900436     0.471335\n",
       "498001           4                    1120       58.73             10                         7                      5                   0.018750        0.052437                  0.476190              21.0                 2.796667                                    0.333333                                 0.238095                0.008929                           0.006250                        0.004464       19.070322              0.170271                         0.119190                      0.085135    0.417751    0.089700     2.159581     0.548878\n",
       "498002           2                     878       82.84              4                         0                      1                   0.012528        0.094351                  0.363636              11.0                 7.530909                                    0.000000                                 0.090909                0.004556                           0.000000                        0.001139       10.598745              0.048286                         0.000000                      0.012071   -0.275274   -0.298547    -1.514400    -1.433863\n",
       "498003           2                     881       35.51              5                         5                      1                   0.030647        0.040306                  0.185185              27.0                 1.315185                                    0.185185                                 0.037037                0.005675                           0.005675                        0.001135       24.809913              0.140805                         0.140805                      0.028161   -0.428842   -0.309141    -1.906032    -1.846047\n",
       "498004           1                     892       95.82             10                         6                      3                   0.023543        0.107422                  0.476190              21.0                 4.562857                                    0.285714                                 0.142857                0.011211                           0.006726                        0.003363        9.309121              0.104362                         0.062617                      0.031309    0.056918   -0.505272     0.323255    -2.369564\n",
       "498005           4                    1154        9.15             11                         9                      1                   0.018198        0.007929                  0.523810              21.0                 0.435714                                    0.428571                                 0.047619                0.009532                           0.007799                        0.000867      126.120219              1.202186                         0.983607                      0.109290   -0.311736    0.102476    -1.858715     0.462043\n",
       "498006           2                     500       50.93              7                         5                      2                   0.032000        0.101860                  0.437500              16.0                 3.183125                                    0.312500                                 0.125000                0.014000                           0.010000                        0.004000        9.817396              0.137444                         0.098174                      0.039270   -0.106676   -0.300231    -0.552954    -1.736231\n",
       "498007           6                     438       23.23              4                         9                      2                   0.025114        0.053037                  0.363636              11.0                 2.111818                                    0.818182                                 0.181818                0.009132                           0.020548                        0.004566       18.854929              0.172191                         0.387430                      0.086096   -0.042063    0.507938    -0.523065     2.124035\n",
       "498008           5                     580       10.63              8                         3                      3                   0.048276        0.018328                  0.285714              28.0                 0.379643                                    0.107143                                 0.107143                0.013793                           0.005172                        0.005172       54.562559              0.752587                         0.282220                      0.282220   -0.082119    0.282185     0.356861     1.975282\n",
       "498009           5                    1607       71.42              8                         6                      3                   0.001867        0.044443                  2.666667               3.0                23.806667                                    2.000000                                 1.000000                0.004978                           0.003734                        0.001867       22.500700              0.112013                         0.084010                      0.042005    0.323571    0.316354     0.399155     1.137945\n",
       "498010           5                     182       56.29              7                        10                      4                   0.043956        0.309286                  0.875000               8.0                 7.036250                                    1.250000                                 0.500000                0.038462                           0.054945                        0.021978        3.233256              0.124356                         0.177651                      0.071061    0.410503    0.308894     1.298575     1.347872\n",
       "498011           2                     873       56.69              5                         5                      0                   0.028637        0.064937                  0.200000              25.0                 2.267600                                    0.200000                                 0.000000                0.005727                           0.005727                        0.000000       15.399541              0.088199                         0.088199                      0.000000   -0.582822   -0.304221    -2.523409    -1.617010\n",
       "498012           5                    1398       27.71             10                         5                      2                   0.018598        0.019821                  0.384615              26.0                 1.065769                                    0.192308                                 0.076923                0.007153                           0.003577                        0.001431       50.451101              0.360881                         0.180440                      0.072176   -0.213236    0.290083    -0.611810     1.999489\n",
       "498013           5                    1014       63.96              2                         6                      2                   0.007890        0.063077                  0.250000               8.0                 7.995000                                    0.750000                                 0.250000                0.001972                           0.005917                        0.001972       15.853659              0.031270                         0.093809                      0.031270   -0.021911    0.308046    -0.489122     1.506339\n",
       "498014           5                     775       40.67              1                        10                      4                   0.016774        0.052477                  0.076923              13.0                 3.128462                                    0.769231                                 0.307692                0.001290                           0.012903                        0.005161       19.055815              0.024588                         0.245881                      0.098353    0.299922    0.301256     1.405477     1.577039\n",
       "498015           4                    1286       58.18              0                         3                      1                   0.006998        0.045241                  0.000000               9.0                 6.464444                                    0.333333                                 0.111111                0.000000                           0.002333                        0.000778       22.103816              0.000000                         0.051564                      0.017188   -0.259565    0.105410    -1.556376     0.435602\n",
       "498016           1                     603       71.47             11                         2                      3                   0.003317        0.118524                  5.500000               2.0                35.735000                                    1.000000                                 1.500000                0.018242                           0.003317                        0.004975        8.437106              0.153911                         0.027984                      0.041976    0.369656   -0.483745     0.332684    -0.963786\n",
       "498017           2                     209       91.97              9                         3                      0                   0.086124        0.440048                  0.500000              18.0                 5.109444                                    0.166667                                 0.000000                0.043062                           0.014354                        0.000000        2.272480              0.097858                         0.032619                      0.000000   -0.487168   -0.298225    -2.261619    -1.476540\n",
       "498018           5                    1001       74.24              7                         6                      0                   0.008991        0.074166                  0.777778               9.0                 8.248889                                    0.666667                                 0.000000                0.006993                           0.005994                        0.000000       13.483297              0.094289                         0.080819                      0.000000   -0.375193    0.313870    -2.112742     1.316592\n",
       "498019           5                    1324       28.98              2                         3                      4                   0.000755        0.021888                  2.000000               1.0                28.980000                                    3.000000                                 4.000000                0.001511                           0.002266                        0.003021       45.686680              0.069013                         0.103520                      0.138026    0.604215    0.314179     3.388991     0.134462\n",
       "498020           5                     951       10.74              9                         2                      5                   0.005258        0.011293                  1.800000               5.0                 2.148000                                    0.400000                                 1.000000                0.009464                           0.002103                        0.005258       88.547486              0.837989                         0.186220                      0.465549    0.580511    0.300506     1.736223     1.153623\n",
       "498021           4                       4       41.16              6                         7                      4                   6.250000       10.290000                  0.240000              25.0                 1.646400                                    0.280000                                 0.160000                1.500000                           1.750000                        1.000000        0.097182              0.145773                         0.170068                      0.097182    0.164367    0.086800     1.208001     0.006072\n",
       "498022           6                    1341       86.28              6                        10                      4                   0.011931        0.064340                  0.375000              16.0                 5.392500                                    0.625000                                 0.250000                0.004474                           0.007457                        0.002983       15.542420              0.069541                         0.115902                      0.046361    0.304757    0.500109     1.370524     2.253173\n",
       "498023           2                      14       56.76              5                         1                      0                   0.357143        4.054286                  1.000000               5.0                11.352000                                    0.200000                                 0.000000                0.357143                           0.071429                        0.000000        0.246653              0.088090                         0.017618                      0.000000   -0.380183   -0.288317    -1.854480    -1.200103\n",
       "498024           4                     588       40.94              1                         4                      5                   0.030612        0.069626                  0.055556              18.0                 2.274444                                    0.222222                                 0.277778                0.001701                           0.006803                        0.008503       14.362482              0.024426                         0.097704                      0.122130    0.378489    0.086512     2.127613     0.537959\n",
       "498025           3                    1580        5.96              0                         4                      5                   0.010759        0.003772                  0.000000              17.0                 0.350588                                    0.235294                                 0.294118                0.000000                           0.002532                        0.003165      265.100671              0.000000                         0.671141                      0.838926    0.372136   -0.112801     2.034317    -0.552250\n",
       "498026           1                    1814       12.55              4                         9                      4                   0.006064        0.006918                  0.363636              11.0                 1.140909                                    0.818182                                 0.363636                0.002205                           0.004961                        0.002205      144.541833              0.318725                         0.717131                      0.318725    0.323437   -0.496505     1.259555    -2.058402\n",
       "498027           1                     847       54.17              0                        10                      2                   0.028335        0.063955                  0.000000              24.0                 2.257083                                    0.416667                                 0.083333                0.000000                           0.011806                        0.002361       15.635961              0.000000                         0.184604                      0.036921   -0.195251   -0.504122    -0.658638    -2.439608\n",
       "498028           2                     856       42.39              0                         5                      5                   0.016355        0.049521                  0.000000              14.0                 3.027857                                    0.357143                                 0.357143                0.000000                           0.005841                        0.005841       20.193442              0.000000                         0.117952                      0.117952    0.428977   -0.308174     2.082868    -1.612717\n",
       "498029           3                    1380       92.66              4                         3                      5                   0.013768        0.067145                  0.210526              19.0                 4.876842                                    0.157895                                 0.263158                0.002899                           0.002174                        0.003623       14.893158              0.043169                         0.032376                      0.053961    0.391932   -0.113856     2.113660    -0.578362\n",
       "498030           2                     560       51.69             10                         5                      1                   0.030357        0.092304                  0.588235              17.0                 3.040588                                    0.294118                                 0.058824                0.017857                           0.008929                        0.001786       10.833817              0.193461                         0.096731                      0.019346   -0.286111   -0.297724    -1.692565    -1.584580\n",
       "498031           3                     865        1.37             11                         7                      2                   0.017341        0.001584                  0.733333              15.0                 0.091333                                    0.466667                                 0.133333                0.012717                           0.008092                        0.002312      631.386861              8.029197                         5.109489                      1.459854   -0.076531   -0.096455    -0.581668    -0.510481\n",
       "498032           1                    1377       44.34              0                         2                      2                   0.021060        0.032200                  0.000000              29.0                 1.528966                                    0.068966                                 0.068966                0.000000                           0.001452                        0.001452       31.055480              0.000000                         0.045106                      0.045106   -0.312384   -0.518153    -0.633360    -2.528799\n",
       "498033           4                    1162       50.89              8                         2                      3                   0.010327        0.043795                  0.666667              12.0                 4.240833                                    0.166667                                 0.250000                0.006885                           0.001721                        0.002582       22.833563              0.157202                         0.039300                      0.058951    0.107338    0.097948     0.396946     0.495607\n",
       "498034           4                     442       93.62             10                        10                      0                   0.042986        0.211810                  0.526316              19.0                 4.927368                                    0.526316                                 0.000000                0.022624                           0.022624                        0.000000        4.721213              0.106815                         0.106815                      0.000000   -0.440895    0.108936    -2.268001     0.498674\n",
       "498035           6                     426       86.54              6                         3                      2                   0.018779        0.203146                  0.750000               8.0                10.817500                                    0.375000                                 0.250000                0.014085                           0.007042                        0.004695        4.922579              0.069332                         0.034666                      0.023111   -0.013686    0.505769    -0.504177     1.956118\n",
       "498036           3                    1115       12.44              2                         7                      0                   0.008072        0.011157                  0.222222               9.0                 1.382222                                    0.777778                                 0.000000                0.001794                           0.006278                        0.000000       89.630225              0.160772                         0.562701                      0.000000   -0.419783   -0.087195    -2.167825    -0.411364\n",
       "498037           2                     994       51.18              9                        10                      4                   0.004024        0.051489                  2.250000               4.0                12.795000                                    2.500000                                 1.000000                0.009054                           0.010060                        0.004024       19.421649              0.175850                         0.195389                      0.078156    0.512460   -0.283281     1.147365    -1.182094\n",
       "498038           6                    1006       76.66              2                        10                      4                   0.020875        0.076203                  0.095238              21.0                 3.650476                                    0.476190                                 0.190476                0.001988                           0.009940                        0.003976       13.122880              0.026089                         0.130446                      0.052178    0.221764    0.493659     1.374972     2.395316\n",
       "498039           3                    1377       87.61              6                         2                      0                   0.007988        0.063624                  0.545455              11.0                 7.964545                                    0.181818                                 0.000000                0.004357                           0.001452                        0.000000       15.717384              0.068485                         0.022828                      0.000000   -0.432407   -0.092852    -2.211316    -0.455448\n",
       "498040           5                     695        3.77              2                         2                      5                   0.011511        0.005424                  0.250000               8.0                 0.471250                                    0.250000                                 0.625000                0.002878                           0.002878                        0.007194      184.350133              0.530504                         0.530504                      1.326260    0.480751    0.294168     1.935878     1.365138\n",
       "498041           4                     235       58.68             10                         9                      3                   0.046809        0.249702                  0.909091              11.0                 5.334545                                    0.818182                                 0.272727                0.042553                           0.038298                        0.012766        4.004772              0.170416                         0.153374                      0.051125    0.189705    0.107790     0.357380     0.473245\n",
       "498042           5                    1580       59.92              5                         9                      4                   0.001266        0.037924                  2.500000               2.0                29.960000                                    4.500000                                 2.000000                0.003165                           0.005696                        0.002532       26.368491              0.083445                         0.150200                      0.066756    0.591405    0.320574     0.931662     0.800498\n",
       "498043           3                    1467       86.97              7                         6                      1                   0.006135        0.059284                  0.777778               9.0                 9.663333                                    0.666667                                 0.111111                0.004772                           0.004090                        0.000682       16.867885              0.080488                         0.068989                      0.011498   -0.184398   -0.088165    -1.532502    -0.446634\n",
       "498044           2                     436       56.54              9                         2                      5                   0.043578        0.129679                  0.473684              19.0                 2.975789                                    0.105263                                 0.263158                0.020642                           0.004587                        0.011468        7.711355              0.159179                         0.035373                      0.088433    0.398903   -0.314244     2.098449    -1.709724\n",
       "498045           1                     276       55.27              0                         9                      5                   0.097826        0.200254                  0.000000              27.0                 2.047037                                    0.333333                                 0.185185                0.000000                           0.032609                        0.018116        4.993667              0.000000                         0.162837                      0.090465    0.313270   -0.516511     1.872921    -2.332967\n",
       "498046           3                    1150       31.81              2                         8                      2                   0.011304        0.027661                  0.153846              13.0                 2.446923                                    0.615385                                 0.153846                0.001739                           0.006957                        0.001739       36.152153              0.062873                         0.251493                      0.062873   -0.082362   -0.095233    -0.550952    -0.499150\n",
       "498047           3                    1257       68.08              2                         4                      1                   0.012729        0.054161                  0.125000              16.0                 4.255000                                    0.250000                                 0.062500                0.001591                           0.003182                        0.000796       18.463572              0.029377                         0.058754                      0.014689   -0.319074   -0.099784    -1.836191    -0.450929\n",
       "498048           6                    1329       90.91              1                        10                      0                   0.020316        0.068405                  0.037037              27.0                 3.367037                                    0.370370                                 0.000000                0.000752                           0.007524                        0.000000       14.618854              0.011000                         0.109999                      0.000000   -0.577908    0.498470    -2.215459     2.116667\n",
       "498049           6                      74       39.14              9                         1                      3                   0.121622        0.528919                  1.000000               9.0                 4.348889                                    0.111111                                 0.333333                0.121622                           0.013514                        0.040541        1.890649              0.229944                         0.025549                      0.076648    0.140629    0.499588     0.382790     1.905114\n",
       "498050           5                     296       90.15              3                         4                      0                   0.101351        0.304561                  0.100000              30.0                 3.005000                                    0.133333                                 0.000000                0.010135                           0.013514                        0.000000        3.283417              0.033278                         0.044370                      0.000000   -0.643651    0.289025    -2.446935     1.607977\n",
       "498051           5                     550       39.44              1                         2                      1                   0.001818        0.071709                  1.000000               1.0                39.440000                                    2.000000                                 1.000000                0.001818                           0.003636                        0.001818       13.945233              0.025355                         0.050710                      0.025355   -0.086355    0.317151    -0.224669     0.960902\n",
       "498052           1                     154       97.15              7                         2                      5                   0.012987        0.630844                  3.500000               2.0                48.575000                                    1.000000                                 2.500000                0.045455                           0.012987                        0.032468        1.585178              0.072054                         0.020587                      0.051467    0.758849   -0.489491     1.198422    -0.940836\n",
       "498053           4                    1674       22.70              7                         4                      5                   0.002389        0.013560                  1.750000               4.0                 5.675000                                    1.000000                                 1.250000                0.004182                           0.002389                        0.002987       73.744493              0.308370                         0.176211                      0.220264    0.618356    0.104645     1.677158     0.414093\n",
       "498054           5                    1542       79.01              6                         4                      5                   0.019455        0.051239                  0.200000              30.0                 2.633667                                    0.133333                                 0.166667                0.003891                           0.002594                        0.003243       19.516517              0.075940                         0.050627                      0.063283    0.279533    0.276435     2.125745     1.806038\n",
       "498055           2                    1210       83.79              9                         6                      0                   0.021488        0.069248                  0.346154              26.0                 3.222692                                    0.230769                                 0.000000                0.007438                           0.004959                        0.000000       14.440864              0.107411                         0.071608                      0.000000   -0.557716   -0.302632    -2.440468    -1.617686\n",
       "498056           5                     289       80.54              3                         0                      0                   0.103806        0.278685                  0.100000              30.0                 2.684667                                    0.000000                                 0.000000                0.010381                           0.000000                        0.000000        3.588279              0.037249                         0.000000                      0.000000   -0.675826    0.284363    -2.426243     1.566113\n",
       "498057           6                    1208       44.88             10                         9                      2                   0.020695        0.037152                  0.400000              25.0                 1.795200                                    0.360000                                 0.080000                0.008278                           0.007450                        0.001656       26.916221              0.222816                         0.200535                      0.044563   -0.167101    0.495667    -0.624764     2.493826\n",
       "498058           2                    1401       17.78             10                         0                      1                   0.000714        0.012691                 10.000000               1.0                17.780000                                    0.000000                                 1.000000                0.007138                           0.000000                        0.000714       78.796400              0.562430                         0.000000                      0.056243   -0.025829   -0.279715     3.210981     0.048958\n",
       "498059           3                      92       26.42              8                         6                      3                   0.021739        0.287174                  4.000000               2.0                13.210000                                    3.000000                                 1.500000                0.086957                           0.065217                        0.032609        3.482210              0.302801                         0.227101                      0.113550    0.356206   -0.081713     0.458718    -0.467975\n",
       "498060           1                     513       88.74              7                         2                      4                   0.015595        0.172982                  0.875000               8.0                11.092500                                    0.250000                                 0.500000                0.013645                           0.003899                        0.007797        5.780933              0.078882                         0.022538                      0.045076    0.358400   -0.499654     1.207683    -1.849035\n",
       "498061           5                    1161       80.91             10                         5                      4                   0.018949        0.069690                  0.454545              22.0                 3.677727                                    0.227273                                 0.181818                0.008613                           0.004307                        0.003445       14.349277              0.123594                         0.061797                      0.049438    0.215484    0.289231     1.471976     1.844743\n",
       "498062           6                    1219       90.48              8                         8                      4                   0.009024        0.074225                  0.727273              11.0                 8.225455                                    0.727273                                 0.363636                0.006563                           0.006563                        0.003281       13.472591              0.088417                         0.088417                      0.044209    0.366828    0.503845     1.267551     1.988822\n",
       "498063           5                    1006       19.05              8                        10                      3                   0.015905        0.018936                  0.500000              16.0                 1.190625                                    0.625000                                 0.187500                0.007952                           0.009940                        0.002982       52.808399              0.419948                         0.524934                      0.157480    0.108321    0.302500     0.356638     1.712943\n",
       "498064           3                    1311        9.85              1                        10                      3                   0.012967        0.007513                  0.058824              17.0                 0.579412                                    0.588235                                 0.176471                0.000763                           0.007628                        0.002288      133.096447              0.101523                         1.015228                      0.304569    0.056719   -0.100213     0.369381    -0.576603\n",
       "498065           5                    1345       61.57              4                         0                      4                   0.017100        0.045777                  0.173913              23.0                 2.676957                                    0.000000                                 0.173913                0.002974                           0.000000                        0.002974       21.845054              0.064967                         0.000000                      0.064967    0.130100    0.280736     1.504133     1.936499\n",
       "498066           6                      80       64.53              4                         0                      5                   0.225000        0.806625                  0.222222              18.0                 3.585000                                    0.000000                                 0.277778                0.050000                           0.000000                        0.062500        1.239733              0.061987                         0.000000                      0.077483    0.372415    0.482587     1.873563     2.192983\n",
       "498067           2                    1623       12.69              8                         0                      3                   0.001232        0.007819                  4.000000               2.0                 6.345000                                    0.000000                                 1.500000                0.004929                           0.000000                        0.001848      127.895981              0.630418                         0.000000                      0.236407    0.273070   -0.290999     0.547632    -1.137739\n",
       "498068           3                     361       79.04             10                         3                      0                   0.022161        0.218947                  1.250000               8.0                 9.880000                                    0.375000                                 0.000000                0.027701                           0.008310                        0.000000        4.567308              0.126518                         0.037955                      0.000000   -0.366288   -0.087476    -2.002003    -0.397231\n",
       "498069           2                    1501       59.95              8                         8                      5                   0.002665        0.039940                  2.000000               4.0                14.987500                                    2.000000                                 1.250000                0.005330                           0.005330                        0.003331       25.037531              0.133445                         0.133445                      0.083403    0.682855   -0.288454     1.605850    -1.109372\n",
       "498070           6                    1137        6.33              5                         4                      0                   0.023747        0.005567                  0.185185              27.0                 0.234444                                    0.148148                                 0.000000                0.004398                           0.003518                        0.000000      179.620853              0.789889                         0.631912                      0.000000   -0.629039    0.491641    -2.294293     2.207567\n",
       "498071           2                     948       38.69              4                         0                      0                   0.006329        0.040812                  0.666667               6.0                 6.448333                                    0.000000                                 0.000000                0.004219                           0.000000                        0.000000       24.502455              0.103386                         0.000000                      0.000000   -0.418069   -0.291281    -2.071975    -1.265852\n",
       "498072           5                    1473       59.90              2                        10                      5                   0.006789        0.040665                  0.200000              10.0                 5.990000                                    1.000000                                 0.500000                0.001358                           0.006789                        0.003394       24.590985              0.033389                         0.166945                      0.083472    0.537267    0.302683     1.916720     1.386184\n",
       "498073           2                     165       95.42              9                         0                      3                   0.157576        0.578303                  0.346154              26.0                 3.670000                                    0.000000                                 0.115385                0.054545                           0.000000                        0.018182        1.729197              0.094320                         0.000000                      0.031440   -0.048566   -0.317767     0.336128    -1.880551\n",
       "498074           6                    1539       42.31              1                         3                      0                   0.014945        0.027492                  0.043478              23.0                 1.839565                                    0.130435                                 0.000000                0.000650                           0.001949                        0.000000       36.374380              0.023635                         0.070905                      0.000000   -0.601733    0.493902    -2.266427     2.160896\n",
       "498075           6                    1543       53.43             10                         8                      1                   0.016850        0.034627                  0.384615              26.0                 2.055000                                    0.307692                                 0.038462                0.006481                           0.005185                        0.000648       28.878907              0.187161                         0.149729                      0.018716   -0.365887    0.496422    -1.670767     2.344311\n",
       "498076           6                    1196       78.10              4                         6                      2                   0.004181        0.065301                  0.800000               5.0                15.620000                                    1.200000                                 0.400000                0.003344                           0.005017                        0.001672       15.313700              0.051216                         0.076825                      0.025608    0.046391    0.512911    -0.487240     1.882704\n",
       "498077           4                     676       65.13              8                         6                      1                   0.013314        0.096346                  0.888889               9.0                 7.236667                                    0.666667                                 0.111111                0.011834                           0.008876                        0.001479       10.379242              0.122831                         0.092123                      0.015354   -0.186779    0.111434    -1.541044     0.441140\n",
       "498078           5                    1100       46.66              7                         8                      2                   0.004545        0.042418                  1.400000               5.0                 9.332000                                    1.600000                                 0.400000                0.006364                           0.007273                        0.001818       23.574796              0.150021                         0.171453                      0.042863    0.068788    0.315959    -0.455508     1.325383\n",
       "498079           6                     311       29.78              1                         8                      4                   0.086817        0.095756                  0.037037              27.0                 1.102963                                    0.296296                                 0.148148                0.003215                           0.025723                        0.012862       10.443251              0.033580                         0.268637                      0.134318    0.119375    0.484379     1.382392     2.459363\n",
       "498080           1                     560       69.82             10                         3                      5                   0.030357        0.124679                  0.588235              17.0                 4.107059                                    0.176471                                 0.294118                0.017857                           0.005357                        0.008929        8.020624              0.143225                         0.042968                      0.071613    0.439937   -0.510433     1.927774    -2.153347\n",
       "498081           6                    1733       43.08              2                         8                      4                   0.012695        0.024859                  0.090909              22.0                 1.958182                                    0.363636                                 0.181818                0.001154                           0.004616                        0.002308       40.227484              0.046425                         0.185701                      0.092851    0.183670    0.490147     1.409201     2.442498\n",
       "498082           3                    1437       36.29              9                         7                      2                   0.016006        0.025254                  0.391304              23.0                 1.577826                                    0.304348                                 0.086957                0.006263                           0.004871                        0.001392       39.597685              0.248002                         0.192891                      0.055112   -0.167162   -0.104527    -0.600645    -0.548197\n",
       "498083           5                     101       46.25             10                         2                      3                   0.019802        0.457921                  5.000000               2.0                23.125000                                    1.000000                                 1.500000                0.099010                           0.019802                        0.029703        2.183784              0.216216                         0.043243                      0.064865    0.341486    0.313902     0.400110     0.867679\n",
       "498084           4                     587       60.88              1                         8                      3                   0.045997        0.103714                  0.037037              27.0                 2.254815                                    0.296296                                 0.111111                0.001704                           0.013629                        0.005111        9.641919              0.016426                         0.131406                      0.049277   -0.053252    0.087779     0.351923     0.526065\n",
       "498085           1                    1270       25.98              5                         1                      4                   0.010236        0.020457                  0.384615              13.0                 1.998462                                    0.076923                                 0.307692                0.003937                           0.000787                        0.003150       48.883757              0.192456                         0.038491                      0.153965    0.246751   -0.507737     1.323659    -2.250099\n",
       "498086           4                     185       63.43             10                         9                      2                   0.027027        0.342865                  2.000000               5.0                12.686000                                    1.800000                                 0.400000                0.054054                           0.048649                        0.010811        2.916601              0.157654                         0.141889                      0.031531    0.105984    0.118611    -0.462521     0.424089\n",
       "498087           6                    1480       19.87              7                         0                      2                   0.000676        0.013426                  7.000000               1.0                19.870000                                    0.000000                                 2.000000                0.004730                           0.000000                        0.001351       74.484147              0.352290                         0.000000                      0.100654    0.160370    0.515485     3.165331     0.183863\n",
       "498088           4                     403       50.53              1                         0                      3                   0.002481        0.125385                  1.000000               1.0                50.530000                                    0.000000                                 3.000000                0.002481                           0.000000                        0.007444        7.975460              0.019790                         0.000000                      0.059371    0.342761    0.110730     3.198606     0.121134\n",
       "498089           6                     154       84.63              6                         2                      4                   0.006494        0.549545                  6.000000               1.0                84.630000                                    2.000000                                 4.000000                0.038961                           0.012987                        0.025974        1.819686              0.070897                         0.023632                      0.047265    0.710191    0.519869     3.510299     0.123292\n",
       "498090           3                     285       74.97              4                         7                      2                   0.003509        0.263053                  4.000000               1.0                74.970000                                    7.000000                                 2.000000                0.014035                           0.024561                        0.007018        3.801521              0.053355                         0.093371                      0.026677    0.307022   -0.068999     3.409179     0.105333\n",
       "498091           4                    1691       22.89             10                         6                      5                   0.007688        0.013536                  0.769231              13.0                 1.760769                                    0.461538                                 0.384615                0.005914                           0.003548                        0.002957       73.875055              0.436872                         0.262123                      0.218436    0.494801    0.096569     1.999168     0.488506\n",
       "498092           1                    1685       61.99              6                         0                      4                   0.001187        0.036789                  3.000000               2.0                30.995000                                    0.000000                                 2.000000                0.003561                           0.000000                        0.002374       27.181804              0.096790                         0.000000                      0.064527    0.495132   -0.492042     1.173011    -1.210670\n",
       "498093           4                     275       48.23              6                         1                      3                   0.040000        0.175382                  0.545455              11.0                 4.384545                                    0.090909                                 0.272727                0.021818                           0.003636                        0.010909        5.701845              0.124404                         0.020734                      0.062202    0.101001    0.096966     0.374923     0.507155\n",
       "498094           5                    1085       50.15              3                         6                      2                   0.012903        0.046221                  0.214286              14.0                 3.582143                                    0.428571                                 0.142857                0.002765                           0.005530                        0.001843       21.635095              0.059821                         0.119641                      0.039880   -0.097823    0.301640    -0.543317     1.711947\n",
       "498095           5                     687       33.47              7                         1                      1                   0.036390        0.048719                  0.280000              25.0                 1.338800                                    0.040000                                 0.040000                0.010189                           0.001456                        0.001456       20.525844              0.209143                         0.029878                      0.029878   -0.427005    0.288301    -1.870431     1.752422\n",
       "498096           6                     425       18.91              7                         9                      4                   0.009412        0.044494                  1.750000               4.0                 4.727500                                    2.250000                                 1.000000                0.016471                           0.021176                        0.009412       22.474881              0.370175                         0.475939                      0.211528    0.472386    0.513056     1.143937     1.454934\n",
       "498097           2                     671       86.03             11                         1                      5                   0.038748        0.128212                  0.423077              26.0                 3.308846                                    0.038462                                 0.192308                0.016393                           0.001490                        0.007452        7.799605              0.127862                         0.011624                      0.058119    0.330910   -0.321494     1.989479    -1.814723\n",
       "498098           4                    1705       98.66              5                         3                      2                   0.002346        0.057865                  1.250000               4.0                24.665000                                    0.750000                                 0.500000                0.002933                           0.001760                        0.001173       17.281573              0.050679                         0.030407                      0.020272    0.058991    0.112030    -0.503091     0.457804\n",
       "498099           1                    1822       75.30              9                         2                      4                   0.016465        0.041328                  0.300000              30.0                 2.510000                                    0.066667                                 0.133333                0.004940                           0.001098                        0.002195       24.196547              0.119522                         0.026560                      0.053121    0.096394   -0.521803     1.458583    -2.411263\n",
       "498100           3                    1151       11.37              3                         1                      0                   0.021720        0.009878                  0.120000              25.0                 0.454800                                    0.040000                                 0.000000                0.002606                           0.000869                        0.000000      101.231310              0.263852                         0.087951                      0.000000   -0.637499   -0.109895    -2.545646    -0.485906\n",
       "498101           1                     796       85.72              9                         3                      1                   0.001256        0.107688                  9.000000               1.0                85.720000                                    3.000000                                 1.000000                0.011307                           0.003769                        0.001256        9.286048              0.104993                         0.034998                      0.011666    0.095151   -0.469309     3.355964     0.082610\n",
       "498102           2                    1379       48.64             11                         7                      5                   0.014503        0.035272                  0.550000              20.0                 2.432000                                    0.350000                                 0.250000                0.007977                           0.005076                        0.003626       28.351151              0.226151                         0.143914                      0.102796    0.431208   -0.308803     2.077825    -1.749740\n",
       "498103           6                     885       92.41              6                         5                      0                   0.016949        0.104418                  0.400000              15.0                 6.160667                                    0.333333                                 0.000000                0.006780                           0.005650                        0.000000        9.576886              0.064928                         0.054107                      0.000000   -0.454381    0.506017    -2.100359     2.010543\n",
       "498104           3                     441       36.91              0                         8                      4                   0.004535        0.083696                  0.000000               2.0                18.455000                                    4.000000                                 2.000000                0.000000                           0.018141                        0.009070       11.947982              0.000000                         0.216743                      0.108372    0.517835   -0.084884     1.092574    -0.370675\n",
       "498105           2                     319       57.74              2                         1                      0                   0.094044        0.181003                  0.066667              30.0                 1.924667                                    0.033333                                 0.000000                0.006270                           0.003135                        0.000000        5.524766              0.034638                         0.017319                      0.000000   -0.680588   -0.314602    -2.465426    -1.618736\n",
       "498106           4                    1228       74.44              6                         5                      3                   0.022801        0.060619                  0.214286              28.0                 2.658571                                    0.178571                                 0.107143                0.004886                           0.004072                        0.002443       16.496507              0.080602                         0.067168                      0.040301   -0.057299    0.084976     0.348254     0.540835\n",
       "498107           4                     708       19.56              8                         1                      3                   0.002825        0.027627                  4.000000               2.0                 9.780000                                    0.500000                                 1.500000                0.011299                           0.001412                        0.004237       36.196319              0.408998                         0.051125                      0.153374    0.290756    0.110333     0.517168     0.377922\n",
       "498108           2                    1726       99.80              1                        10                      1                   0.007532        0.057822                  0.076923              13.0                 7.676923                                    0.769231                                 0.076923                0.000579                           0.005794                        0.000579       17.294589              0.010020                         0.100200                      0.010020   -0.232505   -0.289245    -1.525648    -1.405017\n",
       "498109           4                     961       18.62              1                         0                      1                   0.003122        0.019376                  0.333333               3.0                 6.206667                                    0.000000                                 0.333333                0.001041                           0.000000                        0.001041       51.611171              0.053706                         0.000000                      0.053706   -0.213660    0.107843    -1.371567     0.448997\n",
       "498110           3                     585       26.17              0                         9                      2                   0.022222        0.044735                  0.000000              13.0                 2.013077                                    0.692308                                 0.153846                0.000000                           0.015385                        0.003419       22.353840              0.000000                         0.343905                      0.076423   -0.086967   -0.094858    -0.582623    -0.487519\n",
       "498111           4                     983       57.72              4                         9                      4                   0.030519        0.058718                  0.133333              30.0                 1.924000                                    0.300000                                 0.133333                0.004069                           0.009156                        0.004069       17.030492              0.069300                         0.155925                      0.069300    0.117063    0.084078     1.641820     0.505170\n",
       "498112           1                    1342       69.89              8                        10                      5                   0.002981        0.052079                  2.000000               4.0                17.472500                                    2.500000                                 1.250000                0.005961                           0.007452                        0.003726       19.201603              0.114466                         0.143082                      0.071541    0.707169   -0.485454     1.496246    -1.416488\n",
       "498113           5                     553       95.44              9                         5                      5                   0.045208        0.172586                  0.360000              25.0                 3.817600                                    0.200000                                 0.200000                0.016275                           0.009042                        0.009042        5.794216              0.094300                         0.052389                      0.052389    0.364355    0.283246     2.064971     1.756742\n",
       "498114           6                    1767       65.98              0                         0                      1                   0.013016        0.037340                  0.000000              23.0                 2.868696                                    0.000000                                 0.043478                0.000000                           0.000000                        0.000566       26.780843              0.000000                         0.000000                      0.015156   -0.438262    0.487835    -1.685405     2.273375\n",
       "498115           6                     879       96.34              2                         9                      1                   0.020478        0.109602                  0.111111              18.0                 5.352222                                    0.500000                                 0.055556                0.002275                           0.010239                        0.001138        9.123936              0.020760                         0.093419                      0.010380   -0.294484    0.503851    -1.571458     2.208047\n",
       "498116           4                     783       59.10              2                         2                      0                   0.012771        0.075479                  0.200000              10.0                 5.910000                                    0.200000                                 0.000000                0.002554                           0.002554                        0.000000       13.248731              0.033841                         0.033841                      0.000000   -0.453159    0.106284    -2.261474     0.454349\n",
       "498117           1                     278       98.90              9                         5                      4                   0.050360        0.355755                  0.642857              14.0                 7.064286                                    0.357143                                 0.285714                0.032374                           0.017986                        0.014388        2.810920              0.091001                         0.050556                      0.040445    0.313497   -0.502220     1.283820    -2.074560\n",
       "498118           3                     300        5.41              6                         6                      4                   0.030000        0.018033                  0.666667               9.0                 0.601111                                    0.666667                                 0.444444                0.020000                           0.020000                        0.013333       55.452865              1.109057                         1.109057                      0.739372    0.336591   -0.098049     1.371059    -0.498357\n",
       "498119           2                    1524       33.97              2                         5                      3                   0.005906        0.022290                  0.222222               9.0                 3.774444                                    0.555556                                 0.333333                0.001312                           0.003281                        0.001969       44.863115              0.058875                         0.147189                      0.088313    0.130232   -0.296911     0.386430    -1.575245\n",
       "498120           3                    1456       56.97             11                         7                      2                   0.019231        0.039128                  0.392857              28.0                 2.034643                                    0.250000                                 0.071429                0.007555                           0.004808                        0.001374       25.557311              0.193084                         0.122872                      0.035106   -0.206077   -0.108752    -0.618502    -0.542440\n",
       "\n",
       "[498121 rows x 24 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(test[best_model_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498121"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497871</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497872</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497873</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497874</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497875</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497876</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497877</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497878</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497879</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497880</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497881</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497882</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497883</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497884</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497885</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497886</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497887</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497888</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497889</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497890</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497891</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497892</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497893</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497894</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497895</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497896</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497897</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497898</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497899</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497900</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497901</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497902</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497903</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497904</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497905</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497906</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497907</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497908</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497909</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497910</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497911</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497912</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497913</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497914</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497915</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497916</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497917</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497918</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497919</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497920</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497921</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497922</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497923</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497924</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497925</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497926</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497927</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497928</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497929</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497930</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497931</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497932</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497933</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497934</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497935</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497936</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497937</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497938</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497939</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497940</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497941</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497942</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497943</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497944</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497945</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497946</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497947</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497948</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497949</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497950</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497951</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497952</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497953</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497954</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497955</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497956</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497957</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497958</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497959</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497960</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497961</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497962</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497963</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497964</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497965</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497966</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497967</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497968</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497969</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497970</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497971</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497972</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497973</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497974</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497975</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497976</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497977</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497978</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497979</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497980</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497981</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497982</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497983</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497984</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497985</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497986</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497987</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497988</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497989</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497990</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497991</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497992</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497993</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497994</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497995</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497996</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497997</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497998</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497999</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498000</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498001</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498002</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498003</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498004</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498005</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498006</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498007</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498008</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498009</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498010</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498011</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498012</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498013</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498014</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498015</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498016</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498017</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498018</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498019</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498020</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498021</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498022</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498023</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498024</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498025</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498026</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498027</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498028</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498029</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498030</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498031</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498032</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498033</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498034</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498035</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498036</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498037</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498038</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498039</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498040</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498041</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498042</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498043</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498044</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498045</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498046</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498047</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498048</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498049</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498050</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498051</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498052</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498053</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498054</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498055</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498056</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498057</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498058</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498059</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498060</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498061</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498062</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498063</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498064</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498065</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498066</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498067</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498068</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498069</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498070</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498071</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498072</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498073</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498074</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498075</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498076</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498077</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498078</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498079</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498080</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498081</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498082</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498083</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498084</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498085</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498086</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498087</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498088</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498089</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498090</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498091</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498092</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498093</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498094</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498095</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498096</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498097</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498098</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498099</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498100</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498101</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498102</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498103</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498104</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498105</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498106</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498107</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498108</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498109</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498110</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498111</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498112</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498113</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498114</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498115</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498116</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498117</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498118</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498119</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498120</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498121 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fraud\n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           0\n",
       "5           0\n",
       "6           0\n",
       "7           0\n",
       "8           0\n",
       "9           0\n",
       "10          0\n",
       "11          0\n",
       "12          0\n",
       "13          0\n",
       "14          0\n",
       "15          0\n",
       "16          0\n",
       "17          0\n",
       "18          0\n",
       "19          0\n",
       "20          0\n",
       "21          0\n",
       "22          0\n",
       "23          0\n",
       "24          0\n",
       "25          0\n",
       "26          0\n",
       "27          0\n",
       "28          0\n",
       "29          0\n",
       "30          0\n",
       "31          0\n",
       "32          0\n",
       "33          0\n",
       "34          0\n",
       "35          0\n",
       "36          0\n",
       "37          0\n",
       "38          0\n",
       "39          0\n",
       "40          0\n",
       "41          0\n",
       "42          0\n",
       "43          0\n",
       "44          0\n",
       "45          0\n",
       "46          0\n",
       "47          0\n",
       "48          0\n",
       "49          0\n",
       "50          0\n",
       "51          0\n",
       "52          0\n",
       "53          0\n",
       "54          0\n",
       "55          0\n",
       "56          0\n",
       "57          0\n",
       "58          0\n",
       "59          0\n",
       "60          0\n",
       "61          0\n",
       "62          0\n",
       "63          0\n",
       "64          0\n",
       "65          0\n",
       "66          0\n",
       "67          0\n",
       "68          0\n",
       "69          0\n",
       "70          0\n",
       "71          0\n",
       "72          0\n",
       "73          0\n",
       "74          0\n",
       "75          0\n",
       "76          0\n",
       "77          1\n",
       "78          0\n",
       "79          0\n",
       "80          1\n",
       "81          0\n",
       "82          0\n",
       "83          0\n",
       "84          0\n",
       "85          0\n",
       "86          0\n",
       "87          0\n",
       "88          0\n",
       "89          0\n",
       "90          0\n",
       "91          0\n",
       "92          0\n",
       "93          0\n",
       "94          0\n",
       "95          0\n",
       "96          0\n",
       "97          0\n",
       "98          0\n",
       "99          0\n",
       "100         0\n",
       "101         0\n",
       "102         0\n",
       "103         0\n",
       "104         0\n",
       "105         0\n",
       "106         0\n",
       "107         0\n",
       "108         0\n",
       "109         0\n",
       "110         1\n",
       "111         0\n",
       "112         0\n",
       "113         0\n",
       "114         0\n",
       "115         0\n",
       "116         0\n",
       "117         0\n",
       "118         0\n",
       "119         0\n",
       "120         0\n",
       "121         0\n",
       "122         0\n",
       "123         0\n",
       "124         0\n",
       "125         0\n",
       "126         0\n",
       "127         0\n",
       "128         0\n",
       "129         0\n",
       "130         0\n",
       "131         0\n",
       "132         0\n",
       "133         0\n",
       "134         0\n",
       "135         0\n",
       "136         0\n",
       "137         0\n",
       "138         0\n",
       "139         0\n",
       "140         0\n",
       "141         0\n",
       "142         0\n",
       "143         0\n",
       "144         0\n",
       "145         0\n",
       "146         0\n",
       "147         0\n",
       "148         0\n",
       "149         0\n",
       "150         0\n",
       "151         0\n",
       "152         0\n",
       "153         0\n",
       "154         0\n",
       "155         0\n",
       "156         0\n",
       "157         0\n",
       "158         0\n",
       "159         0\n",
       "160         1\n",
       "161         0\n",
       "162         0\n",
       "163         0\n",
       "164         0\n",
       "165         0\n",
       "166         0\n",
       "167         0\n",
       "168         0\n",
       "169         0\n",
       "170         0\n",
       "171         0\n",
       "172         0\n",
       "173         0\n",
       "174         0\n",
       "175         0\n",
       "176         0\n",
       "177         0\n",
       "178         0\n",
       "179         0\n",
       "180         0\n",
       "181         0\n",
       "182         0\n",
       "183         0\n",
       "184         0\n",
       "185         0\n",
       "186         0\n",
       "187         0\n",
       "188         0\n",
       "189         0\n",
       "190         0\n",
       "191         0\n",
       "192         0\n",
       "193         0\n",
       "194         0\n",
       "195         0\n",
       "196         0\n",
       "197         0\n",
       "198         0\n",
       "199         0\n",
       "200         0\n",
       "201         0\n",
       "202         0\n",
       "203         0\n",
       "204         0\n",
       "205         0\n",
       "206         0\n",
       "207         0\n",
       "208         0\n",
       "209         0\n",
       "210         0\n",
       "211         0\n",
       "212         0\n",
       "213         0\n",
       "214         0\n",
       "215         0\n",
       "216         0\n",
       "217         0\n",
       "218         0\n",
       "219         0\n",
       "220         0\n",
       "221         0\n",
       "222         0\n",
       "223         0\n",
       "224         0\n",
       "225         0\n",
       "226         0\n",
       "227         0\n",
       "228         0\n",
       "229         0\n",
       "230         0\n",
       "231         0\n",
       "232         0\n",
       "233         0\n",
       "234         0\n",
       "235         0\n",
       "236         0\n",
       "237         0\n",
       "238         0\n",
       "239         0\n",
       "240         0\n",
       "241         0\n",
       "242         0\n",
       "243         0\n",
       "244         0\n",
       "245         0\n",
       "246         0\n",
       "247         0\n",
       "248         0\n",
       "249         0\n",
       "...       ...\n",
       "497871      0\n",
       "497872      0\n",
       "497873      0\n",
       "497874      0\n",
       "497875      0\n",
       "497876      0\n",
       "497877      0\n",
       "497878      0\n",
       "497879      0\n",
       "497880      0\n",
       "497881      0\n",
       "497882      0\n",
       "497883      1\n",
       "497884      0\n",
       "497885      0\n",
       "497886      0\n",
       "497887      0\n",
       "497888      0\n",
       "497889      0\n",
       "497890      0\n",
       "497891      0\n",
       "497892      0\n",
       "497893      0\n",
       "497894      0\n",
       "497895      0\n",
       "497896      0\n",
       "497897      0\n",
       "497898      0\n",
       "497899      0\n",
       "497900      0\n",
       "497901      0\n",
       "497902      1\n",
       "497903      0\n",
       "497904      0\n",
       "497905      0\n",
       "497906      0\n",
       "497907      0\n",
       "497908      0\n",
       "497909      0\n",
       "497910      0\n",
       "497911      0\n",
       "497912      0\n",
       "497913      0\n",
       "497914      0\n",
       "497915      0\n",
       "497916      1\n",
       "497917      0\n",
       "497918      0\n",
       "497919      0\n",
       "497920      0\n",
       "497921      0\n",
       "497922      0\n",
       "497923      1\n",
       "497924      0\n",
       "497925      0\n",
       "497926      0\n",
       "497927      0\n",
       "497928      0\n",
       "497929      1\n",
       "497930      0\n",
       "497931      0\n",
       "497932      0\n",
       "497933      0\n",
       "497934      0\n",
       "497935      0\n",
       "497936      0\n",
       "497937      0\n",
       "497938      0\n",
       "497939      0\n",
       "497940      0\n",
       "497941      0\n",
       "497942      0\n",
       "497943      0\n",
       "497944      0\n",
       "497945      0\n",
       "497946      0\n",
       "497947      1\n",
       "497948      0\n",
       "497949      0\n",
       "497950      0\n",
       "497951      0\n",
       "497952      0\n",
       "497953      0\n",
       "497954      0\n",
       "497955      0\n",
       "497956      0\n",
       "497957      0\n",
       "497958      0\n",
       "497959      0\n",
       "497960      1\n",
       "497961      0\n",
       "497962      0\n",
       "497963      0\n",
       "497964      0\n",
       "497965      0\n",
       "497966      0\n",
       "497967      0\n",
       "497968      0\n",
       "497969      0\n",
       "497970      0\n",
       "497971      0\n",
       "497972      0\n",
       "497973      0\n",
       "497974      0\n",
       "497975      0\n",
       "497976      0\n",
       "497977      0\n",
       "497978      0\n",
       "497979      0\n",
       "497980      0\n",
       "497981      1\n",
       "497982      0\n",
       "497983      0\n",
       "497984      0\n",
       "497985      0\n",
       "497986      0\n",
       "497987      0\n",
       "497988      0\n",
       "497989      0\n",
       "497990      0\n",
       "497991      0\n",
       "497992      0\n",
       "497993      0\n",
       "497994      0\n",
       "497995      0\n",
       "497996      1\n",
       "497997      0\n",
       "497998      0\n",
       "497999      0\n",
       "498000      0\n",
       "498001      0\n",
       "498002      0\n",
       "498003      0\n",
       "498004      0\n",
       "498005      0\n",
       "498006      0\n",
       "498007      0\n",
       "498008      0\n",
       "498009      0\n",
       "498010      0\n",
       "498011      0\n",
       "498012      0\n",
       "498013      0\n",
       "498014      0\n",
       "498015      0\n",
       "498016      0\n",
       "498017      0\n",
       "498018      0\n",
       "498019      0\n",
       "498020      0\n",
       "498021      0\n",
       "498022      0\n",
       "498023      0\n",
       "498024      0\n",
       "498025      0\n",
       "498026      0\n",
       "498027      1\n",
       "498028      0\n",
       "498029      0\n",
       "498030      0\n",
       "498031      0\n",
       "498032      1\n",
       "498033      0\n",
       "498034      0\n",
       "498035      0\n",
       "498036      0\n",
       "498037      0\n",
       "498038      0\n",
       "498039      0\n",
       "498040      0\n",
       "498041      0\n",
       "498042      0\n",
       "498043      0\n",
       "498044      0\n",
       "498045      1\n",
       "498046      0\n",
       "498047      0\n",
       "498048      0\n",
       "498049      0\n",
       "498050      0\n",
       "498051      0\n",
       "498052      0\n",
       "498053      0\n",
       "498054      0\n",
       "498055      0\n",
       "498056      0\n",
       "498057      0\n",
       "498058      0\n",
       "498059      0\n",
       "498060      0\n",
       "498061      0\n",
       "498062      0\n",
       "498063      0\n",
       "498064      0\n",
       "498065      0\n",
       "498066      0\n",
       "498067      0\n",
       "498068      0\n",
       "498069      0\n",
       "498070      0\n",
       "498071      0\n",
       "498072      0\n",
       "498073      0\n",
       "498074      0\n",
       "498075      0\n",
       "498076      0\n",
       "498077      0\n",
       "498078      0\n",
       "498079      0\n",
       "498080      0\n",
       "498081      0\n",
       "498082      0\n",
       "498083      0\n",
       "498084      0\n",
       "498085      0\n",
       "498086      0\n",
       "498087      0\n",
       "498088      0\n",
       "498089      0\n",
       "498090      0\n",
       "498091      0\n",
       "498092      0\n",
       "498093      0\n",
       "498094      0\n",
       "498095      0\n",
       "498096      0\n",
       "498097      0\n",
       "498098      0\n",
       "498099      1\n",
       "498100      0\n",
       "498101      0\n",
       "498102      0\n",
       "498103      0\n",
       "498104      0\n",
       "498105      0\n",
       "498106      0\n",
       "498107      0\n",
       "498108      0\n",
       "498109      0\n",
       "498110      0\n",
       "498111      0\n",
       "498112      0\n",
       "498113      0\n",
       "498114      0\n",
       "498115      0\n",
       "498116      0\n",
       "498117      0\n",
       "498118      0\n",
       "498119      0\n",
       "498120      0\n",
       "\n",
       "[498121 rows x 1 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.columns = ['fraud']\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    475226\n",
      "1     22895\n",
      "Name: fraud, dtype: int64\n",
      "0    0.954037\n",
      "1    0.045963\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(predictions_df.fraud.value_counts())\n",
    "print(predictions_df.fraud.value_counts() / len(predictions_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv('TU_Muenchen_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2: Creating a voting classifier out of the 5 best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Oversampling Strategy</th>\n",
       "      <th>Data Preparation</th>\n",
       "      <th>Feature Count</th>\n",
       "      <th>Feature Selection Technique</th>\n",
       "      <th>Features</th>\n",
       "      <th>Optimal Parameters</th>\n",
       "      <th>Monetary Value Per Instance - Mean</th>\n",
       "      <th>Monetary Value Per Instance - Standard Deviation</th>\n",
       "      <th>Raw Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>11</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[totalScanTimeInSeconds, lineItemVoids, scansW...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'base_estimator': Dec...</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>22</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_score': 0.5, 'eval_metric': 'error', 'l...</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.108912</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>17</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'C': 1.8000000000000003, 'fit_intercept': Tru...</td>\n",
       "      <td>0.159659</td>\n",
       "      <td>0.094489</td>\n",
       "      <td>LogisticRegression(C=1.8000000000000003, class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>14</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'base_estimator': LogisticRegression(C=1.0, c...</td>\n",
       "      <td>0.138371</td>\n",
       "      <td>0.091208</td>\n",
       "      <td>(LogisticRegression(C=1.0, class_weight=None, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>15</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'loss': 'deviance', 'n_estimators': 200}</td>\n",
       "      <td>0.130389</td>\n",
       "      <td>0.084181</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>10</td>\n",
       "      <td>RFE</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'gini', 'n_estimators': 190}</td>\n",
       "      <td>0.090474</td>\n",
       "      <td>0.090753</td>\n",
       "      <td>(ExtraTreeClassifier(class_weight=None, criter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'n_estimators': 40}</td>\n",
       "      <td>0.082491</td>\n",
       "      <td>0.097579</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>13</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 29, 'min_sa...</td>\n",
       "      <td>0.034593</td>\n",
       "      <td>0.126854</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>19</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'activation': 'logistic', 'learning_rate': 'i...</td>\n",
       "      <td>0.023949</td>\n",
       "      <td>0.108119</td>\n",
       "      <td>MLPClassifier(activation='logistic', alpha=0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>18</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, totalScanTimeInSeconds, lineItemV...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 13, 'min...</td>\n",
       "      <td>-0.026610</td>\n",
       "      <td>0.090638</td>\n",
       "      <td>ExtraTreeClassifier(class_weight=None, criteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVC</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>-0.162320</td>\n",
       "      <td>0.181452</td>\n",
       "      <td>SVC(C=0.1, cache_size=200, class_weight=None, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.180947</td>\n",
       "      <td>0.210302</td>\n",
       "      <td>KNeighborsClassifier(algorithm='auto', leaf_si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{'alpha': 0.0005, 'fit_intercept': False, 'max...</td>\n",
       "      <td>-0.207557</td>\n",
       "      <td>0.130813</td>\n",
       "      <td>Perceptron(alpha=0.0005, class_weight=None, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>2</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[pca_axis_2, tsne_axis_2]</td>\n",
       "      <td>{'reg_param': 0.8}</td>\n",
       "      <td>-0.220862</td>\n",
       "      <td>0.102408</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>1</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[tsne_axis_2]</td>\n",
       "      <td>{'alpha': 0, 'binarize': 0.0}</td>\n",
       "      <td>-0.276743</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>BernoulliNB(alpha=0, binarize=0.0, class_prior...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>No Oversampling</td>\n",
       "      <td>No Scaling</td>\n",
       "      <td>4</td>\n",
       "      <td>SelectKBest</td>\n",
       "      <td>[trustLevel, scannedLineItems, pca_axis_2, tsn...</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.968600</td>\n",
       "      <td>0.301185</td>\n",
       "      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model Oversampling Strategy Data Preparation Feature Count Feature Selection Technique                                           Features                                 Optimal Parameters  Monetary Value Per Instance - Mean  Monetary Value Per Instance - Standard Deviation                                          Raw Model\n",
       "13             AdaBoostClassifier       No Oversampling       No Scaling            11                         RFE  [totalScanTimeInSeconds, lineItemVoids, scansW...  {'algorithm': 'SAMME.R', 'base_estimator': Dec...                            0.202235                                          0.038051  (DecisionTreeClassifier(class_weight=None, cri...\n",
       "0                   XGBClassifier       No Oversampling       No Scaling            22                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_score': 0.5, 'eval_metric': 'error', 'l...                            0.175625                                          0.108912  XGBClassifier(base_score=0.5, booster='gbtree'...\n",
       "10             LogisticRegression       No Oversampling       No Scaling            17                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'C': 1.8000000000000003, 'fit_intercept': Tru...                            0.159659                                          0.094489  LogisticRegression(C=1.8000000000000003, class...\n",
       "11              BaggingClassifier       No Oversampling       No Scaling            14                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'base_estimator': LogisticRegression(C=1.0, c...                            0.138371                                          0.091208  (LogisticRegression(C=1.0, class_weight=None, ...\n",
       "9      GradientBoostingClassifier       No Oversampling       No Scaling            15                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...          {'loss': 'deviance', 'n_estimators': 200}                            0.130389                                          0.084181  ([DecisionTreeRegressor(criterion='friedman_ms...\n",
       "8            ExtraTreesClassifier       No Oversampling       No Scaling            10                         RFE  [trustLevel, totalScanTimeInSeconds, lineItemV...         {'criterion': 'gini', 'n_estimators': 190}                            0.090474                                          0.090753  (ExtraTreeClassifier(class_weight=None, criter...\n",
       "5          RandomForestClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...       {'criterion': 'entropy', 'n_estimators': 40}                            0.082491                                          0.097579  (DecisionTreeClassifier(class_weight=None, cri...\n",
       "7          DecisionTreeClassifier       No Oversampling       No Scaling            13                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'criterion': 'gini', 'max_depth': 29, 'min_sa...                            0.034593                                          0.126854  DecisionTreeClassifier(class_weight=None, crit...\n",
       "12                  MLPClassifier       No Oversampling       No Scaling            19                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'activation': 'logistic', 'learning_rate': 'i...                            0.023949                                          0.108119  MLPClassifier(activation='logistic', alpha=0.0...\n",
       "6             ExtraTreeClassifier       No Oversampling       No Scaling            18                 SelectKBest  [trustLevel, totalScanTimeInSeconds, lineItemV...  {'criterion': 'entropy', 'max_depth': 13, 'min...                           -0.026610                                          0.090638  ExtraTreeClassifier(class_weight=None, criteri...\n",
       "14                            SVC       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                         {'C': 0.1}                           -0.162320                                          0.181452  SVC(C=0.1, cache_size=200, class_weight=None, ...\n",
       "15           KNeighborsClassifier       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...  {'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}                           -0.180947                                          0.210302  KNeighborsClassifier(algorithm='auto', leaf_si...\n",
       "3                      Perceptron       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...  {'alpha': 0.0005, 'fit_intercept': False, 'max...                           -0.207557                                          0.130813  Perceptron(alpha=0.0005, class_weight=None, ea...\n",
       "4   QuadraticDiscriminantAnalysis       No Oversampling       No Scaling             2                 SelectKBest                          [pca_axis_2, tsne_axis_2]                                 {'reg_param': 0.8}                           -0.220862                                          0.102408  QuadraticDiscriminantAnalysis(priors=None, reg...\n",
       "1                     BernoulliNB       No Oversampling       No Scaling             1                 SelectKBest                                      [tsne_axis_2]                      {'alpha': 0, 'binarize': 0.0}                           -0.276743                                          0.011709  BernoulliNB(alpha=0, binarize=0.0, class_prior...\n",
       "2                      GaussianNB       No Oversampling       No Scaling             4                 SelectKBest  [trustLevel, scannedLineItems, pca_axis_2, tsn...                                                 {}                           -0.968600                                          0.301185       GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table_aggregated = result_table_aggregated.sort_values(by = \"Monetary Value Per Instance - Mean\", ascending = False)\n",
    "\n",
    "result_table_aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "['totalScanTimeInSeconds' 'lineItemVoids' 'scansWithoutRegistration'\n",
      " 'scannedLineItems' 'scansWithoutRegistrationPerScannedLineItem'\n",
      " 'quantityModificationsPerSecond' 'secondsPerEuro'\n",
      " 'quantityModificationsPerEuro' 'pca_axis_2' 'tsne_axis_1' 'tsne_axis_2']\n",
      "{'C': 1.8000000000000003, 'fit_intercept': False, 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "model1 = result_table_aggregated.loc[result_table_aggregated[\"Monetary Value Per Instance - Mean\"].argmax()][\"Raw Model\"]\n",
    "model1_features = result_table_aggregated.loc[result_table_aggregated[\"Monetary Value Per Instance - Mean\"].argmax()][\"Features\"]\n",
    "model1_parameters = result_table.loc[result_table_aggregated[\"Monetary Value Per Instance - Mean\"].argmax()][\"Optimal Parameters\"]\n",
    "\n",
    "print(model1)\n",
    "print(model1_features)\n",
    "print(model1_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, eval_metric='error', gamma=0, learning_rate=0.3,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=nan,\n",
      "       n_estimators=100, n_jobs=1, nthread=None,\n",
      "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
      "       subsample=1)\n",
      "['trustLevel' 'totalScanTimeInSeconds' 'lineItemVoids'\n",
      " 'scansWithoutRegistration' 'scannedLineItemsPerSecond' 'valuePerSecond'\n",
      " 'lineItemVoidsPerPosition' 'scannedLineItems' 'pricePerScannedLineItem'\n",
      " 'scansWithoutRegistrationPerScannedLineItem'\n",
      " 'quantityModificationsPerScannedLineItem' 'lineItemVoidsPerSecond'\n",
      " 'scansWithoutRegistrationPerSecond' 'quantityModificationsPerSecond'\n",
      " 'secondsPerEuro' 'lineItemVoidsPerEuro' 'scansWithoutRegistrationPerEuro'\n",
      " 'quantityModificationsPerEuro' 'pca_axis_1' 'pca_axis_2' 'tsne_axis_1'\n",
      " 'tsne_axis_2']\n",
      "{'base_score': 0.5, 'eval_metric': 'error', 'learning_rate': 0.3, 'n_estimators': 100, 'objective': 'binary:logistic'}\n"
     ]
    }
   ],
   "source": [
    "model2 = result_table_aggregated[1:].loc[result_table_aggregated[1:][\"Monetary Value Per Instance - Mean\"].argmax()][\"Raw Model\"]\n",
    "model2_features = result_table_aggregated[1:].loc[result_table_aggregated[1:][\"Monetary Value Per Instance - Mean\"].argmax()][\"Features\"]\n",
    "model2_parameters = result_table_aggregated[1:].loc[result_table_aggregated[1:][\"Monetary Value Per Instance - Mean\"].argmax()][\"Optimal Parameters\"]\n",
    "\n",
    "print(model2)\n",
    "print(model2_features)\n",
    "print(model2_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.8000000000000003, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='newton-cg', tol=0.0001, verbose=0, warm_start=False)\n",
      "['trustLevel' 'totalScanTimeInSeconds' 'lineItemVoids'\n",
      " 'scansWithoutRegistration' 'scannedLineItemsPerSecond' 'valuePerSecond'\n",
      " 'lineItemVoidsPerPosition' 'scannedLineItems' 'pricePerScannedLineItem'\n",
      " 'scansWithoutRegistrationPerScannedLineItem'\n",
      " 'quantityModificationsPerScannedLineItem' 'lineItemVoidsPerSecond'\n",
      " 'quantityModificationsPerSecond' 'pca_axis_1' 'pca_axis_2' 'tsne_axis_1'\n",
      " 'tsne_axis_2']\n",
      "{'C': 1.8000000000000003, 'fit_intercept': True, 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "model3 = result_table_aggregated[2:].loc[result_table_aggregated[2:][\"Monetary Value Per Instance - Mean\"].argmax()][\"Raw Model\"]\n",
    "model3_features = result_table_aggregated[2:].loc[result_table_aggregated[2:][\"Monetary Value Per Instance - Mean\"].argmax()][\"Features\"]\n",
    "model3_parameters = result_table_aggregated[2:].loc[result_table_aggregated[2:][\"Monetary Value Per Instance - Mean\"].argmax()][\"Optimal Parameters\"]\n",
    "\n",
    "print(model3)\n",
    "print(model3_features)\n",
    "print(model3_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=1.0, n_estimators=5, n_jobs=None, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)\n",
      "['trustLevel' 'totalScanTimeInSeconds' 'lineItemVoids'\n",
      " 'scansWithoutRegistration' 'valuePerSecond' 'lineItemVoidsPerPosition'\n",
      " 'scannedLineItems' 'pricePerScannedLineItem'\n",
      " 'scansWithoutRegistrationPerScannedLineItem'\n",
      " 'quantityModificationsPerScannedLineItem'\n",
      " 'quantityModificationsPerSecond' 'pca_axis_1' 'pca_axis_2' 'tsne_axis_2']\n",
      "{'base_estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), 'n_estimators': 5}\n"
     ]
    }
   ],
   "source": [
    "model4 = result_table_aggregated[3:].loc[result_table_aggregated[3:][\"Monetary Value Per Instance - Mean\"].argmax()][\"Raw Model\"]\n",
    "model4_features = result_table_aggregated[3:].loc[result_table_aggregated[3:][\"Monetary Value Per Instance - Mean\"].argmax()][\"Features\"]\n",
    "model4_parameters = result_table_aggregated[3:].loc[result_table_aggregated[3:][\"Monetary Value Per Instance - Mean\"].argmax()][\"Optimal Parameters\"]\n",
    "\n",
    "print(model4)\n",
    "print(model4_features)\n",
    "print(model4_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifth Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "['trustLevel' 'totalScanTimeInSeconds' 'lineItemVoids'\n",
      " 'scansWithoutRegistration' 'valuePerSecond' 'lineItemVoidsPerPosition'\n",
      " 'scannedLineItems' 'pricePerScannedLineItem'\n",
      " 'scansWithoutRegistrationPerScannedLineItem'\n",
      " 'quantityModificationsPerScannedLineItem'\n",
      " 'quantityModificationsPerSecond' 'pca_axis_1' 'pca_axis_2' 'tsne_axis_1'\n",
      " 'tsne_axis_2']\n",
      "{'loss': 'deviance', 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "model5 = result_table_aggregated[4:].loc[result_table_aggregated[4:][\"Monetary Value Per Instance - Mean\"].argmax()][\"Raw Model\"]\n",
    "model5_features = result_table_aggregated[4:].loc[result_table_aggregated[4:][\"Monetary Value Per Instance - Mean\"].argmax()][\"Features\"]\n",
    "model5_parameters = result_table_aggregated[4:].loc[result_table_aggregated[4:][\"Monetary Value Per Instance - Mean\"].argmax()][\"Optimal Parameters\"]\n",
    "\n",
    "print(model5)\n",
    "print(model5_features)\n",
    "print(model5_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicting the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_train_predictions = model1.predict(train[model1_features])\n",
    "model_2_train_predictions = model2.predict(train[model2_features])\n",
    "model_3_train_predictions = model3.predict(train[model3_features])\n",
    "model_4_train_predictions = model4.predict(train[model4_features])\n",
    "model_5_train_predictions = model5.predict(train[model5_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "train_predictions = pd.DataFrame({ 'Model_1_Prediction': model_1_train_predictions,\n",
    "                                   'Model_2_Prediction': model_2_train_predictions,\n",
    "                                   'Model_3_Prediction': model_3_train_predictions,\n",
    "                                   'Model_4_Prediction': model_4_train_predictions,\n",
    "                                   'Model_5_Prediction': model_5_train_predictions\n",
    "                                 })\n",
    "\n",
    "train_predictions['Voting_Prediction'] = train_predictions.apply(statistics.mode, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_1_Prediction</th>\n",
       "      <th>Model_2_Prediction</th>\n",
       "      <th>Model_3_Prediction</th>\n",
       "      <th>Model_4_Prediction</th>\n",
       "      <th>Model_5_Prediction</th>\n",
       "      <th>Voting_Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1658</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1672</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1840</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1866</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1879 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model_1_Prediction  Model_2_Prediction  Model_3_Prediction  Model_4_Prediction  Model_5_Prediction  Voting_Prediction\n",
       "0                      0                   0                   0                   0                   0                  0\n",
       "1                      0                   0                   0                   0                   0                  0\n",
       "2                      0                   0                   0                   0                   0                  0\n",
       "3                      0                   0                   0                   0                   0                  0\n",
       "4                      0                   0                   0                   0                   0                  0\n",
       "5                      1                   1                   1                   1                   1                  1\n",
       "6                      0                   0                   0                   0                   0                  0\n",
       "7                      0                   0                   0                   0                   0                  0\n",
       "8                      0                   0                   0                   0                   0                  0\n",
       "9                      0                   0                   0                   0                   0                  0\n",
       "10                     0                   0                   0                   0                   0                  0\n",
       "11                     0                   0                   0                   0                   0                  0\n",
       "12                     0                   0                   0                   0                   0                  0\n",
       "13                     0                   0                   0                   0                   0                  0\n",
       "14                     0                   0                   0                   0                   0                  0\n",
       "15                     0                   0                   0                   0                   0                  0\n",
       "16                     0                   0                   0                   0                   0                  0\n",
       "17                     0                   0                   0                   0                   0                  0\n",
       "18                     0                   0                   0                   0                   0                  0\n",
       "19                     0                   0                   0                   0                   0                  0\n",
       "20                     0                   0                   0                   0                   0                  0\n",
       "21                     0                   0                   0                   0                   0                  0\n",
       "22                     0                   0                   0                   0                   0                  0\n",
       "23                     0                   0                   0                   0                   0                  0\n",
       "24                     0                   0                   0                   0                   0                  0\n",
       "25                     0                   0                   0                   0                   0                  0\n",
       "26                     0                   0                   0                   0                   0                  0\n",
       "27                     0                   0                   0                   0                   0                  0\n",
       "28                     0                   0                   0                   0                   0                  0\n",
       "29                     0                   0                   0                   0                   0                  0\n",
       "30                     0                   0                   0                   0                   0                  0\n",
       "31                     0                   0                   0                   0                   0                  0\n",
       "32                     0                   0                   0                   0                   0                  0\n",
       "33                     0                   0                   0                   0                   0                  0\n",
       "34                     0                   0                   0                   0                   0                  0\n",
       "35                     0                   0                   0                   0                   0                  0\n",
       "36                     0                   0                   0                   0                   0                  0\n",
       "37                     1                   1                   1                   0                   1                  1\n",
       "38                     0                   0                   0                   0                   0                  0\n",
       "39                     0                   0                   0                   0                   0                  0\n",
       "40                     0                   0                   0                   0                   0                  0\n",
       "41                     0                   0                   0                   0                   0                  0\n",
       "42                     0                   0                   0                   0                   0                  0\n",
       "43                     0                   0                   0                   0                   0                  0\n",
       "44                     0                   0                   0                   0                   0                  0\n",
       "45                     0                   0                   0                   0                   0                  0\n",
       "46                     0                   0                   0                   0                   0                  0\n",
       "47                     0                   0                   0                   0                   0                  0\n",
       "48                     0                   0                   0                   0                   0                  0\n",
       "49                     0                   0                   0                   0                   0                  0\n",
       "50                     0                   0                   0                   0                   0                  0\n",
       "51                     0                   0                   0                   0                   0                  0\n",
       "52                     0                   0                   0                   0                   0                  0\n",
       "53                     1                   1                   1                   1                   1                  1\n",
       "54                     0                   0                   0                   0                   0                  0\n",
       "55                     0                   0                   0                   0                   0                  0\n",
       "56                     0                   0                   0                   0                   0                  0\n",
       "57                     0                   0                   0                   0                   0                  0\n",
       "58                     0                   0                   0                   0                   0                  0\n",
       "59                     0                   0                   0                   0                   0                  0\n",
       "60                     0                   0                   0                   0                   0                  0\n",
       "61                     0                   0                   0                   0                   0                  0\n",
       "62                     0                   0                   0                   0                   0                  0\n",
       "63                     0                   0                   0                   0                   0                  0\n",
       "64                     0                   0                   0                   0                   0                  0\n",
       "65                     0                   0                   0                   0                   0                  0\n",
       "66                     0                   0                   0                   0                   0                  0\n",
       "67                     0                   0                   0                   0                   0                  0\n",
       "68                     0                   0                   0                   0                   0                  0\n",
       "69                     0                   0                   0                   0                   0                  0\n",
       "70                     0                   0                   0                   0                   0                  0\n",
       "71                     0                   0                   0                   0                   0                  0\n",
       "72                     0                   0                   0                   0                   0                  0\n",
       "73                     0                   0                   0                   0                   0                  0\n",
       "74                     0                   0                   0                   0                   0                  0\n",
       "75                     0                   0                   0                   0                   0                  0\n",
       "76                     0                   0                   0                   0                   0                  0\n",
       "77                     0                   0                   0                   0                   0                  0\n",
       "78                     0                   0                   0                   0                   0                  0\n",
       "79                     0                   0                   0                   0                   0                  0\n",
       "80                     0                   0                   0                   0                   0                  0\n",
       "81                     0                   0                   0                   0                   0                  0\n",
       "82                     0                   0                   0                   0                   0                  0\n",
       "83                     0                   0                   0                   0                   0                  0\n",
       "84                     0                   0                   0                   0                   0                  0\n",
       "85                     0                   0                   0                   0                   0                  0\n",
       "86                     0                   0                   0                   0                   0                  0\n",
       "87                     0                   0                   0                   0                   0                  0\n",
       "88                     0                   0                   0                   0                   0                  0\n",
       "89                     0                   0                   0                   0                   0                  0\n",
       "90                     0                   0                   0                   0                   0                  0\n",
       "91                     1                   1                   1                   1                   1                  1\n",
       "92                     0                   0                   0                   0                   0                  0\n",
       "93                     0                   0                   0                   0                   0                  0\n",
       "94                     0                   0                   0                   0                   0                  0\n",
       "95                     0                   0                   0                   0                   0                  0\n",
       "96                     0                   0                   0                   0                   0                  0\n",
       "97                     1                   1                   1                   0                   1                  1\n",
       "98                     0                   0                   0                   0                   0                  0\n",
       "99                     0                   0                   0                   0                   0                  0\n",
       "100                    0                   0                   0                   0                   0                  0\n",
       "101                    0                   0                   0                   0                   0                  0\n",
       "102                    0                   0                   0                   1                   0                  0\n",
       "103                    0                   0                   0                   0                   0                  0\n",
       "104                    0                   0                   0                   0                   0                  0\n",
       "105                    0                   0                   0                   0                   0                  0\n",
       "106                    0                   0                   0                   0                   0                  0\n",
       "107                    0                   0                   0                   0                   0                  0\n",
       "108                    0                   0                   0                   0                   0                  0\n",
       "109                    0                   0                   0                   0                   0                  0\n",
       "110                    0                   0                   0                   0                   0                  0\n",
       "111                    0                   0                   0                   0                   0                  0\n",
       "112                    0                   0                   0                   0                   0                  0\n",
       "113                    0                   0                   0                   0                   0                  0\n",
       "114                    0                   0                   0                   0                   0                  0\n",
       "115                    0                   0                   0                   0                   0                  0\n",
       "116                    0                   0                   0                   0                   0                  0\n",
       "117                    0                   0                   0                   0                   0                  0\n",
       "118                    0                   0                   0                   0                   0                  0\n",
       "119                    0                   0                   0                   0                   0                  0\n",
       "120                    0                   0                   0                   0                   0                  0\n",
       "121                    0                   0                   0                   0                   0                  0\n",
       "122                    0                   0                   0                   0                   0                  0\n",
       "123                    0                   0                   0                   0                   0                  0\n",
       "124                    0                   0                   0                   0                   0                  0\n",
       "125                    0                   0                   0                   0                   0                  0\n",
       "126                    1                   1                   0                   1                   1                  1\n",
       "127                    0                   0                   0                   0                   0                  0\n",
       "128                    0                   0                   0                   0                   0                  0\n",
       "129                    0                   0                   0                   0                   0                  0\n",
       "130                    0                   0                   0                   0                   0                  0\n",
       "131                    0                   0                   0                   0                   0                  0\n",
       "132                    0                   0                   0                   0                   0                  0\n",
       "133                    0                   0                   0                   0                   0                  0\n",
       "134                    0                   0                   0                   0                   0                  0\n",
       "135                    0                   0                   0                   0                   0                  0\n",
       "136                    0                   0                   0                   0                   0                  0\n",
       "137                    0                   0                   0                   0                   0                  0\n",
       "138                    0                   0                   0                   0                   0                  0\n",
       "139                    0                   0                   0                   0                   0                  0\n",
       "140                    0                   0                   0                   0                   0                  0\n",
       "141                    0                   0                   0                   0                   0                  0\n",
       "142                    0                   0                   0                   0                   0                  0\n",
       "143                    1                   1                   1                   1                   1                  1\n",
       "144                    0                   0                   0                   0                   0                  0\n",
       "145                    0                   0                   0                   0                   0                  0\n",
       "146                    0                   0                   0                   0                   0                  0\n",
       "147                    0                   0                   0                   0                   0                  0\n",
       "148                    0                   0                   0                   0                   0                  0\n",
       "149                    0                   0                   0                   0                   0                  0\n",
       "150                    0                   0                   0                   0                   0                  0\n",
       "151                    1                   1                   1                   1                   1                  1\n",
       "152                    0                   0                   0                   0                   0                  0\n",
       "153                    0                   0                   0                   0                   0                  0\n",
       "154                    0                   0                   0                   0                   0                  0\n",
       "155                    0                   0                   0                   0                   0                  0\n",
       "156                    0                   0                   0                   0                   0                  0\n",
       "157                    0                   0                   0                   0                   0                  0\n",
       "158                    0                   0                   0                   0                   0                  0\n",
       "159                    0                   0                   0                   0                   0                  0\n",
       "160                    0                   0                   0                   0                   0                  0\n",
       "161                    0                   0                   0                   0                   0                  0\n",
       "162                    0                   0                   0                   0                   0                  0\n",
       "163                    0                   0                   0                   0                   0                  0\n",
       "164                    0                   0                   0                   0                   0                  0\n",
       "165                    0                   0                   0                   0                   0                  0\n",
       "166                    0                   0                   0                   0                   0                  0\n",
       "167                    0                   0                   0                   0                   0                  0\n",
       "168                    0                   0                   0                   0                   0                  0\n",
       "169                    0                   0                   0                   0                   0                  0\n",
       "170                    0                   0                   0                   0                   0                  0\n",
       "171                    0                   0                   0                   0                   0                  0\n",
       "172                    0                   0                   0                   0                   0                  0\n",
       "173                    0                   0                   0                   0                   0                  0\n",
       "174                    0                   0                   0                   0                   0                  0\n",
       "175                    0                   0                   0                   0                   0                  0\n",
       "176                    0                   0                   0                   0                   0                  0\n",
       "177                    1                   1                   1                   1                   1                  1\n",
       "178                    1                   1                   1                   1                   1                  1\n",
       "179                    0                   0                   0                   0                   0                  0\n",
       "180                    0                   0                   0                   0                   0                  0\n",
       "181                    0                   0                   0                   0                   0                  0\n",
       "182                    0                   0                   0                   0                   0                  0\n",
       "183                    0                   0                   0                   0                   0                  0\n",
       "184                    0                   0                   0                   0                   0                  0\n",
       "185                    0                   0                   0                   0                   0                  0\n",
       "186                    0                   0                   0                   0                   0                  0\n",
       "187                    0                   0                   0                   0                   0                  0\n",
       "188                    0                   0                   0                   0                   0                  0\n",
       "189                    0                   0                   0                   0                   0                  0\n",
       "190                    0                   0                   0                   0                   0                  0\n",
       "191                    0                   0                   0                   0                   0                  0\n",
       "192                    0                   0                   0                   0                   0                  0\n",
       "193                    0                   0                   0                   0                   0                  0\n",
       "194                    0                   0                   0                   0                   0                  0\n",
       "195                    0                   0                   0                   0                   0                  0\n",
       "196                    0                   0                   0                   0                   0                  0\n",
       "197                    0                   0                   0                   0                   0                  0\n",
       "198                    0                   0                   0                   0                   0                  0\n",
       "199                    0                   0                   0                   0                   0                  0\n",
       "200                    0                   0                   0                   0                   0                  0\n",
       "201                    0                   0                   0                   0                   0                  0\n",
       "202                    0                   0                   0                   0                   0                  0\n",
       "203                    0                   0                   0                   0                   0                  0\n",
       "204                    0                   0                   0                   0                   0                  0\n",
       "205                    1                   1                   1                   1                   1                  1\n",
       "206                    0                   0                   0                   0                   0                  0\n",
       "207                    0                   0                   0                   0                   0                  0\n",
       "208                    0                   0                   0                   0                   0                  0\n",
       "209                    0                   0                   0                   0                   0                  0\n",
       "210                    0                   0                   0                   0                   0                  0\n",
       "211                    0                   0                   0                   0                   0                  0\n",
       "212                    0                   0                   0                   0                   0                  0\n",
       "213                    0                   0                   0                   0                   0                  0\n",
       "214                    0                   0                   0                   0                   0                  0\n",
       "215                    0                   0                   0                   0                   0                  0\n",
       "216                    1                   1                   1                   1                   1                  1\n",
       "217                    0                   0                   0                   0                   0                  0\n",
       "218                    0                   0                   0                   0                   0                  0\n",
       "219                    0                   0                   0                   0                   0                  0\n",
       "220                    0                   0                   0                   0                   0                  0\n",
       "221                    0                   0                   0                   0                   0                  0\n",
       "222                    0                   0                   0                   0                   0                  0\n",
       "223                    0                   0                   0                   0                   0                  0\n",
       "224                    0                   0                   0                   0                   0                  0\n",
       "225                    1                   1                   0                   1                   1                  1\n",
       "226                    0                   0                   0                   0                   0                  0\n",
       "227                    1                   1                   1                   1                   1                  1\n",
       "228                    0                   0                   0                   0                   0                  0\n",
       "229                    0                   0                   0                   0                   0                  0\n",
       "230                    1                   1                   1                   1                   1                  1\n",
       "231                    0                   0                   0                   0                   0                  0\n",
       "232                    0                   0                   0                   0                   0                  0\n",
       "233                    0                   0                   0                   0                   0                  0\n",
       "234                    0                   0                   0                   0                   0                  0\n",
       "235                    0                   0                   0                   0                   0                  0\n",
       "236                    0                   0                   0                   0                   0                  0\n",
       "237                    0                   0                   0                   0                   0                  0\n",
       "238                    0                   0                   0                   0                   0                  0\n",
       "239                    0                   0                   0                   0                   0                  0\n",
       "240                    0                   0                   0                   0                   0                  0\n",
       "241                    0                   0                   0                   0                   0                  0\n",
       "242                    0                   0                   0                   0                   0                  0\n",
       "243                    0                   0                   0                   0                   0                  0\n",
       "244                    0                   0                   0                   0                   0                  0\n",
       "245                    0                   0                   0                   0                   0                  0\n",
       "246                    0                   0                   0                   0                   0                  0\n",
       "247                    0                   0                   0                   0                   0                  0\n",
       "248                    0                   0                   0                   0                   0                  0\n",
       "249                    0                   0                   0                   0                   0                  0\n",
       "...                  ...                 ...                 ...                 ...                 ...                ...\n",
       "1629                   0                   0                   0                   0                   0                  0\n",
       "1630                   0                   0                   0                   0                   0                  0\n",
       "1631                   0                   0                   0                   0                   0                  0\n",
       "1632                   0                   0                   0                   0                   0                  0\n",
       "1633                   0                   0                   0                   0                   0                  0\n",
       "1634                   0                   0                   0                   0                   0                  0\n",
       "1635                   0                   0                   0                   0                   0                  0\n",
       "1636                   0                   0                   0                   0                   0                  0\n",
       "1637                   0                   0                   0                   0                   0                  0\n",
       "1638                   0                   0                   0                   0                   0                  0\n",
       "1639                   0                   0                   0                   0                   0                  0\n",
       "1640                   0                   0                   0                   0                   0                  0\n",
       "1641                   0                   0                   0                   0                   0                  0\n",
       "1642                   0                   0                   0                   0                   0                  0\n",
       "1643                   0                   0                   0                   0                   0                  0\n",
       "1644                   0                   0                   0                   0                   0                  0\n",
       "1645                   0                   0                   0                   0                   0                  0\n",
       "1646                   0                   0                   0                   0                   0                  0\n",
       "1647                   0                   0                   0                   0                   0                  0\n",
       "1648                   0                   0                   0                   0                   0                  0\n",
       "1649                   0                   0                   0                   0                   0                  0\n",
       "1650                   0                   0                   0                   0                   0                  0\n",
       "1651                   0                   0                   0                   0                   0                  0\n",
       "1652                   0                   0                   0                   0                   0                  0\n",
       "1653                   0                   0                   0                   0                   0                  0\n",
       "1654                   0                   0                   0                   0                   0                  0\n",
       "1655                   0                   0                   0                   0                   0                  0\n",
       "1656                   0                   0                   0                   0                   0                  0\n",
       "1657                   0                   0                   0                   0                   0                  0\n",
       "1658                   0                   0                   0                   0                   0                  0\n",
       "1659                   0                   0                   0                   0                   0                  0\n",
       "1660                   0                   0                   0                   0                   0                  0\n",
       "1661                   0                   0                   0                   0                   0                  0\n",
       "1662                   0                   0                   0                   0                   0                  0\n",
       "1663                   0                   0                   0                   0                   0                  0\n",
       "1664                   0                   0                   0                   0                   0                  0\n",
       "1665                   0                   0                   0                   0                   0                  0\n",
       "1666                   0                   0                   0                   0                   0                  0\n",
       "1667                   0                   0                   0                   0                   0                  0\n",
       "1668                   0                   0                   0                   0                   0                  0\n",
       "1669                   0                   0                   0                   0                   0                  0\n",
       "1670                   0                   0                   0                   0                   0                  0\n",
       "1671                   0                   0                   0                   0                   0                  0\n",
       "1672                   0                   0                   0                   0                   0                  0\n",
       "1673                   0                   0                   0                   0                   0                  0\n",
       "1674                   0                   0                   0                   0                   0                  0\n",
       "1675                   0                   0                   0                   0                   0                  0\n",
       "1676                   0                   0                   0                   0                   0                  0\n",
       "1677                   0                   0                   0                   0                   0                  0\n",
       "1678                   0                   0                   0                   0                   0                  0\n",
       "1679                   0                   0                   0                   0                   0                  0\n",
       "1680                   0                   0                   0                   0                   0                  0\n",
       "1681                   0                   0                   0                   0                   0                  0\n",
       "1682                   0                   0                   0                   0                   0                  0\n",
       "1683                   0                   0                   0                   0                   0                  0\n",
       "1684                   0                   0                   0                   0                   0                  0\n",
       "1685                   0                   0                   0                   0                   0                  0\n",
       "1686                   0                   0                   0                   0                   0                  0\n",
       "1687                   0                   0                   0                   0                   0                  0\n",
       "1688                   0                   0                   0                   0                   0                  0\n",
       "1689                   0                   0                   0                   0                   0                  0\n",
       "1690                   0                   0                   0                   0                   0                  0\n",
       "1691                   0                   0                   0                   0                   0                  0\n",
       "1692                   0                   0                   0                   0                   0                  0\n",
       "1693                   0                   0                   0                   0                   0                  0\n",
       "1694                   0                   0                   0                   0                   0                  0\n",
       "1695                   0                   0                   0                   0                   0                  0\n",
       "1696                   0                   0                   0                   0                   0                  0\n",
       "1697                   1                   1                   1                   1                   1                  1\n",
       "1698                   0                   0                   0                   0                   0                  0\n",
       "1699                   0                   0                   0                   0                   0                  0\n",
       "1700                   0                   0                   0                   0                   0                  0\n",
       "1701                   0                   0                   0                   0                   0                  0\n",
       "1702                   0                   0                   0                   0                   0                  0\n",
       "1703                   0                   0                   0                   0                   0                  0\n",
       "1704                   0                   0                   0                   0                   0                  0\n",
       "1705                   0                   0                   0                   0                   0                  0\n",
       "1706                   0                   0                   0                   0                   0                  0\n",
       "1707                   0                   0                   0                   0                   0                  0\n",
       "1708                   0                   0                   0                   0                   0                  0\n",
       "1709                   0                   0                   0                   0                   0                  0\n",
       "1710                   0                   0                   0                   0                   0                  0\n",
       "1711                   0                   0                   0                   0                   0                  0\n",
       "1712                   0                   0                   0                   0                   0                  0\n",
       "1713                   0                   0                   0                   0                   0                  0\n",
       "1714                   0                   0                   0                   0                   0                  0\n",
       "1715                   0                   0                   0                   0                   0                  0\n",
       "1716                   0                   0                   0                   0                   0                  0\n",
       "1717                   0                   0                   0                   0                   0                  0\n",
       "1718                   0                   0                   0                   0                   0                  0\n",
       "1719                   0                   0                   0                   0                   0                  0\n",
       "1720                   0                   0                   0                   0                   0                  0\n",
       "1721                   0                   0                   0                   0                   0                  0\n",
       "1722                   0                   0                   0                   0                   0                  0\n",
       "1723                   0                   0                   0                   0                   0                  0\n",
       "1724                   0                   0                   0                   0                   0                  0\n",
       "1725                   0                   0                   0                   0                   0                  0\n",
       "1726                   1                   1                   1                   1                   1                  1\n",
       "1727                   0                   0                   0                   0                   0                  0\n",
       "1728                   0                   0                   0                   0                   0                  0\n",
       "1729                   0                   0                   0                   0                   0                  0\n",
       "1730                   0                   0                   0                   0                   0                  0\n",
       "1731                   1                   1                   1                   1                   1                  1\n",
       "1732                   0                   0                   0                   0                   0                  0\n",
       "1733                   0                   0                   0                   0                   0                  0\n",
       "1734                   0                   0                   0                   0                   0                  0\n",
       "1735                   0                   0                   0                   0                   0                  0\n",
       "1736                   0                   0                   0                   0                   0                  0\n",
       "1737                   0                   0                   0                   0                   0                  0\n",
       "1738                   0                   0                   0                   0                   0                  0\n",
       "1739                   0                   0                   0                   0                   0                  0\n",
       "1740                   0                   0                   0                   0                   0                  0\n",
       "1741                   0                   0                   0                   0                   0                  0\n",
       "1742                   0                   0                   0                   0                   0                  0\n",
       "1743                   0                   0                   0                   0                   0                  0\n",
       "1744                   0                   0                   0                   0                   0                  0\n",
       "1745                   0                   0                   0                   0                   0                  0\n",
       "1746                   0                   0                   0                   0                   0                  0\n",
       "1747                   0                   0                   0                   0                   0                  0\n",
       "1748                   0                   0                   0                   0                   0                  0\n",
       "1749                   0                   0                   0                   0                   0                  0\n",
       "1750                   0                   0                   0                   0                   0                  0\n",
       "1751                   0                   0                   0                   0                   0                  0\n",
       "1752                   0                   0                   0                   0                   0                  0\n",
       "1753                   0                   0                   0                   0                   0                  0\n",
       "1754                   1                   1                   1                   1                   1                  1\n",
       "1755                   0                   0                   0                   0                   0                  0\n",
       "1756                   0                   0                   0                   0                   0                  0\n",
       "1757                   0                   0                   0                   0                   0                  0\n",
       "1758                   0                   0                   0                   0                   0                  0\n",
       "1759                   0                   0                   0                   0                   0                  0\n",
       "1760                   0                   0                   0                   0                   0                  0\n",
       "1761                   0                   0                   0                   0                   0                  0\n",
       "1762                   0                   0                   0                   0                   0                  0\n",
       "1763                   0                   0                   0                   0                   0                  0\n",
       "1764                   0                   0                   0                   0                   0                  0\n",
       "1765                   0                   0                   0                   0                   0                  0\n",
       "1766                   0                   0                   0                   0                   0                  0\n",
       "1767                   0                   0                   0                   0                   0                  0\n",
       "1768                   0                   0                   0                   0                   0                  0\n",
       "1769                   1                   1                   1                   1                   1                  1\n",
       "1770                   0                   0                   0                   0                   0                  0\n",
       "1771                   0                   0                   0                   0                   0                  0\n",
       "1772                   0                   0                   0                   0                   0                  0\n",
       "1773                   0                   0                   0                   0                   0                  0\n",
       "1774                   0                   0                   0                   0                   0                  0\n",
       "1775                   0                   0                   0                   0                   0                  0\n",
       "1776                   0                   0                   0                   0                   0                  0\n",
       "1777                   0                   0                   0                   0                   0                  0\n",
       "1778                   0                   0                   0                   0                   0                  0\n",
       "1779                   1                   1                   0                   1                   1                  1\n",
       "1780                   0                   0                   0                   0                   0                  0\n",
       "1781                   0                   0                   0                   0                   0                  0\n",
       "1782                   0                   0                   0                   0                   0                  0\n",
       "1783                   0                   0                   0                   0                   0                  0\n",
       "1784                   0                   0                   0                   0                   0                  0\n",
       "1785                   0                   0                   0                   0                   0                  0\n",
       "1786                   0                   0                   0                   0                   0                  0\n",
       "1787                   0                   0                   0                   0                   0                  0\n",
       "1788                   0                   0                   0                   0                   0                  0\n",
       "1789                   1                   1                   1                   1                   1                  1\n",
       "1790                   0                   0                   0                   0                   0                  0\n",
       "1791                   0                   0                   0                   0                   0                  0\n",
       "1792                   0                   0                   0                   0                   0                  0\n",
       "1793                   0                   0                   0                   0                   0                  0\n",
       "1794                   0                   0                   0                   0                   0                  0\n",
       "1795                   0                   0                   0                   0                   0                  0\n",
       "1796                   0                   0                   0                   0                   0                  0\n",
       "1797                   0                   0                   0                   0                   0                  0\n",
       "1798                   0                   0                   0                   0                   0                  0\n",
       "1799                   0                   0                   0                   0                   0                  0\n",
       "1800                   1                   1                   1                   1                   1                  1\n",
       "1801                   0                   0                   0                   0                   0                  0\n",
       "1802                   0                   0                   0                   0                   0                  0\n",
       "1803                   1                   1                   1                   1                   1                  1\n",
       "1804                   0                   0                   0                   0                   0                  0\n",
       "1805                   0                   0                   0                   0                   0                  0\n",
       "1806                   0                   0                   0                   0                   0                  0\n",
       "1807                   0                   0                   0                   0                   0                  0\n",
       "1808                   0                   0                   0                   0                   0                  0\n",
       "1809                   0                   0                   0                   0                   0                  0\n",
       "1810                   0                   0                   0                   0                   0                  0\n",
       "1811                   1                   1                   1                   0                   1                  1\n",
       "1812                   0                   0                   0                   0                   0                  0\n",
       "1813                   0                   0                   0                   0                   0                  0\n",
       "1814                   0                   0                   0                   0                   0                  0\n",
       "1815                   0                   0                   0                   0                   0                  0\n",
       "1816                   0                   0                   0                   0                   0                  0\n",
       "1817                   1                   1                   1                   1                   1                  1\n",
       "1818                   0                   0                   0                   0                   0                  0\n",
       "1819                   0                   0                   0                   0                   0                  0\n",
       "1820                   0                   0                   0                   0                   0                  0\n",
       "1821                   0                   0                   0                   0                   0                  0\n",
       "1822                   1                   1                   1                   1                   1                  1\n",
       "1823                   0                   0                   0                   0                   0                  0\n",
       "1824                   0                   0                   0                   0                   0                  0\n",
       "1825                   0                   0                   0                   0                   0                  0\n",
       "1826                   0                   0                   0                   0                   0                  0\n",
       "1827                   0                   0                   0                   0                   0                  0\n",
       "1828                   0                   0                   0                   0                   0                  0\n",
       "1829                   0                   0                   0                   0                   0                  0\n",
       "1830                   0                   0                   0                   0                   0                  0\n",
       "1831                   0                   0                   0                   0                   0                  0\n",
       "1832                   0                   0                   0                   0                   0                  0\n",
       "1833                   0                   0                   0                   0                   0                  0\n",
       "1834                   0                   0                   0                   0                   0                  0\n",
       "1835                   0                   0                   0                   0                   0                  0\n",
       "1836                   0                   0                   0                   0                   0                  0\n",
       "1837                   0                   0                   0                   0                   0                  0\n",
       "1838                   1                   1                   1                   1                   1                  1\n",
       "1839                   0                   0                   0                   0                   0                  0\n",
       "1840                   0                   0                   0                   0                   0                  0\n",
       "1841                   0                   0                   0                   0                   0                  0\n",
       "1842                   0                   0                   0                   0                   0                  0\n",
       "1843                   0                   0                   0                   0                   0                  0\n",
       "1844                   0                   0                   0                   0                   0                  0\n",
       "1845                   0                   0                   0                   0                   0                  0\n",
       "1846                   0                   0                   0                   0                   0                  0\n",
       "1847                   0                   0                   0                   0                   0                  0\n",
       "1848                   0                   0                   0                   0                   0                  0\n",
       "1849                   0                   0                   0                   0                   0                  0\n",
       "1850                   0                   0                   0                   0                   0                  0\n",
       "1851                   0                   0                   0                   0                   0                  0\n",
       "1852                   1                   1                   1                   1                   1                  1\n",
       "1853                   0                   0                   0                   0                   0                  0\n",
       "1854                   0                   0                   0                   0                   0                  0\n",
       "1855                   0                   0                   0                   0                   0                  0\n",
       "1856                   0                   0                   0                   0                   0                  0\n",
       "1857                   1                   1                   1                   1                   1                  1\n",
       "1858                   0                   0                   0                   0                   0                  0\n",
       "1859                   0                   0                   0                   0                   0                  0\n",
       "1860                   0                   0                   0                   0                   0                  0\n",
       "1861                   0                   0                   0                   0                   0                  0\n",
       "1862                   0                   0                   0                   0                   0                  0\n",
       "1863                   0                   0                   0                   0                   0                  0\n",
       "1864                   0                   0                   0                   0                   0                  0\n",
       "1865                   0                   0                   0                   0                   0                  0\n",
       "1866                   0                   0                   0                   0                   0                  0\n",
       "1867                   0                   0                   0                   0                   0                  0\n",
       "1868                   0                   0                   0                   0                   0                  0\n",
       "1869                   0                   0                   0                   0                   0                  0\n",
       "1870                   0                   0                   0                   0                   0                  0\n",
       "1871                   0                   0                   0                   0                   0                  0\n",
       "1872                   0                   0                   0                   0                   0                  0\n",
       "1873                   0                   0                   0                   0                   0                  0\n",
       "1874                   0                   0                   0                   0                   0                  0\n",
       "1875                   1                   1                   0                   1                   1                  1\n",
       "1876                   0                   0                   0                   0                   0                  0\n",
       "1877                   0                   0                   0                   0                   0                  0\n",
       "1878                   0                   0                   0                   0                   0                  0\n",
       "\n",
       "[1879 rows x 6 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negative:  1775\n",
      "False positive:  0\n",
      "False negative:  0\n",
      "True positive:  104\n",
      "520 for 1879 instances in the test set\n",
      "0.2767429483767962 per instance in the test set\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_monetary_value(cm):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        print(\"True negative: \", tn)\n",
    "        print(\"False positive: \", fp)\n",
    "        print(\"False negative: \", fn)\n",
    "        print(\"True positive: \", tp)\n",
    "        score = (-25)*fp + (-5)*fn + 5*tp\n",
    "        print(str(score) + \" for \" + str(sum(sum(cm))) + \" instances in the test set\")\n",
    "        print(str(score/sum(sum(cm))) + \" per instance in the test set\")\n",
    "        return score\n",
    "    \n",
    "cm = confusion_matrix(train['fraud'], train_predictions['Voting_Prediction'].values)\n",
    "\n",
    "monetary_value = get_monetary_value(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_test_predictions = model1.predict(test[model1_features])\n",
    "model_2_test_predictions = model2.predict(test[model2_features])\n",
    "model_3_test_predictions = model3.predict(test[model3_features])\n",
    "model_4_test_predictions = model4.predict(test[model4_features])\n",
    "model_5_test_predictions = model5.predict(test[model5_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "test_predictions = pd.DataFrame({ 'Model_1_Prediction': model_1_test_predictions,\n",
    "                                  'Model_2_Prediction': model_2_test_predictions,\n",
    "                                  'Model_3_Prediction': model_3_test_predictions,\n",
    "                                  'Model_4_Prediction': model_4_test_predictions,\n",
    "                                  'Model_5_Prediction': model_5_test_predictions\n",
    "                                })\n",
    "\n",
    "test_predictions['Voting_Prediction'] = test_predictions.apply(statistics.mode, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_1_Prediction</th>\n",
       "      <th>Model_2_Prediction</th>\n",
       "      <th>Model_3_Prediction</th>\n",
       "      <th>Model_4_Prediction</th>\n",
       "      <th>Model_5_Prediction</th>\n",
       "      <th>Voting_Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497871</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497872</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497873</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497874</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497875</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497876</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497877</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497878</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497879</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497880</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497881</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497882</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497883</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497884</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497885</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497886</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497887</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497888</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497889</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497890</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497891</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497892</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497893</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497894</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497895</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497896</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497897</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497898</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497899</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497900</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497901</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497902</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497903</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497904</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497905</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497906</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497907</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497908</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497909</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497910</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497911</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497912</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497914</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497915</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497916</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497917</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497918</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497919</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497920</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497921</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497922</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497923</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497924</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497925</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497926</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497927</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497928</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497929</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497930</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497931</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497932</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497933</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497934</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497935</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497936</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497937</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497938</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497939</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497940</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497941</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497942</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497943</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497944</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497945</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497946</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497947</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497948</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497949</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497950</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497951</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497952</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497953</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497954</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497955</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497956</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497957</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497958</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497959</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497960</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497961</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497962</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497963</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497964</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497965</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497966</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497967</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497968</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497969</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497970</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497971</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497972</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497973</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497974</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497975</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497976</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497977</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497978</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497979</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497980</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497981</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497982</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497983</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497984</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497985</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497986</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497987</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497988</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497989</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497990</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497991</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497992</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497993</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497994</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498002</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498003</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498004</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498005</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498006</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498007</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498008</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498009</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498011</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498012</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498013</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498014</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498015</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498016</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498017</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498018</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498019</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498020</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498021</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498022</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498023</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498024</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498025</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498026</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498027</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498028</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498029</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498030</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498031</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498032</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498033</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498034</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498036</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498037</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498038</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498039</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498040</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498041</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498042</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498043</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498044</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498045</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498046</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498047</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498048</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498049</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498050</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498051</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498052</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498053</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498054</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498055</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498056</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498057</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498058</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498059</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498060</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498061</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498062</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498063</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498064</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498065</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498066</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498067</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498068</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498069</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498070</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498071</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498072</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498073</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498074</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498075</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498076</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498077</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498078</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498079</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498080</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498081</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498082</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498083</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498084</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498085</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498086</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498087</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498088</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498089</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498090</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498091</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498092</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498093</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498094</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498095</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498096</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498097</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498098</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498099</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498103</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498104</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498105</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498109</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498110</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498112</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498113</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498114</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498116</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498117</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498118</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498120</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498121 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model_1_Prediction  Model_2_Prediction  Model_3_Prediction  Model_4_Prediction  Model_5_Prediction  Voting_Prediction\n",
       "0                        0                   0                   0                   0                   0                  0\n",
       "1                        0                   0                   0                   0                   0                  0\n",
       "2                        0                   0                   0                   0                   0                  0\n",
       "3                        0                   0                   0                   0                   0                  0\n",
       "4                        0                   0                   0                   0                   0                  0\n",
       "5                        0                   0                   0                   0                   0                  0\n",
       "6                        0                   0                   0                   0                   0                  0\n",
       "7                        0                   0                   0                   0                   0                  0\n",
       "8                        0                   0                   0                   0                   0                  0\n",
       "9                        0                   0                   0                   0                   0                  0\n",
       "10                       0                   0                   0                   0                   0                  0\n",
       "11                       0                   0                   0                   0                   0                  0\n",
       "12                       0                   0                   0                   0                   0                  0\n",
       "13                       0                   0                   0                   0                   0                  0\n",
       "14                       0                   0                   0                   0                   0                  0\n",
       "15                       0                   0                   0                   0                   0                  0\n",
       "16                       0                   0                   0                   0                   0                  0\n",
       "17                       0                   0                   0                   0                   0                  0\n",
       "18                       0                   0                   0                   0                   0                  0\n",
       "19                       0                   0                   0                   0                   0                  0\n",
       "20                       0                   0                   0                   0                   0                  0\n",
       "21                       0                   0                   0                   0                   0                  0\n",
       "22                       0                   0                   0                   0                   0                  0\n",
       "23                       0                   0                   0                   0                   0                  0\n",
       "24                       0                   0                   0                   0                   0                  0\n",
       "25                       0                   0                   0                   0                   0                  0\n",
       "26                       0                   0                   0                   0                   0                  0\n",
       "27                       0                   0                   0                   0                   0                  0\n",
       "28                       0                   0                   0                   0                   0                  0\n",
       "29                       0                   0                   0                   0                   0                  0\n",
       "30                       0                   0                   0                   0                   0                  0\n",
       "31                       0                   0                   0                   0                   0                  0\n",
       "32                       0                   0                   0                   0                   0                  0\n",
       "33                       0                   0                   0                   0                   0                  0\n",
       "34                       0                   0                   0                   0                   0                  0\n",
       "35                       0                   0                   0                   0                   0                  0\n",
       "36                       0                   0                   0                   0                   0                  0\n",
       "37                       0                   0                   1                   1                   0                  0\n",
       "38                       0                   0                   0                   0                   0                  0\n",
       "39                       0                   0                   0                   0                   0                  0\n",
       "40                       0                   0                   0                   0                   0                  0\n",
       "41                       0                   0                   0                   0                   0                  0\n",
       "42                       0                   0                   0                   0                   0                  0\n",
       "43                       0                   0                   0                   0                   0                  0\n",
       "44                       0                   0                   0                   0                   0                  0\n",
       "45                       0                   0                   0                   0                   0                  0\n",
       "46                       0                   0                   0                   0                   0                  0\n",
       "47                       0                   0                   0                   0                   0                  0\n",
       "48                       0                   0                   0                   0                   0                  0\n",
       "49                       0                   0                   0                   0                   0                  0\n",
       "50                       0                   0                   0                   0                   0                  0\n",
       "51                       0                   0                   0                   0                   0                  0\n",
       "52                       0                   0                   0                   0                   0                  0\n",
       "53                       0                   0                   0                   0                   0                  0\n",
       "54                       0                   0                   0                   0                   0                  0\n",
       "55                       0                   0                   0                   0                   0                  0\n",
       "56                       0                   0                   0                   0                   0                  0\n",
       "57                       0                   0                   0                   0                   0                  0\n",
       "58                       0                   0                   0                   0                   0                  0\n",
       "59                       0                   0                   0                   0                   0                  0\n",
       "60                       0                   0                   0                   0                   0                  0\n",
       "61                       0                   0                   0                   0                   0                  0\n",
       "62                       0                   0                   0                   0                   0                  0\n",
       "63                       0                   0                   0                   0                   0                  0\n",
       "64                       0                   0                   0                   0                   0                  0\n",
       "65                       0                   0                   0                   0                   0                  0\n",
       "66                       0                   0                   0                   0                   0                  0\n",
       "67                       0                   0                   0                   0                   0                  0\n",
       "68                       0                   0                   0                   0                   0                  0\n",
       "69                       0                   0                   0                   0                   0                  0\n",
       "70                       0                   0                   0                   0                   0                  0\n",
       "71                       0                   0                   0                   0                   0                  0\n",
       "72                       0                   0                   0                   0                   0                  0\n",
       "73                       0                   0                   0                   0                   0                  0\n",
       "74                       0                   0                   0                   0                   0                  0\n",
       "75                       0                   0                   0                   0                   0                  0\n",
       "76                       0                   0                   0                   0                   0                  0\n",
       "77                       1                   1                   0                   0                   1                  1\n",
       "78                       0                   0                   0                   0                   0                  0\n",
       "79                       0                   0                   0                   0                   0                  0\n",
       "80                       1                   1                   1                   1                   1                  1\n",
       "81                       0                   0                   0                   0                   0                  0\n",
       "82                       0                   0                   0                   0                   0                  0\n",
       "83                       0                   0                   0                   0                   0                  0\n",
       "84                       0                   0                   0                   0                   0                  0\n",
       "85                       0                   0                   0                   0                   0                  0\n",
       "86                       0                   0                   0                   0                   0                  0\n",
       "87                       0                   0                   0                   0                   0                  0\n",
       "88                       0                   0                   0                   0                   0                  0\n",
       "89                       0                   0                   0                   0                   0                  0\n",
       "90                       0                   0                   0                   0                   0                  0\n",
       "91                       0                   0                   0                   0                   0                  0\n",
       "92                       0                   0                   0                   0                   0                  0\n",
       "93                       0                   0                   0                   0                   0                  0\n",
       "94                       0                   0                   0                   0                   0                  0\n",
       "95                       0                   0                   0                   0                   0                  0\n",
       "96                       0                   0                   0                   0                   0                  0\n",
       "97                       0                   0                   0                   0                   0                  0\n",
       "98                       0                   0                   0                   0                   0                  0\n",
       "99                       0                   0                   0                   0                   0                  0\n",
       "100                      0                   0                   0                   0                   0                  0\n",
       "101                      0                   0                   0                   0                   0                  0\n",
       "102                      0                   0                   0                   0                   0                  0\n",
       "103                      0                   0                   0                   0                   0                  0\n",
       "104                      0                   0                   0                   0                   0                  0\n",
       "105                      0                   0                   0                   0                   0                  0\n",
       "106                      0                   0                   0                   0                   0                  0\n",
       "107                      0                   0                   0                   0                   0                  0\n",
       "108                      0                   0                   0                   0                   0                  0\n",
       "109                      0                   0                   0                   0                   0                  0\n",
       "110                      1                   1                   1                   1                   1                  1\n",
       "111                      0                   0                   0                   0                   0                  0\n",
       "112                      0                   0                   0                   0                   0                  0\n",
       "113                      0                   0                   0                   0                   0                  0\n",
       "114                      0                   0                   0                   0                   0                  0\n",
       "115                      0                   0                   0                   0                   0                  0\n",
       "116                      0                   0                   0                   0                   0                  0\n",
       "117                      0                   0                   0                   0                   0                  0\n",
       "118                      0                   0                   0                   0                   0                  0\n",
       "119                      0                   0                   0                   0                   0                  0\n",
       "120                      0                   0                   0                   0                   0                  0\n",
       "121                      0                   0                   0                   0                   0                  0\n",
       "122                      0                   0                   0                   0                   0                  0\n",
       "123                      0                   0                   0                   0                   0                  0\n",
       "124                      0                   0                   0                   0                   0                  0\n",
       "125                      0                   0                   0                   0                   0                  0\n",
       "126                      0                   0                   0                   0                   0                  0\n",
       "127                      0                   0                   0                   0                   0                  0\n",
       "128                      0                   0                   0                   0                   0                  0\n",
       "129                      0                   0                   0                   0                   0                  0\n",
       "130                      0                   0                   0                   0                   0                  0\n",
       "131                      0                   0                   0                   0                   0                  0\n",
       "132                      0                   0                   0                   0                   0                  0\n",
       "133                      0                   0                   0                   0                   0                  0\n",
       "134                      0                   0                   0                   0                   0                  0\n",
       "135                      0                   0                   0                   0                   0                  0\n",
       "136                      0                   0                   0                   0                   0                  0\n",
       "137                      0                   0                   0                   0                   0                  0\n",
       "138                      0                   0                   0                   0                   0                  0\n",
       "139                      0                   0                   0                   0                   0                  0\n",
       "140                      0                   0                   0                   0                   0                  0\n",
       "141                      0                   0                   0                   0                   0                  0\n",
       "142                      0                   0                   0                   0                   0                  0\n",
       "143                      0                   0                   0                   0                   0                  0\n",
       "144                      0                   0                   0                   0                   0                  0\n",
       "145                      0                   0                   0                   0                   0                  0\n",
       "146                      0                   0                   0                   0                   0                  0\n",
       "147                      0                   0                   0                   0                   0                  0\n",
       "148                      0                   0                   0                   0                   0                  0\n",
       "149                      0                   0                   0                   0                   0                  0\n",
       "150                      0                   0                   0                   0                   0                  0\n",
       "151                      0                   0                   0                   0                   0                  0\n",
       "152                      0                   0                   0                   0                   0                  0\n",
       "153                      0                   0                   0                   0                   0                  0\n",
       "154                      0                   0                   0                   0                   0                  0\n",
       "155                      0                   0                   0                   0                   0                  0\n",
       "156                      0                   0                   0                   0                   0                  0\n",
       "157                      0                   0                   0                   0                   0                  0\n",
       "158                      0                   0                   0                   0                   0                  0\n",
       "159                      0                   0                   0                   0                   0                  0\n",
       "160                      1                   1                   1                   1                   1                  1\n",
       "161                      0                   0                   0                   0                   0                  0\n",
       "162                      0                   0                   0                   0                   0                  0\n",
       "163                      0                   0                   0                   0                   0                  0\n",
       "164                      0                   0                   0                   0                   0                  0\n",
       "165                      0                   0                   0                   0                   0                  0\n",
       "166                      0                   0                   0                   0                   0                  0\n",
       "167                      0                   0                   0                   0                   0                  0\n",
       "168                      0                   0                   0                   0                   0                  0\n",
       "169                      0                   0                   0                   0                   0                  0\n",
       "170                      0                   0                   0                   0                   0                  0\n",
       "171                      0                   0                   0                   0                   0                  0\n",
       "172                      0                   0                   0                   0                   0                  0\n",
       "173                      0                   0                   0                   0                   0                  0\n",
       "174                      0                   0                   0                   0                   0                  0\n",
       "175                      0                   0                   0                   0                   0                  0\n",
       "176                      0                   0                   0                   0                   0                  0\n",
       "177                      0                   0                   0                   0                   0                  0\n",
       "178                      0                   0                   0                   0                   0                  0\n",
       "179                      0                   0                   0                   0                   0                  0\n",
       "180                      0                   0                   0                   0                   0                  0\n",
       "181                      0                   0                   0                   0                   0                  0\n",
       "182                      0                   0                   0                   0                   0                  0\n",
       "183                      0                   0                   0                   0                   0                  0\n",
       "184                      0                   0                   0                   0                   0                  0\n",
       "185                      0                   0                   0                   0                   0                  0\n",
       "186                      0                   0                   0                   0                   0                  0\n",
       "187                      0                   0                   0                   0                   0                  0\n",
       "188                      0                   0                   0                   0                   0                  0\n",
       "189                      0                   0                   0                   0                   0                  0\n",
       "190                      0                   0                   0                   0                   0                  0\n",
       "191                      0                   0                   0                   0                   0                  0\n",
       "192                      0                   0                   0                   0                   0                  0\n",
       "193                      0                   0                   0                   0                   0                  0\n",
       "194                      0                   0                   0                   0                   0                  0\n",
       "195                      0                   0                   0                   0                   0                  0\n",
       "196                      0                   0                   0                   0                   0                  0\n",
       "197                      0                   0                   0                   0                   0                  0\n",
       "198                      0                   0                   0                   0                   0                  0\n",
       "199                      0                   0                   0                   0                   0                  0\n",
       "200                      0                   0                   0                   0                   0                  0\n",
       "201                      0                   0                   0                   0                   0                  0\n",
       "202                      0                   0                   0                   0                   0                  0\n",
       "203                      0                   0                   0                   0                   0                  0\n",
       "204                      0                   0                   0                   0                   0                  0\n",
       "205                      0                   0                   0                   0                   0                  0\n",
       "206                      0                   0                   0                   0                   0                  0\n",
       "207                      0                   0                   0                   0                   0                  0\n",
       "208                      0                   0                   0                   0                   0                  0\n",
       "209                      0                   0                   0                   0                   0                  0\n",
       "210                      0                   0                   0                   0                   0                  0\n",
       "211                      0                   0                   0                   0                   0                  0\n",
       "212                      0                   0                   0                   0                   0                  0\n",
       "213                      0                   0                   0                   0                   0                  0\n",
       "214                      0                   0                   0                   0                   0                  0\n",
       "215                      0                   0                   0                   0                   0                  0\n",
       "216                      0                   0                   0                   0                   0                  0\n",
       "217                      0                   0                   0                   0                   0                  0\n",
       "218                      0                   0                   0                   0                   0                  0\n",
       "219                      0                   0                   0                   0                   0                  0\n",
       "220                      0                   0                   0                   0                   0                  0\n",
       "221                      0                   0                   0                   0                   0                  0\n",
       "222                      0                   0                   0                   0                   0                  0\n",
       "223                      0                   0                   0                   0                   0                  0\n",
       "224                      0                   0                   0                   0                   0                  0\n",
       "225                      0                   0                   0                   0                   0                  0\n",
       "226                      0                   0                   0                   0                   1                  0\n",
       "227                      0                   0                   0                   0                   0                  0\n",
       "228                      0                   0                   0                   0                   0                  0\n",
       "229                      0                   0                   0                   0                   0                  0\n",
       "230                      0                   0                   0                   0                   0                  0\n",
       "231                      0                   0                   0                   0                   0                  0\n",
       "232                      0                   0                   0                   0                   0                  0\n",
       "233                      0                   0                   0                   0                   0                  0\n",
       "234                      0                   0                   0                   0                   0                  0\n",
       "235                      0                   0                   0                   0                   0                  0\n",
       "236                      0                   0                   0                   0                   0                  0\n",
       "237                      0                   0                   0                   0                   0                  0\n",
       "238                      0                   0                   0                   0                   0                  0\n",
       "239                      0                   0                   0                   0                   0                  0\n",
       "240                      0                   0                   0                   0                   0                  0\n",
       "241                      0                   0                   0                   0                   0                  0\n",
       "242                      0                   0                   0                   0                   0                  0\n",
       "243                      0                   0                   0                   0                   0                  0\n",
       "244                      0                   0                   0                   0                   0                  0\n",
       "245                      0                   0                   0                   0                   0                  0\n",
       "246                      0                   0                   0                   0                   0                  0\n",
       "247                      0                   0                   0                   0                   0                  0\n",
       "248                      0                   0                   0                   0                   0                  0\n",
       "249                      0                   0                   0                   0                   0                  0\n",
       "...                    ...                 ...                 ...                 ...                 ...                ...\n",
       "497871                   0                   0                   0                   0                   0                  0\n",
       "497872                   0                   0                   0                   0                   0                  0\n",
       "497873                   0                   0                   0                   0                   0                  0\n",
       "497874                   0                   0                   0                   0                   0                  0\n",
       "497875                   0                   0                   0                   0                   0                  0\n",
       "497876                   0                   0                   0                   0                   0                  0\n",
       "497877                   0                   0                   0                   0                   0                  0\n",
       "497878                   0                   0                   0                   0                   0                  0\n",
       "497879                   0                   0                   0                   0                   0                  0\n",
       "497880                   0                   0                   0                   0                   0                  0\n",
       "497881                   0                   0                   0                   0                   0                  0\n",
       "497882                   0                   0                   0                   0                   0                  0\n",
       "497883                   1                   1                   1                   1                   1                  1\n",
       "497884                   0                   0                   0                   0                   0                  0\n",
       "497885                   0                   0                   0                   0                   0                  0\n",
       "497886                   0                   0                   0                   0                   0                  0\n",
       "497887                   0                   0                   0                   0                   0                  0\n",
       "497888                   0                   0                   0                   0                   0                  0\n",
       "497889                   0                   0                   0                   0                   0                  0\n",
       "497890                   0                   0                   0                   0                   0                  0\n",
       "497891                   0                   0                   0                   0                   0                  0\n",
       "497892                   0                   0                   0                   0                   0                  0\n",
       "497893                   0                   0                   0                   0                   0                  0\n",
       "497894                   0                   0                   0                   0                   0                  0\n",
       "497895                   0                   0                   0                   0                   0                  0\n",
       "497896                   0                   0                   0                   0                   0                  0\n",
       "497897                   0                   0                   0                   0                   0                  0\n",
       "497898                   0                   0                   0                   0                   0                  0\n",
       "497899                   0                   0                   0                   0                   0                  0\n",
       "497900                   0                   0                   0                   0                   0                  0\n",
       "497901                   0                   0                   0                   0                   0                  0\n",
       "497902                   1                   1                   1                   1                   1                  1\n",
       "497903                   0                   0                   0                   0                   0                  0\n",
       "497904                   0                   0                   0                   0                   0                  0\n",
       "497905                   0                   0                   0                   0                   0                  0\n",
       "497906                   0                   0                   0                   0                   0                  0\n",
       "497907                   0                   0                   0                   0                   0                  0\n",
       "497908                   0                   0                   0                   0                   0                  0\n",
       "497909                   0                   0                   0                   0                   0                  0\n",
       "497910                   0                   0                   0                   0                   0                  0\n",
       "497911                   0                   0                   0                   0                   0                  0\n",
       "497912                   0                   0                   0                   0                   0                  0\n",
       "497913                   0                   0                   0                   0                   0                  0\n",
       "497914                   0                   0                   0                   0                   0                  0\n",
       "497915                   0                   0                   0                   0                   0                  0\n",
       "497916                   1                   1                   1                   1                   1                  1\n",
       "497917                   0                   0                   0                   0                   0                  0\n",
       "497918                   0                   0                   0                   0                   0                  0\n",
       "497919                   0                   0                   0                   0                   0                  0\n",
       "497920                   0                   0                   0                   0                   0                  0\n",
       "497921                   0                   0                   0                   0                   0                  0\n",
       "497922                   0                   0                   0                   0                   0                  0\n",
       "497923                   1                   0                   1                   1                   0                  1\n",
       "497924                   0                   0                   0                   0                   0                  0\n",
       "497925                   0                   0                   0                   0                   0                  0\n",
       "497926                   0                   0                   0                   0                   0                  0\n",
       "497927                   0                   0                   0                   0                   0                  0\n",
       "497928                   0                   0                   0                   0                   0                  0\n",
       "497929                   1                   1                   1                   1                   1                  1\n",
       "497930                   0                   0                   0                   0                   0                  0\n",
       "497931                   0                   0                   0                   0                   0                  0\n",
       "497932                   0                   1                   1                   1                   0                  1\n",
       "497933                   0                   0                   0                   0                   0                  0\n",
       "497934                   0                   0                   0                   0                   0                  0\n",
       "497935                   0                   0                   0                   0                   0                  0\n",
       "497936                   0                   0                   0                   0                   0                  0\n",
       "497937                   0                   0                   0                   0                   0                  0\n",
       "497938                   0                   0                   0                   0                   0                  0\n",
       "497939                   0                   0                   0                   0                   0                  0\n",
       "497940                   0                   0                   0                   0                   0                  0\n",
       "497941                   0                   0                   0                   0                   0                  0\n",
       "497942                   0                   0                   0                   0                   0                  0\n",
       "497943                   0                   0                   0                   0                   0                  0\n",
       "497944                   0                   0                   0                   0                   0                  0\n",
       "497945                   0                   0                   0                   0                   0                  0\n",
       "497946                   0                   0                   0                   0                   0                  0\n",
       "497947                   1                   0                   1                   0                   0                  0\n",
       "497948                   0                   0                   0                   0                   0                  0\n",
       "497949                   0                   0                   0                   0                   0                  0\n",
       "497950                   0                   0                   0                   0                   0                  0\n",
       "497951                   0                   0                   0                   0                   0                  0\n",
       "497952                   0                   0                   0                   0                   0                  0\n",
       "497953                   0                   0                   0                   0                   0                  0\n",
       "497954                   0                   0                   0                   0                   0                  0\n",
       "497955                   0                   0                   0                   0                   0                  0\n",
       "497956                   0                   0                   0                   0                   0                  0\n",
       "497957                   0                   0                   0                   0                   0                  0\n",
       "497958                   0                   0                   0                   0                   0                  0\n",
       "497959                   0                   0                   0                   0                   0                  0\n",
       "497960                   1                   1                   1                   0                   0                  1\n",
       "497961                   0                   0                   0                   0                   0                  0\n",
       "497962                   0                   0                   0                   0                   0                  0\n",
       "497963                   0                   0                   0                   0                   0                  0\n",
       "497964                   0                   0                   0                   0                   0                  0\n",
       "497965                   0                   0                   0                   0                   0                  0\n",
       "497966                   0                   0                   0                   0                   0                  0\n",
       "497967                   0                   0                   0                   0                   0                  0\n",
       "497968                   0                   0                   0                   0                   0                  0\n",
       "497969                   0                   0                   0                   0                   0                  0\n",
       "497970                   0                   0                   0                   0                   0                  0\n",
       "497971                   0                   0                   0                   0                   0                  0\n",
       "497972                   0                   0                   0                   0                   0                  0\n",
       "497973                   0                   0                   0                   0                   0                  0\n",
       "497974                   0                   0                   0                   0                   0                  0\n",
       "497975                   0                   0                   0                   0                   0                  0\n",
       "497976                   0                   0                   0                   0                   0                  0\n",
       "497977                   0                   0                   0                   0                   0                  0\n",
       "497978                   0                   0                   0                   0                   0                  0\n",
       "497979                   0                   0                   0                   0                   0                  0\n",
       "497980                   0                   0                   0                   0                   0                  0\n",
       "497981                   1                   1                   1                   0                   1                  1\n",
       "497982                   0                   0                   0                   0                   0                  0\n",
       "497983                   0                   0                   0                   0                   0                  0\n",
       "497984                   0                   0                   0                   0                   0                  0\n",
       "497985                   0                   0                   0                   0                   0                  0\n",
       "497986                   0                   0                   0                   0                   0                  0\n",
       "497987                   0                   0                   0                   0                   0                  0\n",
       "497988                   0                   0                   0                   0                   0                  0\n",
       "497989                   0                   0                   0                   0                   0                  0\n",
       "497990                   0                   0                   0                   0                   0                  0\n",
       "497991                   0                   0                   0                   0                   0                  0\n",
       "497992                   0                   0                   0                   0                   0                  0\n",
       "497993                   0                   0                   0                   0                   0                  0\n",
       "497994                   0                   0                   0                   0                   0                  0\n",
       "497995                   0                   0                   0                   0                   0                  0\n",
       "497996                   1                   0                   1                   0                   0                  0\n",
       "497997                   0                   0                   0                   0                   0                  0\n",
       "497998                   0                   0                   0                   0                   0                  0\n",
       "497999                   0                   0                   0                   0                   0                  0\n",
       "498000                   0                   0                   0                   0                   0                  0\n",
       "498001                   0                   0                   0                   0                   0                  0\n",
       "498002                   0                   0                   0                   0                   0                  0\n",
       "498003                   0                   0                   0                   0                   0                  0\n",
       "498004                   0                   1                   1                   0                   1                  1\n",
       "498005                   0                   0                   0                   0                   0                  0\n",
       "498006                   0                   0                   0                   0                   0                  0\n",
       "498007                   0                   0                   0                   0                   0                  0\n",
       "498008                   0                   0                   0                   0                   0                  0\n",
       "498009                   0                   0                   0                   0                   0                  0\n",
       "498010                   0                   0                   0                   0                   0                  0\n",
       "498011                   0                   0                   0                   0                   0                  0\n",
       "498012                   0                   0                   0                   0                   0                  0\n",
       "498013                   0                   0                   0                   0                   0                  0\n",
       "498014                   0                   0                   0                   0                   0                  0\n",
       "498015                   0                   0                   0                   0                   0                  0\n",
       "498016                   0                   0                   0                   0                   0                  0\n",
       "498017                   0                   0                   0                   0                   0                  0\n",
       "498018                   0                   0                   0                   0                   0                  0\n",
       "498019                   0                   0                   0                   0                   0                  0\n",
       "498020                   0                   0                   0                   0                   0                  0\n",
       "498021                   0                   0                   0                   0                   0                  0\n",
       "498022                   0                   0                   0                   0                   0                  0\n",
       "498023                   0                   0                   0                   0                   0                  0\n",
       "498024                   0                   0                   0                   0                   0                  0\n",
       "498025                   0                   0                   0                   0                   0                  0\n",
       "498026                   0                   0                   0                   0                   0                  0\n",
       "498027                   1                   1                   1                   1                   1                  1\n",
       "498028                   0                   0                   0                   0                   0                  0\n",
       "498029                   0                   0                   0                   0                   0                  0\n",
       "498030                   0                   0                   0                   0                   0                  0\n",
       "498031                   0                   0                   0                   0                   0                  0\n",
       "498032                   1                   1                   1                   1                   1                  1\n",
       "498033                   0                   0                   0                   0                   0                  0\n",
       "498034                   0                   0                   0                   0                   0                  0\n",
       "498035                   0                   0                   0                   0                   0                  0\n",
       "498036                   0                   0                   0                   0                   0                  0\n",
       "498037                   0                   0                   0                   0                   0                  0\n",
       "498038                   0                   0                   0                   0                   0                  0\n",
       "498039                   0                   0                   0                   0                   0                  0\n",
       "498040                   0                   0                   0                   0                   0                  0\n",
       "498041                   0                   0                   0                   0                   0                  0\n",
       "498042                   0                   0                   0                   0                   0                  0\n",
       "498043                   0                   0                   0                   0                   0                  0\n",
       "498044                   0                   0                   0                   0                   0                  0\n",
       "498045                   1                   1                   0                   1                   1                  1\n",
       "498046                   0                   0                   0                   0                   0                  0\n",
       "498047                   0                   0                   0                   0                   0                  0\n",
       "498048                   0                   0                   0                   0                   0                  0\n",
       "498049                   0                   0                   0                   0                   0                  0\n",
       "498050                   0                   0                   0                   0                   0                  0\n",
       "498051                   0                   0                   0                   0                   0                  0\n",
       "498052                   0                   0                   0                   0                   0                  0\n",
       "498053                   0                   0                   0                   0                   0                  0\n",
       "498054                   0                   0                   0                   0                   0                  0\n",
       "498055                   0                   0                   0                   0                   0                  0\n",
       "498056                   0                   0                   0                   0                   0                  0\n",
       "498057                   0                   0                   0                   0                   0                  0\n",
       "498058                   0                   0                   0                   0                   0                  0\n",
       "498059                   0                   0                   0                   0                   0                  0\n",
       "498060                   0                   0                   0                   0                   0                  0\n",
       "498061                   0                   0                   0                   0                   0                  0\n",
       "498062                   0                   0                   0                   0                   0                  0\n",
       "498063                   0                   0                   0                   0                   0                  0\n",
       "498064                   0                   0                   0                   0                   0                  0\n",
       "498065                   0                   0                   0                   0                   0                  0\n",
       "498066                   0                   0                   0                   0                   0                  0\n",
       "498067                   0                   0                   0                   0                   0                  0\n",
       "498068                   0                   0                   0                   0                   0                  0\n",
       "498069                   0                   0                   0                   0                   0                  0\n",
       "498070                   0                   0                   0                   0                   0                  0\n",
       "498071                   0                   0                   0                   0                   0                  0\n",
       "498072                   0                   0                   0                   0                   0                  0\n",
       "498073                   0                   0                   0                   0                   0                  0\n",
       "498074                   0                   0                   0                   0                   0                  0\n",
       "498075                   0                   0                   0                   0                   0                  0\n",
       "498076                   0                   0                   0                   0                   0                  0\n",
       "498077                   0                   0                   0                   0                   0                  0\n",
       "498078                   0                   0                   0                   0                   0                  0\n",
       "498079                   0                   0                   0                   0                   0                  0\n",
       "498080                   0                   0                   0                   0                   0                  0\n",
       "498081                   0                   0                   0                   0                   0                  0\n",
       "498082                   0                   0                   0                   0                   0                  0\n",
       "498083                   0                   0                   0                   0                   0                  0\n",
       "498084                   0                   0                   0                   0                   0                  0\n",
       "498085                   0                   0                   0                   0                   0                  0\n",
       "498086                   0                   0                   0                   0                   0                  0\n",
       "498087                   0                   0                   0                   0                   0                  0\n",
       "498088                   0                   0                   0                   0                   0                  0\n",
       "498089                   0                   0                   0                   0                   0                  0\n",
       "498090                   0                   0                   0                   0                   0                  0\n",
       "498091                   0                   0                   0                   0                   0                  0\n",
       "498092                   0                   0                   0                   0                   0                  0\n",
       "498093                   0                   0                   0                   0                   0                  0\n",
       "498094                   0                   0                   0                   0                   0                  0\n",
       "498095                   0                   0                   0                   0                   0                  0\n",
       "498096                   0                   0                   0                   0                   0                  0\n",
       "498097                   0                   0                   0                   0                   0                  0\n",
       "498098                   0                   0                   0                   0                   0                  0\n",
       "498099                   1                   1                   1                   1                   1                  1\n",
       "498100                   0                   0                   0                   0                   0                  0\n",
       "498101                   0                   0                   0                   0                   0                  0\n",
       "498102                   0                   0                   0                   0                   0                  0\n",
       "498103                   0                   0                   0                   0                   0                  0\n",
       "498104                   0                   0                   0                   0                   0                  0\n",
       "498105                   0                   0                   0                   0                   0                  0\n",
       "498106                   0                   0                   0                   0                   0                  0\n",
       "498107                   0                   0                   0                   0                   0                  0\n",
       "498108                   0                   0                   0                   0                   0                  0\n",
       "498109                   0                   0                   0                   0                   0                  0\n",
       "498110                   0                   0                   0                   0                   0                  0\n",
       "498111                   0                   0                   0                   0                   0                  0\n",
       "498112                   0                   0                   0                   0                   0                  0\n",
       "498113                   0                   0                   0                   0                   0                  0\n",
       "498114                   0                   0                   0                   0                   0                  0\n",
       "498115                   0                   0                   0                   0                   0                  0\n",
       "498116                   0                   0                   0                   0                   0                  0\n",
       "498117                   0                   0                   0                   0                   0                  0\n",
       "498118                   0                   0                   0                   0                   0                  0\n",
       "498119                   0                   0                   0                   0                   0                  0\n",
       "498120                   0                   0                   0                   0                   0                  0\n",
       "\n",
       "[498121 rows x 6 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction_values = test_predictions['Voting_Prediction'].values\n",
    "\n",
    "test_prediction_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497871</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497872</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497873</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497874</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497875</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497876</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497877</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497878</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497879</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497880</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497881</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497882</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497883</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497884</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497885</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497886</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497887</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497888</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497889</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497890</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497891</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497892</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497893</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497894</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497895</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497896</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497897</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497898</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497899</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497900</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497901</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497902</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497903</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497904</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497905</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497906</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497907</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497908</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497909</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497910</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497911</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497912</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497913</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497914</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497915</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497916</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497917</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497918</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497919</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497920</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497921</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497922</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497923</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497924</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497925</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497926</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497927</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497928</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497929</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497930</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497931</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497932</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497933</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497934</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497935</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497936</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497937</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497938</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497939</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497940</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497941</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497942</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497943</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497944</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497945</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497946</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497947</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497948</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497949</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497950</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497951</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497952</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497953</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497954</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497955</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497956</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497957</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497958</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497959</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497960</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497961</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497962</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497963</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497964</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497965</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497966</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497967</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497968</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497969</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497970</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497971</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497972</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497973</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497974</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497975</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497976</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497977</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497978</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497979</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497980</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497981</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497982</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497983</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497984</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497985</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497986</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497987</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497988</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497989</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497990</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497991</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497992</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497993</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497994</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497995</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497996</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497997</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497998</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497999</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498000</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498001</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498002</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498003</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498004</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498005</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498006</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498007</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498008</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498009</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498010</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498011</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498012</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498013</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498014</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498015</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498016</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498017</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498018</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498019</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498020</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498021</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498022</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498023</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498024</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498025</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498026</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498027</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498028</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498029</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498030</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498031</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498032</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498033</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498034</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498035</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498036</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498037</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498038</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498039</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498040</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498041</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498042</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498043</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498044</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498045</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498046</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498047</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498048</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498049</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498050</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498051</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498052</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498053</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498054</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498055</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498056</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498057</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498058</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498059</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498060</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498061</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498062</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498063</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498064</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498065</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498066</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498067</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498068</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498069</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498070</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498071</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498072</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498073</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498074</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498075</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498076</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498077</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498078</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498079</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498080</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498081</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498082</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498083</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498084</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498085</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498086</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498087</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498088</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498089</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498090</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498091</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498092</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498093</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498094</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498095</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498096</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498097</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498098</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498099</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498100</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498101</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498102</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498103</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498104</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498105</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498106</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498107</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498108</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498109</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498110</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498111</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498112</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498113</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498114</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498115</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498116</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498117</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498118</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498119</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498120</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498121 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fraud\n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           0\n",
       "5           0\n",
       "6           0\n",
       "7           0\n",
       "8           0\n",
       "9           0\n",
       "10          0\n",
       "11          0\n",
       "12          0\n",
       "13          0\n",
       "14          0\n",
       "15          0\n",
       "16          0\n",
       "17          0\n",
       "18          0\n",
       "19          0\n",
       "20          0\n",
       "21          0\n",
       "22          0\n",
       "23          0\n",
       "24          0\n",
       "25          0\n",
       "26          0\n",
       "27          0\n",
       "28          0\n",
       "29          0\n",
       "30          0\n",
       "31          0\n",
       "32          0\n",
       "33          0\n",
       "34          0\n",
       "35          0\n",
       "36          0\n",
       "37          0\n",
       "38          0\n",
       "39          0\n",
       "40          0\n",
       "41          0\n",
       "42          0\n",
       "43          0\n",
       "44          0\n",
       "45          0\n",
       "46          0\n",
       "47          0\n",
       "48          0\n",
       "49          0\n",
       "50          0\n",
       "51          0\n",
       "52          0\n",
       "53          0\n",
       "54          0\n",
       "55          0\n",
       "56          0\n",
       "57          0\n",
       "58          0\n",
       "59          0\n",
       "60          0\n",
       "61          0\n",
       "62          0\n",
       "63          0\n",
       "64          0\n",
       "65          0\n",
       "66          0\n",
       "67          0\n",
       "68          0\n",
       "69          0\n",
       "70          0\n",
       "71          0\n",
       "72          0\n",
       "73          0\n",
       "74          0\n",
       "75          0\n",
       "76          0\n",
       "77          1\n",
       "78          0\n",
       "79          0\n",
       "80          1\n",
       "81          0\n",
       "82          0\n",
       "83          0\n",
       "84          0\n",
       "85          0\n",
       "86          0\n",
       "87          0\n",
       "88          0\n",
       "89          0\n",
       "90          0\n",
       "91          0\n",
       "92          0\n",
       "93          0\n",
       "94          0\n",
       "95          0\n",
       "96          0\n",
       "97          0\n",
       "98          0\n",
       "99          0\n",
       "100         0\n",
       "101         0\n",
       "102         0\n",
       "103         0\n",
       "104         0\n",
       "105         0\n",
       "106         0\n",
       "107         0\n",
       "108         0\n",
       "109         0\n",
       "110         1\n",
       "111         0\n",
       "112         0\n",
       "113         0\n",
       "114         0\n",
       "115         0\n",
       "116         0\n",
       "117         0\n",
       "118         0\n",
       "119         0\n",
       "120         0\n",
       "121         0\n",
       "122         0\n",
       "123         0\n",
       "124         0\n",
       "125         0\n",
       "126         0\n",
       "127         0\n",
       "128         0\n",
       "129         0\n",
       "130         0\n",
       "131         0\n",
       "132         0\n",
       "133         0\n",
       "134         0\n",
       "135         0\n",
       "136         0\n",
       "137         0\n",
       "138         0\n",
       "139         0\n",
       "140         0\n",
       "141         0\n",
       "142         0\n",
       "143         0\n",
       "144         0\n",
       "145         0\n",
       "146         0\n",
       "147         0\n",
       "148         0\n",
       "149         0\n",
       "150         0\n",
       "151         0\n",
       "152         0\n",
       "153         0\n",
       "154         0\n",
       "155         0\n",
       "156         0\n",
       "157         0\n",
       "158         0\n",
       "159         0\n",
       "160         1\n",
       "161         0\n",
       "162         0\n",
       "163         0\n",
       "164         0\n",
       "165         0\n",
       "166         0\n",
       "167         0\n",
       "168         0\n",
       "169         0\n",
       "170         0\n",
       "171         0\n",
       "172         0\n",
       "173         0\n",
       "174         0\n",
       "175         0\n",
       "176         0\n",
       "177         0\n",
       "178         0\n",
       "179         0\n",
       "180         0\n",
       "181         0\n",
       "182         0\n",
       "183         0\n",
       "184         0\n",
       "185         0\n",
       "186         0\n",
       "187         0\n",
       "188         0\n",
       "189         0\n",
       "190         0\n",
       "191         0\n",
       "192         0\n",
       "193         0\n",
       "194         0\n",
       "195         0\n",
       "196         0\n",
       "197         0\n",
       "198         0\n",
       "199         0\n",
       "200         0\n",
       "201         0\n",
       "202         0\n",
       "203         0\n",
       "204         0\n",
       "205         0\n",
       "206         0\n",
       "207         0\n",
       "208         0\n",
       "209         0\n",
       "210         0\n",
       "211         0\n",
       "212         0\n",
       "213         0\n",
       "214         0\n",
       "215         0\n",
       "216         0\n",
       "217         0\n",
       "218         0\n",
       "219         0\n",
       "220         0\n",
       "221         0\n",
       "222         0\n",
       "223         0\n",
       "224         0\n",
       "225         0\n",
       "226         0\n",
       "227         0\n",
       "228         0\n",
       "229         0\n",
       "230         0\n",
       "231         0\n",
       "232         0\n",
       "233         0\n",
       "234         0\n",
       "235         0\n",
       "236         0\n",
       "237         0\n",
       "238         0\n",
       "239         0\n",
       "240         0\n",
       "241         0\n",
       "242         0\n",
       "243         0\n",
       "244         0\n",
       "245         0\n",
       "246         0\n",
       "247         0\n",
       "248         0\n",
       "249         0\n",
       "...       ...\n",
       "497871      0\n",
       "497872      0\n",
       "497873      0\n",
       "497874      0\n",
       "497875      0\n",
       "497876      0\n",
       "497877      0\n",
       "497878      0\n",
       "497879      0\n",
       "497880      0\n",
       "497881      0\n",
       "497882      0\n",
       "497883      1\n",
       "497884      0\n",
       "497885      0\n",
       "497886      0\n",
       "497887      0\n",
       "497888      0\n",
       "497889      0\n",
       "497890      0\n",
       "497891      0\n",
       "497892      0\n",
       "497893      0\n",
       "497894      0\n",
       "497895      0\n",
       "497896      0\n",
       "497897      0\n",
       "497898      0\n",
       "497899      0\n",
       "497900      0\n",
       "497901      0\n",
       "497902      1\n",
       "497903      0\n",
       "497904      0\n",
       "497905      0\n",
       "497906      0\n",
       "497907      0\n",
       "497908      0\n",
       "497909      0\n",
       "497910      0\n",
       "497911      0\n",
       "497912      0\n",
       "497913      0\n",
       "497914      0\n",
       "497915      0\n",
       "497916      1\n",
       "497917      0\n",
       "497918      0\n",
       "497919      0\n",
       "497920      0\n",
       "497921      0\n",
       "497922      0\n",
       "497923      1\n",
       "497924      0\n",
       "497925      0\n",
       "497926      0\n",
       "497927      0\n",
       "497928      0\n",
       "497929      1\n",
       "497930      0\n",
       "497931      0\n",
       "497932      1\n",
       "497933      0\n",
       "497934      0\n",
       "497935      0\n",
       "497936      0\n",
       "497937      0\n",
       "497938      0\n",
       "497939      0\n",
       "497940      0\n",
       "497941      0\n",
       "497942      0\n",
       "497943      0\n",
       "497944      0\n",
       "497945      0\n",
       "497946      0\n",
       "497947      0\n",
       "497948      0\n",
       "497949      0\n",
       "497950      0\n",
       "497951      0\n",
       "497952      0\n",
       "497953      0\n",
       "497954      0\n",
       "497955      0\n",
       "497956      0\n",
       "497957      0\n",
       "497958      0\n",
       "497959      0\n",
       "497960      1\n",
       "497961      0\n",
       "497962      0\n",
       "497963      0\n",
       "497964      0\n",
       "497965      0\n",
       "497966      0\n",
       "497967      0\n",
       "497968      0\n",
       "497969      0\n",
       "497970      0\n",
       "497971      0\n",
       "497972      0\n",
       "497973      0\n",
       "497974      0\n",
       "497975      0\n",
       "497976      0\n",
       "497977      0\n",
       "497978      0\n",
       "497979      0\n",
       "497980      0\n",
       "497981      1\n",
       "497982      0\n",
       "497983      0\n",
       "497984      0\n",
       "497985      0\n",
       "497986      0\n",
       "497987      0\n",
       "497988      0\n",
       "497989      0\n",
       "497990      0\n",
       "497991      0\n",
       "497992      0\n",
       "497993      0\n",
       "497994      0\n",
       "497995      0\n",
       "497996      0\n",
       "497997      0\n",
       "497998      0\n",
       "497999      0\n",
       "498000      0\n",
       "498001      0\n",
       "498002      0\n",
       "498003      0\n",
       "498004      1\n",
       "498005      0\n",
       "498006      0\n",
       "498007      0\n",
       "498008      0\n",
       "498009      0\n",
       "498010      0\n",
       "498011      0\n",
       "498012      0\n",
       "498013      0\n",
       "498014      0\n",
       "498015      0\n",
       "498016      0\n",
       "498017      0\n",
       "498018      0\n",
       "498019      0\n",
       "498020      0\n",
       "498021      0\n",
       "498022      0\n",
       "498023      0\n",
       "498024      0\n",
       "498025      0\n",
       "498026      0\n",
       "498027      1\n",
       "498028      0\n",
       "498029      0\n",
       "498030      0\n",
       "498031      0\n",
       "498032      1\n",
       "498033      0\n",
       "498034      0\n",
       "498035      0\n",
       "498036      0\n",
       "498037      0\n",
       "498038      0\n",
       "498039      0\n",
       "498040      0\n",
       "498041      0\n",
       "498042      0\n",
       "498043      0\n",
       "498044      0\n",
       "498045      1\n",
       "498046      0\n",
       "498047      0\n",
       "498048      0\n",
       "498049      0\n",
       "498050      0\n",
       "498051      0\n",
       "498052      0\n",
       "498053      0\n",
       "498054      0\n",
       "498055      0\n",
       "498056      0\n",
       "498057      0\n",
       "498058      0\n",
       "498059      0\n",
       "498060      0\n",
       "498061      0\n",
       "498062      0\n",
       "498063      0\n",
       "498064      0\n",
       "498065      0\n",
       "498066      0\n",
       "498067      0\n",
       "498068      0\n",
       "498069      0\n",
       "498070      0\n",
       "498071      0\n",
       "498072      0\n",
       "498073      0\n",
       "498074      0\n",
       "498075      0\n",
       "498076      0\n",
       "498077      0\n",
       "498078      0\n",
       "498079      0\n",
       "498080      0\n",
       "498081      0\n",
       "498082      0\n",
       "498083      0\n",
       "498084      0\n",
       "498085      0\n",
       "498086      0\n",
       "498087      0\n",
       "498088      0\n",
       "498089      0\n",
       "498090      0\n",
       "498091      0\n",
       "498092      0\n",
       "498093      0\n",
       "498094      0\n",
       "498095      0\n",
       "498096      0\n",
       "498097      0\n",
       "498098      0\n",
       "498099      1\n",
       "498100      0\n",
       "498101      0\n",
       "498102      0\n",
       "498103      0\n",
       "498104      0\n",
       "498105      0\n",
       "498106      0\n",
       "498107      0\n",
       "498108      0\n",
       "498109      0\n",
       "498110      0\n",
       "498111      0\n",
       "498112      0\n",
       "498113      0\n",
       "498114      0\n",
       "498115      0\n",
       "498116      0\n",
       "498117      0\n",
       "498118      0\n",
       "498119      0\n",
       "498120      0\n",
       "\n",
       "[498121 rows x 1 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(test_prediction_values)\n",
    "predictions_df.columns = ['fraud']\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    475919\n",
      "1     22202\n",
      "Name: fraud, dtype: int64\n",
      "0    0.955429\n",
      "1    0.044571\n",
      "Name: fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(predictions_df.fraud.value_counts())\n",
    "print(predictions_df.fraud.value_counts() / len(predictions_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
